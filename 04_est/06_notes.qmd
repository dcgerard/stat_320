---
title: "Chapter 6 Notes: Estimation"
author: "David Gerard"
date: today
bibliography: est_bib.bib
---

```{r}
#| echo: false
#| message: false
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE, 
            fig.width = 4, 
            fig.height = 3, 
            fig.align = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
plt_norm <- function(mu = 0, sig = 1, lb = -Inf, ub = Inf) {
  tibble(x = seq(mu - 3 * sig, mu + 3 * sig, length.out = 500)) |>
    mutate(y = dnorm(x = x, mean = mu, sd = sig)) ->
    df1
  df1 |>
    filter(x > lb, x < ub) ->
    df2
  ggplot() +
    geom_line(data = df1, mapping = aes(x = x, y = y)) +
    geom_area(data = df2, mapping = aes(x = x, y = y)) +
    theme_classic() +
    theme(axis.title = element_blank())
}
```



# Estimation Set up

<div style="text-align: center;">
```{mermaid}
stateDiagram-v2
    state "Data Generating Process" as Process
    Process --> Data: Probability
    Data --> Process: Inference
```
</div>

- Probability (Chapters 3, 4, and 5): 
  - We know that $X \sim N(80, 12)$  
  - What is $\Pr(X > 90)$?

- Inference (Chapters 6 through 14): 
  - We observe $X_i = 81, 78, 77, 89, \ldots$  
  - We assume $X_i \sim N(\mu, \sigma^2)$ 
    - But the values of $\mu$ and $\sigma^2$ are unknown.
  - What are $\mu$ and $\sigma^2$?

- **Estimation**: Guess parameter values from data

  - **Point estimation**: A single number is your best guess  
    - E.g., estimate $\mu$ with $\bar{X}$

  - **Interval estimation**: Get a range of likely values of a parameter  
    - E.g., confidence intervals

- **Hypothesis testing**: How sure are we a parameter is different from some value?  
  - E.g., $H_0: \mu = 0$
  
![](./04_figs/pop_to_sample/pop_to_sample.png){fig-alt="Population versus sample"}\ 

::: {.callout-tip}
# Reference / Target / Study Population
Group we are interested in
:::

::: {.callout-tip}
## Sample
Group we have data about
:::

::: {.callout-tip}
## Parameter
Numeric summary of population  

E.g.: $\mu$, $\sigma^2$, $p$
:::

::: {.callout-tip}
## Statistic
Numeric summary of sample  

E.g. $\bar{X}$, $s^2$, $\hat{p}$
:::

- Simplest way to get a sample is by a simple random sample  
  - Each unit has an equal chance of being in sample

- Random selection (e.g., via SRS) is distinct from Random Assignment.

- Random assignment: randomly assign units to different groups (e.g., treatment vs. control)

- Random selection: results generalizable to target population  
  - Because sample is similar to population in terms of demographic variables

- Random assignment: allows for claims of causality  
  - Because all possible confounders are equal (on average) in the groups

- Randomized Clinical Trial (RCT): Random assignment of treatments to compare them.

- No causal claims without random assignment

- Example: tobramycin and gentamicin are antibiotics.  
  - Tobramycin is more aggressive and has more side effects.  
  - Early studies were not randomized and showed tobramycin performed worse. Why?  
    - Doctors gave sicker patients tobramycin
  - Randomization guarantees equal number of sicker and less sick in each group (on average)

- Double blind: neither doctor nor patient know treatment  
  - Guards against placebo effect

- Single blind: doctor knows

- Unblinded: both know

___

[Sampling in R](./04_sample.qmd)

___

# Estimating the Mean

- Suppose $E(X_i) = \mu$  
  - Not necessarily normal

- Estimate $\mu$ with $\hat{\mu} = \bar{X} = \frac{1}{n} \sum X_i$  
  - $\hat{\mu}$ is an estimate of $\mu$  
  - $\bar{X}$ is the sample mean
  - Often (almost always), $\hat{\mu} = \bar{X}$.

::: {.callout-tip}
## Sampling distribution

Distribution of statistic across many (hypothetical) samples
:::

![](./04_figs/sampling_dist/sampling_dist.png){fig-alt="Sampling distribution visualization."}\ 

- The sampling distribution is used to describe properties of statistics

- Properties of $\bar{X}$

  - Let $X_1, X_2, \ldots, X_n$ be a random sample with  
    $E(X_i) = \mu$, $\operatorname{Var}(X_i) = \sigma^2$

  - Then:
    1. $E(\bar{X}) = \mu$ (unbiased)
    2. $\operatorname{Var}(\bar{X}) = \sigma^2/n$  
       - More precise for larger $n$ (consistent)
    3. $\bar{X} \approx N(\mu, \sigma^2/n)$ for large $n$, even if $X_i$ are not also normal  
       - (Central Limit Theorem)

::: {.callout-tip}
## Standard error
Standard deviation of a statistic (its sampling distribution).
:::

- $\operatorname{SE}(\bar{X}) = \sigma / \sqrt{n}$

::: {.callout-tip}
## Estimated Standard Error: 
Estimated standard deviation of a statistic  
:::

- Most of the time, when folks say "standard error", they mean estimated standard error.
  - The book does this too.

- $\operatorname{SE}(\bar{X}) = s / \sqrt{n}$

- Typically don't know $\sigma^2$, so estimate it with $s^2$


| Statistic          | $X_i$      | $\bar{X}$                          |
|-------------------|------------|------------------------------------|
| Mean              | $\mu$      | $\mu$                              |
| Variance          | $\sigma^2$ | $\sigma^2 / n$                     |
| Estimated Variance| $s^2$      | $s^2 / n$                          |
| Distribution      | Unknown    | $N(\mu, \sigma^2 / n)$ (for large $n$) |

::: {.panel-tabset}
## Exercise:

Mean birthweight is 112 oz with standard deviation 20.6. What is the probability the mean of 10 birthweights will be between 98 and 126?

## Hint
The distribution of $\bar{X}$ is:
$$
\bar{X} \sim N\left(112, \left(\frac{20.6}{\sqrt{10}}\right)^2\right) = N(112, 6.514^2)
$$

## Solution
The distribution of $\bar{X}$ is:
$$
\bar{X} \sim N\left(112, \left(\frac{20.6}{\sqrt{10}}\right)^2\right) = N(112, 6.514^2)
$$

We want the area between 98 and 126
```{r}
#| echo: false
#| fig-height: 2
#| fig-width: 4
#| fig-alt: "Area under normal curve between 98 and 126."
plt_norm(mu = 112, sig = 20.6/sqrt(10), lb = 98, ub = 126)
```

Which is the area to the left of 126
```{r}
#| echo: false
#| fig-height: 2
#| fig-width: 4
#| fig-alt: "Area under the normal curve less than 126."
plt_norm(mu = 112, sig = 20.6/sqrt(10), ub = 126)
```
Minus the area to the left of 98
```{r}
#| echo: false
#| fig-height: 2
#| fig-width: 4
#| fig-alt: "Area under the normal curve less than 98."
plt_norm(mu = 112, sig = 20.6/sqrt(10), ub = 98)
```
We can calculate this in R:
```{r}
pnorm(q = 126, mean = 112, sd = 6.514) - pnorm(q = 98, mean = 112, sd = 6.514)
```
:::

# Interval Estimation

- $\bar{X}$ is not exactly equal to $\mu$
- We want range of likely values of $\mu$
- We know $\bar{X} \sim N(\mu, \sigma^2 / n)$ for large $n$


\begin{align}
&\Pr\left(-1.96 \leq \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \leq 1.96\right) \approx 0.95\\
&\Leftrightarrow \Pr\left(-1.96 \cdot \frac{\sigma}{\sqrt{n}} \leq \bar{X} - \mu \leq 1.96 \cdot \frac{\sigma}{\sqrt{n}}\right) \approx 0.95\\
&\Leftrightarrow \Pr(\bar{X} - 1.96 \cdot \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + 1.96 \cdot \frac{\sigma}{\sqrt{n}}) \approx 0.95
\end{align}

- Thus, $\bar{X} \pm 1.96 \cdot \frac{\sigma}{\sqrt{n}}$ captures $\mu$ with probability 0.95.
    - This is a 95% Confidence Interval (CI)


::: {.callout-tip}
## Common CI Format
Estimate ± multiplier × standard error  
:::

-   More generally, a $(1 - \alpha) \cdot 100\%$ CI for $\mu$

  $$
  \bar{X} \pm \texttt{qnorm}(1 - \frac{\alpha}{2}) \cdot \frac{\sigma}{\sqrt{n}}
  $$
  
```{r}
#| echo: false
#| fig-alt: "Quantiles of standard normal so that 1-alpha area is between them."
plt_norm(lb = -1.5, ub = 1.5) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        axis.text.x = element_text(size = 12)) +
  scale_x_continuous(breaks = c(-1.5, 1.5), labels = c(expression(atop(qnorm(alpha/2),-qnorm(1-alpha/2))), expression(qnorm(1-alpha/2)))) +
  annotate(geom = "text", x = 0, y = 0.15, label = "1-alpha", color = "white", size = 9, parse = TRUE)
```


- CI Interpretation:

  ![](./04_figs/ci_graphic.png){fig-alt="Confidence interval interpretation."}\ 
  
## When Variance is Not Known

- The above only works when $\sigma^2$ is known  
  - $\sigma^2$ is never known

- $\frac{\bar{X} - \mu}{s / \sqrt{n}} \sim t_{n-1}$ (not $N(0, 1)$)  
  - $t$-distribution with $n - 1$ degrees of freedom

- Only an exact result when $X_1, \ldots, X_n \overset{\text{iid}}{\sim} N(\mu, \sigma^2)$  
  - But $t$-distribution tends to work better in small samples even when $X_i$ are not normal

- For large $n$, $t_{n-1} \approx N(0, 1)$, so CLT is OK

- Bell-shaped, centered at 0

- As $\text{df} \downarrow$, extreme values more likely  
  As $\text{df} \uparrow$, extreme values less likely

- Use $t$ because of added variability from using $s^2$ instead of $\sigma^2$

---

[R Code for t-distribution](./04_t.qmd)

---

- Rosner uses notation $t_{\text{df}, p}$ for the $p$ quantile of a $t_{\text{df}}$ distribution

$$
t_{\text{df}, p} = \texttt{qt(p, df)}
$$

```{r}
#| echo: false
#| fig-alt: "Quantiles of t distribution so that 1-alpha area is between them."
plt_norm(lb = -1.5, ub = 1.5) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        axis.text.x = element_text(size = 12)) +
  scale_x_continuous(breaks = c(-1.5, 1.5), labels = c(expression(atop(qt(alpha/2,n-1),-qt(1-alpha/2,n-1))), expression(qt(1-alpha/2,n-1)))) +
  annotate(geom = "text", x = 0, y = 0.15, label = "1-alpha", color = "white", size = 9, parse = TRUE)
```

\begin{align}
&\Pr\left(-t_{n-1, 1 - \alpha/2} \leq \frac{\bar{X} - \mu}{s/\sqrt{n}} \leq t_{n-1, 1 - \alpha/2} \right) = 1 - \alpha\\
&\Leftrightarrow \Pr\left(-t_{n-1, 1 - \alpha/2} \cdot \frac{s}{\sqrt{n}} \leq \bar{X} - \mu \leq t_{n-1, 1 - \alpha/2} \cdot \frac{s}{\sqrt{n}} \right) = 1 - \alpha\\
&\Leftrightarrow \Pr\left(\bar{X} - t_{n-1, 1 - \alpha/2} \cdot \frac{s}{\sqrt{n}} \leq \mu \leq \bar{X} + t_{n-1, 1 - \alpha/2} \cdot \frac{s}{\sqrt{n}} \right) = 1 - \alpha
\end{align}

- Thus, the following is a $(1-\alpha)100\%$ confidence interval for $\mu$ when $\sigma^2$ is not known:
  \begin{align}
  &\bar{X} \pm t_{n-1, 1 - \alpha/2} \cdot \frac{s}{\sqrt{n}} \text{ or, equivalently}\\
  &\bar{X} \pm \texttt{qt(1 - }\alpha/2\texttt{, n-1)} \cdot \frac{s}{\sqrt{n}}
  \end{align}

::: {.panel-tabset}
## Exercise
Suppose $n = 10$, $\bar{X} = 116.9$, $s = 21.70$. Calculate 90%, 95%, 99% CIs

## Hint

The degrees of freedom of the appropriate $t$ distribution is $10 - 1 = 9$.

E.g., for a 90% CI, $\alpha = 1 - 0.9 = 0.1$, and you'll need the $1 - \alpha/2 = 1 - 0.1/2 = 0.95$ quantile of that $t$ distribution.

## Solution
Since $n = 10$, the degrees of freedom of the appropriate $t$ distribution is $10 - 1 = 9$., For 90% CI's, $\alpha = 0.1$ and the appropriate quantile is
```{r}
qt(p = 1 - 0.1/2, df = 9)
```
We can thus get a 90% CI by
```{r}
116.9 - 1.833 * 21.70 / sqrt(10)
116.9 + 1.833 * 21.70 / sqrt(10)
```

Similarly, for a 95% CI we have
```{r}
116.9 - qt(p = 1 - 0.05/2, df = 9) * 21.70 / sqrt(10)
116.9 + qt(p = 1 - 0.05/2, df = 9) * 21.70 / sqrt(10)
```

And, for a 99% CI we have
```{r}
116.9 - qt(p = 1 - 0.01/2, df = 9) * 21.70 / sqrt(10)
116.9 + qt(p = 1 - 0.01/2, df = 9) * 21.70 / sqrt(10)
```
:::

- Note:
  - CI level $\uparrow$ (so $\alpha \downarrow$) $\Rightarrow$ larger intervals
  - $n \uparrow$ $\Rightarrow$ smaller intervals
  - $s^2 \uparrow$ $\Rightarrow$ larger intervals


---

[Bone Density Case Study](./04_boneden.html)

---

# Estimate Variance:

- We estimate $\sigma^2$ with $s^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})^2$

-   Why $n - 1$?

    $$
    \bar{X} = \operatorname{argmin}_a \sum (X_i - a)^2
    $$
    
    The ideal estimate of $\sigma^2$ is $\frac{1}{n} \sum (X_i - \mu)^2$. However,
    
    $$
    \frac{1}{n} \sum (X_i - \bar{X})^2 \leq \frac{1}{n} \sum (X_i - \mu)^2
    $$
    
    So, $\frac{1}{n} \sum (X_i - \bar{X})^2$ is too small. Dividing by $n-1$ instead of $n$ makes it a little larger, and it turns out it makes it larger by the correct amount.


-   Property of the variance:
    $$
    \frac{s^2}{\sigma^2 / (n - 1)} \sim \chi^2_{n - 1}
    $$
- That is, the chi-squared distribution with $n - 1$ degrees of freedom.

- This property **only** holds when the original data ($X_1,X_2,\ldots,X_n$) or normally distributed.

- Properties of chi-squared:
  1. Support $\geq 0$
  2. $E(\chi^2_{\text{df}}) = \text{df}$
  3. df $\downarrow$ $\Rightarrow$ thicker tails (extreme events happen more often)

- Can use this to get confidence intervals for $\sigma^2$ (**only** when your original data are normal).

- Let $\chi^2_{\text{df}, p}$ = $p^\text{th}$ quantile of a $\chi^2_{\text{df}}$ distribution
  - `qchisq(p, df)`
  
```{r}
#| echo: false
#| fig-alt: "Chi-squared quantiles for 1-alpha probability between them."
nu <- 3
lb <- qchisq(p = 0.1, df = nu)
ub <- qchisq(p = 0.9, df = nu)
tibble(x = seq(0, 9, length.out = 200)) |>
  mutate(y = dchisq(x = x, df = nu)) ->
  df1
df1 |>
  filter(
    x > lb,
    x < ub
  ) ->
  df2

ggplot() +
  geom_line(data = df1, mapping = aes(x = x, y = y)) +
  geom_area(data = df2, mapping = aes(x = x, y = y)) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 12),
        axis.title = element_blank()) +
  scale_x_continuous(breaks = c(lb, ub), labels = c(expression(qchisq(alpha/2,n-1)), expression(qchisq(1-alpha/2,n-1)))) +
  annotate(geom = "text", x = 3, y = 0.05, label = "1-alpha", color = "white", size = 9, parse = TRUE)
```

\begin{align*}
&\Pr\left( \chi^2_{n-1, \alpha/2} \leq \frac{s^2}{\sigma^2/(n-1)} \leq \chi^2_{n-1, 1-\alpha/2} \right) = 1 - \alpha\\
&\Leftrightarrow  \Pr\left( \frac{(n - 1)s^2}{\chi^2_{n-1, 1 - \alpha/2}} \leq \sigma^2 \leq \frac{(n - 1)s^2}{\chi^2_{n-1, \alpha/2}} \right) = 1 - \alpha
\end{align*}

- Thus a $(1-\alpha)100\%$ CI for the population variance (only when the original data are normally distributed) is:
  $$
  \frac{(n - 1)s^2}{\chi^2_{n-1, 1 - \alpha/2}} \leq \sigma^2 \leq \frac{(n - 1)s^2}{\chi^2_{n-1, \alpha/2}}
  $$

- Notes:
  - Less often constructed than CI for $\mu$
  - **Very** sensitive to violations of normality  
    - CLT does not save you
  - In real life, folks use a bootstrap if they want a CI for the variance.

# Estimate Binomial Proportion

-   Let $X_1, X_2, \ldots, X_n$ be independent Bernoulli trials s.t.:

    $$
    X_i = 
    \begin{cases}
    1 & \text{w.p. } p \\
    0 & \text{w.p. } 1 - p
    \end{cases}
    $$

  - Then $X = \sum_{i=1}^n X_i$ = number of 1's out of $n$ trials  
  - $X \sim \text{Bin}(n, p)$

- Example: Estimate prevalence of malignant melanoma  
  - Sample $n = 5000$ individuals.  
  - $X_i = 1$ if has melanoma, $0$ if no

- Goal: Observe $X_i$, estimate $p$

-   We estimate $p$ with
    $$
    \hat{p} = \frac{1}{n} \sum X_i = \frac{X}{n} = \text{proportion of 1's}
    $$

- Properties:
  - $E(\hat{p}) = E\left(\frac{X}{n}\right) = \frac{1}{n} E(X) = \frac{1}{n} n p = p \Rightarrow$ unbiased
  - $\operatorname{Var}(\hat{p}) = \operatorname{Var}\left(\frac{X}{n}\right) = \frac{1}{n^2} \operatorname{Var}(X) = \frac{1}{n^2} n p(1 - p) = \frac{p(1 - p)}{n}$
    - More precise for larger $n$ $\Rightarrow$ consistent

-   Estimated standard error:
    $$
    \sqrt{ \frac{ \hat{p}(1 - \hat{p}) }{n} }
    $$

- Goal: Interval estimate of $p$

- For large $n$, $\hat{p} \sim N\left(p,\ \frac{p(1 - p)}{n} \right)$  
  - Rule of thumb: $n \hat{p}(1 - \hat{p}) \geq 5$

- Let $z_p =$ `qnorm(p)` $= p^\text{th}$ quantile of $N(0, 1)$

-   Then, subracting the mean from $\hat{p}$ and dividing by the estimated standard error, we have:
    $$
    \frac{ \hat{p} - p }{ \sqrt{ \frac{ \hat{p}(1 - \hat{p}) }{n} } } \approx N(0, 1)
    $$

-   Thus:
    $$
    \Pr\left( z_{1 - \alpha/2} \leq \frac{ \hat{p} - p }{ \sqrt{ \hat{p}(1 - \hat{p}) / n } } \leq z_{\alpha/2} \right) \approx 1-\alpha
    $$

-   Solving for $p$, we get the following $(1-\alpha)100\%$ CI for $p$:
    $$
    \hat{p} \pm z_{1 - \alpha/2} \cdot \sqrt{ \frac{ \hat{p}(1 - \hat{p}) }{n} }
    $$

- This follows the "Estimate $\pm$ Multiplier $\times$ Standard Error" format for a confidence interval.

::: {.panel-tabset}
## Exercise
Suppose out of 10,000 women, 400 have breast cancer. What is a 95% CI for breast cancer incidence?

## Hint

$n = 10000$ and $x = 400$. We need the $1 - \alpha = 1 - 0.05 / 2 = 0.975$ quantile of the standard normal.

## Solution
Our estimated incidence is $\hat{p} = 400 / 10000 = 0.04$  

Our estimated standard error is
```{r}
sqrt(0.04 * (1 - 0.04) / 10000)
```


Checking for our rule of thumb: $n \hat{p}(1 - \hat{p}) = 384 > 5$, so we can use the CLT.

For a 95% confidence interval, $\alpha = 0.05$ and we need the $1 - \alpha/2 = 1 - 0.05/2 = 0.975$ quantile of the standard normal distribution, denoted $z_{0.975}$, or
```{r}
qnorm(0.975)
```

Thus, our 95% confidence interval is
```{r}
0.04 - 1.96 * 0.00196
0.04 + 1.96 * 0.00196
```
:::

### Exact test

- What if $n$ is small? Use an exact method

-   Find $p_1$ and $p_2$ such that:
    \begin{align*}
    \frac{\alpha}{2} &= \text{pbinom}(x,\ \text{size} = n,\ \text{prob} = p_1)\\
    \frac{\alpha}{2} &= 1 - \text{pbinom}(x - 1,\ \text{size} = n,\ \text{prob} = p_2)
    \end{align*}
    
- E.g., $n = 10$ and $X = 3$. Suppose we want a 80% CI (so $\alpha = 0.2$).

-   The value of $p_1$ such that the probability $X$ being at least $3$ equals $\alpha/2=0.1$ is 0.1158. Visually, the sum of the heights of the orange bars is 0.1:

    ```{r}
    #| echo: false
    #| fig-alt: "P(X >= 3) when X ~ Binom(10, 0.1158) is 0.1."
    n <- 10
    x <- 3
    p <- 0.2
    bout <- binom.test(x = x, n = n, conf.level = 0.8)
    lb <- bout$conf.int[[1]]
    
    tibble(xv = 0:n) |>
      mutate(
        yv = dbinom(x = xv, size = n, prob = lb),
        islarge = xv >= x) |>
      ggplot(aes(x = xv, xend = xv, yend = yv, color = islarge)) +
      geom_segment(y = 0, linewidth = 2) +
      theme_bw() +
      scale_color_manual(values = palette.colors(n = 2)) +
      xlab("X") +
      ylab("Probability") +
      theme(legend.position = "none") +
      ggtitle(paste0("PMF of Binomial with p = ", round(lb, digits = 4))) +
      scale_x_continuous(breaks = 0:n, minor_breaks = NULL)
    ``` 
    
-   The value of $p_1$ such that the probability $X$ being at most $3$ equals $\alpha/2=0.1$ is 0.5517. Visually, the sum of the heights of the orange bars is 0.1:

    ```{r}
    #| echo: false
    #| fig-alt: "P(X <= 3) when X ~ Binom(10, 0.5517) is 0.1."
    n <- 10
    x <- 3
    p <- 0.2
    bout <- binom.test(x = x, n = n, conf.level = 1 - p)
    ub <- bout$conf.int[[2]]
    
    tibble(xv = 0:n) |>
      mutate(
        yv = dbinom(x = xv, size = n, prob = ub),
        islarge = xv > x) |>
      ggplot(aes(x = xv, xend = xv, yend = yv, color = islarge)) +
      geom_segment(y = 0, linewidth = 2) +
      theme_bw() +
      scale_color_manual(values = rev(palette.colors(n = 2))) +
      xlab("X") +
      ylab("Probability") +
      theme(legend.position = "none") +
      ggtitle(paste0("PMF of Binomial with p = ", round(ub, digits = 4))) +
      scale_x_continuous(breaks = 0:n, minor_breaks = NULL)
    ``` 



---

[Binomial R code](./04_binom.html)

---

# Estimate Poisson Mean

- $Y \sim \text{Poi}(\mu)$ where $\mu = \lambda T$

- Goal: Estimate $\lambda$

  - $\lambda$ = incidence rate per unit time  
  - $T$ = time

- Example: Leukemia rate in Woburn, MA.  
  - 12,000 residents, 10 years, 12 children got leukemia

- A common unit in biostatistics is a **person-year**
  - 1 person being followed for 1 year  
  - Normalizes by number of people in a study  
  - E.g., 12 cases out of 20 is a lot more than 12 out of 20000

- Example: Woburn study had 12,000 people × 10 years = 120,000 person-years

-   We estimate $\lambda$ with:
    $$
    \hat{\lambda} = \frac{X}{T}
    $$

- $T$ can be in any unit of time. E.g., person-year.

-   $\hat{\lambda}$ is unbiased:
    $$
    E(\hat{\lambda}) = E\left( \frac{X}{T} \right) = \frac{E(X)}{T} = \frac{\lambda T}{T} = \lambda \quad \text{(unbiased)}
    $$

- Woburn Example: $\hat{\lambda} = \frac{12 \text{ cases}}{120{,}000 \text{ person-years}} = 0.0001 \text{ cases per person-year}$

- Cancer rates are typically low, so it is useful to multiply by 100,000 to get human-readable units

-   Woburn Example:
    $$
    0.0001 \text{ cases/person-year} = 10 \text{ cases / 100,000 person-years}
    $$

- Exact interval uses same strategy as exact binomial case:
  - Find $\mu_1$ such that $\Pr(X \geq x \mid \mu_1) = \alpha/2$
  - Find $\mu_2$ such that $\Pr(X \leq x \mid \mu_2) = \alpha/2$
  - Interval: $(\mu_1,\ \mu_2)$  

-   The interval for $\lambda$ is then:
    $$
    \left( \frac{\mu_1}{T},\ \frac{\mu_2}{T} \right)
    $$

- Use `poisson.test()` in R

# One-sided CI (if we have time)

- Suppose we are only interested in lower or upper bounds  
  - E.g., compare cancer treatment survival rate against baseline of 30%.  
    Want lower bound on treatment effect to see if it's better than baseline

- Find $p_1$ such that $\Pr(p > p_1) = 1 - \alpha$  
  - $p_1$ is the random variable and $p$ is the fixed parameter.

- Or, find $p_2$ such that $\Pr(p < p_2) = 1 - \alpha$  
  - E.g., suppose we want an upper bound on the incidence rate of some treatment group

-   Since $\frac{\hat{p} - p}{\sqrt{ \hat{p}(1 - \hat{p}) / n }} \sim N(0, 1)$, we can get a one-sided confidence interval by knowing the following:
    $$
    \Pr\left(-z_{1 - \alpha} < \frac{\hat{p} - p}{\sqrt{ \hat{p}(1 - \hat{p}) / n }} \right) = 1 - \alpha
    $$
    and solving for $p$:
    \begin{align*}
    &\Leftrightarrow \Pr\left(z_{1 - \alpha} > \frac{p - \hat{p}}{\sqrt{ \hat{p}(1 - \hat{p}) / n }} \right) = 1 - \alpha\\
    &\Leftrightarrow \Pr\left(\hat{p} + z_{1 - \alpha} \cdot \sqrt{ \hat{p}(1 - \hat{p}) / n } > p \right) = 1 - \alpha
    \end{align*}
    Thus, $p_2$ (the upper bound of a lower one sided confidence interval) is:
    $$
    p_2 = \hat{p} + z_{1 - \alpha} \cdot \sqrt{ \hat{p}(1 - \hat{p}) / n }
    $$

-   Similarly, $p_1$ (the lower bound of an upper one-sided confidence interval) is:
    $$
    p_1 = \hat{p} - z_{1 - \alpha} \cdot \sqrt{ \hat{p}(1 - \hat{p}) / n }
    $$

::: {.panel-tabset}
## Exercise
40 out of 100 patients survive a new cancer treatment. What is the **lower bound** on $\Pr(\text{survive})$? (Upper 1-sided 95% CI)

## Hint

Use the fact that
$$
\frac{\hat{p} - p}{\sqrt{\hat{p}(1-\hat{p})/n}} \approx N(0,1)
$$
for large $n$.

## Solution
We have that
\begin{align*}
&\Pr\left(\frac{\hat{p} - p}{\sqrt{\hat{p}(1-\hat{p})/n}} \leq z_{1-\alpha}\right) = 1-\alpha\\
&\Leftrightarrow \Pr\left(\hat{p} - z_{1-\alpha} \sqrt{\hat{p}(1-\hat{p})/n} \leq p\right) = 1-\alpha
\end{align*}

Let's find $\hat{p}$
$$
\hat{p} = \frac{40}{100} = 0.4
$$

Let's find $z_{1-\alpha}$ when $\alpha = 0.05$
```{r}
qnorm(1 - 0.05)
```

This gives us the following lower bound:
```{r}
0.4 - 1.645 * sqrt(0.4 * (1 - 0.4) / 100)
```

So the one-sided confidence interval is $p > 0.3194$.
:::

# Exercises 6.5 -- 6.10

The data in Table 6.10 concern the mean triceps skin-fold thickness in a group of normal men and a group of men with chronic airflow limitation [@arora1984effect].


| Group                      | Mean | sd  | n  |
|---------------------------:|-----:|----:|---:|
| Normal                     | 1.35 | 0.5 | 40 |
| Chronic airflow limitation | 0.92 | 0.4 | 32 |

: Table 6.10: Triceps skin-fold thickness in normal men and men with chronic airflow limitation

::: {.panel-tabset}
## Exercise 6.5 
What is the standard error of the mean for each group?

## Solution

- Normal group: $0.5 / \sqrt{40}$ = 0.07906
- Chronic airflow limitation group: $0.4 / \sqrt{32}$ = 0.07071
:::

::: {.panel-tabset}
## Exercise 6.6 
Assume that the central-limit theorem is applicable. What does it mean in this context?

## Solution

That the sample mean skin-fold thickness within each group is (approximately) normally distributed.

:::

::: {.panel-tabset}
## Exercise 6.7 
Find the upper 1st percentile of a $t$ distribution with 16 df.

## Solution

```{r}
qt(p = 0.99, df = 16)
```
:::

::: {.panel-tabset}
## Exercise 6.8 
Find the lower 10th percentile of a $t$ distribution with 28 df.

## Solution

```{r}
qt(p = 0.1, df = 28)
```

:::

::: {.panel-tabset}
## Exercise 6.9 
Find the upper 2.5th percentile of a $t$ distribution with 7 df.

## Solution

```{r}
qt(p = 0.975, df = 7)
```
:::

::: {.panel-tabset}
## Exercise 6.10 
What are the upper and lower 2.5th percentiles for a chi-square distribution with 2 df? What notation is used to denote these percentiles?

## Solution

```{r}
qchisq(p = c(0.025, 0.975), df = 2)
```

$$
\chi^2_{2,0.025} \text{ and } \chi^2_{2,0.975}
$$
:::

# Exercises 6.11 -- 6.17

Consider the hospital data that you can download from [here](https://dcgerard.github.io/stat_320/data/hospital.csv) and read about [here](https://dcgerard.github.io/stat_320/data.html#hospital). Regard this hospital as typical of Pennsylvania hospitals.

::: {.panel-tabset}
## Exercise 6.11 
Compute a 95% CI for the mean age.

## Solution by Hand

The CI is:
$$
\bar{x} \pm t_{n - 1, 1 - 0.05/2}s/\sqrt{n}
$$
Let's calculate these elements

```{r}
#| message: false
library(tidyverse)
library(broom)
hospital <- read_csv("https://dcgerard.github.io/stat_320/data/hospital.csv")
hospital |>
  summarize(
    xbar = mean(Age),
    s = sd(Age),
    n = n(),
    t = qt(p = 1 - 0.05 / 2, df = n() - 1)
  )
```

So the CI is
$$
41.24 \pm 2.064 \times 20.1 / \sqrt{25}
$$

Plugging in R, we get:
```{r}
c(
  41.24 - 2.064 * 20.1 / sqrt(25),
  41.24 + 2.064 * 20.1 / sqrt(25)
)
```

## Solution in R

```{r}
t.test(Age ~ 1, data = hospital) |>
  tidy() |>
  select(conf.low, conf.high)
```


:::

::: {.panel-tabset}
## Exercise 6.12 
Compute a 95% CI for the mean white blood count following admission.

## Solution by Hand

The CI is again
$$
\bar{x} \pm t_{n - 1, 1 - 0.05/2}s/\sqrt{n}
$$

Calculating these summaries, we get
```{r}
hospital |>
  summarize(
    xbar = mean(WBC),
    s = sd(WBC),
    n = n(),
    t = qt(p = 1 - 0.05 / 2, df = n() - 1)
  )
```

So the CI is
$$
7.84 \pm 2.064 \times 3.21 / \sqrt{25}
$$

Plugging in R, we get:
```{r}
c(
  7.84 - 2.064 * 3.21 / sqrt(25),
  7.84 + 2.064 * 3.21 / sqrt(25)
)
```

## Solution in R

```{r}
t.test(WBC ~ 1, data = hospital) |>
  tidy() |>
  select(conf.low, conf.high)
```


You might think that since we have counts, you could try to assume that $X_i \sim \mathrm{Poi}(\mu)$ and try to come up with an exact confidence interval based on this assumption. This could be done by calculating $X = \sum_{i=1}^n X_i$, which follows a Poisson with mean parameter $n\mu$. You can get an exact CI for $n\mu$ and divide it by $n$ to get an exact CI for $\mu$. But, the normal approximation would be appropriate for this approach anyway. More importantly, the $t$-method is probably better anyway because it allows for over/underdispersion. Anyway, for comparison, this approach would result in:
```{r}
x <- sum(hospital$WBC)
poisson.test(x = x) |>
  tidy() |>
  select(conf.low, conf.high) |>
  mutate(conf.low = conf.low / 25, conf.high = conf.high / 25)
```

:::

::: {.panel-tabset}
## Exercise 6.13 
Answer Problem 6.12 for a 90% CI.

## Solution

For the by-hand way, we just use the 0.95 quantile of the $t$ distribution with $n-1$ degrees of freedom.
```{r}
qt(p = 1 - 0.1 / 2, df = 25 - 1)
```
```{r}
c(
  7.84 - 1.711 * 3.21 / sqrt(25),
  7.84 + 1.711 * 3.21 / sqrt(25)
)
```

For the R way, we change the `conf.level` argument
```{r}
t.test(WBC ~ 1, data = hospital, conf.level = 0.9) |>
  tidy() |>
  select(conf.low, conf.high)
```
:::

::: {.panel-tabset}
## Exercise 6.14 
What is the relationship between your answers to Problems 6.12 and 6.13?

## Solution

The CI in 6.12 is larger than the CI in 6.13 because it has a higher confidence level (you need wider intervals to capture the parameter more often).

:::

::: {.panel-tabset}
## Exercise 6.15 
What is the best point estimate of the percentage of males among patients discharged from Pennsylvania hospitals?

## Solution

This is $\hat{p}$, the proportion of patients that are male:
```{r}
hospital |>
  summarize(x = sum(Sex == "male"), n = n())
```
Since $\hat{p} = 11 / 25 = 0.44$, 44\% is the best estimate.
:::

::: {.panel-tabset}
## Exercise 6.16 
What is the standard error of the estimate obtained in Problem 6.15?

## Solution

This is $\sqrt{\hat{p}(1-\hat{p})/n} = \sqrt{0.44 * (1 - 0.44) / 25}$
```{r}
sqrt(0.44 * (1 - 0.44) / 25)
```
This is the standard error for $\hat{p}$. If you really want to keep it in in units of percent (instead of proportions), then standard error is 9.928\%.
:::

::: {.panel-tabset}
## Exercise 6.17 
Provide a 95% CI for the percentage of males among patients discharged from Pennsylvania hospitals.

## Solution by Hand

If we want to keep things in terms of proportions, we havee
$$
\hat{p} \pm z_{1-0.5/2} \times\sqrt{\hat{p}(1-\hat{p})/n}
$$

If we really want to keep things in terms of percent, then this is

$$
100 \times \hat{p} \pm z_{1-0.5/2}\times 100 \times\sqrt{\hat{p}(1-\hat{p})/n}
$$
That normal quantile $z_{1-0.05/2}$ is
```{r}
qnorm(0.975)
```

So, we have
```{r}
c(
  44 - 1.96 * 9.928,
  44 + 1.96 * 9.928
)
```

## Solution in R

```{r}
binom.test(x = 11, n = 25) |>
  tidy() |>
  select(conf.low, conf.high) |>
  mutate(conf.low = 100 * conf.low, conf.high = 100 * conf.high)
```

:::

# Exercises 6.77 -- 6.80

The value of mammography as a screening test for breast cancer has been controversial, particularly among young women. A study was recently performed looking at the rate of false positives for repeated screening mammograms among approximately 10,000 women who were members of Harvard Pilgrim Health Care, a large health-maintenance organization in New England [@elmore1998ten]. The study reported that of a total of 1996 tests given to 40- to 49-year-old women, 156 yielded false-positive results.

::: {.panel-tabset}
## Exercise 6.77
What does a false-positive test result mean, in words, in this context?

## Solutions

The mammogram result is positive (indicating breast cancer), but the woman does not have breast cancer.

:::

::: {.panel-tabset}
## Exercise 6.78 
Some physicians feel a mammogram is not cost effective unless one can be reasonably certain (e.g., 95\% certain) that the false-positive rate is less than 10%. Can you address this issue based on the preceding data? (Hint: Use a CI approach.)

## Solutions

We will create a 95\% lower confidence interval. If the upper bound of this is less than 0.1, then we can be reasonably certain (at the 0.05 significance level) that the test is cost effective.

The lower one-sided CI has an upper bound of

$$
\hat{p} + z_{1 - \alpha} \sqrt{\hat{p}(1 - \hat{p})/n}
$$

Calculating values we get
```{r}
phat <- 156 / 1996
phat
z <- qnorm(1 - 0.05)
z
n <- 1996
```

So the upper bound is
$$
0.07816 + 1.645 \times \sqrt{0.07816(1 - 0.07816) / 1996}
$$
```{r}
0.07816 + 1.645 * sqrt(0.07816 * (1 - 0.07816) / 1996)
```

Since this upper bound is less than 0.1, we are reasonably certain (at the 0.05 significance level) that the test is cost effective.

In R, the upper bound is
```{r}
binom.test(x = 156, n = 1996, alternative = "less") |>
  tidy() |>
  select(conf.low, conf.high)
```

:::

::: {.panel-tabset}
## Exercise 6.79
Suppose a woman is given a mammogram every 2 years starting at age 40. What is the probability that she will have at least one false-positive test result among 5 screening tests during her forties? (Assume the repeated screening tests are independent.)

## Solutions

Let $p$ be the probability of a false positive. Let $X$ be the number of tests, out of 5, whihch is a false positive. Then $X \sim \mathrm{Binomial}(5, p)$. We want $\mathrm{P}(X \geq 1) = 1 - \mathrm{P}(X = 0)$. This is, using the binomial PMF,
$$
1 - (1 - p)^5.
$$

We can estimate this probability with $\hat{p}$
$$
1 - (1 - \hat{p})^5.
$$

In R, this estimate is
```{r}
phat <- 156 / 1996
1 - (1 - phat)^5
```

It is very important to note that 0.3343 is *not* the answer. It is an *estimate* of the probability of at least one false positive. It is not the probability of at least one false positive.


:::

::: {.panel-tabset}
## Exercise 6.80 
Provide a two-sided 95% CI for the probability estimate in Problem 6.79.

## Solutions

A 95\% confidence interval for $p$ is
```{r}
bindf <- binom.test(x = 156, n = 1996) |>
  tidy() |>
  select(conf.low, conf.high)
bindf
```

So we just transform these bounds using the formula from part 6.79
```{r}
1 - (1 - bindf)^5
```

So a 95\% confidence interval for the probability that a woman will have at least one false positive in her forties is (0.2921, 0.3787).

:::

# Exercises 6.102 -- 6.103

Injuries are common in football and may be related to a number of factors, including the type of playing surface, the number of years of playing experience, and whether any previous injury exists. A study of factors affecting injury among Canadian football players was recently reported [@hagel2003injury]. The rate of injury to the upper extremity (that is, shoulder to hand) on a dry field consisting of natural grass was 2.73 injuries per 1000 games. Assume this rate is known without error.

::: {.panel-tabset}
## Exercise 6.102 
The study reported 45 injuries to the upper extremity on a dry field consisting of artificial turf over the course of 10,112 games. What procedure can be used to assess whether the risk of injury is different on artificial turf versus natural grass?

## Solutions

Let $X$ be the number of injuries to the upper extremity on turf. If it were the same as natural grass, then we would expect
$$
X \sim \mathrm{Poisson}(t\lambda) = \mathrm{Poisson}\left(\frac{2.73}{1000} \times 10112\right) = \mathrm{Poisson}(27.61)
$$
Suppose $X \sim \mathrm{Poisson}(\mu)$. If this were chapter seven, we would run a hypothesis test of the null that $\mu = 27.61$ versus the alternative that $\mu \neq 27.61$. But this is chapter 6, so we just calculate a 95\% confidence interval and see that 27.61 is included in it.

```{r}
poisson.test(x = 45, r = 27.61) |>
  tidy() |>
  select(conf.low, conf.high)
```

The confidence interval does not include 27.61, so we have evidence (at the 0.05 significance level) that the two rates differ.

:::

::: {.panel-tabset}
## Exercise 6.103 
Provide a 95% CI for the rate of injury to the upper extremity on artificial turf. (Hint: Use the Poisson distribution.) Express each rate as the number of injuries per 1000 games.

## Solutions

The time period in 1000 game increments for this study is 10112 / 1000 = 10.112. We can just plug this into `poisson.test()` using this time increment.
```{r}
poisson.test(x = 45, T = 10.112, r = 2.73) |>
  tidy()
```

The 95\% CI is 3.246 to 5.955 injuries per 1000 games.

You could also use this CI, like in the previous question, and note that since it does not include 2.73, we have evidence (at the 0.05 significance level) that the injury rates differ between the two fields.

:::

# References
