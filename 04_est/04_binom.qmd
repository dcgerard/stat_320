---
title: "Estimates and Intervals for Binomial Proportions"
author: "David Gerard"
date: today
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
            fig.width = 4, 
            fig.height = 3, 
            fig.align = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

# Normal approach

- We want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer. 

- Let $X$ be the number of rats with bladder cancer. Then $X \sim \mathrm{Binom}(20, p)$ (our observed $x = 2$) and our goal is to estimate $p$.

- We estimate $p$ with $\hat{p} = 2 / 20$

  ```{r}
  phat = 2 / 20
  phat
  ```

- The standard error of this estimate is $\sqrt{\hat{p}(1-\hat{p})/n} = \sqrt{0.1 * (1 - 0.1)/20}$

  ```{r}
  n <- 20
  se <- sqrt(phat * (1 - phat) / n)
  se
  ```

- Suppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have $\alpha = 1 - 0.9 = 0.1$, so we need the $1 - \alpha/2 = 1 - 0.1 / 2 = 0.95$ quantile of the standard normal distribution.

  ```{r}
  z <- qnorm(0.95)
  z
  ```

- Now we can obtain an approximate 90% confidence interval by $\hat{p} \pm z_{0.95} \sqrt{\hat{p}(1 - \hat{p}) / n}$

  ```{r}
  phat - z * se
  phat + z * se
  ```

# Normal Real way

- It's crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the `{broom}` package

  ```{r}
  #| message: false
  library(tidyverse)
  library(broom)
  ```

- You use the `prop.test()` function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into `tidy()`.
  ```{r}
  pout <- prop.test(x = 2, n = 20, conf.level = 0.9)
  tidy(pout) |>
    select(estimate, conf.low, conf.high)
  ```

- The results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error.
  $$
  \mathrm{Pr}\left(-z_{1-\alpha/2} \leq \frac{\hat{p} - p}{\sqrt{p(1-p)/2}}\leq z_{1-\alpha/2} \right) \approx 1-\alpha
  $$
  You then solve for $p$ on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.
  
- The Wald and the Wilson approaches are approximately the same for large $n$.
  
  ```{r}
  # Wald
  n <- 5000
  x <- 2000
  phat <- x / n
  se <- sqrt(phat * (1 - phat) / n)
  z <- qnorm(0.95)
  phat - z * se
  phat + z * se
  
  # Wilson
  prop.test(x = x, n = n, conf.level = 0.9) |>
    tidy() |>
    select(estimate, conf.low, conf.high)
  ```
  
# Exact approach

- The rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since $n\hat{p}(1-\hat{p}) = 1.8 < 5$. Thus, the above intervals would be suspect.

- The exact approach finds a $p_1$ such that $\mathrm{Pr}(X \leq x|p_1) = \alpha/2$ and a $p_2$ such $\mathrm{Pr}(X \geq x|p_1) = \alpha/2$. The interval is then $(p_1, p_2)$.

```{r}
#| echo: false
#| eval: false
# library(tidyverse)
# n <- 20
# xt <- 2
# bout <- binom.test(x = xt, n = n)
# low <- bout$conf.int[[1]]
# high <- bout$conf.int[[2]]
# 
# pal <- palette.colors(n = 2, palette = "Okabe-Ito")
# attributes(pal) <- NULL
# 
# ## Lower
# pseq <- seq(0.1, low, length.out = 10)
# pl_list <- list()
# for (i in seq_along(pseq)) {
#   tibble(x = 0:n, pmf = dbinom(x = 0:n, size = n, prob = pseq[[i]])) |>
#     mutate(isg = x >= xt) ->
#     df
#   prob <- sum(df$pmf[df$isg])
#   ggplot(df, aes(x = x, xend = x, yend = pmf, color = isg)) +
#     geom_vline(xintercept = x - 0.5, lty = 2, col = 2) +
#     geom_segment(y = 0, lwd = 2) +
#     guides(color = "none") +
#     scale_color_manual(values = pal) +
#     ggtitle(paste0("p1=", round(pseq[[i]], digits = 2), ", Pr(X>=", x, "|p1)=", round(prob, digits = 3))) +
#     ylim(0, 0.8) +
#     scale_x_continuous(breaks = 0:n)  ->
#     pl_list[[i]]
# }
# 
# library(animation)
# saveGIF(expr = {
#   for (i in seq_along(pl_list)) {
#     print(pl_list[[i]])
#   }
# }, movie.name = "lower_binom.gif", ani.height = 200, ani.width = 300)
# 
# 
# ## Upper
# pseq <- seq(0.2, high, length.out = 10)
# pl_list <- list()
# for (i in seq_along(pseq)) {
#   tibble(x = 0:n, pmf = dbinom(x = 0:n, size = n, prob = pseq[[i]])) |>
#     mutate(isg = x <= xt) ->
#     df
#   prob <- sum(df$pmf[df$isg])
#   ggplot(df, aes(x = x, xend = x, yend = pmf, color = isg)) +
#     geom_vline(xintercept = x + 0.5, lty = 2, col = 2) +
#     geom_segment(y = 0, lwd = 2) +
#     guides(color = "none") +
#     scale_color_manual(values = pal) +
#     ggtitle(paste0("p2=", round(pseq[[i]], digits = 2), ", Pr(X<=", x, "|p2)=", round(prob, digits = 3))) +
#     ylim(0, 0.8) +
#     scale_x_continuous(breaks = 0:n)  ->
#     pl_list[[i]]
# }
# 
# library(animation)
# saveGIF(expr = {
#   for (i in seq_along(pl_list)) {
#     print(pl_list[[i]])
#   }
# }, movie.name = "upper_binom.gif", ani.height = 200, ani.width = 300)
```

- Let's visualize finding this $p_1$ an $p_2$ for a 95% confidence interval where $\alpha / 2 = 0.025$.

- Find a $p_1$ such that being greater than or equal to $x$ is 0.025.

  ![](./04_figs/lower_binom.gif)\ 

- Find a $p_2$ such that being less than or equal to $x$ is 0.025.

  ![](./04_figs/upper_binom.gif)\ 

- In practice, you do this using `binom.test()`.

  ```{r}
  binom.test(x = 2, n = 20, conf.level = 0.95) |>
    tidy() |>
    select(estimate, conf.low, conf.high)
  
  binom.test(x = 2, n = 20, conf.level = 0.9) |>
    tidy() |>
    select(estimate, conf.low, conf.high)
  ```


- **Exercise**: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?

  ```{r}
  #| echo: false
  #| eval: false
  # Wald
  n <- 10
  x <- 6
  phat <- x / n
  se <- sqrt(phat * (1 - phat) / n)
  z <- qnorm(0.9)
  phat - z * se
  phat + z * se
  
  # Wilson
  prop.test(x = x, n = n, conf.level = 0.8) |>
    tidy() |>
    select(estimate, conf.low, conf.high)

  # Exact
  binom.test(x = x, n = n, conf.level = 0.8) |>
    tidy() |>
    select(estimate, conf.low, conf.high)

  # Yes, should use the binomial approach since 10 * 0.6 * 0.4 = 2.4 < 5
  ```













