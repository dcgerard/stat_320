---
title: "Chapter 10: Categorical Data"
author: "David Gerard"
date: today
---

```{r}
#| message: false
#| echo: false
library(tidyverse)
library(broom)
knitr::opts_chunk$set(echo = TRUE, 
            fig.width = 4, 
            fig.height = 3, 
            fig.align = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
plt_t <- function(mu = 0, sig = 1, lb = -Inf, ub = Inf, df = Inf, rng = c(-3, 3), two_sided = FALSE, col = "#E69F00", lwd = 1) {
  tibble(x = seq(mu + rng[[1]] * sig, mu + rng[[2]] * sig, length.out = 500)) |>
    mutate(y = dt(x = (x - mu) / sig, df = df)) ->
    df1
  df1 |>
    filter(x > lb, x < ub) ->
    df2
  ggplot() +
    geom_line(data = df1, mapping = aes(x = x, y = y), linewidth = lwd) +
    geom_area(data = df2, mapping = aes(x = x, y = y), fill = col) +
    geom_line(data = df2, mapping = aes(x = x, y = y), color = col, linewidth = lwd) +
    theme_classic() +
    theme(axis.title = element_blank()) ->
    pl
  
  if (two_sided) {
    df1 |>
      filter(x > 2 * mu - ub, x < 2 * mu - lb) ->
      df3
    pl <- pl + 
      geom_area(data = df3, mapping = aes(x = x, y = y), fill = col) +
      geom_line(data = df3, mapping = aes(x = x, y = y), color = col, linewidth = lwd)
  }
  pl
}
```

# Introduction

::: {.callout-tip}
## Categorical Variable
A variable that groups units into different categories.
:::

- Examples:
  - Oral contraceptive user vs. Non-OC-user
  - Has cancer vs. Does not have cancer

- There are lots of inference problems that are interesting using categorical variables:

- Example 1: Test if cancer incidence is the same between OC users and non-OC users.
  - Test the association between two categorical variables (cancer status and OC status).

- Example 2: Test if heavy OC users, light OC users, and non-OC users have the same cancer rates.
  - Same as exame 1 above, but now OC status has three categories instead of 2.

- Example 3: Are the observed frequencies below consistent with the theoretical probabilities?

  | Diastolic Blood Pressure | Frequency | Expected Probability |
  |--------------------------|-----------|----------------------|
  | < 50                     | 57        | 1%                   |
  | ≥ 50, < 60               | 330       | 7%                   |
  | ≥ 60, < 70               | 2132      | 30%                  |
  | ≥ 70, < 80               | 4584      | 62%                  |


# Two-sample Test for Binomial Proportions

- Example: Age at first birth vs. breast cancer incidence.

  |                             | Control (No Cancer) | Case (Cancer) |
  |-----------------------------|---------------------|---------------|
  | Age at 1st birth ≤ 29 years | 8747                | 2537          |
  | Age at 1st birth ≥ 30 years | 1498                | 683           |
  | Total                       | 10245               | 3220          |

- Data collection scheme: Chose women with cancer and women without cancer, then measured age of their first births

- Goal: Test if cancer is associated with age.

- Two populations:
  - Those with cancer
  - Those without cancer

- Let:
  - $p_1 = P(\text{age} \geq 30 \mid \text{cancer})$
  - $p_2 = P(\text{age} \geq 30 \mid \text{no cancer})$

- Hypotheses:
  - $H_0$: $p_1 = p_2$ vs. $H_1$: $p_1 \neq p_2$

- Sample proportions:
  - $\hat{p}_1 = \frac{683}{3220} = 0.212$
  - $\hat{p}_2 = \frac{1498}{10245} = 0.146$

- Idea: Base test on $\hat{p}_1 - \hat{p}_2$.

- Let:
  - $X_1$ = number of women age $\geq 30$ with cancer
  - $X_2$ = number of women age $\geq 30$ without cancer

- $X_1 \sim \text{Binom}(n_1, p_1)$, where $n_1 = 3220$

  - $\hat{p}_1 = X_1 / n_1$

- $X_2 \sim \text{Binom}(n_2, p_2)$, where $n_2 = 10245$

  - $\hat{p}_2 = X_2 / n_2$

- We have very large sample sizes, so we can use normal theory.
  - $\hat{p}_1 \sim N\left(p_1, \frac{1}{n_1} p_1 (1 - p_1) \right)$
  - $\hat{p}_2 \sim N\left(p_2, \frac{1}{n_2} p_2 (1 - p_2) \right)$

- Under, $H_0$, $p_1 = p_2 = p$, so
  - $\hat{p}_1 \sim N\left(p, \frac{1}{n_1} p (1 - p) \right)$
  - $\hat{p}_2 \sim N\left(p, \frac{1}{n_2} p (1 - p) \right)$

- Thus,
  $$
  \hat{p}_1 - \hat{p}_2 \sim N\left(0, \left(\frac{1}{n_1} + \frac{1}{n_2} \right)p(1 - p)\right)
  $$

- So we divide by the standard error to get a $z$ statistic that follows a standard normal distribution (but only if $H_0$ is true):
  $$
  \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\left( \frac{1}{n_1} + \frac{1}{n_2} \right)p(1 - p)}} \sim N(0,1)
  $$

- We estimate $p$ under $H_0$ by noting that, if $H_0$ were ture, then
  $$
  X_1 + X_2 \sim \text{Binom}(n_1 + n_2, p)
  $$
  Thus,
  $$
  \hat{p} = \frac{X_1 + X_2}{n_1 + n_2}
  $$

- Plugging in $\hat{p}$ for $p$ we get something that is asymptotically standard normal
  $$
  \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\left( \frac{1}{n_1} + \frac{1}{n_2} \right)\hat{p}(1 - \hat{p})}} \sim N(0,1)
  $$
  
- Note: don't use the $t$-distribution here. That depends on the data actually being normal.

- In practice, we need to do a continuity correction for better performance.

::: {.callout-important}
## Two-sample Binomial Test
Observe $X_1 \sim \mathrm{Binom}(n_1,p_1)$ and $X_2 \sim \mathrm{Binom}(n_2, p_2)$

- Null Hypothesis: $H_0: p_1 = p_2$

Calculate
  - $\hat{p_1} = X_1 / n_1$
  - $\hat{p_2} = X_2 / n_2$
  - $\hat{p} = (X_1 + X_2) / (n_1 + n_2)$

- $z$-statistic
  $$
  z = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\left( \frac{1}{n_1} + \frac{1}{n_2} \right)\hat{p}(1 - \hat{p})}}
  $$

-   $H_1: p_1 \neq p_2$
    $$
    \text{p-value} = 2 \times \texttt{pnorm}(-|z|)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-z", "z")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: p_1 > p_2$
    $$
    \text{p-value} = 1 - \texttt{pnorm}(z)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("z")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```
    
-   $H_1: p_1 < p_2$
    $$
    \text{p-value} = \texttt{pnorm}(z)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("z")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```
    
- A $(1 - \alpha)100\%$ confidence interval for the difference is
  $$
  \hat{p}_1 - \hat{p_2} \pm z_{1-\alpha/2}\sqrt{\left( \frac{1}{n_1} + \frac{1}{n_2} \right)\hat{p}(1 - \hat{p})}
  $$

- Rule of thumb: The normal approximation works if $n_1\hat{p}(1-\hat{p}) \geq 5$ and $n_2\hat{p}(1-\hat{p}) \geq 5$
:::

- Age at first birth vs cancer example continued:
  - $X_1 = 683$, $n_1 = 3220$
  - $X_2 = 1498$, $n_2 = 10245$
  - $\hat{p}_1 = 683/3220 = 0.2121$
  - $\hat{p}_2 = 1498 / 10245 = 0.1462$
  - $\hat{p} = (683 + 1498) / (3220 + 10245) = 0.162$ 
  - Test statistic:
    $$
    z = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\left( \frac{1}{n_1} + \frac{1}{n_2} \right)\hat{p}(1 - \hat{p})}} 
    = \frac{0.2121 - 0.1462}{\sqrt{\left( \frac{1}{3220} + \frac{1}{10245} \right)(0.162)(1 - 0.162)}}
    = 8.866
    $$
  - p-value:
    ```{r}
    2 * pnorm(-8.866)
    ```
  - We have strong evidence that women with cancer are more likely to have had their first child after age 30.

---

[2-sample binomial tests in R](./07_binom.qmd#two-sample-binomial-test)

---

# Test for Homogeneity in 2 $\times$ 2 Tables

- What I showed before was a $2 \times 2$ contingency table:

  | Status    | Age ≥ 30 | Age < 30 | Row Total |
  |-----------|----------|----------|-----------|
  | Case      | 683      | 2537     | 3220      |
  | Control   | 1498     | 8747     | 10245     |
  | Col Total | 2181     | 11284    | 13465     |

- Row margins and column margins are shown above. The total is the grand total.

- An equivalent (exact same $p$-value) method is to test for **homogeneity** or **independence** between the two variables.

- Suppose our table looks like this:

  |           | Z = A    | Z = B    | Row Total |
  |-----------|----------|----------|-----------|
  | W = C     | $X_{11}$ | $X_{12}$ | $m_1$     |
  | W = D     | $X_{21}$ | $X_{22}$ | $m_2$     |
  | Col Total | $n_1$    | $n_2$    | $N$       |

- Definitions:
  - $Z$ is a variable with possible values of $A$ and $B$
  - $W$ is a variable with possible values $C$ and $D$.
  - $n_1 = X_{11} + X_{21}$
  - $n_2 = X_{12} + X_{22}$
  - $m_1 = X_{11} + X_{12}$
  - $m_2 = X_{21} + X_{22}$
  - $N = n_1 + n_2 = m_1 + m_2$

- If $Z \perp W$, then:
  - $P(Z = A \cap W = C) = P(Z = A) \cdot P(W = C)$
  - $P(Z = A \cap W = D) = P(Z = A) \cdot P(W = D)$
  - $P(Z = b \cap W = C) = P(Z = b) \cdot P(W = C)$
  - $P(Z = b \cap W = D) = P(Z = b) \cdot P(W = D)$

- Idea: Calculate expected counts from the assumption of independendence based on the margin totals.
  - $P(Z = A) = \frac{n_1}{N}$
  - $P(Z = B) = \frac{n_2}{N}$
  - $P(W = C) = \frac{m_1}{N}$
  - $P(W = D) = \frac{m_2}{N}$

- The expected count under independence:
  - $E[X_{11} \mid Z \perp W] = N \Pr(Z = A \cap W = C) = N\Pr(Z = A)\Pr(W=C) =  N \cdot \frac{n_1}{N} \cdot \frac{m_1}{N} = \frac{n_1 m_1}{N}$
  - $E[X_{12} \mid Z \perp W] = N \Pr(Z = B \cap W = C) = N\Pr(Z = B)\Pr(W=C) =  N \cdot \frac{n_2}{N} \cdot \frac{m_1}{N} = \frac{n_2 m_1}{N}$
  - $E[X_{21} \mid Z \perp W] = N \Pr(Z = A \cap W = D) = N\Pr(Z = A)\Pr(W=D) =  N \cdot \frac{n_1}{N} \cdot \frac{m_2}{N} = \frac{n_1 m_2}{N}$
  - $E[X_{22} \mid Z \perp W] = N \Pr(Z = B \cap W = D) = N\Pr(Z = A)\Pr(W=C) =  N \cdot \frac{n_2}{N} \cdot \frac{m_2}{N} = \frac{n_2 m_2}{N}$


- Compare $X_{ij}$ to the expected value of $X_{ij}$ if variables were independent.

- Expected counts from age at first birth example:

  |          | ≥ 30                            | < 30                             |
  |----------|---------------------------------|----------------------------------|
  | Case     | $\frac{2181 \cdot 3220}{13465}$ | $\frac{11284 \cdot 3220}{13465}$ |
  | Control  | $\frac{2181 \cdot 10245}{13465}$| $\frac{11284 \cdot 10245}{13465}$|

- This results in:

  |          | ≥ 30   | < 30   |
  |----------|--------|--------|
  | Case     | 521.6  | 2698.4 |
  | Control  | 1659.4 | 8585.6 |

- How do these compare to observed counts?

- Whenever you compare observed counts to expected counts, do a Pearson $\chi^2$ test.

  $$
  X^2 = \sum_{\text{categories}} \frac{(O - E)^2}{E}
  $$

  - $O$ = observed count
  - $E$ = expected count

- If $H_0$ is true, then $X^2 \sim \chi^2_\nu$
  - $\nu$ = degrees of freedom

- For a test of homogeneity in $2 \times 2$ tables, $\nu = 1$

-   Birth example:

    $$
    X^2 = \frac{(683 - 521.6)^2}{521.6} + \frac{(2537 - 2698.4)^2}{2698.4} 
    + \frac{(1498 - 1659.4)^2}{1659.4} + \frac{(8747 - 8585.6)^2}{8585.6}
    = 77.89
    $$

    ```{r}
    #| echo: false
    tibble(x = seq(0, 80, length.out = 500)) |>
      mutate(y = dchisq(x = x, df = 1),
             up = x > 77.89) |>
      ggplot(aes(x = x, y = y, color = up)) +
      geom_line(linewidth = 1) +
      scale_color_manual(values = palette.colors(2)) +
      guides(color = "none") +
      xlab(expression(x^2)) +
      ylab("Density") +
      ggtitle(expression(chi[1]^2-Distribution)) +
      geom_vline(xintercept = 77.89, linetype = "dashed") +
      scale_x_continuous(breaks = 77.89, labels = "77.89")
    ```


-   In R:
    ```{r}
    1 - pchisq(77.89, df = 1)
    ```
    
-   It's more numerically accurate to use `lower.tail = FALSE` because R has better precision for numbers close to 0 than for numbers close to 1.
    ```{r}
    pchisq(77.89, df = 1, lower.tail = FALSE)
    ```

::: {.panel-tabset}
## Exercise
What are the expected counts of the following OC use vs. MI table?

| OC Use    | MI Yes | MI No | Row Total |
|-----------|--------|-------|-----------|
| Yes       | 13     | 4987  | 5000      |
| No        | 7      | 9993  | 10000     |
| Col Total | 20     | 14980 | 15000     |

## Hint

Use:  
$$
E = \frac{\text{row total} \cdot \text{column total}}{\text{grand total}}
$$  
Apply for each cell.

## Solution

| OC Use | MI Yes                         | MI No                             |
|--------|--------------------------------|-----------------------------------|
| Yes    | $\frac{20 \cdot 5000}{15000}$  | $\frac{14980 \cdot 5000}{15000}$  |
| No     | $\frac{20 \cdot 10000}{15000}$ | $\frac{14980 \cdot 10000}{15000}$ |


| OC Use | MI Yes | MI No   |
|--------|--------|---------|
| Yes    | 6.7    | 4993.3  |
| No     | 13.3   | 9986.7  |

:::


::: {.panel-tabset}
## Exercise
Calculate the $\chi^2$ statistic from the previous exercise.

## Solution:
Chi-squared test statistic:

$$
\chi^2 =
\frac{(13 - 6.7)^2}{6.7} +
\frac{(4987 - 4993.3)^2}{4993.3} +
\frac{(7 - 13.3)^2}{13.3} +
\frac{(9993 - 9986.7)^2}{9986.7} = 8.92
$$
:::

---

[Contingency test perspective in R](07_binom.qmd#contingency-table-perspective)

---

# Fisher's Exact Test

- What if $n$ is small?

- Rule of thumb for "small": Any **expected** count $< 5$
  - It's OK if the observed counts are $< 5$.

- Example: Salt diet vs. cardiovascular disease (CVD) death

  | Cause of Death | High Salt | Low Salt | Row Total |
  |----------------|-----------|----------|-----------|
  | Non-CVD        | 2         | 23       | 25        |
  | CVD            | 5         | 30       | 35        |
  | Col Total      | 7         | 53       | 60        |

- Expected count for top-left cell:
  $$
  E_{11} = \frac{7 \cdot 25}{60} = 2.92 < 5
  $$

  - Use $2.92$ (not $2$) to determine normality approximation validity.

::: {.callout-tip}
## Exact Test
An exact test is one that controls Type I error for **any** $n$, not just large $n$.
:::


::: {.callout-important}
## Fisher's Exact Test
1. Fix the margin totals.
2. Enumerate all possible tables with those same margin totals. 
3. Each table has a known probability (under $H_0$).
4. Find how likely our observed table is (if $H_0$ true) by summing over all tables less than or as probable as our observed table.
:::

- To find all tables that maintain margin totals, you can start from your observed table and perform one of these two operations until the operations become impossible (because doing so would create negative counts).

  $$
  \pmatrix{x_{11} - 1 & x_{12} + 1 \\ x_{21} + 1 & x_{22} - 1}
  $$
  or
  $$
  \pmatrix{x_{11} + 1 & x_{12} - 1 \\ x_{21} - 1 & x_{22} + 1}
  $$

- Our observe table is

  $$
  \pmatrix{2 & 23 \\ 5 & 30}
  $$
- All possible $2 \times 2$ tables with the exact same margins

  | Table                        | Probability Under Null     |
  |------------------------------|----------------------------|
  | $\pmatrix{0 & 25 \\ 7 & 28}$ | **0.017**                  |
  | $\pmatrix{1 & 24 \\ 6 & 29}$ | **0.105**                  |
  | $\pmatrix{2 & 23 \\ 5 & 30}$ | **0.252** (observed table) |
  | $\pmatrix{3 & 22 \\ 4 & 31}$ | 0.312                      |
  | $\pmatrix{4 & 21 \\ 3 & 32}$ | **0.214**                  |
  | $\pmatrix{5 & 20 \\ 2 & 33}$ | **0.082**                  |
  | $\pmatrix{6 & 19 \\ 1 & 34}$ | **0.016**                  |
  | $\pmatrix{7 & 18 \\ 0 & 35}$ | **0.001**                  |

- Sum all probabilities $\leq$ the observed one (in bold) to get the p-value.

- These probabilities come from the hypergeometric distribution (we won't cover the exact formula).



::: {.panel-tabset}
## Exercise
What are the possible tables with fixed margins from the following?

$$
\pmatrix{2 & 1 \\ 2 & 2}
$$

## Solution

- Valid tables with these margins:
  - $\pmatrix{3 & 0 \\ 1 & 3}$
  - $\pmatrix{2 & 1 \\ 2 & 2}$
  - $\pmatrix{1 & 2 \\ 3 & 1}$
  - $\pmatrix{0 & 3 \\ 4 & 0}$
:::

---

[Fisher Exact Test in R](07_binom.qmd#fishers-exact-test)

---

# McNemar's Test

- Study design: Have matched samples, where each unit has two binary variables.

- Example:  
  - Variable 1: Treatment A vs. Treatment B  
  - Variable 2: Survive vs. Not
  - Choose a woman to go into A.
  - **Match** another woman with similar characteristics to go into B.
  - Observe survival for both.

- "Similar characteristics": age, weight, clinical condition, etc.

- Naive Way: Treat each individual as an independent unit and put these in a 2 $\times$ 2 table:

  | Treatment | Survive | Die | Row Total |
  |-----------|---------|-----|-----------|
  | A         | 526     | 95  | 621       |
  | B         | 515     | 106 | 621       |
  | Col Total | 1041    | 201 | 1242      |

  - You could run a $\chi^2$ test for homogeneity, but this is **wrong** — the data are matched, not independent.

- Instead, use a **matched-pair contingency table**.
  - Above, each count is a person.
  - Instead, each count should be a pair.

- Correct way: Each matched pair is a unit

  |           | B Survive | B Die | Row Total |
  |-----------|-----------|-------|-----------|
  | A Survive | 510       | 16    | 526       |
  | A Die     | 5         | 90    | 95        |
  | Col Total | 515       | 106   | 621       |
  
::: {.callout-important}
## Note
Don’t use $\chi^2$ on this table because we know observations are associated (matched).  
We only want to know which treatment does better.
:::

::: {.callout-tip}
## Concordant pair
Same outcome (e.g., both survive or both die)
:::

::: {.callout-tip}
## Discordant pair
Different outcomes (e.g. one dies, one survives)
:::

- Concordant pairs tell you nothing about which treatment is better.

- Idea: Treatments are the same if they the discordant cells have approximately the same counts.

- Let:
  - $X$ = number of pairs where A survives and B dies
  - $n$ = number of discordant pairs

  $$
  X \sim \text{Binom}(n, p)
  $$

- Hypotheses:
  - $H_0$: $p = \frac{1}{2}$ (treatments A and B equally effective)
  - $H_1$: $p \ne \frac{1}{2}$

- Just use **binomial methods** on discordant pairs.
  - Use normal approximation for large counts
  - Use exact test for small counts

---

[McNemar's Test in R](07_mcnemar.qmd)

---

::: {.panel-tabset}
## Exercise
**Hypertension diagnosis**: Each person is assessed (i) by trained observer and (ii) by a machine. Each person is a unit and the assessments are matched. Test if they give the same result (on average).

|             | Trained + | Trained − |
|-------------|-----------|-----------|
| Machine +   | 3         | 7         |
| Machine −   | 1         | 9         |

## Hint
Only discordant pairs (7 and 1) matter.

## Solution

Let:
- $X \sim \text{Binom}(8, p)$ (discordant total = 8)
- $H_0: p = \frac{1}{2}$ vs. $H_1: p \ne \frac{1}{2}$

Run an exact binomial test in R:

```{r}
binom.test(x = 7, n = 8, p = 0.5)
```

The p-value is 0.07, so we only have weak evidence that the machines and technicians differ.

Cannot use `mcnemar.test()` here because it uses a normal approximation.
:::

- Summary: If you can group individuals into concordant vs. discordant pairs, then use McNemar’s test.

**Skip Power calculations §10.5**

# Larger Contingency Tables

- Instead of 2 binary variables, we now have 2 categorical variables.
  - Variable 1 has $R$ levels  
  - Variable 2 has $C$ levels

- **Example**: Cancer vs. Age at First Birth

| Age       | < 20 | 20–24 | 25–29 | 30–34 | ≥ 35 | Total |
|-----------|------|-------|-------|-------|------|--------|
| Case      | 326  | 1206  | 1011  | 463   | 220  | 3226   |
| Control   | 1422 | 4432  | 2893  | 1092  | 906  | 10245  |
| **Total** | 1748 | 5638  | 3904  | 1555  | 1126 | 13465  |

- $H_0$: Cancer status ⟂ Age at first birth  
- $H_1$: The two variables are **related**

- Use the same contingency table approach as for $2 \times 2$ tables:

1. Calculate expected counts assuming independence  
2. Compare to observed counts using $\chi^2$ statistic:

$$
\chi^2 = \sum_{\text{cells}} \frac{(O - E)^2}{E}
$$

3. $\chi^2 \sim \chi^2_\nu$ if $H_0$ is true

- Degrees of freedom:
$$
\nu = (R - 1)(C - 1)
$$

**XYZ IMAGE HERE**

- Example expected cell computations:

$$
E_{11} = \frac{1742 \cdot 3220}{13465} = 416.6
$$
- $P(\text{<20}) \cdot P(\text{case})$

$$
E_{12} = \frac{1742 \cdot 10245}{13465} = 1348.3
$$
- $P(\text{<20}) \cdot P(\text{control})$

::: {.panel-tabset}
## Exercise
Compute $E_{25}$:

$$
E_{25} = \frac{626 \cdot 10245}{13465} = 476.3
$$

## Solution
Plug into $\chi^2$ formula:

$$
\chi^2 = \sum_{\text{cell}} \frac{(O - E)^2}{E} 
= \frac{(416.6 - 326)^2}{416.6} + \cdots + \frac{(476.3 - 406)^2}{476.3} = 130.3
$$

Degrees of freedom:
$$
\nu = (2 - 1)(5 - 1) = 4
$$

R code:
```r
pchisq(130.3, df = 4, lower.tail = FALSE)
```
:::

---

Larger contingency tables in R

---

Skip: $\chi^2$ test for trend

# $\chi^2$ Goodness-of-Fit Test

- The $\chi^2$ tests of homogeneity are **special cases** of $\chi^2$ goodness-of-fit tests.

- Let $e$ be the expected counts under the null hypothesis.  
  Then:
  $$
  \chi^2 = \sum_{\text{categories}} \frac{(e - o)^2}{e}, \quad o = \text{observed count}
  $$

- Under $H_0$, $\chi^2 \sim \chi^2_\nu$

- Degrees of freedom:
  $$
  \nu = \text{\# parameters under } H_1 - \text{\# parameters under } H_0
  $$

- $H_1$ usually has more parameters (is more complex).

---

### Example: $2 \times 2$ Table

|     | C     | D     |
|-----|-------|-------|
| A   | $x_{11}$ | $x_{12}$ |
| B   | $x_{21}$ | $x_{22}$ |

- **$H_0$: Independence**

  - 2 parameters:
    - $P(A)$, $P(C)$
    - $P(B) = 1 - P(A)$
    - $P(D) = 1 - P(C)$

- **$H_1$: Association**

  - 3 parameters:
    - $P(A \cap C)$, $P(A \cap D)$, $P(B \cap C)$
    - $P(B \cap D) = 1 - \left[ P(A \cap C) + P(A \cap D) + P(B \cap C) \right]$

- So:
  $$
  \nu = 3 - 2 = 1
  $$

# Cohen's Kappa

- Want a measure of how **reliable** a test is
- Or measure how **similar** two judges rate something

- **Example**: 2 surveys measuring beef consumption

|              | Survey 2: ≤ 1 serving/week | Survey 2: > 1 serving/week | Total |
|--------------|----------------------------|-----------------------------|--------|
| Survey 1: ≤ 1 serving/week | 136                        | 92                          | 228    |
| Survey 1: > 1 serving/week | 64                         | 240                         | 304    |
| **Total**     | 205                        | 332                         | 537    |

---

- **Idea**: The “amount” of concordance reflects reliability of the survey

- Observed proportion concordant:
  $$
  p_o = \frac{136 + 240}{537} = 0.7
  $$

- Expected concordance under independence:
  $$
  p_e = \frac{205 \cdot 228}{537^2} + \frac{332 \cdot 304}{537^2} = 0.518
  $$

- **Cohen’s kappa**:
  $$
  \kappa = \frac{p_o - p_e}{1 - p_e}
  $$

## Properties of Cohen's Kappa

1. Bounds:
   $$
   \frac{-p_e}{1 - p_e} \leq \kappa \leq 1
   $$
   - Set $p_o = 0$ or $1$ for bounds.

2. $\kappa = 1$ implies **perfect concordance**.

3. **Rules of thumb**:
   - $\kappa > 0.75$ → excellent reproducibility
   - $0.4 < \kappa \leq 0.75$ → good reproducibility
   - $0 \leq \kappa \leq 0.4$ → marginal reproducibility

- Confidence intervals and tests for $\kappa$ are possible.

- Use $\kappa$ for **repeated measures** of the **same variable**.

- For **two different variables**, use **sensitivity** and **specificity**.

---

Cohen's Kappa in R

---

# Exercises 10.8--10.13
