[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Unless otherwise noted, all data are from Rosner (2015), downloaded from the book’s companion site.\nI did some cleaning (changing 9’s to NA’s, recoding numerics to informative characters, etc).\n\nbetacar\nHigh doses of beta-carotene in food have been linked to a reduced cancer risk in some observational studies. A study considered four beta-carotene capsule preparations: Solatene (30 mg), Roche (60 mg), and two from BASF (30 mg and 60 mg). To test their effectiveness in raising plasma-carotene levels, 23 volunteers were randomized to one of the four preparations, taking one pill every other day for 12 weeks. The primary endpoint was the plasma carotene level after prolonged ingestion.\n\nbetacar.csv\n\nPrepar: Preparation.\n\nPossible values: SOL, ROCHE, BASF-30, BASF-60.\n\nId: Subject #\nBase1lvl: 1st Baseline Level\nBase2lvl: 2nd Baseline Level\nWk6lvl: Week 6 Level\nWk8lvl: Week 8 Level\nWk10lvl: Week 10 Level\nWk12lvl: Week 12 Level\n\n\n\n\nbirthweight\nThe birthweights of 1000 consecutive infants born at Boston City Hospital, which serves a low-income population.\n\nbirthweight.csv\n\nid: ID\nweight: Birthweight (oz)\n\n\n\n\nblood\nData from a case-control study investigated various plasma risk factors for breast cancer. The women were matched approximately by age at the time of blood draw, fasting status, and, when possible, current postmenopausal hormone (PMH) use at the time of blood draw. Each matched set included one case and either one or two controls, although some sets are incomplete due to missing data. The matching variable is matchid.\n\nblood.csv\n\nId: ID\nmatchid: Matched ID\ncase: Case/control.\n\nPossible values: case, control.\n\ncurpmh: Current PMH use.\n\nPossible values: yes, no.\n\nageblood: Age at blood draw\nestradol: Estradiol\nestrone: Estrone\ntestost: Testosterone\nprolactn: Prolactin\n\n\n\n\nboneden\nA study in Australia examined the relationship between bone density and cigarette smoking in middle-aged female twins with different smoking histories. Forty-one pairs of twins visited a hospital in Victoria, Australia, where their bone density was measured. Participants also completed questionnaires providing information on their tobacco use, alcohol, coffee, and tea consumption, calcium intake from dairy products, menopausal and reproductive history, fracture history, use of oral contraceptives or estrogen replacement therapy, and physical activity levels. Tobacco consumption was measured in pack-years, with one pack-year defined as smoking one pack of cigarettes per day for one year.\n\nboneden.csv\n\nID: ID\nAge: Age (yrs)\nzyg: Twin type.\n\nPossible values: mz, dz\n\nTwin 1 Lighter Smoking Twin\n\nht1: Height (cm)\nwt1: Weight (kg)\ntea1: Tea (cups/week)\ncof1: Coffee (cups/week)\nalc1: Alcohol (drinks/week)\ncur1: Current Smoking (cigarettes/day)\nmen1: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr1: Pack-years smoking\nls1: Lumbar spine (g/cm\\(^2\\))\nfn1: Femoral neck (g/cm\\(^2\\))\nfs1: Femoral shaft (g/cm\\(^2\\))\n\nTwin 2 Heavier Smoking Twin\n\nht2: Height (cm)\nwt2: Weight (kg)\ntea2: Tea (cups/week)\ncof2: Coffee (cups/week)\nalc2: Alcohol (drinks/week)\ncur2: Current Smoking (cigarettes/day)\nmen2: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr2: Pack-years smoking\nls2: Lumbar spine (g/cm\\(^2\\))\nfn2: Femoral neck (g/cm\\(^2\\))\nfs2: Femoral shaft (g/cm\\(^2\\))\n\n\n\n\n\nbotox\nA study on patients with piriformis syndrome compared the effects of three types of injections: triamcinolone with lidocaine (TL), a placebo (Placebo), and Botox (Botox). The patients were randomly assigned to these groups in a 3:1:2 ratio and received injections directly into the piriformis muscle. They were evaluated at 2 weeks, 1 month, and monthly up to 17 months, though many missed visits. Improvement in pain was measured on a scale from 0% to 100% (higher means more improved). The study involved 69 patients, with one having the condition in both legs. The goal was to compare the efficacy between the groups, considering age, gender, and affected side as potential influencing factors.\n\nbotox.csv\n\nID: ID\ngroup:\n\nPossible values: TL, Placebo, Botox\n\nside: left (L), middle (M), or right (R).\ngender: male or female\nage: in years\npain0: pain score month 0\npain05: pain score month 0.5\npain1: pain score month 1\npain2: pain score month 2\npain3: pain score month 3\npain4: pain score month 4\npain5: pain score month 5\npain6: pain score month 6\npain7: pain score month 7\npain8: pain score month 8\npain9: pain score month 9\npain10: pain score month 10\npain11: pain score month 11\npain12: pain score month 12\npain13: pain score month 13\npain14: pain score month 14\npain15: pain score month 15\npain16: pain score month 16\npain17: pain score month 17\n\n\n\n\nbreast\nThe data set includes 1200 postmenopausal women from the NHS, free of cancer in 1990. Of these, 200 were using postmenopausal hormones (PMH) in 1990, and 1000 had never used them. The study aimed to link PMH use in 1990 to breast cancer incidence from 1990 to 2000. Fifty-three women developed breast cancer during this period. PMH use was categorized by current use and duration of use in 1990, with separate durations for estrogen and estrogen plus progesterone. Each woman has a return date for the 1990 questionnaire and a follow-up date, which is either the date of breast cancer diagnosis or the date of the last questionnaire by 2000. The file also includes data on other breast cancer risk factors as of 1990.\n\nbreast.csv\n\nId: ID\ncase: Whether a woman had breast cancer or not.\n\nPossible values: case,control\n\nage: age\nagemenar: age at menarche\nagemenop: age at menopause\nafb: age at first birth\nparity: parity\nbbd: Benign Breast disease.\n\nPossible values: yes, no\n\nfamhx: family history breast cancer.\n\nPossible values: yes, no\n\nbmi: BMI (kg/m**2)\nhgt: Height (inches)\nalcohol: Alcohol use (grams/day)\npmh: PMH status.\n\nPossible values: never user, current user\n\ndur3: Duration of Estrogen use (months)\ndur4: Duration of Estrogen + progesterone use (months)\ncsmk: Current Smoker.\n\nPossible values: yes, no\n\npsmk: Past smoker.\n\nPossible values: yes, no\n\nfoluptm: Months of follow up. Note: Some subjects provided no follow up after the 1990 questionnaire and foluptm = 0 for these people\n\n\n\n\ncholesterol\nCholesterol levels from 24 hospital employees who switched from a standard American diet to a vegetarian diet for one month. Their serum cholesterol was measured before and after the diet change.\n\ncholesterol.csv\n\nSubject: ID\nBefore: Serum-cholesterol levels before diet change (mg/dL)\nAfter: Serum-cholesterol levels after diet change (mg/dL)\nDifference: Difference in serum-cholesterol levels, Before - After, where positive numbers indicate a reduction in serum-cholesterol levels.\n\n\n\n\ncorneal\nFluoroquinolones, antibiotics for bacterial infections, are FDA-approved for systemic use. However, post-approval studies indicate a risk of peripheral neuropathy, leading to updated safety labeling.\nA small clinical trial tested the safety and effectiveness of two fluoroquinolone eye drops (drugs M and G) and a placebo (drug P) for bacterial eye infections. Ninety-three subjects were randomly assigned to three groups, each receiving one active drug and a placebo in opposite eyes. Participants used the drops four times daily for 10 days. The primary outcome was corneal sensitivity, measured in millimeters, with higher values indicating more normal sensitivity.\nCorneal sensitivity was assessed at baseline, 7 days, and 14 days, with measurements taken from the central cornea and four quadrants (superior, inferior, temporal, nasal).\n\ncorneal.csv\n\nid: ID\ntr: Treatment.\n\nPossible values: M, G, P\n\nc1: Central visit 1\ns1: Superior visit 1\ni1: Inferior Visit 1\nt1: Temporal visit 1\nn1: Nasal Visit 1\nc2: Central Visit 2(day 7)\ns2: Superior Visit 2\ni2: Inferior Visit 2\nt2: Temporal Visit 2\nn2: Nasal Visit 2\nc3: Central Visit 3(day 14)\ns3: Superior Visit 3\ni3: Inferior Visit 3\nt3: Temporal Visit 3\nn3: Nasal Visit 3\n\n\n\n\ndiabetes\nType I diabetes is common in children and requires regular insulin shots to prevent long-term complications like neurologic, vision, kidney issues, heart disease, and premature death.\nThe impact of diabetes control on childhood growth is less clear. To study this, adolescent boys aged 9−15 were examined about every 3 months. Each exam measured diabetes control using glycosylated hemoglobin (HgbA1c), where higher HgbA1c indicates poorer control (normal &lt;7.0). Age, height, and weight were also recorded. Data includes 94 boys over 910 visits.\nThe main question is the overall relationship between glycemic control and growth, primarily weight, for the entire group, not individual cases.\n\ndiabetes.csv\n\nID: ID\nmon_a1c: Month A1c\nday_a1c: Day A1c\nyr_a1c: Yr A1c\nage_yrs: Age in years\ngly_a1c: Hemoglobin A1c\nht_cm: Height in cm\nwt_kg: Weight in kg\n\n\n\n\near\nThis dataset includes 203 children with acute otitis media (OME) from a randomized clinical trial. Each child had OME in one or both ears and received a 14-day course of either cefaclor (CEF) or amoxicillin (AMO). Middle-ear status was assessed after the 14-day treatment.\n\near.csv\n\nId: ID\nClear: Clearance by 14 days,\n\nPossible values: yes, no\n\nAntibo: Antibiotic.\n\nPossible values: CEF, AMO\n\nAge: Age. Possible values: &lt;2 yrs, 2-5 yrs, 6+ yrs.\nEar: Ear,\n1 = 1st ear\n2 = 2nd ear\n\n\n\n\neff, nephro, and oto\nAminoglycoside antibiotics are crucial for treating severe gram-negative bacterial infections in hospitalized patients. Despite their toxicity and the development of new antibiotics, aminoglycosides remain widely used. Choosing the right aminoglycoside depends on the clinical situation, antimicrobial spectrum, cost, and side effects like nephrotoxicity and auditory toxicity. Numerous trials comparing these antibiotics have varied in design and conclusions, often lacking sufficient sample sizes to detect small differences.\nTo better understand their true effects, a meta-analysis of all randomized trials was conducted. This analysis included 45 trials from 1975 to 1985, comparing amikacin, gentamicin, netilmicin, sisomicin, and tobramycin. Data from 37 trials were suitable for comparison, focusing on efficacy, nephrotoxicity, and auditory toxicity. Efficacy was defined by bacterial or clinical response as reported by each trial, nephrotoxicity by the percentage of kidney-related toxic events reported, and auditory toxicity by the diffence in pre- and post-treatment audiograms. The data is organized into three sets: eff.csv, nephro.csv, and oto.csv, with separate records for each antibiotic and endpoint.\n\neff.csv\n\nName: Study name\nId: Study Number\nEndpnt: Endpoint, 1 = efficacy\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nCured: Number Cured\n\nnephro.csv\n\nname: Study name\nid: Study number\nEndpnt: Endpoint 2 = nephrotoxicity\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin\n\nSamp_sz: Sample size\nSide_eff: Number with side effects\n\noto.csv\n\nName: Study Name\nId: Study Number\nEndpnt: Endpoint.\n\nPossible values: efficacy, nephrotoxicity, ototoxicity.\n\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nSide_eff: Number with side effect\n\n\n\n\nendocrin\nThe data set contains split-sample plasma measurements of four hormones for each of five subjects, all from one laboratory.\n\nendocrin.csv\n\nSubject: Subject number\nReplicat: Replicate number\nEstrone: Estrone\nEstradol: Estradiol\nAndroste: Androstenedione\nTestost: Testosterone\n\n\n\n\nestradl\nObesity is common in American society and a risk factor for breast cancer in postmenopausal women, possibly due to increased estrogen levels, particularly serum estradiol. Researchers studied 151 African American and 60 Caucasian premenopausal women, measuring adiposity using body mass index (BMI) (a measure of overall adioposity) and waist-hip ratio (WHR) (a measure of abdominal adioposity). They also obtained a complete hormonal profile, including serum estradiol, and assessed other breast cancer risk factors: ethnicity, age, number of children, age at first birth, presence of children, and age at menarche.\n\nestradl.csv\n\nId: Identification number\nEstradl: Estradiol\nEthnic: Ethnicity.\n\nPossible values: African-American, Caucasian\n\nEntage: Age\nNumchild: Parity, number of children\nAgefbo: Age at 1st birth (=0 if numchild=0)\nAnykids: any children.\n\nPossible values: yes, no.\n\nAgemenar: age at menarche\nBMI: Body Mass Index\nWHR: waist-hip ratio\n\n\n\n\nestrogen\nThree distinct two-period crossover studies were conducted with different subject groups, measuring systolic and diastolic blood pressure. In Study 1, 0.625 mg of estrogen was compared with a placebo. Study 2 compared 1.25 mg of estrogen with a placebo. Study 3 compared 1.25 mg of estrogen with 0.625 mg of estrogen. Each active treatment period lasted for four weeks, with a two-week washout period between them.\n\nestrogen.csv\n\nId: ID\nstd_typ: Study type.\n\nPossible values: 0.625MG VS PLACEBO, 1.25MG VS PLACEBO, 1.25MG VS 0.625MG\n\nperiod: Period\ntrtgrp: Treatment.\n\nPossible values: PLACEBO, 0.625MG, 1.25MG\n\nsysd1r1: Systolic blood pressure day 1 reading 1\ndiasd1r1: Diastolic blood pressure day 1 reading 1\nsysd1r2: Systolic blood pressure day 1 reading 2\ndiasd1r2: Diastolic blood pressure day 1 reading 2\nsysd1r3: SYSTOLIC BP DAY 1 reading 3\ndiasd1r3: Diastolic blood pressure day 1 reading 3\nsysd2r1: Systolic blood pressure day 2 reading 1\ndiasd2r1: Diastolic blood pressure day 2 reading 1\nsysd2r2: Systolic blood pressure day 2 reading 2\ndiasd2r2: Diastolic blood pressure day 2 reading 2\nsysd2r3: Systolic blood pressure day 2 reading 3\ndiasd2r3: Diastolic blood pressure day 2 reading 3\nsysd3r1: Systolic blood pressure day 3 reading 1\ndiasd3r1: Diastolic blood pressure day 3 reading 1\nsysd3r2: Systolic blood pressure day 3 reading 2\ndiasd3r2: Diastolic blood pressure day 3 reading 2\nsysd3r3: Systolic blood pressure day 3 reading 3\ndiasd3r3: Diastolic blood pressure day 3 reading 3\n\n\n\n\nfev\nForced expiratory volume (FEV) is a measure of pulmonary function that quantifies the volume of air expelled in one second of sustained effort. This dataset includes FEV measurements from 1980 for 654 children aged 3 to 19 who participated in the Childhood Respiratory Disease (CRD) Study in East Boston, Massachusetts. The data are part of a longitudinal study aimed at tracking changes in pulmonary function over time in children.\n\nfev.csv\n\nId: ID number\nAge: Age (yrs)\nFEV: FEV (liters)\nHgt: Height (inches)\nSex: Sex.\n\nPossible values: female, male\n\nSmoke: Smoking Status.\n\nPossible values: non-current, current smoker.\n\n\n\n\n\nfield\nRetinitis pigmentosa (RP) is a hereditary eye disease that can lead to substantial vision loss or blindness. It has dominant, recessive, and sex-linked forms, with mutations in the rhodopsin (RHO) gene linked to dominant cases and RPGR gene mutations linked to sex-linked cases.\nThe data file field.csv contains visual field data for approximately 100 patients each from the RHO and RPGR groups. Visual field, measured in degrees\\(^2\\), indicates the area of vision. The dataset includes longitudinal data with varying follow-up times from a minimum of 3 years to a maximum of about 25-30 years. Measurements are provided separately for the right eye (OD) and the left eye (OS).\n\nfield.csv\n\nid: ID\ngroup: group.\n\nPossible values: RHO, RPGR\n\nage: age at visit (years)\ngender: gender. Note: all RPGR individuals have to be male.\n\nPossible values: male, female.\n\ndtvisit: date of visit (YYYY-MM-DD)\nfolowup: time from 1st visit in years\ntotfldod: total field area right eye (OD) in degrees\ntotfldos: total field area left eye (OS) in degrees\n\n\n\n\nheart\n\nheart.csv\n\nDiagnosis: Possible values:\n\nY1 = normal\nY2 = atrial septal defect without pulmonary stenosis or pulmonary hypertension\nY3 = ventricular septal defect with valvular pulmonary stenosis\nY4 = isolated pulmonary hypertension\nY5 = transposed great vessels\nY6 = ventricular septal defect without pulmonary hypertension\nY7 = ventricular septal defect with pulmonary hypertension\n\nPrevalence: Prevalence\nX1: age 1-20 years old\nX2: age&gt;20 years old\nX3: mild cyanosis\nX4: easy fatigue\nX5: chest pain\nX6: repeated respiratory infections\nX7: EKG axis more than 110\n\n\n\n\nhormone\nAn experiment was conducted to study the effects of avian pancreatic polypeptide (aPP), cholecystokinin (CCK), vasoactive intestinal peptide (VIP), and secretin on pancreatic and biliary secretions in laying hens. Researchers aimed to determine how these hormones affect the flow rates and pH values of these secretions.\nWhite Leghorn hens, aged 14-29 weeks, were surgically fitted with cannulas for collecting biliary and pancreatic secretions, and a jugular cannula for hormone infusion. Each hen underwent one trial per day, as long as her cannulas remained functional, leading to varying trial numbers per hen.\nEach trial started with a 20-minute saline infusion, followed by the collection of pancreatic and biliary secretions. The flow rates (in microliters per minute) and pH values were measured. This was followed by a 40-minute hormone infusion, with measurements repeated afterward.\nThe data set “hormone.csv” includes data for the four hormones and saline, with each trial recorded as one entry and 11 associated variables.\n\nhormone.csv\n\nID: ID of hen\nBilsecpr: Biliary secretion-pre (microliters per minute)\nBilphpr: Biliary pH-pre\nPansecpr: Pancreatic secretion-pre (microliters per minute)\nPanphpr: Pancreatic pH-pre\nDose: Dose of hormone\nBilsecpt: Biliary secretion-post (microliters per minute)\nBilphpt: Biliary pH-post\nPansecpt: Pancreatic secretion-post (microliters per minute)\nPanphpt: Pancreatic pH-post\nHormone: Hormone.\n\nPossible values: SAL, APP, CCK, SEC, VIP.\n\n\n\n\n\nhospital\nThese data are part of a larger data set gathered from individuals discharged from a specific Pennsylvania hospital. It was collected as part of a retrospective chart review focusing on antibiotic usage in hospitals.\n\nhospital.csv\n\nId: id no.\nDur_stay: Duration of hospital stay\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nTemp: First temperature following admission\nWBC: First WBC(x1000) following admission\nAntibio: Received antibiotic.\n\nPossible values: yes, no\n\nBact_cul: Received bacterial culture.\n\nPossible values: yes, no\n\nService: Service.\n\nPossible values: med, surg\n\n\n\n\n\ninfantbp\nResearchers investigated the link between high blood pressure and sodium intake by measuring infants’ responses to salt and sugar solutions. They measured the vigor of infants’ sucking (mean sucks per burst, MSB) when exposed to different solutions: water, 0.1 molar salt, 0.3 molar salt, and sugar. The responses were recorded over a series of periods using different stimuli: (i) nonnutritive sucking (ii) water, (iii) 5% sucrose + water, (iv) 15% sucrose + water, and (v) nonnutritive sucking.\n\ninfantbp.csv\n\nID: Infant ID\nMn_sbp: Mean Systolic Blood Pressure\nMn_dbp: Mean Diastolic Blood Pressure\nSalt Taste Variables\n\nMSB1slt: MSB-trial 1 water\nMSB2slt: MSB-trial 2 water\nMSB3slt: MSB-trial 3 0.1 molar salt + water\nMSB4slt: MSB-trial 4 0.1 molar salt + water\nMSB5slt: MSB-trial 5 water\nMSB6slt: MSB-trial 6 water\nMSB7slt: MSB-trial 7 0.3 molar salt + water\nMSB8slt: MSB-trial 8 0.3 molar salt + water\nMSB9slt: MSB-trial 9 water\nMSB10slt: MSB-trial 10 water\n\nSugar Taste Variables\n\nMSB1sug: MSB-trial 1 non-nutritive sucking\nMSB2sug: MSB-trial 2 water\nMSB3sug: MSB-trial 3 5% sucrose + water\nMSB4sug: MSB-trial 4 15% sucrose + water\nMSB5sug: MSB-trial 5 non-nutritive sucking\n\nNOTE: For MSB, 0 indicates the baby did not suck.\n\n\n\n\nlead\nA study examined the psychological and neurological effects of lead exposure on children near a lead smelter in El Paso, Texas. Blood lead levels were measured, categorizing 46 children with levels ≥ 40 μg/mL as the exposed group. Another 78 children with levels &lt; 40 μg/mL functioned as the control group. Key outcomes included finger–wrist taps (neurological function) and Wechsler full-scale IQ scores.\nOther behavioral effects of lead include hyperactivity. In this study, parents also rated their children’s hyperactivity on a scale from 0 (normal) to 3 (very hyperactive). Hyperactivity measures are available for 49 control children and 35 exposed children.\n\nlead.csv\n\nPatient information\n\nid: Identification number\nageyrs: Age in years\nsex: Sex.\n\nPossible values: male, female\n\n\nLead data\n\narea: Distance of esidence from smelter on august 1972. Possible values:\n\n0-1 Miles from smelter\n1-2.5 Miles\n2.5-4.1 Miles\n\nlead_grp: Blood lead level group. possible values:\n\ncontrol = Blood lead levels below 40 micrograms/100ml in both 1972 & 1973 (control group),\ncurrent exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in both 72 & 73 or a level greater than or equal to 40 in 73 alone (3 cases only) (currently exposed group),\nprevious exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in 72 and less than 40 in 73 (previously exposed group)\n\nGroup: group.\n\nPossible values: control, exposed\n\nld72: Blood lead values (micrograms/100ml) in 72\nld73: Blood lead values (micrograms/100ml) in 73\nfst2yrs: Did child live for 1st 2 years within 1 mile of smelter.\n\nPossible values: yes, no.\n\ntotyrs: Total number of years spent within 4.1 miles of smelter\n\nIQ Test Results\n\niqv_inf: INF - information subtest in WISC and WPPSI\niqv_comp: COMP - comprehension subtest in WISC and WPPSI\niqv_ar: AR - arithmetic subtest in WISC and WPPSI\niqv_ds: DS - digit span subtest(WISC) and sentence completion(WPPSI)\niqv_raw: V/RAW - raw score/verbal IQ\niqp_pc: PC - picture completion subtest in WISC and WPPSI\niqp_bd: BD - block design subtest in WISC and WPPSI\niqp_oa: OA - object assembly subtest(WISC), animal house subtest(WPPSI)\niqp_cod: COD - coding subtest(WISC), geometric design subtest(WPPSI)\niqp_raw: P/RAW - raw score/performance IQ (total of scores PC, BD, OA, & COD)\nhh_index: HH/INDEX - Hollingshead index of social status\niqv: IQV - verbal IQ\niqp: IQP - performance IQ\niqf: IQF - full scale IQ (not sum or average of IQV D IQP)\niq_type: Type of IQ test (WISC usually given to children \\(\\geq\\) 5 years and 1 month of age WPPSI usually given to children \\(\\leq\\) 5 years of age).\n\nPossible values: WISC, WPPSI\n\n\nSymptom data (as reported by parents)\n\npica: Pica.\n\nPossible values: yes, no.\n\ncolic: Colic.\n\nPossible values: yes, no.\n\nclumsi: Clumsiness.\n\nPossible values: yes, no.\n\nirrit: Irritability.\n\nPossible values: yes, no.\n\nconvul: Convulsions.\n\nPossible values: yes, no.\n\n\nNeurological test data\n\n_2plat_r: Number of taps for right hand in the 2-plate tapping test (number of taps in one 10 second trial)\n_2plat_l: Number OF taps for left hand in the 2-plate tapping test (number taps in one 10 second trial)\nvisrea_r: Visual reaction time right hand (milliseconds)\nvisrea_l: Visual reation time left hand (milliseconds)\naudrea_r: Auditory reaction time right hand (milliseconds)\naudrea_l: Auditory reaction time left hand (milliseconds)\nfwt_r: Finger-wrist tapping test right hand (number of taps in one 10 second trial)\nfwt_l: Finger-wrist tapping test left hand (#taps in one 10 second trial)\nhyperact: WWPS - Werry-Weiss-Peters scale for hyperactivity\n\n0 = no activity, \\(\\ldots\\), 4 = severly hyperactive (as reported by parents)\n\nmaxfwt: Finger-wrist tapping test in dominant hand (max of fwt_r,fwt_l)\n\n\n\n\n\nlvm\nThe Left Ventricular Mass Index (LVMI) measures the enlargement of the heart’s left side, expressed in gm/ht(m)^2.7. High LVMI values can predict future cardiovascular disease in children. A study investigated the relationship between LVMI levels and blood pressure categories in children and adolescents aged 10-18. Blood pressure was categorized as Normal (bpcat = normal, bp percentile &lt; 80%), Pre-Hypertensive (bpcat = pre-hypertensive, bp percentile ≥ 80% and &lt; 90%), or Hypertensive (bpcat = hypertensive, bp percentile ≥ 90%)..\n\nlvm.csv\n\nID: ID\nlvmht27: Left ventricular mass – height corrected = Left Ventricular Mass/Height(m)\\(^{2.7}\\), \\(g/m^{2.7}\\)\nbpcat: Blood pressure category.\n\nPossible values: normal, pre-hypertensive, hypertensive.\n\ngender: Bender.\n\nPossible values: male, female\n\nage: in years\nBMI: kg/m\\(^2\\)\n\n\n\n\nmice\nRetinitis pigmentosa (RP) is a hereditary eye condition causing night blindness and visual field loss, typically between ages 10 and 40. Some patients become legally blind by 30, while others retain central vision past 60. A specific gene linked RP has been identified whose transmision is autosomal dominant. The disease progression is tracked using electroretinogram (ERG), measuring retinal electrical activity, which decreases as RP advances, affecting routine activities like driving and walking at night.\nTo test if sunlight exposure harms RP patients, researchers introduced the RP gene into mice, creating “RP mice.” These mice were divided into light, dim, and dark lighting conditions from birth. A control group of normal mice was also subjected to similar conditions. ERG amplitudes (BAMP and AAMP) were measured at 15, 20, and 35 days of life for RP mice, and only BAMP was measured for normal mice.\n\nmice.csv\n\nId: ID\nGroup: GROUP.\n\nPossible values: RP, NORMAL\n\nTrtgrp: TREATMENT GROUP.\n\nPossible values: LIGHT, DIM, DARK.\n\nAge: AGE (days)\nB_amp: B AMP\nA_amp: A AMP\n\n\n\n\nnifed\nA clinical trial tested the effectiveness of nifedipine in reducing chest pain in hospitalized angina patients. The study lasted 14 days unless patients were withdrawn, discharged, or died. Patients were randomly assigned to receive either nifedipine or propranolol, starting at a standard dosage. If pain persisted or recurred, the dosage was increased in pre-specified steps. Patients in both groups could also receive nitrates as needed to control pain. The primary goal was to compare pain relief between nifedipine and propranolol, with a secondary goal of examining their effects on heart rate and blood pressure.\n\nnifed.csv\n\nId: ID\ntrtgrp: Treatment group,\n\nN = nifedipine\nP = placebo\n\nbashrtrt: Baseline heart rate immediately prior to randomization (beats/min)\nlv1hrtrt: Level 1 heart rate, Highest heart rate and systolic blood pressure at baseline and each level of therapy respectively (beats/min)\nlv2hrtrt: Level 2 heart rate (beats/min)\nlv3hrtrt: Level 3 heart rate (beats/min)\nbassys: Baseline systolic bp immediately prior to randomization (mm Hg)\nlv1sys: Level 1 systolic bp (mm Hg)\nlv2sys: Level 2 systolic bp (mm Hg)\nlv3sys: Level 3 systolic bp (mm Hg)\n\n\nMissing values indicate that either (a) the patient withdrew from the study prior to entering this level of therapy (b) the patient achieved pain relief prior to reaching this level or therapy, (c) the patient encountered this level of therapy, but this particular piece of data was missing.\n\n\npiriform\nA study evaluated the FAIR test (hip flexion, adduction, and internal rotation) for diagnosing piriformis syndrome (PS), which affects the piriformis muscle in the buttock, causing lumbar and sciatic pain. The test measures nerve-conduction velocity differences between an aggravating and a neutral posture, with higher scores indicating a greater likelihood of PS. Data from 142 participants without PS and 489 with PS (diagnosed clinically) are available. A FAIR test score of ≥ 1.86 ms is proposed to define a positive result. The FAIR test value, MAXCHG, is recorded in milliseconds.\n\npiriform.csv\n\nID: ID\npiriform: Piriformis Syndrome.\n\nPossible values: Negative, Positive\n\nsex: Sex.\n\nPossible values: male, female\n\nage: Age\nmaxchg: Max change between tibia and peroneal\n\n\n\n\nsexrat\nIt is often assumed that the gender distribution of consecutive children is independent. To test this hypothesis, birth records from the first five children in 51,868 families were analyzed. These data contain the frequency of how many times a pattern of child sexes showed up. E.g., MM (only two males) showed up 4400 times.\n\nsexrat.csv\n\nnm_chld: Number of children. For families with 5+ children, the sex of the first 5 children are listed. The number of children is given as 5 for such families.\nsx_1: Sex of 1st born\nsx_2: Sex of 2nd born\nsx_3: Sex of 3rd born\nsx_4: sex of 4th born\nsx_5: sex of 5th born\nsexchldn: Sex of all children. The sex of successive births is given. Thus, MMMF means that the first three children were males and the fourth child was a female. There were 484 such families.\nnum_fam: Number of families. Number of families with specific gender contribution of children\n\n\n\n\nsmoke\nA study was conducted among 234 individuals who wanted to quit smoking but had not yet done so. On the day they quit, their carbon monoxide (CO) levels were measured, and the time since their last cigarette was recorded. This CO level serves as an indicator of the number of cigarettes smoked daily before quitting but is influenced by the time since the last cigarette. Therefore, a “corrected CO level” was provided, adjusted for this time. Participant age, sex, and self-reported daily cigarette consumption were also recorded. The participants were followed for a year to determine the number of days they remained abstinent, ranging from 0 to 365 days.\n\nsmoke.csv\n\nID: ID number\nAge: age\nGender: Gender.\n\nPossible values: male, female\n\nCig_day: Cigarettes/day\nCO: Carbon monoxide (CO) (X 10)\nMin_last: Minutes elapsed since last cigarette\nLogCOadj: Log CO Adj * (X 1000). This variable represents adjusted carbon monoxide (CO) values. CO values were adjusted for minutes elapsed since last cigarette smoked using the formula Log 10 CO (Adjusted) = Log 10 CO - (-0.000638) X (Min - 80), where Min is the number of minutes elapsed since the last cigarette smoked.\nDay_abs: Days abstinent Those abstinent less than 1 day were given a value of zero.\n\n\n\n\nswiss\nThe Swiss Analgesic Study aimed to evaluate the impact of phenacetin-containing analgesics on kidney function and health. It involved 624 women from Basel, Switzerland, who had high phenacetin intake (study group) and 626 women with low or no phenacetin intake (control group). The study used urine N-acetyl-P-aminophenyl (NAPAP) levels to measure recent phenacetin use, dividing the study group into high-NAPAP and low-NAPAP subgroups. Both subgroups had higher NAPAP levels than the control group. The women were examined in 1967-1968 and again in 1969, 1970, 1971, 1972, 1975, and 1978, with kidney function assessed through various laboratory tests, including serum-creatinine levels.\n\nswiss.csv\n\nID: ID\nage: age (yrs)\ngroup: Group.\n\nPossible values: High NAPAP, Low NAPAP, control\n\ncreat_68: Serum Creatinine 1968 (mg/dL)\ncreat_69: Serum Creatinine 1969 (mg/dL)\ncreat_70: Serum Creatinine 1970 (mg/dL)\ncreat_71: Serum Creatinine 1971 (mg/dL)\ncreat_72: Serum Creatinine 1972 (mg/dL)\ncreat_75: Serum Creatinine 1975 (mg/dL)\ncreat_78: Serum Creatinine 1978 (mg/dL)\n\n\n\n\ntear\nA pilot study was conducted to evaluate an eye drop’s effectiveness in preventing dry eye, measured by tear breakup time (TBUT). Fourteen participants tested three protocols:\n\nProtocol A: No blinking for 3 seconds before placebo instillation.\nProtocol B: No blinking for 6 seconds before placebo instillation (standard protocol).\nProtocol C: No blinking for 10 seconds before placebo instillation.\n\nTBUT was recorded at baseline, immediately after, and at 5, 10, and 15 minutes post-instillation in a low-humidity environment. Each protocol was tested on the same participants, measuring both eyes with two replicates.\n\ntear.csv\n\nID: ID\nod3bas1: OD 3sec baseline 1\nod3bas2: OD 3 sec baseline 2\nod3im1: OD 3 sec immediately post 1\nod3im2: OD 3 sec immediately post 2\nod3pst51: OD 3 sec 5min post 1\nod3pst52: OD 3 sec 5min post 2\nod3pt101: OD 3 sec 10min post 1\nod3pt102: OD 3 sec 10min post 2\nod3pt151: OD 3 sec 15min post 1\nod3pt152: OD 3 sec 15min post 2\nos3bas1: OS 3sec baseline 1\nos3bas2: OS 3 sec baseline 2\nos3im1: OS 3 sec immediately post 1\nos3im2: OS 3 sec immediately post 2\nos3pst51: OS 3 sec 5min post 1\nos3pst52: OS 3 sec 5min post 2\nos3pt101: OS 3 sec 10min post 1\nos3pt102: OS 3 sec 10min post 2\nos3pt151: OS 3 sec 15min post 1\nos3pt152: OS 3 sec 15min post 2\nod6bas1: OD 6 sec baseline 1\nod6bas2: OD 6 sec baseline 2\nod6im1: OD 6 sec immediately post 1\nod6im2: OD 6 sec immediately post 2\nod6pst51: OD 6 sec 5min post 1\nod6pst52: OD 6 sec 5min post 2\nod6pt101: OD 6 sec 10min post 1\nod6pt102: OD 6 sec 10min post 2\nod6pt151: OD 6 sec 15min post 1\nod6pt152: OD 6 sec 15min post 2\nos6bas1: OS 6 sec baseline 1\nos6bas2: OS 6 sec baseline 2\nos6im1: OS 6 sec immediately post 1\nos6im2: OS 6 sec immediately post 2\nos6pst51: OS 6 sec 5min post 1\nos6pst52: OS 6 sec 5min post 2\nos6pt101: OS 6 sec 10min post 1\nos6pt102: OS 6 sec 10min post 2\nos6pt151: OS 6 sec 15min post 1\nos6pt152: OS 6 sec 15min post 2\nod10bas1: OD 10 sec baseline 1\nod10bas2: OD 10 sec baseline 2\nod10im1: OD 10 sec immediately post 1\nod10im2: OD 10 sec immediately post 2\nod10ps51: OD 10 sec 5min post 1\nod10ps52: OD 10 sec 5min post 2\nod10p101: OD 10 sec 10min post 1\nod10p102: OD 10 sec 10min post 2\nod10p151: OD 10 sec 15min post 1\nod10p152: OD 10 sec 15min post 2\nos10bas1: OS 10 sec baseline 1\nos10bas2: OS 10 sec baseline 2\nos10im1: OS 10 sec immediately post 1\nos10im2: OS 10 sec immediately post 2\nos10ps51: OS 10 sec 5min post 1\nos10ps52: OS 10 sec 5min post 2\nos10p101: OS 10 sec 10min post 1\nos10p102: OS 10 sec 10min post 2\nos10p151: OS 10 sec 15min post 1\nos10p152: OS 10 sec 15min post 2\n\n\n\n\ntemperat\nA student records temperatures at 20 sites within her house for 30 days each. She records the outside temperature and the weather condition.\n\ntemperat.csv\n\nDate: Date (MDY)\nOut_temp: Outside temerature (Degrees Fahrenheit)\nRoom: Room location\nIn_temp: Inside temperature (Degrees Fahrenheit)\nCor_fac: Correction factor added.\n\nPossible values: yes, no\n\nTyp_wea: Type of weather.\n\nPossible values: SUNNY, PARTLY CLOUDY, CLOUDY, RAINY, FOGGY\n\n\n\n\n\ntennis1\nA survey of tennis club members in the Boston area examined the occurrence of tennis elbow. Subjects reported anywhere from 0 to 8 episodes of tennis elbow. They were also asked about demographic factors and racquet characteristics.\n\ntennis1.csv\n\nId: ID\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nNum_epis: Number of episodes of tennis elbow\nTyp_last: Type of racquet used during last episode.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_last: Weight of racquet used during last episode.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_last: Material of racquet used during last episode.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_last: String type of racquet used during last episode.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\nTyp_curr: Type of racquet used currently.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_curr: Weight of racquet used currently.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_curr: Material of racquet used currently.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_curr: String type of racquet used currently.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\n\n\n\n\ntennis2\nTennis elbow is a painful condition common among tennis players. Treatments include rest, heat, and anti-inflammatory medications like Motrin (ibuprofen). A clinical trial with 87 participants compared the effectiveness of Motrin vs. placebo. Participants were randomly divided into two groups: Group A received Motrin for 3 weeks, followed by a 2-week washout period, and then placebo for 3 weeks; Group B received the treatments in reverse order. Pain levels were measured on a 1-6 scale (1 = worse, 6 = completely improved) at the end of each treatment and washout period. The comparison was made (i) during maximum activity, (ii) 12 hours following maximum activity, (iii) during the average day, and (iv) by overall impression of drug efficacy.\n\ntennis2.csv\n\nid: ID\nage: Age\nsex: Sex.\n\nPossible values: male, female\n\ndrg_ord: Drug order.\n\nPossible values: MOTRIN-PLACEBO, PLACEBO-MOTRIN\n\nPeriod 2 = Pain scores after the first active drug period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_2: During study period, pain during maximum activity vs baseline\n\n1 = Worse\n2 = Unchanged\n3 = Slightly improved (25%)\n4 = Moderately improved (50%)\n5 = Mostly improved (75%)\n6 = Completely improved\n\npain12_2: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_2: During the average day of study period pain vs. baseline (same code as painmx_2)\npainov_2: Overall impression of drug efficacy vs. baseline (same code as painmx_2)\n\nPeriod 3 = Pain scores after the washout period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_3: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_3: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_3: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_3: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\nPeriod 4 = Pain scores after the second active drug period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_4: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_4: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_4: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_4: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\n\n\n\n\nvalid\nThe food-frequency questionnaire (FFQ) is a common tool in dietary epidemiology to assess food consumption. It asks individuals to report their typical daily servings of over 100 food items from the past year, and a food-composition table calculates nutrient intakes. While FFQs are inexpensive, they are less accurate than diet records (DR), where participants document their weekly food intake, and a nutritionist calculates nutrient intakes. In a validation study, 173 nurses from the Nurses’ Health Study completed 4 weeks of diet records and an FFQ. Data for saturated fat, total fat, alcohol consumption, and caloric intake from both methods recorded here.\n\nvalid.csv\n\nId: ID number\nsfat_dr: Saturated fat-DR (g)\nsfat_ffq: Saturated fat-FFQ (g)\ntfat_dr: Total fat-DR (g)\ntfat_ffq: Total fat-FFQ (g)\nalco_dr: Alcohol consumption-DR (oz)\nalco_ffq: Alcohol consumption-FFQ (oz)\ncal_dr: Total calories-DR (K-cal)\ncal_ffq: Total calories-FFQ (K-cal)\n\n\n\n\nwales\nA study in South Wales investigated the hereditary factors of blood pressure in 623 individuals (propositii) over age 5 from two populations. The participants and their first-degree relatives had their blood pressure measured at home by one observer, with a baseline and three follow-up exams from the mid-1950s to the early 1960s. The dataset WALES.DAT includes familial blood pressure data from the Rhondda Fach and Vale of Glamorgan communities.\n\nwales.csv\n\nID: ID\nrecord:\nsex:\narea:\npropositus:\nsexofprop:\nrelationship:\ndoubler:\nsurveystat:\nmaritalstt:\nage:\nparity:\nheight:\nweight:\narmgrth:\ntriceps:\nsysbp:\ndiasbp:\npulse:\nalbumin:\nglucose:\nbacteria:\noccupation:\nhypertension:\ndiabetes:\npregnancy:\npet_fibroids:\nangina:\npmi:\ninter_claud:\n\n\n\n\n\n\n\n\n\n\nReferences\n\nRosner, B. 2015. Fundamentals of Biostatistics. 8th ed. Cengage Learning."
  },
  {
    "objectID": "01_r/01_r_intro.html",
    "href": "01_r/01_r_intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a statistical programming language designed to analyze data.\nThis is not an R course. But you need to know some tools to summarize/plot/model data.\nR is free, widely used, more generally applicable (beyond linear regression), and a useful tool for reproducibility. So this is what we will use.\nPython would have been a good choice too, but it is worse at basic stats (this is controversial)."
  },
  {
    "objectID": "01_r/01_r_intro.html#variables",
    "href": "01_r/01_r_intro.html#variables",
    "title": "Introduction to R",
    "section": "Variables",
    "text": "Variables\n\nA variable stores a value. You use the assignment operator “&lt;-” to assign values to variables. For example, we can assign the value of 10 to the variable x.\n\nx &lt;- 10\n\n\nIt is possible to use =, and I think there is nothing wrong with that. But for some reason the field has decided to only use &lt;-, so you should too.\n\nWhenever we use x later, it will use the value of 10\n\nx\n\n[1] 10\n\n\nThis is useful because you can reuse this value over and over again:\n\ny &lt;- 0\nx + y\nx * y\nx / y\nx - y\n\nTo assign a “string” (a fancy way to say a word) to x, put the string in quotes. For example, we can assign the value of \"Hello World\" to x.\n\nx &lt;- \"Hello World\"\nx\n\n[1] \"Hello World\""
  },
  {
    "objectID": "01_r/01_r_intro.html#functions",
    "href": "01_r/01_r_intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\n\nFunctions take objects (such as numbers or variables) as input and output new objects. Let’s look at a simple function that takes the log of a number:\n\nlog(x = 4, base = 2)\n\nThe inputs are called “arguments”. Generally, every function will be for the form:\n\nfunction_name(arg1 = val1, arg2 = val2, ...)\n\nIf you do not specify the name of the argument, R will assume you are assigning in their order.\n\nlog(4, 2)\n\nYou can change the order of the arguments if you specify them.\n\nlog(base = 2, x = 4)\n\nTo see the list of all possible arguments of a function, use the help() function:\n\nhelp(log)\n\nIn the help file, there are often default values for an argument. For example, the following indicates the the default value of base is exp(1).\n\nlog(x, base = exp(1))\n\nThis indicates that you can omit the base argument and R will assume that it should be exp(1).\n\nlog(x = 4, base = exp(1))\n\n[1] 1.386\n\nlog(x = 4)\n\n[1] 1.386\n\n\nIf an argument does not have a default, then it must be specified when calling a function.\nType this:\n\nlog(x = 4,\n\nThe “+” indicates that R is expecting more input (you forgot either a parentheses or a quotation mark). You can get back to the prompt by hitting the ESCAPE key."
  },
  {
    "objectID": "01_r/01_r_intro.html#useful-functions",
    "href": "01_r/01_r_intro.html#useful-functions",
    "title": "Introduction to R",
    "section": "Useful Functions",
    "text": "Useful Functions\n\nc() creates a vector (sequence of values)\n\ny &lt;- c(8, 1, 3, 4, 2)\ny\n\n[1] 8 1 3 4 2\n\n\nYou can perform vectorized operations on these vectors\n\ny + 2\n\n[1] 10  3  5  6  4\n\ny / 2\n\n[1] 4.0 0.5 1.5 2.0 1.0\n\ny - 2\n\n[1]  6 -1  1  2  0\n\n\nexp(): Exponentiation. This is the inverse of log().\n\nexp(10)\n\n[1] 22026\n\nlog(exp(10))\n\n[1] 10\n\n\nsqrt(): Square root\n\nsqrt(9)\n\n[1] 3\n\n\nmean(): The mean of a vector\n\nmean(y)\n\n[1] 3.6\n\n\nsd() The standard deviation of a vector\n\nsd(y)\n\n[1] 2.702\n\n\nsum(): Sum the values of a vector.\n\nsum(y)\n\n[1] 18\n\n\nseq(): Create a sequence of numbers\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nhead(): Show the first six values of an object.\n\n\nExerciseSolution\n\n\nCalculate this in R (hint: pi is \\(\\pi\\) in R) \\[\n\\frac{1}{\\sqrt{2\\pi}}e^{-1.3^2}\n\\]\n\n\n\nexp(-1.3^2) / sqrt(2 * pi)\n\n[1] 0.07361\n\n\n\n\n\n\nExerciseSolution\n\n\nCreate the following vector \\(x = (1, -2, 1.3)\\). What is the mean and standard deviation of this variable?\n\n\n\nx &lt;- c(1, -2, 1.3)\nmean(x)\n\n[1] 0.1\n\nsd(x)\n\n[1] 1.825\n\n\n\n\n\n\nExerciseSolution\n\n\nCreate a vector from 1 to 1000, take ths square root of each element, then sum them up.\n\n\n\nsum(sqrt(seq(1, 1000)))\n\n[1] 21097\n\n\n\n\n\n\nExerciseSolution\n\n\nWhat does the by argument do in seq()? Try it out.\n\n\nCreates increments of 2 instead of 1.\n\nseq(1, 10, by = 2)\n\n[1] 1 3 5 7 9"
  },
  {
    "objectID": "01_r/01_r_intro.html#r-packages",
    "href": "01_r/01_r_intro.html#r-packages",
    "title": "Introduction to R",
    "section": "R Packages",
    "text": "R Packages\n\nA package is a collection of functions that don’t come with R by default.\nThere are many many packages available. If you need to do any data analysis, there is probably an R package for it.\nUsing install.packages(), you can install packages that contain functions and datasets that are not available by default. Do this now with the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nYou will only need to install a package once per computer. Once it is installed you can gain access to all of the functions and datasets in a package by using the library() function.\n\nlibrary(tidyverse)\n\nYou will need to run library() at the start of every R session if you want to use the functions in a package.\nWhen I want to write the name of a function, I will write it like this()."
  },
  {
    "objectID": "01_r/01_r_intro.html#data-frames",
    "href": "01_r/01_r_intro.html#data-frames",
    "title": "Introduction to R",
    "section": "Data Frames",
    "text": "Data Frames\n\nThe fundamental unit object of data analysis is the data frame.\nA data frame has variables in the columns, and observations in the rows.\n \nR comes with a bunch of famous datasets in the form of a data frame. Such as the airquality dataset, which contains daily air quality measurements in New York from 1973.\n\ndata(\"airquality\")\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nYou can extract individual variables from a data frame using $\n\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6\n [19]  30  11   1  11   4  32  NA  NA  NA  23  45 115  37  NA  NA  NA  NA  NA\n [37]  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA\n [55]  NA  NA  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA\n [73]  10  27  NA   7  48  35  61  79  63  16  NA  NA  80 108  20  52  82  50\n [91]  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22\n[109]  59  23  31  44  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73\n[127]  91  47  32  20  23  21  24  44  21  28   9  13  46  18  13  24  16  13\n[145]  23  36   7  14  30  NA  14  18  20\n\n\nYou can explore these in a spreadsheet format using View() (note the capital “V”). Don’t ever have this in a file though, directly write it in the console.\n\nView(airquality)"
  },
  {
    "objectID": "01_r/01_r_intro.html#reading-in-data-frames",
    "href": "01_r/01_r_intro.html#reading-in-data-frames",
    "title": "Introduction to R",
    "section": "Reading in Data Frames",
    "text": "Reading in Data Frames\n\nMost datasets will nead to be loaded into R. To do so, we will use the {readr} package.\n\nlibrary(readr)\n\nThe only function I will require you to know from this package is read_csv(), which loads in data from a CSV file (“Comma-separated values”), a very popular format for storing data.\nIf you have the CSV file somewhere on your computer, then specify the path from the current working directory, and assign the data frame to a variable.\nFor other file formats, you need to use other functions, such as read_tsv(), read_table(), read_fwf(), etc. I will try to make sure read_csv() works for all datasets in this course.\nI will typicaly post course datasets at https://dcgerard.github.io/stat_320/data.html. You can load those data into R by pasting their URL’s into read_csv().\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nhead(lead)\n\n# A tibble: 6 × 40\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 29 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;\n\n\n\n\nExerciseSolution\n\n\nLoad in the birthweight data into R and print out the first six rows.\n\n\n\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nRows: 1000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): id, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(birthweight)\n\n# A tibble: 6 × 2\n     id weight\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     0    116\n2     1    124\n3     2    119\n4     3    100\n5     4    127\n6     5    103"
  },
  {
    "objectID": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "href": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "title": "Introduction to R",
    "section": "Basic Data Frame Manipulations",
    "text": "Basic Data Frame Manipulations\n\nYou will need to know just a few data frame manipulations, which we will perform using the {dplyr} package.\n\nlibrary(dplyr)\n\nThe first argument for {dplyr} functions is always the data frame you are modifying. The following arguments typically involve the columns of that data frame.\nUse the mutate() function from the {dplyr} package to make variable transformations.\n\nlead &lt;- mutate(lead, log_iqv_inf = log(iqv_inf))\nhead(lead)\n\n# A tibble: 6 × 41\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse glimpse() to get a brief look at the data frame.\n\nglimpse(lead)\n\nRows: 124\nColumns: 41\n$ id          &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112…\n$ area        &lt;chr&gt; \"2.5-4.1\", \"2.5-4.1\", \"2.5-4.1\", \"1-2.5\", \"0-1\", \"1-2.5\", …\n$ ageyrs      &lt;dbl&gt; 11.08, 9.42, 11.08, 6.92, 11.25, 6.50, 6.92, 15.00, 7.17, …\n$ sex         &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"f…\n$ iqv_inf     &lt;dbl&gt; 3, 7, 4, 4, 5, 5, 7, 3, 13, 7, 6, 11, 11, 6, 9, 4, 13, 4, …\n$ iqv_comp    &lt;dbl&gt; 4, 9, 9, 6, 4, 12, 9, 1, 10, 9, 10, 14, 12, 4, 11, 6, 17, …\n$ iqv_ar      &lt;dbl&gt; 3, 7, 5, 6, 8, 11, 10, 3, 14, 12, 6, 14, 8, 5, 11, 4, 13, …\n$ iqv_ds      &lt;dbl&gt; 5, 6, 3, 6, 5, 9, 7, 6, 13, 9, 7, 11, 8, 8, 9, 8, 14, 12, …\n$ iqv_raw     &lt;dbl&gt; 15, 29, 21, 22, 22, 37, 33, 13, 50, 37, 29, 50, 39, 23, 40…\n$ iqp_pc      &lt;dbl&gt; 10, 8, 10, 5, 5, 14, 10, 6, 8, 6, 6, 13, 8, 9, 14, 9, 16, …\n$ iqp_bd      &lt;dbl&gt; 8, 7, 7, 8, 10, 7, 8, 2, 15, 9, 8, 13, 9, 7, 17, 8, 16, 9,…\n$ iqp_oa      &lt;dbl&gt; 8, 10, 7, 5, 13, 7, 7, 3, 14, 12, 3, 15, 11, 6, 13, 13, 16…\n$ iqp_cod     &lt;dbl&gt; 5, 9, 20, 13, 12, 10, 16, 8, 9, 13, 9, 20, 12, 12, 16, 12,…\n$ iqp_raw     &lt;dbl&gt; 31, 34, 44, 31, 40, 38, 41, 19, 46, 40, 26, 61, 40, 34, 60…\n$ hh_index    &lt;dbl&gt; 77, 77, 30, 77, 62, 72, 54, 73, 22, 77, 63, 48, 48, 48, 48…\n$ iqv         &lt;dbl&gt; 61, 82, 70, 72, 72, 95, 89, 57, 116, 95, 82, 116, NA, 74, …\n$ iqp         &lt;dbl&gt; 85, 90, 107, 85, 100, 97, 101, 64, 111, 100, 76, 136, 100,…\n$ iqf         &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, …\n$ iq_type     &lt;chr&gt; \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"W…\n$ lead_grp    &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ Group       &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ ld72        &lt;dbl&gt; 25, 31, 30, 29, 2, 29, 25, 24, 24, 31, 21, 29, 32, 36, 30,…\n$ ld73        &lt;dbl&gt; 18, 28, 29, 30, 34, 25, 24, 15, 16, 24, 19, 27, 29, 32, 25…\n$ fst2yrs     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ totyrs      &lt;dbl&gt; 11, 6, 5, 5, 11, 6, 6, 15, 7, 7, 12, 10, 12, 12, 10, 10, 1…\n$ pica        &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ colic       &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ clumsi      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no…\n$ irrit       &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ convul      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ `_2plat_r`  &lt;dbl&gt; 16, 17, 16, 11, 17, 16, 10, 19, 15, 16, 17, 17, 15, 23, 19…\n$ `_2plar_l`  &lt;dbl&gt; 16, 16, 17, 9, 16, 14, 13, 14, 13, 11, 16, 17, 14, 21, 20,…\n$ visrea_r    &lt;dbl&gt; 36, 23, 20, 34, 26, 29, 29, 30, 31, 26, 19, 22, 19, 26, 17…\n$ visrea_l    &lt;dbl&gt; 38, 19, 24, 42, 34, 26, 29, 32, 28, 25, 19, 24, 17, 23, 16…\n$ audrea_r    &lt;dbl&gt; 27, 18, 16, 35, 31, 28, 30, 33, 31, 27, 16, 22, 18, 25, 17…\n$ audrea_l    &lt;dbl&gt; 25, 28, 17, 30, 33, 27, 27, 24, 29, 21, 19, 23, 20, 28, 16…\n$ fwt_r       &lt;dbl&gt; 72, 61, 46, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 44, 74…\n$ fwt_l       &lt;dbl&gt; 52, 48, 49, 41, 42, 35, 39, 58, 40, 37, 44, 48, 47, 53, 63…\n$ hyperact    &lt;dbl&gt; NA, 0, NA, 2, NA, 0, 0, NA, 0, 0, NA, 1, NA, NA, NA, 2, NA…\n$ maxfwt      &lt;dbl&gt; 72, 61, 49, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 53, 74…\n$ log_iqv_inf &lt;dbl&gt; 1.0986, 1.9459, 1.3863, 1.3863, 1.6094, 1.6094, 1.9459, 1.…\n\n\nUse View() to see a spreadsheet of the data frame (never put this in a Quarto file). Note the capital “V”.\n\nView(lead)\n\nUse rename() to rename variables.\n\nlead &lt;- rename(lead, ID = id)\nhead(lead)\n\n# A tibble: 6 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse filter() to remove rows.\n\nUse == to select rows based on equality\nUse &lt; and &gt; to select rows based on inequality\nUse &lt;= and &gt;= to select rows based on inequality/equality.\n\n\nfilter(lead, Group == \"control\")\n\n# A tibble: 78 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   101 2.5-…  11.1  male        3        4      3      5      15     10      8\n 2   102 2.5-…   9.42 male        7        9      7      6      29      8      7\n 3   103 2.5-…  11.1  male        4        9      5      3      21     10      7\n 4   104 1-2.5   6.92 male        4        6      6      6      22      5      8\n 5   105 0-1    11.2  male        5        4      8      5      22      5     10\n 6   106 1-2.5   6.5  male        5       12     11      9      37     14      7\n 7   107 2.5-…   6.92 male        7        9     10      7      33     10      8\n 8   108 0-1    15    fema…       3        1      3      6      13      6      2\n 9   109 1-2.5   7.17 fema…      13       10     14     13      50      8     15\n10   110 1-2.5   7.25 male        7        9     12      9      37      6      9\n# ℹ 68 more rows\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;, …\n\nfilter(lead, ageyrs &lt; 4)  \n\n# A tibble: 7 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   403 0-1      3.92 male        5        6     10      7      28     12      8\n2   406 1-2.5    3.75 male        9        4     10      7      30      8     11\n3   504 1-2.5    3.75 male        8        7      8      7      30      8      7\n4   505 1-2.5    3.75 fema…       6        5      6      3      20      8     12\n5   602 1-2.5    3.75 fema…       6       11      8     11      36     11     10\n6   606 2.5-4…   3.83 male       12        9     18      8      47     12     10\n7   607 0-1      3.92 male        8        4     11      1      24     13      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\nfilter(lead, ageyrs &gt; 4, ageyrs &lt; 5)\n\n# A tibble: 14 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   401 2.5-…   4.33 male        6       13     13      9      41     11     11\n 2   402 1-2.5   4.83 fema…       9        7     13      8      37     11     12\n 3   404 0-1     4.58 male        6        3      4      2      15     12      7\n 4   405 0-1     4.5  male        6        9      9     12      36      8      9\n 5   407 2.5-…   4.25 male        7        4      8      6      25     10     12\n 6   408 2.5-…   4.33 male        7        6      4      5      22      9      6\n 7   409 1-2.5   4.33 fema…       8        8     11     11      38     11      9\n 8   411 1-2.5   4.33 fema…       8        9     14      7      38     14      9\n 9   412 2.5-…   4.75 fema…       7        7     10      8      32     13     13\n10   414 0-1     4.5  male        6        3      7      2      18      6      8\n11   501 0-1     4.17 male       11        7      8      5      31     11     10\n12   502 2.5-…   4.58 male        7        7      9      3      26     10      3\n13   601 1-2.5   4.33 male        9        6      9      8      32      8      9\n14   604 1-2.5   4.58 male        7       10      6     12      35      9     11\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\n\n\nExerciseSolution\n\n\nThe birthweight data is in ounces. There are abour 28.3495 grams in an ounce. Create a new variable called weight_g that is the weight of the baby in grams.\n\n\n\nbirthweight &lt;- mutate(birthweight, weight_g = 28.3495 * weight)\nglimpse(birthweight)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the birthweight data, select just babies that are greater than or equal to 150 ounces.\n\n\n\nfilter(birthweight, weight &gt;= 150)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the lead data, select individuals who are both in the control group and are at least 15.\n\n\n\nfilter(lead, Group == \"control\", ageyrs &gt;= 15)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the lead data, rename ageyrs to just age\n\n\n\nrename(lead, age = ageyrs)"
  },
  {
    "objectID": "00_course_outline/00_math_prereqs.html",
    "href": "00_course_outline/00_math_prereqs.html",
    "title": "00 Math Prerequisites",
    "section": "",
    "text": "We do not emphasize the mathematics underlying statistics in this course. However, you should have some familiarity with pre-calculus topics. Here are some facts you should know off the top of your head:\n\nPowers, Exponentials, Logarithms\n\n\\(e^{ab} = {e^{a}}^{b} = {e^{b}}^{a}\\)\n\\(e^{a+b+c} = e^a(e^{b+c}) = e^ae^be^c\\)\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(a/b) = \\log(a) - \\log(b)\\)\n\\(x^ny^n = (xy)^n\\)\n\n\n\nSummations\n\nCapital-sigma notation is useful for writing sums of many numbers/variables: \\[\\sum_{i = 1}^n x_i = x_1 + x_2 + \\cdots x_n\\]\nIf you sum a constant \\(n\\) times, you get \\(n\\) times that constant: \\[\\sum_{i = 1}^n a = an\\]\nYou can factor out multiplicative constants that don’t depend on the summing index: \\[\\sum_{i = 1}^n cx_i = c\\sum_{i = 1}^n x_i\\]\nThe order that you sum elements does not matter: \\[\\sum_{i = 1}^n (x_i + y_i) = \\sum_{i = 1}^n x_i + \\sum_{i = 1}^n y_i\\]"
  },
  {
    "objectID": "00_course_outline/00_overview.html",
    "href": "00_course_outline/00_overview.html",
    "title": "Course Overview",
    "section": "",
    "text": "Learning Objectives\n\nOverview of the course plus some reminders from STAT 202/203/204\nP-values/confidence intervals.\n\\(t\\)-tests for means in R.\nProportion tests in R.\n\n\n\nProbability and Distributions in R.\n\nDistribution: The possible values of a variable and how often it takes those values.\nA density describes the distribution of a quantitative variable. You can think of it as approximating a histogram. It is a curve where\n\nThe area under the curve between any two points is approximately the probability of being between those two points.\nThe total area under the curve is 1 (something must happen).\nThe curve is never negative (can’t have negative probabilities).\n\nThe density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nExercise: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?\nThe \\(t\\)-distribution shows up a lot in Statistics.\n\nIt is also bell-curved but has “thicker tails” (more extreme observations are more likely).\nIt is always centered at 0.\nIt only has one parameter, called the “degrees of freedom”, which determines how thick the tails are.\nSmaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.\nIf the degrees of freedom is large enough, the \\(t\\)-distribution is approximately the same as a normal distribution with mean 0 and variance 1.\n\n\\(t\\)-distributions with different degrees of freedom:\n\n\n\n\n\n\n\n\n\nDensity Function\n\ndt(x = -6, df = 2)\n\n[1] 0.004269\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation\n\nsamp &lt;- rt(n = 1000, df = 2)\nhead(samp)\n\n[1]  0.89857 -1.07176  0.09639  0.79371 -0.42428 -0.64561\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\npt(q = 2, df = 2)\n\n[1] 0.9082\n\n\n\n\n\n\n\n\n\n\n\nQuantile Function\n\nqt(p = 0.9082, df = 2)\n\n[1] 1.999\n\n\n\n\n\n\n\n\n\n\n\nThere are many other distributions implemented in R. To see the most common, run:\n\nhelp(\"Distributions\")\n\n\n\n\nAll of Statistics\n\nObservational/experimental Units: The people/places/things/animals/groups that we collect information about. Also known as “individuals” or “cases”. Sometimes I just say “units”.\nVariable: A property of the observational/experimental units.\n\nE.g.: height of a person, area of a country, marital status.\n\nValue: The specific level of a variable for an observational/experimental unit.\n\nE.g.: Bob is 5’11’’, China has an area of 3,705,407 square miles, Jane is divorced.\n\nQuantitative Variable: The variable takes on numerical values where arithmetic operations (plus/minus/divide/times) make sense.\n\nE.g.: height, weight, area, income.\nCounterexample: Phone numbers, social security numbers.\n\nCategorical Variable: The variable puts observational/experimental units into different groups/categories based on the values of that variable.\n\nE.g.: race/ethnicity, marital status, religion.\n\nBinary Variable: A categorical variable that takes on only two values.\n\nE.g.: dead/alive, treatment/control.\n\nPopulation: The collection of all observational units we are interested in.\nParameter: A numerical summary of the population.\n\nE.g.: Average height, proportion of people who are divorced, standard deviation of weight.\n\nSample: A subset of the population (some observational units, but not all of them).\nStatistic: A numeric summary of the sample.\n\nE.g.: Average height of the sample, proportion of people who are divorced in the sample, standard deviation of weight of a sample.\n\nGraphic:\n \nSampling Distribution: The distribution of a statistic over many hypothetical random samples from the population.\n \nAll of Statistics: We see a pattern in the sample.\n\nEstimation: Guess the pattern in the population based on the sample. Guess a parameter with a statistic. A statistic which is a guess for a parameter is called an estimate.\nHypothesis Testing: Ask if the pattern we see in the sample also exists in the population. Test if a parameter is some value.\nConfidence Intervals: Quantify our (un)certainty of the pattern in the population based on the sample. Provide a range of likely parameter values.\n\nWe will go through a lot of examples of this below\n\nlibrary(tidyverse)\nlibrary(broom)\n\nExercise: Read about the boneden data here What are the observational units? What are the variables? Which are quantitative and which are categorical?\nExercise: Read about the lead data here. What are the observational units? What are the variables? Which are quantitative and which are categorical?\n\n\n\nPattern: Mean is shifted (one quantitative variable)\n\nExample: The boneden data explores the difference in bone density between a heavier and a lighter smoking twin.\nObservational Units: The twins.\nPopulation: All twins where one smokes more than the other.\nSample: The 41 twins in our study.\nVariable: The difference in lumbar spine density (in g/cm2) between the twins. We derived this quantitative variable by subtracting one density from another.\n\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\n\nPattern: Use a histogram/boxplot to visualize the shift from 0.\n\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(bins = 20, fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = 0, lty = 2) +\n  xlab(\"Difference in Bone Density\")\n\n\n\n\n\n\n\n\nGraphic:\n \nParameter of interest: Mean difference in bone density for all twins.\nEstimate: Use sample mean\n\nboneden %&gt;%\n  summarize(meandiff = mean(ls_diff))\n\n# A tibble: 1 × 1\n  meandiff\n     &lt;dbl&gt;\n1   0.0359\n\n\n0.03585 is our “best guess” for the parameter, but it is almost certainly not the value of the parameter (since we didn’t measure everyone).\nHypothesis Testing:\n\nWe are interested in if the mean difference is different from 0.\nTwo possibilities:\n\nNull Hypothesis: Mean is not different from 0, we just happened by chance to get twins that had some difference in density.\nAlternative Hypothesis: Mean is different from 0.\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a \\(p\\)-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\np-value: the probability that you would see data as or more supportive of the alternative hypothesis than what you saw assuming that the null hypothesis is true.\n\nGraphic:\n \nThe distribution of possible null sample means is given by statistical theory. Specifically, the \\(t\\)-statistic (mean divided by the standard deviation of the sampling distribution of the mean) has a \\(t\\) distribution with \\(n - 1\\) degrees of freedom (\\(n\\) is the sample size). It works as long as your data aren’t too skewed or if you have a large enough sample size.\nFunction: t.test()\n\ntout &lt;- t.test(ls_diff ~ 1, data = boneden)\ntout\n\n\n  One Sample t-test\n\ndata:  ls_diff\nt = 2.6, df = 40, p-value = 0.01\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.007986 0.063721\nsample estimates:\nmean of x \n  0.03585 \n\n\nThe tidy() function from the broom package will format the output of common procedures to a convenient data frame.\n\ntdf &lt;- tidy(tout)\ntdf$estimate\n\nmean of x \n  0.03585 \n\ntdf$p.value\n\n[1] 0.01299\n\n\nWe often want a range of “likely” values. These are called confidence intervals. t.test() will return these confidence intervals, giving lowest and highest likely values for the mean difference in bone density:\n\ntdf$conf.low\n\n[1] 0.007986\n\ntdf$conf.high\n\n[1] 0.06372\n\n\nInterpreting confidence intervals:\n\nCORRECT: We used a procedure that would capture the true parameter in 95% of repeated samples.\nCORRECT: Prior to sampling, the probability of capturing the true parameter is 0.95.\nWRONG: After sampling, the probability of capturing the true parameter is 0.95.\n\nBecause after sampling the parameter is either in the interval or it’s not. We just don’t know which.\n\nWRONG: 95% of twins have bone density differences within the bounds of the 95% confidence interval.\n\nBecause confidence intervals are statements about parameters, not observational units or statistics.\n\n\nGraphic:\n \nIntuition: Statistical theory tells us that the sample mean will be within (approximately) 2 standard deviations of the population mean in 95% of repeated samples. This is two standard deviations of the sampling distribution of the sample mean, not two standard deviations of the sample. So we just add and subtract (approximately) two standard deviations of the sampling distribution from the sample mean.\nExercise: The birthweight data available here contains the birthweights (in ounces) of 1000 newborns born in a Boston area hospital. Wikipedia says the average birthweight for individuals of European and African descent is 123 ounces. Does this Boston hospital have the same mean as what Wikipedia says? Explain.\n\n\n\nPattern: Means of two groups are different (one quantitative, one binary)\n\nExample: IQ differences between children with high levels of lead (exposed) and those with low levels of lead (control).\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead |&gt;\n  select(Group, iqf) |&gt;\n  glimpse()\n\nRows: 124\nColumns: 2\n$ Group &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"control\"…\n$ iqf   &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, 118, 8…\n\n\nObservational Units: Children\nPopulation: All children\nSample: The 120 children for whom we have both lead and IQ measurements.\nVariables: The lead level group (binary/categorical) and the full scale IQ (quantitative).\nPattern: Use a boxplot to see if the groups differ.\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nParameter of interest: Difference in mean IQ levels between the control and exposed groups.\nEstimate: The difference in mean IQ between the two groups in our sample.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(meaniq = mean(iqf, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  Group   meaniq\n  &lt;chr&gt;    &lt;dbl&gt;\n1 control   92.6\n2 exposed   88.0\n\n92.55 - 88.02\n\n[1] 4.53\n\n\nThe control group is about 4.5 points higher on average\nHypothesis Test:\n\nWe want to know if the difference in the mean IQ in the two groups is actually different.\nTwo possibilities:\n\nNull Hypothesis: The mean IQs are the same in the two groups. We just happened by chance to get a lower IQ lead group and a higher IQ control group.\nAlternative Hypothesis: The mean IQ are different in the two groups.\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\n\n\nGraphic:\n \nThe distribution of possible null sample means comes from statistical theory. The t-statistic has a \\(t\\) distribution with a complicated degrees of freedom.\nFunction: t.test(). The quantitative variable goes to the left of the tilde and the binary variable goes to the right of the tilde.\n\ntout &lt;- t.test(iqf ~ Group, data = lead)\ntdf &lt;- tidy(tout)\ntdf$estimate\n\n[1] 4.532\n\ntdf$p.value\n\n[1] 0.07966\n\n\nt.test() also returns a 95% confidence interval for the difference in means. This has the exact same interpretation as in the previous section.\n\nc(tdf$conf.low, tdf$conf.high)\n\n[1] -0.5448  9.6094\n\n\nAssumptions (in decreasing order of importance):\n\nIndependence: conditional on group, IQ of one child doesn’t give us any information on the IQs of any other children (reasonable).\nApproximate normality: The distribution of IQ’s is bell-curved in group. Doesn’t matter for moderate-large sample sizes because of the central limit theorem.\n\nExercise: Is there a difference between control and exposed groups when it comes to the finger-wrist tapping test in the dominant hand (maxfwt).\n\n\n\nPattern: Proportion is shifted (one binary variable).\n\nThe exposed individuals were mostly males. There were 30 males and 16 females. In the US, about 51.22% of all births are boys. Are boys more likely to be recruited to the study than girls?\n\nlead |&gt;\n  filter(Group == \"exposed\") |&gt;\n  group_by(sex) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female    16\n2 male      30\n\n\nObservational Units: U.S. children exposed to lead\nPopulation: All U.S. children exposed to lead\nSample: The 46 children in our sample who were exposed to lead.\nVariable: Sex (male/female)\nPattern: Calculate sample proportion.\n\n30 / 46\n\n[1] 0.6522\n\n\nParameter of interest: Proportion of children exposed to lead who are boys\nEstimate with sample proportion, 0.6522\nHypothesis Testing:\n\nWe are interested in if our sample had some bias in selecting more boys.\nTwo possibilities:\n\nNull Hypothesis: Probability of a boy being included in the sample is 0.5122. We just happened by chance to get a lot more boys.\nAlternative Hypothesis: There is bias and the probability of a boy being in the sample is greater than for a girl(because boys are more likely to be exposed to lead, or because they were more likely to be recruited to the study).\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If this probability is high, we don’t conclude anything.\n\nGraphic:\n \nThe distribution of possible null sample proportions comes from statistical theory. The number of successes has a binomial distribution with success probability 0.5122 and size parameter equal to the sample size. The sample proportion is the number successes divided by the sample size.\nFunction: prop.test() (when you have a large number of both successes and failures) or binom.test() (for any number of successes and failures).\n\nbout &lt;- tidy(binom.test(x = 30, n = 46, p = 0.5122))\nbout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0757    0.498     0.786\n\n\n\npout &lt;- tidy(prop.test(x = 30, n = 46, p = 0.5122))\npout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0798    0.497     0.782\n\n\nExercise: Is there a sex bias for the control group?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 320: Biostatistics",
    "section": "",
    "text": "Syllabus\n\n00 R\n\nIntroduction to R\nPlotting in R\nDocuments in R\n\n01 Overview\n\nMath Prereqs\nCourse Overview\n\n02 Descriptive Statistics\n\nChapter 2 Notes (Descriptive Statistics)\n\n03 Probability\n\nChapter 3 Notes (Probability)\nConfusion Matrix\nChapter 4 Notes (Discrete Probability Distributions)\nChapter 5 Notes (Continuous Probability Distributions)\nNormal Distribution in R\n\n04 Estimation\n\nChapter 6 Notes (Estimation)\nRandom Selection/Assignment\nCentral Limit Theorem Illustration\nCI Interpretation\nt-distribution\nBone Density Case Study\nchi-squared distribution\nEstimating Binomial Proportion\n\n05 Testing\n\nChapter 7 Notes (One Sample Hypothesis Tests)\nOne Sample \\(t\\)-Tests in R\nPower Calculations in R\nOne Sample Binomial Tests in R\nOne Sample Binomial Power Calculations\nChapter 8 Notes (Two Sample Hypothesis Tests)\nTwo Sample \\(t\\)-Tests in R\n\n06 Nonparametric Methods\n\nChapter 9 Notes (Nonparametric Methods)\nOne Sample Inference in R\nTwo Sample Inference in R\n\n07 Categorical Tests\n\nChapter 10 Notes (Categorical Methods)\n2x2 Contingency Tables in R\nMcNemar’s Test in R\nLarger Contingency Tables in R\nCohen’s Kappa in R\n\nNext stop:\n\nRegression\n\n\nHandwritten Notes:\n\nChapter 2 Notes (Descriptive Statistics)\nChapter 3 Notes (Probability)\nChapter 4 Notes (Discrete Probability Distributions)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STAT 320 - Biostatistics",
    "section": "",
    "text": "Instructor: Dr. David Gerard\nEmail: dgerard@american.edu\nOffice: DMTI 106E\n\n\nQ1 Learning Outcomes:\n \n\nStudents will solve quantitative problems including approaches that go beyond memorized procedures.\nStudents will demonstrate an understanding of mathematical relationships from multiple perspectives, such as functions from graphical, verbal, numerical, and analytic points of view.\n\n\n\nQ2 Learning Outcomes:\n \n\nTranslate real-world questions or intellectual inquiries into quantitative frameworks.\nSelect and apply appropriate quantitative methods or reasoning.\nDraw appropriate insights from the application of a quantitative framework.\nExplain quantitative reasoning and insights using appropriate forms of representation so that others could replicate the findings.\n\n\n\nCourse Description\nSTAT-320 is an introduction to the statistical methodology commonly used in public health, medical, and biological studies. This course emphasizes working with data and communicating statistical ideas. A breadth of topics will be covered including: study design, tests of significance, confidence intervals, t-procedures, chi-square and Fisher’s exact test, linear regression, logistic regression, analysis of variance, nonparametric methods, and more advanced topics as time permits. The R computer program will be used to conduct analyses.\nThe major focus for this course is the ideas behind, and the methods for, drawing conclusions about a population from a sample. At the end of this course you will be expected to identify the major concepts related to statistical reasoning and to statistical inferences for drawing such conclusions, recognize how these concepts are used in disciplines related to health and medicine, and implement the methods yourself in statistical analyses using the methods covered. In particular, you are expected to be able to (1) identify the appropriate statistical model or models for a given analysis, (2) write the model in the correct notation, (3) implement the model in the R software package on a given set of data, (4) interpret the output in the context of the study, (5) diagnose model deficiences, (6) suggest improvements to the model if necessary, and (7) summarize the results of the analysis. Work will be a balance between understanding the concepts underlying a method, implementation of the method, and interpretation of the results.\n\n\nRequired Text\n\nRosner, B. (2016) Fundamentals of Biostatistics, Eighth Edition. Brooks/Cole, Boston, MA, USA.\n\n\nThere will be occasional readings from other sources, such as journal articles, for class discussion or for homework assignments. These will be posted in Canvas or links will be given to find these online.\n\n\n\nGrading\n\n\n\n\n\nAssignment\nPercent\n\n\n\n\nHomeworks\n20%\n\n\nParticipation\n20%\n\n\nExams 1, 2, and 3\n60%\n\n\n\n\n\n\n\n\n\nParticipation:\n\nShow up to class. Stay off your phones. Engage with the in-class exercises. You don’t need to complete or turn in the exercises—just make a genuine effort to try them.\nParticipation points will only be deducted under the following circumstances:\n\nYou miss many classes without explanation. Occasional absences are fine. I will take attendance most days, and if you start missing a lot of class, I’ll send you warnings about the impact on your grade before deducting any points.\nYou’re not making a good-faith effort on in-class exercises. For example, if you’re clearly working on something else, I’ll make a note and begin sending you warnings before deducting points. Again, you just need to try the exercises—you don’t need to complete them perfectly.\nYou engage in behavior that is clearly disrespectful to me or your classmates. This includes things like repeated interruptions or dismissive comments. Our classroom is a space for learning, where everyone is respected and discourse remains civil and scholarly.\n\n\nExams\n\nExams are not officially cumulative, but since statistical concepts build on one another, they are effectively cumulative.\nYou may bring one handwritten reference sheet (8.5’’ × 11’’, both sides). Typed sheets are not allowed.\nNo other resources are permitted.\n\nIf you touch your calculator, phone, computer, smartwatch, smart glasses, or any similar device during the exam, it will result in an automatic fail for the course.\n\nI will drop your lowest exam score. Because of this:\n\nNo make-up exams will be offered. If you miss an exam, it will count as your drop.\nYou may not leave the room during the exam unless you are turning it in.\n\nIf you leave mid-exam (even for an emergency), it will count as your drop.\nIf you are unable to remain in the room for the full 1 hour and 15 minutes, please contact ASAC to request an official accommodation.\n\nIf you miss two exams, you should consider withdrawing from the course. The last day to withdraw is October 31, 2025.\n\n\nHomeworks\n\nHomework assignments are designed to reinforce your understanding of course concepts and to help you prepare for the exams. You are permitted to use generative AI tools (e.g., ChatGPT) to assist with homework. However, I strongly recommend using such tools only after you have made a sincere effort to solve the problems on your own, such as checking your work or seeking clarification.\nEducational research consistently shows that actively working through practice problems is among the most effective ways to learn quantitative material. In contrast, passively reading solutions—whether written by others or generated by AI—offers minimal learning benefit. If you rely primarily on AI-generated solutions, you may not be adequately prepared for the exams.\nTo allow some flexibility, your lowest homework score will be dropped.\n\n\nUsual grade cutoffs will be used:\n\n\n\n\n\nGrade\nLower\nUpper\n\n\n\n\nA\n93\n100\n\n\nA-\n90\n92\n\n\nB+\n88\n89\n\n\nB\n83\n87\n\n\nB-\n80\n82\n\n\nC+\n78\n79\n\n\nC\n73\n77\n\n\nC-\n70\n72\n\n\nD\n60\n69\n\n\nF\n0\n59\n\n\n\n\n\nIndividual assignments will not be curved. However, at the discretion of the instructor, the overall course grade at the end of the semester may be curved.\n\n\nLate Work Policy\n\nAll assignments must be submitted on the day they are due.\nEach student will have two three-day extensions, where you can turn in the assignment on Thursday by end-of-day.\nPlease just let me know ahead of time that you will be using one of your two extensions.\nPlease do not tell me why you need the extension. Any reason is a fine reason.\nAny homeworks not submitted by the due date will receive a grade of 0.\n\n\n\nImportant Dates\n\n09/01: Labor Day (no classes or office hours).\n09/29: (tentative): Exam 1 (Chapters 1 through 5)\n10/30: (tentative): Exam 2 (Chapters 6 though 8)\n10/31: Last day to withdraw.\n11/24: Classes meat online via Zoom (or a recorded lecture). No in-person class.\n11/27: Thanksgiving break (no classes or office hours).\nTBD (sometime between 12/08 and 12/12): Exam 3 (Chapters 9 through 11).\n\n\n\nComputing and Software\nWe will use the R computing language to complete some assignment questions. R is free and may be downloaded from the R website (http://cran.r-project.org/). In addition, I highly recommend you interface with R through the free RStudio IDE (https://www.rstudio.com/). R and RStudio are also available on computers in the Anderson Computing Complex in addition to various labs across campus. R Studio may also be run from your web browser using American University’s Virtual Applications System. Please see me during office hours if you have questions regarding R.\n\n\nData\nData sets for homeworks assignments and examples from the textbook are available on the Data page. Almost all of these are cleaned versions of the data from the book’s companion website.\n\n\nAcademic Integrity\n\nStandards of academic conduct are set forth in the university’s Academic Integrity Code. By registering for this course, students have acknowledged their awareness of the Academic Integrity Code and they are obliged to become familiar with their rights and responsibilities as defined by the Code. Violations of the Academic Integrity Code will not be treated lightly and disciplinary action will be taken should violations occur. This includes cheating, fabrication, and plagiarism.\nI expect you to work with others and me, and I expect you to use online resources as you work on your assignments. However, your submissions must be composed of your own thoughts, coding, and words. You should be able to explain your work on assignments/projects and your rationale. Based on your explanation (or lack thereof), I may modify your grade.\nYou can use generative AI (e.g. ChatGPT, CoPilot, etc) on the homeworks if you want. But\n\nThese are your only study exercises for the exams. So I wouldn’t use AI to do them for me except to check my work after I am done.\nYou are still expected to “own” all of your responses. I reserve the right to ask you to explain any of your solutions. If you write something weird or too advanced in the homework, I’ll call you in and ask you questions about it. Based on your explanation (or lack thereof), I may modify your grade.\n\nNo resources are allowed for the exam except the 1 page (8.5’’ by 11’’) handwritten cheat sheet (and a pen or pencil, of course). If you touch your phone/computer/smart watch/smart glasses/etc during the exam then that is an automatic fail for the course.\nAll solutions that I provide are under my copyright. These solutions are for personal use only and may not be distributed to anyone else. Giving these solutions to others, including other students or posting them on the internet, is a violation of my copyright and a violation of the student code of conduct.\n\n\n\nSharing Course Content:\nStudents are not permitted to make visual or audio recordings (including livestreams) of lectures or any class-related content or use any type of recording device unless prior permission from the instructor is obtained and there are no objections from any student in the class. If permission is granted, only students registered in the course may use or share recordings and any electronic copies of course materials (e.g., PowerPoints, formulas, lecture notes, and any discussions – online or otherwise). Use is limited to educational purposes even after the end of the course. Exceptions will be made for students who present a signed Letter of Accommodation from the Academic Support and Access Center. Further details are available from the ASAC website.\n\n\nUse of Student Work\nThe professor will use academic work that you complete for educational purposes in this course during this semester. Your registration and continued enrollment constitute your consent.\n\n\nSyllabus Change Policy\nThis syllabus is a guide for the course and is subject to change with advanced notice. These changes may come via email or Canvas. Make sure to check Canvas and your university-supplied email regularly. You are accountable for all such communications."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#learning-objectives",
    "href": "00_course_outline/00_course_outline.html#learning-objectives",
    "title": "Course Outline for Stat 320",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nThree aspects of Statistics\nPopulation/Sample"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics",
    "href": "00_course_outline/00_course_outline.html#statistics",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nData — Numerical or qualitative descriptions of people/places/things that we want to study."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-1",
    "href": "00_course_outline/00_course_outline.html#statistics-1",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nSome examples\n\nLead Exposure\n\nData: Retrospective study measuring lead exposure in children along with variou outcome variables like IQ score, different measures of neurological function, and hyperactivity assessmenets.\nQuestion: What are the neurological and behavioral effects of lead exposure in young children?\n\nSmoking and bone density\n\nData: Pairs of twins, one of whom is a lighter smoker and one of whome is a heavier smoker. Different bone density measures were taken.\nQuestion: Do individuals who smoke have lower densities?"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-2",
    "href": "00_course_outline/00_course_outline.html#statistics-2",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nThree aspects:\n\nData Design\nData Description\nData Inference — informed by Probability"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-design",
    "href": "00_course_outline/00_course_outline.html#data-design",
    "title": "Course Outline for Stat 320",
    "section": "Data Design",
    "text": "Data Design\nWhere do we get data?\n\nWhat is the proper way to collect data?\nWhen can we claim a causal connection between variables? (e.g. Does smoking lead to lower bone density? Does lead lead to increased neurological and behavioral problems?)\nWhat are some sources of bias (unwanted systematic tendencies in the data collection)?\nOnly touched on in this course."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-description",
    "href": "00_course_outline/00_course_outline.html#data-description",
    "title": "Course Outline for Stat 320",
    "section": "Data Description",
    "text": "Data Description\nHow do we describe the data we have?\n\nNumerical summaries — use numbers to describe the data.\nGraphical summaries — use pictures to describe the data.\nExploratory data analysis — play with the data to get a “feel” for it.\nLots of R.\nFirst week of the semester."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-inference-probability",
    "href": "00_course_outline/00_course_outline.html#data-inference-probability",
    "title": "Course Outline for Stat 320",
    "section": "Data Inference (Probability)",
    "text": "Data Inference (Probability)\nHow can we tell if our conclusions from the exploratory data analysis are real?\n\nLast thirteen weeks of the semester.\nProbability — subdiscipline of Mathematics that provides a foundation for modeling chance events.\nInference — describing a population (probabilistically) by using information from sample."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#population",
    "href": "00_course_outline/00_course_outline.html#population",
    "title": "Course Outline for Stat 320",
    "section": "Population",
    "text": "Population\nStatisticians (among others) are interested in characteristics of a large group of people/countries/objects\n\nCharacterize/describe neurological and behavioral health of young children\nCharacterize/describe bone health of smokers.\nCharacterize/describe the effectiveness of a drug on a all adults.\n\nA population is a group of individuals/objects/locations for which you want information."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#sample",
    "href": "00_course_outline/00_course_outline.html#sample",
    "title": "Course Outline for Stat 320",
    "section": "Sample",
    "text": "Sample\nIt is usually expensive/impossible to measure characteristics of every case in a population.\nA sample is a subgroup of individuals/objects/locations of the population.\n\nMeasure lead intake and different measures of neurological and behavioral health in a sample of 124 children.\nFind a group of 41 twins who have different smoking behaviors and compare their bone densities."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference",
    "href": "00_course_outline/00_course_outline.html#inference",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\nFrom the sample, describe the population using probability.\n\nWe have strong evidence that lighter smoking twins have heavier lumbar spine bone density than heavier smoking twins (pair \\(t\\)-test \\(p = 0.006494\\)). The corresponding 95% confidence interval for the difference in bone density is 0.00799 g/cm2 0.06372 g/cm2. Since the twins are not a random sample, the generalizability of this result depends on how representative the twins are of the general population of interest. Since this is an observational study and not an experiment, the statistics alone cannot make a claim for causality — such a claim would have to depend on other arguments."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference-1",
    "href": "00_course_outline/00_course_outline.html#inference-1",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\n\nIn this class, we will learn how to formulate such statements and interpret them."
  },
  {
    "objectID": "01_r/01_quarto.html",
    "href": "01_r/01_quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Quarto is a file format that is a combination of plain text and R code.\nLots of great educational material is available at https://quarto.org/\nYou write code and commentary of code in one file. You may then compile (RStudio calls this “rendering”) the Quarto file to many different kinds of output: pdf (including beamer presentations), html (including various presentation formats), Word, PowerPoint, etc.\nQuarto is useful for:\n\nCommunication of statistical results.\nCollaborating with other data scientists.\nUsing it as a modern lab notebook to do data science.\n\nQuarto can also make “literate programming” documents for python, Julia, JavaScript, etc…\n\n\n\n\nInstall Quarto via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/\n\n\n\n\n\n\nOpen up a new Quarto file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto:\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: today\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_quarto.html#getting-statrted",
    "href": "01_r/01_quarto.html#getting-statrted",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Install Quarto via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/"
  },
  {
    "objectID": "01_r/01_quarto.html#playing-with-quarto",
    "href": "01_r/01_quarto.html#playing-with-quarto",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Open up a new Quarto file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto:\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: today\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_ggplot.html",
    "href": "01_r/01_ggplot.html",
    "title": "R Graphics with {ggplot2}",
    "section": "",
    "text": "Basic plotting in R using the {ggplot2} package."
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous",
    "href": "01_r/01_ggplot.html#continuous",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous",
    "text": "Continuous\n\nHistogram:\n\nVariable should be on the \\(x\\)-axis.\nUse the geom_histogram() function.\n\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nMake the bin lines black and the fill white, and change the number of bins.\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\")\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nLoad in the boneden data (see here for a description) and make a histogram of lumbar spine density for the lighter smoking twin with 20 bins. Make the bins red.\n\n\n\nlibrary(readr)\nlibrary(ggplot2)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nRows: 41 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): zyg, men1, men2\ndbl (22): ID, age, ht1, wt1, tea1, cof1, alc1, cur1, pyr1, ls1, fn1, fs1, ht...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(data = boneden, mapping = aes(x = ls1)) +\n  geom_histogram(bins = 20, fill = \"red\")"
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete",
    "href": "01_r/01_ggplot.html#discrete",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete",
    "text": "Discrete\n\nBarplot:\n\nPut the variable on the \\(x\\)-axis.\nUse geom_bar().\n\n\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nWhat variables from the lead data are appropriately plotted using a bar plot? Plot a couple of them.\n\n\n\nggplot(data = lead, mapping = aes(x = sex)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = area)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = lead_grp)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = Group)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = fst2yrs)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = iq_type)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = pica)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = colic)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = clumsi)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = irrit)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = convul)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = maxfwt)) +\n  geom_bar()"
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "href": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous X, Continuous Y",
    "text": "Continuous X, Continuous Y\n\nScatterplot:\n\nSay what variables should be on the \\(x\\)- and \\(y\\)-axes.\nUse geom_point().\n\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point()\n\n\n\n\n\n\n\n\nJitter points to account for overlaying points.\n\nUse geom_jitter() instead of geom_point().\n\n\nggplot(data = lead, mapping = aes(x = totyrs, y = hyperact)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nAdd a Loess Smoother by adding geom_smooth().\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nUsing the boneden data, make a scatterplot exploring the association between the lumbar spine densities for the two twin types (smoking status).\n\n\n\nggplot(data = boneden, mapping = aes(x = ls1, y = ls2)) +\n  geom_point()"
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "href": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete X, Continuous Y",
    "text": "Discrete X, Continuous Y\n\nBoxplot\n\nPlace one variable on \\(x\\)-axis and other on \\(y\\)-axis.\nTypically, but not always, continuous goes on \\(y\\)-axis.\nUse geom_boxplot().\n\n\nggplot(data = lead, mapping = aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nUsing the boneden data, first calculate the difference in lumbar spine densities between the two twins (you’ll need to use mutate() here). Then plot this difference versus zygosity (monozygotic versus dizygotic).\n\n\n\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\nggplot(data = boneden, mapping = aes(x = zyg, y = ls_diff)) +\n  geom_boxplot()"
  },
  {
    "objectID": "01_r/01_figures/formatting.html",
    "href": "01_r/01_figures/formatting.html",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#text-formatting",
    "href": "01_r/01_figures/formatting.html#text-formatting",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#headings",
    "href": "01_r/01_figures/formatting.html#headings",
    "title": "1st Level Header",
    "section": "Headings",
    "text": "Headings"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#nd-level-header",
    "href": "01_r/01_figures/formatting.html#nd-level-header",
    "title": "1st Level Header",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#lists",
    "href": "01_r/01_figures/formatting.html#lists",
    "title": "1st Level Header",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output."
  },
  {
    "objectID": "01_r/01_figures/formatting.html#links-and-images",
    "href": "01_r/01_figures/formatting.html#links-and-images",
    "title": "1st Level Header",
    "section": "Links and images",
    "text": "Links and images\nhttp://example.com\nlinked phrase\n\n\n\noptional caption text"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#tables",
    "href": "01_r/01_figures/formatting.html#tables",
    "title": "1st Level Header",
    "section": "Tables",
    "text": "Tables\n\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell\nContent Cell\n\n\nContent Cell\nContent Cell"
  },
  {
    "objectID": "02_descriptive/02_descriptive.html",
    "href": "02_descriptive/02_descriptive.html",
    "title": "Summary Statistics in R",
    "section": "",
    "text": "We’ll use the lead data as an example. Read about it here.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nYou calculate the summary statistics (mean/median/quantiles/variance/standard deviation) all within a summarize() call.\n\nsummarize(\n  lead, \n  Mean = mean(iqf, na.rm = TRUE), \n  Min = min(iqf, na.rm = TRUE),\n  Q25 = quantile(iqf, probs = 0.25, na.rm = TRUE),\n  Med = median(iqf, na.rm = TRUE), \n  Q75 = quantile(iqf, probs = 0.75, na.rm = TRUE),\n  Max = max(iqf, na.rm = TRUE),\n  Var = var(iqf, na.rm = TRUE),\n  SD = sd(iqf, na.rm = TRUE)\n)\n\n# A tibble: 1 × 8\n   Mean   Min   Q25   Med   Q75   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  90.8    46    80  89.5  98.5   141  212.  14.6\n\n\nThe values on the left of = are the names of the summaries and are up to you.\nThe values on the right of = are the function calls for the summaries.\n\nmean(): the arithmetic mean.\nmin(): the minimum. Same as quantile(x, probs = 0)\nquantile(): the quantiles. You specify which quantile with the probs argument.\nmedian(): the median. Same as quantile(x, probs = 0.5)\nmax(): the maximum. Same as quantile(x, probs = 1)\nvar(): the sample variance.\nsd(): the sample standard deviation.\n\nI have the na.rm = TRUE argument because there are some children who did not have a iqf score. These are “missing” and encoded with NA. If you do not provide that argument, R doesn’t know what those values are and so returns an NA or errors.\n\nsummarize(\n  lead, \n  Mean = mean(iqf), \n  Min = min(iqf),\n  # Q25 = quantile(iqf, probs = 0.25), # errors\n  Med = median(iqf), \n  # Q75 = quantile(iqf, probs = 0.75), # errors\n  Max = max(iqf),\n  Var = var(iqf),\n  SD = sd(iqf)\n)\n\n# A tibble: 1 × 6\n   Mean   Min   Med   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    NA    NA    NA    NA    NA    NA\n\n\nYou can also apply these functions on vectors that you extract from the data frame.\n\nvar(lead$iqf, na.rm = TRUE)\n\n[1] 212.3\n\n\nLet’s demonstrate some properties. Variance is invariant to shift\n\nvar(lead$iqf + 10000, na.rm = TRUE)\n\n[1] 212.3\n\n\nbut scales with the square of the multiplicative factor\n\nvar(10 * lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n10^2 * var(lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n\nThe standard deviation scales with the multiplicative factor because it is the square root of the variance.\n\nsd(10 * lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n10 * sd(lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n\nThe mean and quantiles shift and scale with the additive and multiplicative factors.\n\nmean(lead$iqf * 10 + 20, na.rm = TRUE)\n\n[1] 928.2\n\n10 * mean(lead$iqf, na.rm = TRUE) + 20\n\n[1] 928.2\n\nquantile(lead$iqf * 10 + 20, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)\n\n 25%  50%  75% \n 820  915 1005 \n\n10 * quantile(lead$iqf, probs = c(0.25, 0.5, 0.75), na.rm = TRUE) + 20\n\n 25%  50%  75% \n 820  915 1005 \n\n\nExercise: Calculate the mean and median of the birthweight data. What is the more appropriate measure of center?\nYou can calculate grouped summaries (a summary for each group) by grouping the data first.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(\n    Mean = mean(iqf, na.rm = TRUE),\n    SD = sd(iqf, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 3\n  Group    Mean    SD\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 control  92.6  15.7\n2 exposed  88.0  12.2\n\n\nGroup summaries are where the power of descriptive statistics really comes into play. Here, we see that the exposed group has a lower IQ on average than the control group. Whether this is real signal will have to be answered via a formal hypothesis test. But the descriptive statistics gives us some initial information on the data.\nExercise: What about different lead groups? Calculate descriptive statistics for the different lead groups."
  },
  {
    "objectID": "hw/hw_ch2/hw_ch2.html",
    "href": "hw/hw_ch2/hw_ch2.html",
    "title": "Homework 01L Chapters 1 and 2",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives: - Chapter 2 of Rosner - Descriptive Statistics - Graphics - R\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: &lt; https://www.adobe.com/acrobat/online/merge-pdf.html&gt;\nUse the tidyverse way to answer these questions. If you use the base R way, that’s fine, but I’ll call you in and have you show me that you really know how the base R way works by doing some practice problems in front of me.\n\n\nQuestion 1: Cardiovascular Disease\nRead in the data using the read_csv() function, which you can read about here: https://dcgerard.github.io/stat_320/data.html#lvm\n\nlibrary(tidyverse)\nlvm &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lvm.csv\")\n\n\nUse R to print what variables are in lvm?\n\n\nnames(lvm)\n\n[1] \"ID\"      \"lvmht27\" \"bpcat\"   \"gender\"  \"age\"     \"BMI\"    \n\n\n\nWhat is the arithmetic mean of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(Mean = mean(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat             Mean\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      34.1\n2 normal            29.3\n3 pre-hypertensive  33.8\n\n\n\nIs it appropriate to use the arithmetic mean (by blood pressure group) for LVMI, or should we have used the median? Explain your reasoning and justify with an appropriate box plot.\n\n\nggplot(lvm, aes(x = bpcat, y = lvmht27)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n## Since the distributions are mostly symmetric, and there \n## do not appear to be any major outliers, it looks like\n## the sample mean is an appropriate measure of center.\n\n\nWhat does the boxplot from the previous questiontell you?\n\n\n## The center of group 3 is larger than group 2 than group 1. \n## But groups 1 and 3 are more spread out than group 2. All \n## groups appear to have roughly symmetric distributions.\n\n\nWhat is the standard deviation of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(SD = sd(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat               SD\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      8.56\n2 normal            6.66\n3 pre-hypertensive  5.75\n\n\n\nDoes there appear to be any association between age and lvmht27? Use an appropriate plot to make your case.\n\n\nggplot(lvm, aes(x = age, y = lvmht27)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x)\n\n\n\n\n\n\n\n## Maybe a weak positive association.\n\n\n\nValidity Data\nRead about the validity study on the food frequency questionnaire here: https://dcgerard.github.io/stat_320/data.html#valid\n\nThe data are available at &lt;&gt;. Load these data into R as the valid data frame.\n\n\nvalid &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/valid.csv\")\n\nRows: 173 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Id, sfat_dr, sfat_ffq, tfat_dr, tfat_ffq, alco_dr, alco_ffq, cal_dr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nUse descriptive statistics to relate nutrient intake for the DR and FFQ. Do you think the FFQ is a reasonably ac- curate approximation to the DR? Why or why not?\nA frequently used method for quantifying dietary intake is in the form of quintiles. Compute quintiles for each nutri- ent and each method of recording, and relate the nutrient composition for DR and FFQ using the quintile scale. (That is, how does the quintile category based on DR relate to the quintile category based on FFQ for the same individual?) Do you get the same impression about the concordance between DR and FFQ using quintiles as in the previous problem, in which raw (ungrouped) nutrient intake is considered?\nIn nutritional epidemiology, it is customary to assess nutrient intake in relation to total caloric intake. One measure used to accomplish this is nutrient density, which is defined as 100% × (caloric intake of a nutrient/total caloric intake). For fat consumption, 1 g of fat is equivalent to 9 calories. Compute the nutrient density for total fat for the DR and FFQ.\n\n\nvalid |&gt;\n  mutate(nd_dr = tfat_dr * 9 / cal_dr * 100,\n         nd_ffq = tfat_ffq * 9 / cal_ffq * 100) -&gt;\n  valid\n\n\nObtain appropriate descriptive statistics for nutrient density for both DR and FFQ. How do they compare?\n\n\nRelate the nutrient density for total fat for the DR versus the FFQ using the quintile approach in Problem 2.28. Is the concordance between total fat for DR and FFQ stronger, weaker, or the same when total fat is expressed in terms of nutrient density as opposed to raw nutrient?"
  },
  {
    "objectID": "03_prob/03_prob.html",
    "href": "03_prob/03_prob.html",
    "title": "Probability Definitions and Properties",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution"
  },
  {
    "objectID": "03_prob/03_prob_discrete.html",
    "href": "03_prob/03_prob_discrete.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts."
  },
  {
    "objectID": "03_prob/03_confusion.html",
    "href": "03_prob/03_confusion.html",
    "title": "Confusion Matrix",
    "section": "",
    "text": "The below is a simplified version of the Confusion matrix table from Wikipedia that emphasizes the terminology more common to biostatistics (e.g. sensitivity/specificity/\\(PV^+\\)/\\(PV^-\\))\n\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\nTest Positive \\(T^+\\)\n\n\nTest Negative \\(T^-\\)\n\n\n\n\n\n\n\n\nTruth\n\n\n\nPositive \\(D^+\\)\n\n\nTrue Positive (TP)\n\n\nFalse Negative (FN)  (Type II Error)\n\n\nSensitivity  (True Positive Rate, Recall, Power)  \\(\\frac{TP}{D^+}\\)\n\n\nFalse Negative Rate  (Type II Error Rate)  \\(\\frac{FN}{D^+}\\)\n\n\n\n\nNegative \\(D^-\\)\n\n\nFalse Positive (FP)  (Type I Error)\n\n\nTrue Negative (TN)\n\n\nFalse Positive Rate  (Type I Error Rate)  \\(\\frac{FP}{D^-}\\)\n\n\nSpecificity (True Negative Rate)  \\(\\frac{TN}{D^-}\\)\n\n\n\n\n\n\nPrevalence  \\(\\frac{D^+}{D^+ + D^-}\\)\n\n\nPositive Predictive Value  (Precision)  \\(PV^+ = \\frac{TP}{T^+}\\)\n\n\nFalse Omission Rate  \\(\\frac{FN}{T^-}\\)\n\n\n\n\n\n\n\n\n\n\nFalse Discovery Rate  \\(\\frac{FP}{T^+}\\)\n\n\nNegative Predictive Value  \\(PV^- = \\frac{TN}{T^-}\\)"
  },
  {
    "objectID": "03_prob/03_prob_cont.html",
    "href": "03_prob/03_prob_cont.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "The density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.9990 0.4384 0.9445 2.3738 1.5226 1.6511\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2"
  },
  {
    "objectID": "hw/hw_ch5/hw_ch5.html",
    "href": "hw/hw_ch5/hw_ch5.html",
    "title": "Homework, Chapter 4",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives:\n\nChapter 5 of Rosner\nNormal Distribution\n\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: https://www.adobe.com/acrobat/online/merge-pdf.html\nI will only grade a randomly chosen subset of these questions. Please complete all of them, since you don’t know which ones I will grade.\n\n\nBlood Chemistry\nIn pharmacologic research a variety of clinical chemistry measurements are routinely monitored closely for evidence of side effects of the medication under study. Suppose typical blood-glucose levels are normally distributed, with mean = 90 mg/dL and standard deviation = 38 mg/dL.\n\nIf the normal range is 65−120 mg/dL, then what percentage of values will fall in the normal range?\n\n\n# X ~ N(90, 38^2)\npnorm(q = 120, mean = 90, sd = 38) - pnorm(q = 65, mean = 90, sd = 38)\n\n[1] 0.5298\n\n\n\nIn some studies only values at least 1.5 times as high as the upper limit of normal are identified as abnormal. What percentage of values would fall in this range?\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 0.008932\n\n\n\nAnswer Problem 2 for values 2.0 times the upper limit of normal.\n\n\n# Calculate upper limit\nu &lt;- 120 * 2\nu\n\n[1] 240\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 3.951e-05\n\n\n\nFrequently, tests that yield abnormal results are re- peated for confirmation. What is the probability that for a normal person a test will be at least 1.5 times as high as the upper limit of normal on two separate occasions? Assume the two tests are independent.\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\np1 &lt;- pnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\np1^2\n\n[1] 7.978e-05\n\n\n\nSuppose that in a pharmacologic study involving 6000 patients, 75 patients have blood-glucose levels at least 1.5 times the upper limit of normal on one occasion. What is the probability that this result could be due to chance? Assume the patient tests are independent.\n\n\n## Y = number of patients above 1.5 times upper limit\n## Y ~ Binom(6000, p1)\n## Want Pr(Y &gt;= 75)\n1 - pbinom(q = 74, size = 6000, prob = p1)\n\n[1] 0.003166\n\n## Only a 0.3% chance, so very unlikely\n\n\n\nOrthopedics\nA study was conducted of a diagnostic test (the FAIR test, i.e., hip flexion, adduction, and internal rotation) used to identify people with piriformis syndrome (PS), a pelvic condition that involves malfunction of the piriformis muscle (a deep buttock muscle), which often causes lumbar and buttock pain with sciatica (pain radiating down the leg) [7]. The FAIR test is based on nerve-conduction velocity and is expressed as a difference score (nerve-conduction velocity in an aggravating posture minus nerve-conduction velocity in a neutral posture). It is felt that the larger the FAIR test score, the more likely a participant will be to have PS. Data are given in the Data Set PIRIFORM.DAT for 142 participants without PS (piriform = 1) and 489 participants with PS (piriform = 2) for whom the diagnosis of PS was based on clinical criteria. The FAIR test value is called MAXCHG and is in milliseconds (ms). A cutoff point of ≥ 1.86 ms on the FAIR test is proposed to define a positive test.\n\nWhat is the sensitivity of the test for this cutoff point?\nWhat is the specificity of the test for this cutoff point?\nSuppose that 70% of the participants who are referred to an orthopedist who specializes in PS will actually have the condition. If a test score of ≥ 1.86 ms is obtained for a par- ticipant, then what is the probability that the person has PS?\nThe criterion of ≥ 1.86 ms to define a positive test is arbitrary. Using different cutoff points to define positivity, obtain the ROC curve for the FAIR test. What is the area under the ROC curve? What does it mean in this context?\nDo you think the distribution of FAIR test scores within a group is normally distributed? Why or why not?"
  },
  {
    "objectID": "04_est/04_sample.html",
    "href": "04_est/04_sample.html",
    "title": "Random Sampling",
    "section": "",
    "text": "Before doing sampling, make sure you set the seed so that you have reproducible results. E.g., this makes it so that your “random selection” is the same every time you first run the seed.\n\nset.seed(3574927)\n\nIf you are having trouble coming up with a random seed, you could NIST’s Interoperable Randomness Beacons API, which generates a new truly random number every 60 seconds. You would only run this once and copy the resulting number in your script. You would not include this script anywhere in any code you have.\n\n## HTTP request for a random number\nlibrary(httr2)\nreqout &lt;- request(base_url = \"https://beacon.nist.gov/beacon/2.0/pulse/last\") |&gt;\n  req_perform()\n\n## A random value represented as a 64-byte (512 bits) hex string\nhex &lt;- resp_body_json(reqout)$pulse$localRandomValue \n\n## select only first few digits to make number small. You can increase this.\nhexsmall &lt;- substr(hex, start = 1, stop = 6) \n\n## convert to an integer. This is your seed.\nstrtoi(x = hexsmall, base = 16) \n\nYou generate ID’s for with seq() or :\n\n# 1:100\nidvec &lt;- seq(from = 1, to = 20)\nidvec\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nYou can sample (without replacement) from this vector with sample()\n\nsample(x = idvec, size = 5)\n\n[1]  6 18 11  1  3\n\n\nIf you don’t give an argument for size, then sample will randomly permute the values.\n\nsample(x = idvec)\n\n [1]  1  5  6 16 13 18 19 17 20 12  7  8  3  2 14 11  4 10  9 15\n\n\nThis is useful for random assignment.\nYou should generally also randomize order, but if you need to see the group ID’s in an easier to read format, use sort().\n\nsample(x = idvec, size = 5) |&gt;\n  sort()\n\n[1]  4 10 12 13 15\n\n\nIf you are doing random assignment, you have a data frame of individuals. E.g., from the birthweight data.\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nThen you create a column with the number of groups via rep(), and then randomly permute it with sample(). E.g., suppose we want three groups:\n\nbirthweight |&gt;\n  mutate(group = rep(1:3, length.out = n())) |&gt; ## choose groups of equal size\n  mutate(group = sample(group)) ## randomly assign\n\n# A tibble: 1,000 × 3\n      id weight group\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1     0    116     2\n 2     1    124     1\n 3     2    119     2\n 4     3    100     1\n 5     4    127     3\n 6     5    103     2\n 7     6    140     1\n 8     7     82     3\n 9     8    107     3\n10     9    132     1\n# ℹ 990 more rows\n\n\nExercise: Randomly assign 400 individuals (with IDs 1 through 400) into two groups, \"treatment\" and \"control\".\nExercise: The treatment is way more expensive than the control, so randomly assign only 100 to \"treatment\" and 300 to \"control\"."
  },
  {
    "objectID": "04_est/04_clt.html",
    "href": "04_est/04_clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Consider the birthweight data:\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\nggplot(birthweight, aes(x = weight)) +\n  geom_histogram(bins = 30, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nLet’s take samples of size \\(n\\) = 1, 5, 10, 20, 50, 100 from this distribution with replacement. For each sample size, we college 10000 repeat samples, each time calculating the sample mean \\(\\bar{X}\\), to obtain \\(\\bar{X}_1,\\bar{X}_2,\\ldots,\\bar{X}_{10000}\\). Below are histograms of these \\(\\bar{X}\\)’s\nThe distribution of the \\(\\bar{X}\\)’s has smaller and smaller variance as the sample size increases since \\(\\mathrm{var}(\\bar{X}) = \\sigma^2/n\\).\n \nThe distribution of the \\(\\bar{X}\\)’s gets closer to a normal as the sample size increases. Though it’s already sufficiently normal for most purposes once \\(n = 10\\).\n \nThe true mean \\(\\mu\\) is the vertical dashed red line. You see that the distribution of the sample mean has a mean of \\(\\mu\\), \\(\\mathrm{E}[\\bar{X}] = \\mu\\)."
  },
  {
    "objectID": "04_est/04_t.html",
    "href": "04_est/04_t.html",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_t.html#learning-objectives",
    "href": "04_est/04_t.html#learning-objectives",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_boneden.html",
    "href": "04_est/04_boneden.html",
    "title": "Bone Density Case Study",
    "section": "",
    "text": "Twin study with \\(n=41\\) where one smoked more than the other. Bone density was measured in both twins. More detail here.\n\nlibrary(tidyverse)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nLet’s explore if lumbar spine bone density differed between the twins. First, we’ll calculate the difference in density between the twins:\n\nboneden |&gt;\n  mutate(diff = ls1 - ls2) -&gt; #ls1 = lighter smoking, ls2 = heavy smoking\n  boneden \n\nLet’s plot the data\n\nggplot(boneden, aes(x = diff)) +\n  geom_histogram(bins = 7, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the difference\n\nboneden |&gt;\n  summarize(xbar = mean(diff), s = sd(diff), n = n())\n\n# A tibble: 1 × 3\n    xbar      s     n\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 0.0359 0.0883    41\n\n\nWe can use this to get a 95% confidence interval for the mean difference in lumbar spine bone density between heavy and light smoking twins.\nThe standard error is\n\n0.08829 / sqrt(41)\n\n[1] 0.01379\n\n\nThe appropriate quantile of the t-distribution is\n\nqt(p = 0.975, df = 41 - 1)\n\n[1] 2.021\n\n\nSo, the 95% confidence interval is\n\n0.03585 - 2.021 * 0.01379 # lower\n\n[1] 0.00798\n\n0.03585 + 2.021 * 0.01379 # upper\n\n[1] 0.06372\n\n\nSince the lower bound of the 95% CI is above 0, we can be fairly confident that the true mean difference is greater than 0. That is, we are pretty sure that the lighter smoking twin has heavier bone density. We will formalize what “pretty sure” means in Chapter 7.\n\n\nReal Way\n\nIt would be crazy to do the above calculations, by hand, every time. For this class, I’ll occasionally ask you do that to solidify your understanding. But in real data analysis we use code to automate inference.\nWe will use the {broom} package to summarize inference output.\n\nlibrary(broom)\n\nYou calculate an interval for a mean using t.test() using one of two ways:\n\n## tout &lt;- t.test(boneden$diff)\ntout &lt;- t.test(diff ~ 1, data = boneden)\n\nYou get a summary of the output with broom::tidy()\n\ntidy(tout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359  0.00799    0.0637\n\n\nYou can change the level with the conf.level argument in t.test()\n\nt.test(diff ~ 1, data = boneden, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359   0.0126    0.0591\n\n\nExercise: Calculate an 80% confidence interval for the mean birth weight of newborns using the birth weight data that you can download from here: https://dcgerard.github.io/stat_320/data/birthweight.csv. Do this both “by hand” (after calculating the summary statistics and t-quantile) and using R’s automated functions."
  },
  {
    "objectID": "04_est/04_chi2.html",
    "href": "04_est/04_chi2.html",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/04_chi2.html#learning-objectives",
    "href": "04_est/04_chi2.html#learning-objectives",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/binom.html",
    "href": "04_est/binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\nExercise: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?"
  },
  {
    "objectID": "04_est/04_binom.html",
    "href": "04_est/04_binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\nExercise: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?"
  },
  {
    "objectID": "05_tests/05_ttest.html",
    "href": "05_tests/05_ttest.html",
    "title": "One Sample t-Tests in R",
    "section": "",
    "text": "Suppose we know the average birthweight in America is 110 oz. We are curious if the babies in a Boston area hospital have a different birthweight. Let \\(X_i\\) be the birthweight of the \\(i\\) Boston baby, then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\) and are independent. We want to test\n\\[\\begin{align}\nH_0: \\mu &= 110\\\\\nH_1: \\mu &\\neq 110\n\\end{align}\\]\nLet’s first read in the data on the \\(n = 1000\\) babies:\n\nlibrary(tidyverse)\nlibrary(broom)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nWe then use t.test() to run the \\(t\\)-test. The arguments are:\n\nformula: a formula object (generated with a tilde ~).\n\nWe put the name of the variable we are exploring to the left of the tilde.\nWe put the number 1 to the right of the tilde.\n\ndata: the name of the data frame containing the variable.\nmu: The null value. The default is 0 since this is the most common test.\nalternative: We use the default \"two.sided\", since our alternative hypothesis is of the form parameter \\(\\neq\\) value.\n\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110)\n\nWe then use broom::tidy() to get a summary of the \\(t\\)-test output.\n\nbout &lt;- tidy(tout)\nbout\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     107.     -3.32 0.000920       999     105.      109. One Samp… two.sided  \n\n\nWe can manually verify these results (though, you wouldn’t do this step in real life):\n\nxbar &lt;- mean(birthweight$weight)\ns &lt;- sd(birthweight$weight)\nn &lt;- length(birthweight$weight)\nmu0 &lt;- 110\ntstat &lt;- (xbar - mu0) / (s / sqrt(n))\npval &lt;- 2 * pt(-abs(tstat), df = n - 1)\ntstat\n\n[1] -3.324\n\npval\n\n[1] 0.0009204\n\n\nIf instead we had the alternative of \\(H_1: \\mu &lt; \\mu_0\\), then we would use the alternative = \"less\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"less\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.0004602\n\n\nIf instead we had the alternative of \\(H_1: \\mu &gt; \\mu_0\\), then we would use the alternative = \"greater\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"greater\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.9995\n\n\n\nExercise: Consider the lead data that you can read about here and download from https://dcgerard.github.io/stat_320/data/lead.csv. IQ tests are designed to have a mean of 100. Use iqf to test if the control group has an average IQ value of 100. Separately test if the exposed group has an average IQ less than 100. State the hypotheses, test results, and conclusions."
  },
  {
    "objectID": "05_tests/05_power.html",
    "href": "05_tests/05_power.html",
    "title": "Power Calculations",
    "section": "",
    "text": "For \\(t\\)-methods, you use power.t.test() to calculate do power and smaple size calculations. It takes as input four of the following:\n\nn: The sample size \\(n\\)\ndelta: The effect size (difference in means) \\(|\\mu_1 - \\mu_0|\\)\nsd: The standard deviation of the data \\(\\sigma\\)\nsig.level: The signficicance level \\(\\alpha\\)\npower: The power \\(1 - \\beta\\).\n\nYou must put values for exactly four of the above. The fifth should be NULL and the function will return the fifth value.\nOther inputs are for the type of test:\n\ntype: Use \"one.sample\" for one-sample \\(t\\)-tests and \"two.sample\" for two-sample \\(t\\)-tests.\nalternative: Either \"two.sided\" or \"one.sided\".\n\nSuppose we plan on running a study with 100 participants of low socioeconomic status (SES). The mean birthweight in america is 120 oz. A pilot study suggested that the average birthweight of low SES babies is 115 oz with a standard deviation of 24 oz. What is the power of a test with a significance level of 0.05?\n\npower.t.test(\n  n = 100, \n  delta = 5, \n  sd = 24, \n  sig.level = 0.05, \n  type = \"one.sample\", \n  alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 5\n             sd = 24\n      sig.level = 0.05\n          power = 0.6643\n    alternative = one.sided\n\n\nLet’s compare that power to the \\(z\\)-test calculation from Rosner\n\npnorm(qnorm(0.05) + sqrt(100) * 5 / 24)\n\n[1] 0.6695\n\n\nIt’s a little different because Rosner uses \\(z\\)-methods instead of \\(t\\)-methods for power and sample size calculations. But it’s not too different to be practically important, especially since power and sample size calculations are mostly just wild educated guesses.\nExercise: What sample size would be needed for a power of at least 0.8?\n\n\nExercise: A new drug is proposed to prevent glaucoma among people with high intraocular pressure (IOP). A pilot study is conducted with 10 individuals. After 1 month of using the drug, their IOP decreases by 5 mm HG with a standard deviation of 10 mm HG. What is the sample size needed to achieve 90% power for a two-sided test with significance level of 0.05."
  },
  {
    "objectID": "05_tests/05_binom.html#example",
    "href": "05_tests/05_binom.html#example",
    "title": "One Sample Binomial Tests in R",
    "section": "Example",
    "text": "Example\nSuppose \\(x\\) = 5 and \\(n\\) = 8.\n\nx &lt;- 5\nn &lt;- 8\nbinom.test(x = x, n = n)\n\n\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 5, number of trials = 8, p-value = 0.7\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2449 0.9148\nsample estimates:\nprobability of success \n                 0.625"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound",
    "href": "05_tests/05_binom.html#finding-lower-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-1",
    "href": "05_tests/05_binom.html#finding-lower-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-2",
    "href": "05_tests/05_binom.html#finding-lower-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-3",
    "href": "05_tests/05_binom.html#finding-lower-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-4",
    "href": "05_tests/05_binom.html#finding-lower-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-5",
    "href": "05_tests/05_binom.html#finding-lower-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-6",
    "href": "05_tests/05_binom.html#finding-lower-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-7",
    "href": "05_tests/05_binom.html#finding-lower-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound",
    "href": "05_tests/05_binom.html#finding-upper-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-1",
    "href": "05_tests/05_binom.html#finding-upper-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-2",
    "href": "05_tests/05_binom.html#finding-upper-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-3",
    "href": "05_tests/05_binom.html#finding-upper-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-4",
    "href": "05_tests/05_binom.html#finding-upper-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-5",
    "href": "05_tests/05_binom.html#finding-upper-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-6",
    "href": "05_tests/05_binom.html#finding-upper-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-7",
    "href": "05_tests/05_binom.html#finding-upper-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval",
    "href": "05_tests/05_binom.html#final-confidence-interval",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(np_1\\)\nRight: \\(np_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval-1",
    "href": "05_tests/05_binom.html#final-confidence-interval-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(p_1\\)\nRight: \\(p_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html",
    "href": "05_tests/05_binom.html",
    "title": "One Sample Binomial Tests in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nWe have \\[\n  X \\sim \\mathrm{Binom}(n,p)\n  \\] and we are testing\n\n\\(H_0\\): \\(p = p_0\\)\n\\(H_1\\): \\(p \\neq p_0\\) or \\(p &gt; p_0\\) or \\(p &lt; p_0\\)\n\n\n\nApproximate Approach\n\nWe use the central limit theorem. If (rule-of-thumb) \\(np_0(1-p_0) \\geq 5\\) then \\[\n  X \\sim N(p_0, p_0(1-p_0)/n)\n  \\] and we calculate the tail probabilities of (for \\(\\hat{p} = X/n\\)) \\[\n  Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\n  \\]\nThis is done via prop.test(), which you can feed into broom::tidy()\n\nx: the observed number of successes\nn: The total number of trials\np: The null value of the success probability\nalternative: Either \"two.sided\", \"less\", or \"greater\"\n\nSuppose that about 20% of women who are trying to concieve take 12 months or more to get pregnant, which we will call infertility. Researchers are interested in if a genetic marker is associated with infertility. Of 40 women with this marker, 10 were infertile. Is this marker associated with infertility?\n\n\\(X \\sim \\mathrm{Binom}(40, p)\\)\n\\(H_0\\): \\(p = 0.2\\)\n\\(H_1\\): \\(p &gt; 0.2\\)\n\n\npout &lt;- prop.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(pout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.277\n\n\nSince the \\(p\\)-value is 0.2766, we don’t have evidence that the marker is associated with infertility.\nWe can do this calculation by hand (but you would never do this):\n\nphat &lt;- 10 / 40 \np0 &lt;- 0.2\nz &lt;- (phat - p0 - 1/80) / sqrt(p0 * (1 - p0) / 40) ## continuity correction\npnorm(q = z, lower.tail = FALSE)\n\n[1] 0.2766\n\n\n\n\n\nExact Approach\n\nThe exact approach calculates the probability under the null of being as more supportive of the alternative as the data we observed.\nE.g., for our infertility example, we would calculate \\(\\mathrm{Pr}(X \\geq 10 | p = 0.2)\\)\n\n1 - pbinom(q = 9, size = 40, prob = 0.2)\n\n[1] 0.2682\n\n\nThis procedure is implemented in the binom.test() function, which has the same inputs as prop.test().\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.268\n\n\nWhen the alternative is 2-sided, \\(H_1 \\neq p_0\\), R is a little different than the procedure Rosner proposes. It calculates the sums the probabilities under the null of all \\(X\\) that are less probable than our observed \\(x\\)\n\\[\n  \\sum_{k \\text{ s.t. } Pr(k) \\leq Pr(x)}\\binom{n}{k} p_0^k(1-p_0)^{n-k}\n  \\]\nIn the infertility example, this would be\n\nprob &lt;- dbinom(x = 0:40, size = 40, prob = 0.2)\nsum(prob[prob &lt;= dbinom(x = 10, size = 40, prob = 0.2)])\n\n[1] 0.4296\n\n\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2)\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.430\n\n\nExercise: Out of 13 deaths at a nuclear facility among men aged 55-64, 5 of them were due to cancer. The proportion of deaths caused by cancer in that group in the greater population is 0.2. Is there more cancer deaths in this nuclear facility? Use both the normal and exact approaches."
  },
  {
    "objectID": "05_tests/05_binom_power.html",
    "href": "05_tests/05_binom_power.html",
    "title": "Power Calculations for Binomial Tests",
    "section": "",
    "text": "There are no base R functions that do power and sample size calculations. But I created some for you:\n\n#' Power/sample size calculation of 1-sample proportion test\n#' \n#' Uses central limit theorem, so make sure `p0 * (1 - p0) * n &gt;= 5`\n#' \n#' Exactly one of `n`, `power`, `p0`, `p1`, or `alpha` needs to be `NULL`.\n#' \n#' @param n The sample size\n#' @param power The power\n#' @param p0 The null proportion\n#' @param p1 The alternative proportion\n#' @param alpha The significance level\n#' @param TOL Tolerance level\n#' \n#' @author David Gerard\n#' \n#' @examples\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' b1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' \n#' ## two p1's\n#' b1power(n = 500, power = 0.9, p0 = 0.02, p1 = NULL, alpha = 0.05)\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.00406, alpha = 0.05)$power\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.044, alpha = 0.05)$power\nb1power &lt;- function(\n    n = NULL, \n    power = NULL,\n    p0 = NULL,\n    p1 = NULL, \n    alpha = 0.05,\n    TOL = 1e-6) {\n  \n  if (is.null(n) + is.null(power) + is.null(p0) + is.null(p1) + is.null(alpha) != 1) {\n    stop(\"exactly one of n, power, p0, p1, and alpha need to be NULL\")\n  }\n  \n  oout &lt;- list(n = n, power = power, p0 = p0, p1 = p1, alpha = alpha)\n  \n  pfun &lt;- function(n, p0, p1, alpha) {\n    za2 &lt;- stats::qnorm(alpha / 2)\n    stats::pnorm(sqrt(p0 * (1 - p0) / (p1 * (1 - p1))) * (za2 +\n                   abs(p0 - p1) * sqrt(n) / sqrt(p0 * (1 - p0))))\n  }\n  \n  if (is.null(power)) {\n    oout$power &lt;- pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)\n  } else if (is.null(n)) {\n    z1a2 &lt;- stats::qnorm(1 - alpha / 2)\n    zp &lt;- stats::qnorm(power)\n    oout$n &lt;- p0 * (1 - p0) * (z1a2 + zp * sqrt(p1 * (1 - p1) / (p0 * (1 - p0))))^2 / (p1 - p0)^2\n    oout$n &lt;- ceiling(oout$n)\n  } else if (is.null(p0)) {\n    rp0 &lt;- function(p0) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp0(p0 = TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp0, interval = c(TOL, p1))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp0(p0 = 1 - TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp0, interval = c(p1, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p0 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(p1)) {\n    rp1 &lt;- function(p1) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp1(p1 = TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp1, interval = c(TOL, p0))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp1(p1 = 1 - TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp1, interval = c(p0, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p1 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(alpha)) {\n    ralpha &lt;- function(alpha) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    rout &lt;- stats::uniroot(f = ralpha, interval = c(TOL, 1-TOL))\n    oout$alpha &lt;- rout$root\n  }\n  \n  if (any(oout$p0[!is.na(oout$p0)] * (1 - oout$p0[!is.na(oout$p0)]) * oout$n &lt; 5)) {\n    warning(\"too small sample size\")\n  }\n  return(oout)\n}\n\n\nAssumes the sample size is large enough to use the central limit theorem (\\(np_0(1-p_0) \\geq 5\\)).\nSuppose we wish to test the hypothesis that women with a sister history of breast cancer are at higher risk of developing breast cancer themselves. Suppose the prevalence rate of breast cancer is 2% among 50 to 54 year-old US women, whereas it is 5% among women with a sister history. We wish to interval 500 women 50 to 54 years old with a sistory history of the disease. What is the power of such a study assuming that we conduct a two-sided test with \\(\\alpha = 0.05\\)?\n\n# 0.9655\nb1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 500\n\n$power\n[1] 0.9655\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05\n\n\nHow many women should we interview in the study proposed to achieve 90% power?\n\n# 341\nb1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 341\n\n$power\n[1] 0.9\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05"
  },
  {
    "objectID": "05_tests/two_sample_t.html",
    "href": "05_tests/two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women containt eh systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\nExercise: A study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)"
  },
  {
    "objectID": "05_tests/05_two_sample_t.html",
    "href": "05_tests/05_two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women containt eh systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\nExercise: A study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)\n\n\n\n\nUnpaired (Equal Variance)\n\nConsider the lead data that you can read about here.\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\n\nWe are interested in if the exposed and control groups have the same mean full scale IQ. Let’s explore the data\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet \\(X_i\\) be the \\(i\\)th IQ score in the control group, let \\(Y_i\\) be the \\(i\\)th IQ score in the exposed group.\nThen we assume that \\(X_i \\sim \\mathrm{N}(\\mu_1, \\sigma^2)\\) and \\(Y_i \\sim \\mathrm{N}(\\mu_2, \\sigma^2)\\), and that all observations are independent.\nWe use t.test() to run a two-sample \\(t\\)-test.\n\nThe quantitative variable is to the left of the tilde ~\nThe variable encoding the two groups is to the right of the tilde\nIf we assume equal variances in each group, we set var.equal = TRUE\n\n\nt.test(iqf ~ Group, data = lead, var.equal = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.67  0.0977       118   -0.845      9.91\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWe can verify this result manually (you would never do this in real life, but you might on an exam).\n\n## Get summary statistics of the two groups\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(mean = mean(iqf), var = var(iqf), n = n()) -&gt;\n  sumdf\nxbar &lt;- sumdf$mean[[1]]\nybar &lt;- sumdf$mean[[2]]\ns2x &lt;- sumdf$var[[1]]\ns2y &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## Calculate pooled sample standard deviation\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\n\n## Calculate t-statistic\ntstat &lt;- (xbar - ybar) / (s * sqrt(1 / n1 + 1 / n2))\n\n## compare to t distribution with n1 + n2 - 2 df\npval &lt;- 2 * pt(-abs(tstat), df = n1 + n2 - 2)\n\n## Get confidence intervals\nlower &lt;- (xbar - ybar) - qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nupper &lt;- (xbar - ybar) + qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\n\nc(pval = pval, lower = lower, upper = upper)\n\n    pval    lower    upper \n 0.09772 -0.84454  9.90917 \n\n\nExercise: Is there any difference between exposed and control groups when it comes to the finger-wrist tapping test in dominant hand (maxfwt)? Assume equal variances.\n\n\nExercise: A sample of eight 35- to 39-year-old non-pregnant, premenoposaul OC users have a mean systoolic blood pressure (SBP) of 132.82 mm Hg and a sample standard deviation of 15.34 mm Hg. A different sample of 21 non-pregnant, premenopausal, non-OC users have a mean SBP of 127.44 mm Hg and a sample standard deviation of 18.23 mm Hg. What can be said about the underlying mean difference in blood pressure between the two groups? Provide a measure of how sure we are that there is a difference, and provide some interval estimate for this difference. Assume equal variances.\n\n\n\nTest for Equal Variance\n\nSuppose we have \\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y_i \\sim N(\\mu_2,\\sigma_2^2)\\).\nIt is possible to test \\(H_0: \\sigma_1 = \\sigma_2\\) versus \\(H_1: \\sigma_1 \\neq \\sigma_2\\).\nFolks don’t typically do this test because:\n\nIt is very sensitive to the normality assumption. Conversely, the \\(t\\)-test is not.\nThe \\(t\\)-test with equal variances is relatively robust to violations in the equal variance assumption.\nFolks typically just use the unequal variances \\(t\\)-test below.\n\nBut if your boss asks you run such a test, use var.test().\n\nvar.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\nMultiple parameters; naming those columns num.df, den.df\n\n\n# A tibble: 1 × 9\n  estimate num.df den.df statistic p.value conf.low conf.high method alternative\n     &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      \n1     1.65     73     45      1.65  0.0719    0.955      2.76 F tes… two.sided  \n\n\nThis test is based on the ratio of the variances \\(s_1^2 / s_2^2\\). Under the null, this follows a \\(F\\)-distribution with \\(n_1 - 1\\) numerator degrees of freedom and \\(n_2 - 1\\) denominator degrees of freedom.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(var = var(iqf), n = n()) -&gt;\n  sumdf\nvar1 &lt;- sumdf$var[[1]]\nvar2 &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## for fstat &gt; 1\nfstat1 &lt;- var1 / var2\n2 * pf(q = fstat1, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)\n\n[1] 0.07194\n\n## For fstat &lt; 1\nfstat2 &lt;- var2 / var1\n2 * pf(q = fstat2, df1 = n2 - 1, df2 = n1 - 1, lower.tail = TRUE)\n\n[1] 0.07194\n\n\n\n\n\nUnpaired (Unequal Variance)\n\nWhen you don’t want to assume equal variances (typically the case), just use the default settings of t.test() that has var.equal = FALSE.\n\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\nt.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.77  0.0797      112.   -0.545      9.61\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\nDon’t bother memorizing Satterthwaite’s approximation for the degrees of freedom. Just do this in the computer.\nExercise: Is there a difference in finger tapping between groups? Don’t assume equal variances.\n\n\n\nPower and Sample Size Calculations in Two-sample \\(t\\)-tests\n\nUse power.t.test().\nIn the two-sample case, n means the sample size per group. It assumes the sample sizes are equal, so the total sample size is 2 * n.\nIt also assumes the standard deviations are equal, so you need to use a pooled estimate of the standard deviation.\nIf you need more precise power or sample size calculations, then those exist.\n\nBut I think these calculations are so much guess work that the error of assuming equal sample sizes is less than the error of the wild guesses you are giving it.\n\nE.g., suppose we have the OC user exercise above as a pilot experiment.\n\nxbar &lt;- 132.82\ns2x &lt;- 15.34^2\nn1 &lt;- 8\n\nybar &lt;- 127.44\ns2y &lt;- 18.23^2\nn2 &lt;- 21\n\nLet’s calculate a pooled estimate of the variance, and we will assume that is the true variance for the power calculations.\n\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\ns\n\n[1] 17.53\n\n\nLet’s suppose we want a power of 0.8. Then the sample size we would need is 168 per group:\n\npower.t.test(\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  power = 0.8, \n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 167.6\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf a researcher can only afford \\(n = 100\\) per gropu, then the power calculation would be 0.58:\n\npower.t.test(\n  n = 100,\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.5793\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExercise: Suppose a new drug is proposed to lower intraocular pressure (IOP) among people with glaucoma. It is anticipated that mean IOP will drop by 8 mm Hg after 1 month with the new drug. The comparison group will get the standard drug, which is anticipated to have a mean drop in IOP of 5 mm Hg after 1 month. It is expected that the sd of change within each group will be 10 mm Hg. How many subjects need to be enrolled to achieve 90% power if an equal sample size is planned within each group and a two-sided test with \\(\\alpha\\) = 0.05 will be used?"
  },
  {
    "objectID": "06_nonparametric/06_wilcoxin_1.html",
    "href": "06_nonparametric/06_wilcoxin_1.html",
    "title": "One-sample Nonparametric Tests",
    "section": "",
    "text": "We will load the boneden data, that you can read about here.\n\nlibrary(tidyverse)\nlibrary(broom)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nRecall that this is a twin study with a heavier smoking twin and a lighter smoking twin. We are interested in the difference in bone density between these pairs of twins.\n\nboneden |&gt;\n  mutate(ls_diff = ls1 - ls2) -&gt;\n  boneden\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 10)\n\n\n\n\n\n\n\n\n\n\nSign test\n\nWe might not be willing to assume a normal distribution for these data. An alternative is the sign test, which tests whether the number of positive values is greater than expected by chance.\nThe sign test is the exact same thing as the binomial test using the number of positive values as the data and \\(p = 1/2\\) as the null.\nFirst, we calculate the number of positive values and the total number of values.\n\nboneden |&gt;\n  summarize(x = sum(ls_diff &gt; 0), n = sum(ls_diff != 0))\n\n# A tibble: 1 × 2\n      x     n\n  &lt;int&gt; &lt;int&gt;\n1    28    41\n\n\nWe can now do a normal approximation proportion test, or an exact binomial test on these data\n\nprop.test(x = 28, n = 41, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.683      4.78  0.0288         1    0.518     0.814 1-sample … two.sided  \n\n\n\nbinom.test(x = 28, n = 41, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.683        28  0.0275        41    0.519     0.819 Exact bin… two.sided  \n\n\nSo we have some evidence that there is a difference in median bone density between the two twins (\\(p \\approx 0.028\\)).\n\n\n\nWilcoxin signed-rank test\n\nIf we are willing to at least assume that the data are symmetric, we can gain some power by doing the Wilcoxin signed-rank test\n\nwilcox.test(ls_diff ~ 1, data = boneden, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      618.  0.0156 Wilcoxon signed rank test with continuity corre… two.sided  \n\n\nI set exact = FALSE because the exact test as implemented in R cannot handle ties.\nThe idea of the Wilcoxin signed-rank test is that if you rank the observations by the magnitude (absolute value), then the rank sum of the negative numbers should be about the same as the rank sum of the positive numbers.\n\nset.seed(77)\n## Draw data from a distribution with median 0\nx &lt;- rnorm(1000) \n## Calculate ranks of absolue values\nr &lt;- rank(abs(x))\n## Mean rank of positive numbers\nsum(r[x &gt; 0])\n\n[1] 256612\n\n## Mean rank of negative numbers\nsum(r[x &lt; 0])\n\n[1] 243888\n\n\nIf the median is positive, you would expect the magnitude of the positive numbers to be larger than the maginude of the negative numbers.\n\nset.seed(77)\n## Draw data from a distribution with median 1\nx &lt;- rnorm(1000, mean = 1) \n## Calculate ranks of absolue values\nr &lt;- rank(abs(x))\n## Mean rank of positive numbers\nsum(r[x &gt; 0])\n\n[1] 465908\n\n## Mean rank of negative numbers\nsum(r[x &lt; 0])\n\n[1] 34592\n\n\nA visualization: The true distribution is the curve, symmetric about 0. A sample of 10 individuals is the rug plot. The ranks of the magnitudes are above the rug plot. The red numbers are the rank sums of the positive and negative values.\n\n\n\n\n\n\n\n\n\nSame as before, but the true distribution is symmetric about 1.\n\n\n\n\n\n\n\n\n\nExercise: Do a sign test, Wilcoxin signed-rank test, and a paired \\(t\\)-test for the difference in femoral neck density between the two twins. Which do you think is more appropriate?\n\n\nExercise: What is the signed-rank sum from these data. Do it by hand then check your work using R.\n\nx &lt;- c(11, 39, 75, 60, 66, -28, 55, 61, -69, -7)"
  },
  {
    "objectID": "06_nonparametric/06_wilcoxin_2.html",
    "href": "06_nonparametric/06_wilcoxin_2.html",
    "title": "Two-sample Nonparametric Methods",
    "section": "",
    "text": "Consider the study on the effects of lead, described here.\n\nlibrary(tidyverse)\nlibrary(broom)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\n\nWilcoxin Rank-sum test (AKA Mann-Whitney \\(U\\) test)\n\nThe Wilcoxin rank-sum test is the nonparametric version of the two-sample \\(t\\)-test.\nThe null is that the distributions of the two samples are the same. The alternative is that they are the same except shifted. That is,\n\n\\(X_i \\sim F_1\\) for \\(i = 1,\\ldots,n_1\\) and \\(Y_j \\sim F_2\\) for \\(j = 1,\\ldots,n_2\\). Here, \\(F_1\\) and \\(F_2\\) are the CDF’s of samples 1 and 2, respectively.\n\\(H_0\\): \\(F_1 = F_1\\)\n\\(H_1\\): \\(F_1(x) = F_2(x + \\Delta)\\) for some \\(\\Delta \\neq 0\\).\n\nThis shift interpretation is only valid if the distributions are about the same but shifted (checkable using histograms). If the distributions vary wildly, the hypothesis test is actually\n\n\\(H_0\\): \\(P(X &gt; Y) = P(Y &gt; X)\\)\n\\(H_1\\): \\(P(X &gt; Y) \\neq P(Y &gt; X)\\)\n\nThe idea of this test is that the rank-sums should be about the same on average (if \\(n_1 = n_2\\))\n\nset.seed(68)\ndf &lt;- tibble(\n  value = c(rnorm(20), rnorm(20)),\n  group = c(rep(1, 20), rep(2, 20))\n)\ndf |&gt;\n  mutate(rank = rank(value)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1   413\n2     2   407\n\n\nBut, if the two distributions differ by some location shift, then the distribution shifted up will have higher ranks on average, and so a larger rank sum.\n\nset.seed(68)\ndf &lt;- tibble(\n  value = c(rnorm(20), rnorm(20, mean = 1)),\n  group = c(rep(1, 20), rep(2, 20))\n)\ndf |&gt;\n  mutate(rank = rank(value)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1   303\n2     2   517\n\n\nVisualization: If the two distributions are the same, the average rankings should be about the same (when \\(n_1 = n_2\\)). Sample of 5 individuals from each group (rug plot) from distribution whose PDF is plotted. The ranks are the numbers above the rug plots. The rank sum of each sample is in red.\n\n\n\n\n\n\n\n\n\nVisualization: If one distribution is shifted, the rankings for the shifted to the right distribution will in general be larger than expected than if the distributions were the same.\n\n\n\n\n\n\n\n\n\nWe can evaluate if the exposed and the control groups have different distributions of IQF\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nUse wilcox.text() to run a Wilcoxin rank-sum test.\n\nResponse variable is to the left of the tilde ~\nGrouping variable is to the right of the tilde\ndata: The data frame that containst hte variables\n\n\nwilcox.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     1996.   0.114 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\nThe \\(p\\)-value above was pretty large, so we don’t have evidence of a difference of IQ between the two groups.\nExercise: Calculate the rank sum statistic using these data. First by hand and then using R.\n\ndf &lt;- tibble(\n  group = c(1, 1, 1, 2, 2, 2),\n  val = c(80, -36, -83, 63, 79, 93)\n)\n\n\n\nExercise: The Wilcoxin rank-sum test is most often used for ordinal data. E.g. consider the Werry-Weiss-Peters scale for hyperactivity (as reported by parents), which goes from 0 for no hyperactivity to 4 for severe hyperactivity. Is there a difference in hyperactivity between the exposed and control groups? Do an EDA and then answer with a formal hypothesis test.\n\n\n\nPermutation Tests\n\nThe the distribution of the two groups are indeed the exact same, then hypothetically we could arbitrarily choose which individuals belong to which group and the distribution of the rank sum should be the same.\nThis is the idea of the permutation test.\nYou generate a null distribution via:\n\nRandomly assign group labels to individuals\nCalculate the rank-sum statistic\nRepeat 1 and 2 many many times.\n\nIf the null is true, then our observed rank sum statistic should be about the same as the rank sum statistics from this null distribution. So we calculate a \\(p\\)-value by seeing how extreme our observed rank sum statistic is.\nYou can randomly assign labels by randomly permuting them with sample().\n\nset.seed(1)\nlead |&gt;\n  filter(!is.na(iqf)) |&gt; ## always remove NA's first\n  mutate(rank = rank(iqf)) |&gt;\n  select(iqf, rank, Group) -&gt;\n  lead_sub\nlead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  head()\n\n# A tibble: 6 × 4\n    iqf  rank Group   new_group\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    \n1    70   4   control exposed  \n2    85  40   control control  \n3    86  46   control control  \n4    76  15.5 control control  \n5    84  36   control exposed  \n6    96  81.5 control control  \n\n\nSo one iteration of generating the null distribution would be to first choose each individual’s group, the calculate the rank sum statistic.\n\nlead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  group_by(new_group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nsumdf$rsum[[1]]\n\n[1] 4622\n\n\nYou can replicate this process with replicate().\n\nrout &lt;- replicate(n = 1000, expr = {\n  lead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  group_by(new_group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nsumdf$rsum[[1]]\n})\n\nOur observed rank-sum statistic is\n\nlead_sub |&gt; \n  group_by(Group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nrobs &lt;- sumdf$rsum[[1]]\nrobs\n\n[1] 4770\n\n\nOur observed rank-sum statistic is a little rare:\n\ndata.frame(rsum = rout) |&gt;\n  ggplot(aes(x = rsum)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\") +\n  geom_vline(xintercept = robs, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nWe can quantify how rare by seeing how many null rank sum statistics are at or above our observed statistic. This is our permutation test \\(p\\)-value\n\n2 * mean(robs &lt;= rout)\n\n[1] 0.12\n\n\nFor large \\(n\\), this will be about the same as the normality approximation from the Wilcoxin rank-sum test:\n\nwilcox.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     1996.   0.114 Wilcoxon rank sum test with continuity correcti… two.sided"
  },
  {
    "objectID": "07_cat/07_binom.html",
    "href": "07_cat/07_binom.html",
    "title": "Categorical Tests",
    "section": "",
    "text": "library(broom)\n\n\nTwo-sample Binomial Test\n\nWe have the following 2x2 contingency table from a study comparing age of a mother at her first birth against breast cancer status. The hypothesis is that women who have their first births later in life are at higher risks of breast cancer.\n\n\n\n\n\n\n\n\nAge at First Birth\nStatus\nTotal\n\n\nCase\nControl\n\n\n\n\n≥30\n683\n1498\n2181\n\n\n≤29\n2537\n8747\n11284\n\n\nTotal\n3220\n10245\n13465\n\n\n\n\n\n\n\nLet \\(n_1 = 3220\\) and \\(n_2 = 10245\\) be the sample sizes among case and control women, respectively.\nLet \\(x_1 = 683\\) and \\(x_2 = 1498\\) be the number of women older than 30 at first birth for case and control women, respectively.\nOur model is the \\(X_1 \\sim \\text{Binom}(n_1, p_1)\\) and \\(X_2 \\sim \\text{Binom}(n_2, p_2)\\).\nOur hypotheses are\n\n\\(H_0\\): \\(p_1 = p_2\\)\n\\(H_1\\): \\(p_1 \\neq p_2\\)\n\nYou can run this test in R via\n\nprop.test(x = c(683, 1498), n = c(3220, 10245)) |&gt;\n  tidy()\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic  p.value parameter conf.low conf.high method    \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1     0.212     0.146      77.9 1.09e-18         1   0.0500    0.0818 2-sample …\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nIf we did this manually (which you will only do for exams and homeworks, not in real life), we first calculate the estimated proportions\n\np1hat &lt;- 683 / 3220\np2hat &lt;- 1498 / 10245\n\nWe then calculate the pooled estimated proportion, which is our estimate if the null is true\n\nphat &lt;- (683 + 1498) / (3220 + 10245)\n\nOur test statistic (I’m skipping the continuity correction)\n\nz &lt;- (p1hat - p2hat) / sqrt((1 / 3220 + 1 / 10245) * phat * (1 - phat))\nz\n\n[1] 8.853\n\n\nAnd our \\(p\\)-value compares this to the standard normal\n\n2 * pnorm(-abs(z))\n\n[1] 8.545e-19\n\n\nExercise: A study looked at the effects of oral contraceptive (OC) use on heart disease in women 40 to 44 years of age. The researchers found that among 5000 current OC users at baseline, 13 women developed a myocardial infarction (MI) over a 3-year period, whereas among 10,000 never-OC users, 7 developed an MI over a 3-year period. Assess the statistical significance of the results. Do this both “by hand” and using prop.test(). State your results.\n\n\n\nContingency Table Perspective\n\nIn this study design, we collected case women and control women, and measured their age.\nIf we would have run the test accidentally assuming that we had collected younger and older women, and measured their cancer status, then it turns out that we would have gotten the exact same \\(p\\)-value.\n\nprop.test(x = c(683, 1498), n = c(3220, 10245))$p.value\n\n[1] 1.092e-18\n\nprop.test(x = c(683, 2537), n = c(2181, 11284))$p.value\n\n[1] 1.092e-18\n\n\nYou can consider the binomial test, then, as a test for association between two variables that each are binary (categorical with only two categories).\nTo run the equivalent test of association using a contingency table, first create it using matrix():\n\ntab &lt;- matrix(c(683, 1498, 2537, 8747), nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(Age = c(\"≥30\", \"≤29\"), Status = c(\"Case\", \"Control\"))\ntab\n\n     Status\nAge   Case Control\n  ≥30  683    1498\n  ≤29 2537    8747\n\n\nThe dimnames() code above sets the row names (first) and the column names (second) via a list object.\nThen just plug it into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                                           \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                            \n1      77.9 1.09e-18         1 Pearson's Chi-squared test with Yates' continuit…\n\n\nExercise: From OC exercise above, insert these data into a 2x2 contingency table. Then run a chi-squared test for homogeneity. Verify that your results are the same as in the first exercise.\n\n\n\nFisher’s Exact Test\n\nThe above methods are only valid for large \\(n\\) (expected counts at least 5 in every cell).\nIf this is not a valid assumption, then you can use fisher.test() to run an exact test (controls Type I error for all \\(n\\), not just large \\(n\\)).\nThe syntax is the exact same as chisq.test()\n\ntab\n\n     Status\nAge   Case Control\n  ≥30  683    1498\n  ≤29 2537    8747\n\nfisher.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 6\n  estimate  p.value conf.low conf.high method                        alternative\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;      \n1     1.57 5.87e-18     1.42      1.74 Fisher's Exact Test for Coun… two.sided  \n\n\nFor large \\(n\\), the chi-squared and Fisher tests will provide about the same values. So why use chisq.test()? Sometimes, approximate methods can be better. But my opinion is that the stated benefits are minor compared to the benefit of controlling type I error exactly. So I would always use the Fisher test.\nExercise: Researchers collected information on salt diet versus cardiovascular death. Run a Fisher’s exact test using the below table to evaluate if diet is associated with cardiovascular death. How does it compare to the chi-squared test?\n\n\n\n\n\n\n\n\nCause of death\nType of diet\nTotal\n\n\nHigh salt\nLow salt\n\n\n\n\nNon-CVD\n2\n23\n25\n\n\nCVD\n5\n30\n35\n\n\nTotal\n7\n53\n60"
  },
  {
    "objectID": "07_cat/07_mcnemar.html",
    "href": "07_cat/07_mcnemar.html",
    "title": "McNemar’s Test",
    "section": "",
    "text": "library(broom)\n\n\nContingency Table Approach\n\nWomen were matched into pairs based on age and clinical characteristics. One woman of each pair was given treatment A and the other treatment B. The doctors then followed the women to see which survived and which died within 5 years.\n\n\n\n\n\n\n\n\nA\nB\nTotal\n\n\nSurvived\nDied\n\n\n\n\nSurvived\n510\n16\n526\n\n\nDied\n5\n90\n95\n\n\nTotal\n515\n106\n621\n\n\n\n\n\n\n\nIf we have a 2x2 contingency taable with matched pairs as the sampling unit, we can put it into R using matrix(), as with other contingency tables.\n\ntab &lt;- matrix(c(510, 16, 5, 90), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(A = c(\"Survived\", \"Died\"), B = c(\"Survived\", \"Died\"))\ntab\n\n          B\nA          Survived Died\n  Survived      510   16\n  Died            5   90\n\n\nTo run McNemar’s test, we can run mcnemar.test() for the large-sample approach\n\nmcnemar.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      4.76  0.0291         1 McNemar's Chi-squared test with continuity correc…\n\n\nThis is the exact same as running prop.test() on the discordant pairs.\n\nprop.test(x = 16, n = 5 + 16, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.762      4.76  0.0291         1    0.525     0.909 1-sample … two.sided  \n\n\nFor an exact test, we can run binom.test() on the discordant pairs.\n\nbinom.test(x = 16, n = 5 + 16, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.762        16  0.0266        21    0.528     0.918 Exact bin… two.sided  \n\n\nThis is best for small sample sizes.\nRule of thumb: 20 or more discordant pairs is enough for McNemar’s test. Fewer than that and use the binomial method.\nBut for this sample size, asymptotic approaches are fine.\n\n\n\nRaw Data Approach\n\nNow suppose we don’t have a 2x2 contingency table of pairs, but just two binary variables.\nE.g., a mall device and a trained observer assess if a person is hypertensive. The data are as follows\n\ndf &lt;- data.frame(\n  mall = c(\"-\", \"-\", \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"-\", \n           \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"-\"),\n  trained = c(\"-\", \"-\", \"-\", \"+\", \"-\", \"-\", \"-\", \"+\", \"+\", \"-\", \n              \"-\", \"-\", \"-\", \"-\", \"+\", \"-\", \"-\", \"-\", \"-\", \"-\")\n)\ndf\n\n   mall trained\n1     -       -\n2     -       -\n3     +       -\n4     +       +\n5     -       -\n6     +       -\n7     -       -\n8     +       +\n9     +       +\n10    -       -\n11    +       -\n12    +       -\n13    -       -\n14    +       -\n15    -       +\n16    +       -\n17    +       -\n18    -       -\n19    -       -\n20    -       -\n\n\nYou can create the contingency table with table() and then run mcnemar.test().\n\ntab &lt;- table(df$mall, df$trained)\ntab\n\n\n    - +\n  - 9 1\n  + 7 3\n\nmcnemar.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      3.12  0.0771         1 McNemar's Chi-squared test with continuity correc…\n\n\nOr run the exact test, which you should here since there are 8 discordant pairs, which is less than our rule of thumb of 20.\n\nbinom.test(x = 7, n = 7 + 1) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.875         7  0.0703         8    0.473     0.997 Exact bin… two.sided  \n\n\nAn alternative to first creating the contingency table is to just put each variable in mcnemar.test() (as long as the sample size is large enough.\n\nmcnemar.test(x = df$mall, y = df$trained) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      3.12  0.0771         1 McNemar's Chi-squared test with continuity correc…\n\n\n\n\n\nExercise\n\nExercise (from Rosner): A twin design is used to study age-related macular degeneration (AMD), a common eye disease of the elderly that results in substantial losses in vision. Suppose we contact 66 twinships in which one twin has AMD and the other twin does not. The twins are given a dietary questionnaire to report their usual diet. We find that in 10 twinships the AMD twin takes multivitamin supplements and the normal twin does not. In 8 twinships the normal twin takes multivitamin supplements and the AMD twin does not. In 3 twinships both twins take multivitamin supplements, and in 45 twinships neither twin takes multivitamin supplements.\n\nWhat test can be used to assess whether there is an association between AMD and taking multivitamin supplements?\nAre AMD and taking multivitamin supplements significantly associated based on these data?"
  },
  {
    "objectID": "07_cat/07_larger.html",
    "href": "07_cat/07_larger.html",
    "title": "Larger Contingency Tables",
    "section": "",
    "text": "library(broom)\n\n\nSuppose we measure case and control status (for breast cancer) for various ages at first birth.\n\n\n\n\n\n\n\n\nAge at First Birth\nAge at First Birth\nTotal\n\n\n&lt;20\n20-24\n25-29\n30-34\n≥35\n\n\n\n\nCase\n320\n1206\n1011\n463\n220\n3220\n\n\nControl\n1422\n4432\n2893\n1092\n406\n10245\n\n\nTotal\n1742\n5638\n3904\n1555\n626\n13465\n\n\n\n\n\n\n\nAs in the 2x2 case, we use matrix() to insert the data. Just change the nrow and ncol arguments to represent the number of rows and columns\n\ntab &lt;- matrix(\n  c(320, 1206, 1011, 463, 220,\n    1422, 4432, 2893, 1092, 406),\n  nrow = 2, ncol = 5, byrow = TRUE)\ndimnames(tab) &lt;- list(status = c(\"Case\", \"Control\"),\n                      age = c(\"&lt;20\", \"20-24\", \"25-29\", \"30-34\", \"≥35\"))\ntab\n\n         age\nstatus     &lt;20 20-24 25-29 30-34 ≥35\n  Case     320  1206  1011   463 220\n  Control 1422  4432  2893  1092 406\n\n\nYou then just plug this into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                    \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      130. 3.30e-27         4 Pearson's Chi-squared test\n\n\nAs in the 2x2 case, you can generate a contingency table from raw data using table().\nE.g., from the lead data, suppose that we are interested in testing if there is an association between lead_grp and sex.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nWe can create this contingency table by table()\n\ntab &lt;- table(lead$sex, lead$lead_grp)\ntab\n\n\n         control current exposed previous exposed\n  female      32               7                9\n  male        46              17               13\n\n\nAnd we can plug this into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.14   0.565         2 Pearson's Chi-squared test\n\n\nAlternatively, we could plug the two variables under consideration from the raw data frame directly into chisq.test().\n\nchisq.test(x = lead$sex, y = lead$lead_grp) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.14   0.565         2 Pearson's Chi-squared test\n\n\nExercise (from Rosner): We are interested in studying the relationship between the prevalence of hypertension in adolescents and ethnic group, where hypertension is defined as being above the 90th percentile for a child’s age, sex, and height, based on national norms.\n\nSuppose that 8 of 100 Caucasian adolescent girls, 12 out of 95 African-American adolescent girls, and 10 of 90 Hispanic adolescent girls are above the 90th percentile for blood pressure. What test can be used to assess whether there is an association between adolescent hypertension and ethnic group?\nImplement this test and report a two-tailed \\(p\\)-value."
  },
  {
    "objectID": "07_cat/07_kappa.html",
    "href": "07_cat/07_kappa.html",
    "title": "Cohen’s Kappa in R",
    "section": "",
    "text": "Some women ate beef and wrote down in two repeat surveys how much beef they ate. We are interested in how reliable this survey is. The data look like this:\n\n\n\n\n\n\n\n\nSurvey 1\nSurvey 2\nTotal\n\n\n≤1 Serving/Week\n&gt;1 Serving/Week\n\n\n\n\n≤1 Serving/Week\n136\n92\n228\n\n\n&gt;1 Serving/Week\n69\n240\n309\n\n\nTotal\n205\n332\n537\n\n\n\n\n\n\n\nThere is no base R function to calculate Cohen’s kappa (though there are some third party packages). I made a function that will do it for you:\n\n#' @param tab The 2x2 contingency table\n#' \n#' @return A list with the following elements\n#' \\itemize{\n#'   \\item{kappa: Cohen's kappa}\n#'   \\item{se: (estimated) standard error}\n#'   \\item{z: test statistic}\n#'   \\item{p_value: upper one-sided p-value against the null of kappa = 0}\n#' }\n#' \n#' @author David Gerard\ncohen_kappa &lt;- function(tab) {\n  stopifnot(nrow(tab) == ncol(tab))\n  n &lt;- sum(tab)\n  po &lt;- sum(diag(tab)) / n\n  a &lt;- rowSums(tab) / n\n  b &lt;- colSums(tab) / n\n  pe &lt;- sum(a * b)\n  kappa &lt;- (po - pe) / (1 - pe)\n  se &lt;- sqrt((pe + pe^2 - sum(a * b * (a + b))) / (n * (1 - pe)^2))\n  z &lt;- kappa / se\n  p_value &lt;- stats::pnorm(z, lower.tail = FALSE)\n  return(list(kappa = kappa, se = se, z = z, p_value = p_value))\n}\n\nFirst, put in contingency table in R as before\n\ntab &lt;- matrix(c(136, 92, 69, 240), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(survey1 = c(\"low\", \"high\"), survey2 = c(\"low\", \"high\"))\ntab\n\n       survey2\nsurvey1 low high\n   low  136   92\n   high  69  240\n\n\nThen use this function I wrote:\n\ncohen_kappa(tab = tab)\n\n$kappa\n[1] 0.3782\n\n$se\n[1] 0.04298\n\n$z\n[1] 8.799\n\n$p_value\n[1] 6.921e-19"
  },
  {
    "objectID": "02_descriptive/02_notes.html",
    "href": "02_descriptive/02_notes.html",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Observe \\(X_1, X_2, \\dots, X_n\\)\n\nSample of numeric values\nSubscript indexes the units\n\nExample: \\(X_i =\\) Birthweight for baby \\(i\\)\nMeasure of location = center of a sample (statistic) or a population (parameter)\nArithmetic Mean \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} (X_1 + X_2 + \\dots + X_n)\n\\]\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\) \\[\n  \\sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 5 + (-4)\n  \\]\n\\[\n  \\sum_{i=1}^2 X_i = X_1 + X_2 = 2 + 5\n  \\]\n\\[\n  \\sum_{i=2}^2 X_i = X_2 = 5\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{3} \\sum_{i=1}^3 X_i = \\frac{1}{3} (2 + 5 - 4) = \\frac{3}{3} = 1\n  \\]\n\\(\\bar{X}\\) is sensitive to extreme observations.\nExample with extreme value:\n\\[\n  X_4 = 3997\n  \\]\n\\[\n  \\frac{1}{4} \\sum_{i=1}^4 X_i = \\frac{1}{4} (2 + 5 - 4 + 3997) = \\frac{4000}{4} = 1000\n  \\]\nMedian\n\nFor \\(n\\) odd \\(\\Rightarrow \\left(\\frac{n+1}{2}\\right)\\)th largest observation.\nFor \\(n\\) even \\(\\Rightarrow\\) Average of \\(\\left(\\frac{n}{2}\\right)\\)th and \\(\\left(\\frac{n}{2} + 1\\right)\\)th largest observations.\n\nExample:\n\n\\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4 \\Rightarrow -4, 2, 5\\)\n\\[\n\\text{Median}(X) = 2\n\\]\nIf \\(X_4 = 3997\\)\n\\[\n\\text{Median}(X) = \\frac{2 + 5}{2} = 3.5\n\\]\n\nIf distribution is symmetric, \\(\\text{median}(X) \\approx \\bar{X}\\).\nMean follows the skew of distribution (dashed is median, dotted is mean):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse mean if total is important.\nUse median if lots of skew.\nA mode is a frequently occurring value.\nTypes of Modalities:\n\nUnimodal:\n\n\n\n\n\n\n\n\n\nBimodal:\n\n\n\n\n\n\n\n\n\nTrimodal:\n\n\n\n\n\n\n\n\n\n\nThe mode is typically not used as a real measure of center but rather as a way to describe distribution.\n\n\n\n\nSuppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\n\n\nExerciseSolution\n\n\nWhat is the mean menstrual cycle deviation from 4 weeks?\n\n\n\\[\n3.92 - 4 = -0.08\n\\]"
  },
  {
    "objectID": "02_descriptive/02_notes.html#arithmetic-mean",
    "href": "02_descriptive/02_notes.html#arithmetic-mean",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "The arithmetic mean ( {X} ) is defined as:\n[ {X} = _{i=1}^n X_i = (X_1 + X_2 + + X_n) ]\n\n\nGiven ( X_1 = 2 ), ( X_2 = 3 ), ( X_3 = 4 ):\n\nSum: ( _{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 3 + 4 )\nMean:\n\n[ {X} = _{i=1}^3 X_i = (2 + 3 + 4) = = 3 ]\nThe arithmetic mean ( {X} ) is sensitive to extreme observations.\n\n\n\nIf ( X_3 = 3997 ):\n[ {X} = _{i=1}^3 X_i = (2 + 3 + 3997) = = 1334 ]\nAs shown, the mean increases significantly due to the extreme value."
  },
  {
    "objectID": "02_descriptive/02_notes.html#mean-and-distribution-skew",
    "href": "02_descriptive/02_notes.html#mean-and-distribution-skew",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "Mean and Distribution Skew",
    "text": "Mean and Distribution Skew\n\nThe mean is affected by the skew of the distribution.\n\n\nRight Skew\nIn a right-skewed distribution:\n\n( &gt; )\nExample: Years of oral contraception use.\n\n\nlibrary(ggplot2)\nset.seed(123)\nright_skew &lt;- data.frame(value = rexp(1000, rate = 0.5))\n\nggplot(right_skew, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(value)), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nLeft Skew\nIn a left-skewed distribution:\nMean&lt;MedianMean&lt;Median\nExample: Relative humidity in summer in Ohio.\n\nset.seed(123)\nleft_skew &lt;- data.frame(value = -rexp(1000, rate = 0.5) + 5)\n\nggplot(left_skew, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(value)), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Left-Skewed Distribution\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nChoosing Mean or Median\nUse the mean if the total is important.\nUse the median if there is a lot of skew."
  },
  {
    "objectID": "02_descriptive/02_notes.html#measures-of-location",
    "href": "02_descriptive/02_notes.html#measures-of-location",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Observe \\(X_1, X_2, \\dots, X_n\\)\n\nSample of numeric values\nSubscript indexes the units\n\nExample: \\(X_i =\\) Birthweight for baby \\(i\\)\nMeasure of location = center of a sample (statistic) or a population (parameter)\nArithmetic Mean \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} (X_1 + X_2 + \\dots + X_n)\n\\]\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\) \\[\n  \\sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 5 + (-4)\n  \\]\n\\[\n  \\sum_{i=1}^2 X_i = X_1 + X_2 = 2 + 5\n  \\]\n\\[\n  \\sum_{i=2}^2 X_i = X_2 = 5\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{3} \\sum_{i=1}^3 X_i = \\frac{1}{3} (2 + 5 - 4) = \\frac{3}{3} = 1\n  \\]\n\\(\\bar{X}\\) is sensitive to extreme observations.\nExample with extreme value:\n\\[\n  X_4 = 3997\n  \\]\n\\[\n  \\frac{1}{4} \\sum_{i=1}^4 X_i = \\frac{1}{4} (2 + 5 - 4 + 3997) = \\frac{4000}{4} = 1000\n  \\]\nMedian\n\nFor \\(n\\) odd \\(\\Rightarrow \\left(\\frac{n+1}{2}\\right)\\)th largest observation.\nFor \\(n\\) even \\(\\Rightarrow\\) Average of \\(\\left(\\frac{n}{2}\\right)\\)th and \\(\\left(\\frac{n}{2} + 1\\right)\\)th largest observations.\n\nExample:\n\n\\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4 \\Rightarrow -4, 2, 5\\)\n\\[\n\\text{Median}(X) = 2\n\\]\nIf \\(X_4 = 3997\\)\n\\[\n\\text{Median}(X) = \\frac{2 + 5}{2} = 3.5\n\\]\n\nIf distribution is symmetric, \\(\\text{median}(X) \\approx \\bar{X}\\).\nMean follows the skew of distribution (dashed is median, dotted is mean):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse mean if total is important.\nUse median if lots of skew.\nA mode is a frequently occurring value.\nTypes of Modalities:\n\nUnimodal:\n\n\n\n\n\n\n\n\n\nBimodal:\n\n\n\n\n\n\n\n\n\nTrimodal:\n\n\n\n\n\n\n\n\n\n\nThe mode is typically not used as a real measure of center but rather as a way to describe distribution.\n\n\n\n\nSuppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\nExercise: What is the mean menstrual cycle deviation from 4 weeks?"
  },
  {
    "objectID": "03_prob/03_notes.html",
    "href": "03_prob/03_notes.html",
    "title": "Chapter 3 Notes: Probability",
    "section": "",
    "text": "Sample Space: Set of all possible outcomes.\nEvent: Any set of outcomes (subset of sample space).\nProbability of Event: Frequency of the event over a large number of trials.\nExample: Tuberculin skin test to detect tuberculosis\n\n\n\nOutcome\nProb\n\n\n\n\nPositive\n0.1\n\n\nNegative\n0.7\n\n\nUncertain\n0.2\n\n\n\nSample Space: \\(\\{ \\text{Positive}, \\text{Negative}, \\text{Uncertain} \\}\\)\nPossible Events:\n\n\n\nEvent\nProb\n\n\n\n\n\\(\\{ \\text{Positive} \\}\\)\n0.1\n\n\n\\(\\{ \\text{Negative} \\}\\)\n0.7\n\n\n\\(\\{ \\text{Uncertain} \\}\\)\n0.2\n\n\n\\(\\{ \\text{Positive, Negative} \\}\\)\n0.8\n\n\n\\(\\{ \\text{Positive, Uncertain} \\}\\)\n0.3\n\n\n\\(\\{ \\text{Negative, Uncertain} \\}\\)\n0.9\n\n\n\\(\\{ \\text{Positive, Negative, Uncertain} \\}\\)\n1\n\n\n\nNotation:\n\n\\(P(E) =\\) Probability of event \\(E\\)\nE.g., if \\(E = \\{\\text{Positive, Negative}\\}\\), then \\(P(E) = 0.8\\)\n\nTwo events are mutually exclusive if they cannot both happen at the same time.\nExample:\n\n\\(E_1 = \\{\\text{Positive, Negative}\\}\\)\n\\(E_2 = \\{\\text{Uncertain}\\}\\)\n\\(E_3 = \\{\\text{Negative, Uncertain}\\}\\)\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\\(E_1\\) and \\(E_3\\) can both happen if the outcome is “Negative.”\n\\(E_2\\) and \\(E_3\\) can both happen if the outcome is “Uncertain.”"
  },
  {
    "objectID": "03_prob/04_notes.html",
    "href": "03_prob/04_notes.html",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "A random variable assigns numbers to outcomes in the sample space.\nExample:\n\nNumber of children with retinitis pigmentosa\nNumber of individuals with leukemia\netc…\n\nBasically, an event that is a number.\nDiscrete random variable: Can count them (but may be infinite).\n\nTypically \\(0, 1, 2, 3, \\dots\\)\n\nContinuous random variable: Cannot count them.\n\nTypically some interval \\((-\\infty, \\infty)\\), \\([0,1]\\), etc.\n\nDenote random variables with capital letters \\(X, Y, Z\\), etc.\n\n\n\n\n\n\n\nProbability Mass Function (PMF)\n\n\n\nAssigns a probability to a possible value \\(r\\). Denote this probability by \\(P(X = r)\\).\n\n\n\nA PMF is a function of \\(r\\), not \\(X\\). \\(X\\) is used to denote the random variable.\nHypertensive Example: Let \\(X =\\) number of patients in a trial of 4 who have improved blood pressure.\n\n\\(P(X = 0) = 0.008\\)\n\\(P(X = 1) = 0.076\\)\n\\(P(X = 2) = 0.265\\)\n\\(P(X = 3) = 0.411\\)\n\\(P(X = 4) = 0.240\\)\n\n\\(0 \\leq P(X = r) \\leq 1\\) for all \\(r\\)\n\\(\\sum_r P(X = r) = 1\\)\n\nSum over all possible \\(r\\) is 1.\n\n\n\n\n\n\n\n\nExpected Value\n\n\n\nMeasure of center of a PMF (also known as mean).\n\\[\nE(X) = \\sum_r r \\cdot P(X = r) = \\mu\n\\] - Again, summing over all possible \\(r\\).\n\n\n\nHypertensive Example:\n\\[\nE(X) = 0 \\cdot 0.008 + 1 \\cdot 0.076 + 2 \\cdot 0.265 + 3 \\cdot 0.411 + 4 \\cdot 0.240 = 2.8\n\\]\nCan be interpreted as the average value of \\(X\\) across many trials.\nNote: \\(\\mu\\) is a population parameter, not a statistic, which is a function of observed data.\nExample: Across many trials, we might see:\n\n\n\n\\(x\\)\nfrequency\n\n\n\n\n0\n0\n\n\n1\n9\n\n\n2\n24\n\n\n3\n48\n\n\n4\n19\n\n\n\n\\[\n\\bar{X} = 0 \\cdot 0 + 1 \\cdot \\frac{9}{100} + 2 \\cdot \\frac{24}{100} + 3 \\cdot \\frac{48}{100} + 4 \\cdot \\frac{19}{100} = 2.77\n\\]\n\n\n\n\n\n\n\nVariance\n\n\n\nMeasure of spread.\n\\[\n  \\text{Var}(X) = \\sum_r (r - \\mu)^2 \\, P(X = r) = \\sigma^2\n  \\]\n\n\n\n\\(SD(X) = \\sqrt{\\sigma^2} = \\sigma\\)\nNote: \\(\\text{Var}(X) = E(X^2) - E(X)^2 = \\sum_r r^2 \\, P(X = r) - \\left(\\sum_r r \\, P(X = r)\\right)^2\\)\nLarger \\(\\sigma\\) means more variable.\n\n\n\n\n\n\n\nCumulative Distribution Function (CDF)\n\n\n\n\\[\nF(x) = P(X \\leq x) = \\text{Probability } X \\text{ is less than or equal to } x\n\\]\n\n\n\nExample: Hypergeometric distribution\n\n\\(F(0) = 0.008\\)\n\\(F(1) = 0.008 + 0.076\\)\n\\(F(2) = 0.008 + 0.076 + 0.265\\)\n\\(F(3) = 0.008 + 0.076 + 0.265 + 0.411\\)\n\\(F(4) = 0.008 + 0.076 + 0.265 + 0.411 + 0.240\\)\n\nUseful for probability calculation:\n\\[\nP(1 \\leq X \\leq 3) = P(X \\leq 3) - P(X \\leq 0)\n\\]\n\n\nExerciseSolution\n\n\n\\(X =\\) number of boys in a family of 4\n\n\n\n\\(r\\)\n\\(P(X = r)\\)\n\n\n\n\n0\n\\(\\frac{1}{16}\\)\n\n\n1\n\\(\\frac{1}{4}\\)\n\n\n2\n\\(\\frac{3}{8}\\)\n\n\n3\n\\(\\frac{1}{4}\\)\n\n\n4\n\\(\\frac{1}{16}\\)\n\n\n\nCalculate \\(E(X)\\), \\(SD(X)\\), \\(F(X)\\)\n\n\n\n\\(E[X] = 0 \\cdot \\frac{1}{16} + 1 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{4} + 4 \\cdot \\frac{1}{16} = 2\\)\n\\(E[X^2] = 0^2 \\cdot \\frac{1}{16} + 1^2 \\cdot \\frac{1}{4} + 2^2 \\cdot \\frac{3}{8} + 3^2 \\cdot \\frac{1}{4} + 4^2 \\cdot \\frac{1}{16} = 5\\)\n\\(Var(X) = E[X^2] - E[X]^2 = 5 - 2^2 = 1\\)\n\\(SD(X) = \\sqrt{Var(X)} = \\sqrt{1} = 1\\)\nCDF:\n\n\\(F(0) = \\frac{1}{16}\\)\n\\(F(1) = \\frac{1}{16} + \\frac{1}{4}\\)\n\\(F(2) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8}\\)\n\\(F(3) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8} + \\frac{1}{4}\\)\n\\(F(4) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8} + \\frac{1}{4} + \\frac{1}{16}\\)\n\n\n\n\n\n\nSome distributions are seen in real data over and over again:\n\nBinomial: count out of \\(n\\)\nPoisson: count during some time interval\n\nThese have specific PDFs/CDFs.\nWe need to know about permutations/combinations to understand them.\nNumber of permutations of \\(n\\) things taken \\(k\\) times:\n\\[\n{}_nP_k = n(n-1)(n-2) \\dots (n-k+1) = \\frac{n!}{(n-k)!}\n\\]\nExample: Individuals \\(= A, B, C, D\\)\n\\[\n{}_4P_2 = \\frac{4!}{2!} = \\frac{4 \\cdot 3}{1} = 12\n\\]\nPossible permutations:\n\n\\(A, B\\)\n\\(A, C\\)\n\\(A, D\\)\n\\(B, A\\)\n\\(B, C\\)\n\\(B, D\\)\n\\(C, A\\)\n\\(C, B\\)\n\\(C, D\\)\n\\(D, A\\)\n\\(D, B\\)\n\\(D, C\\)\n\n\\({}_4P_3\\) Example:\n\nTree diagram shows all possible arrangements of \\(A, B, C, D\\) taken 3 at a time:\n\n \nWhat if order does not matter? For example, \\(\\{A, B, C\\} = \\{C, B, A\\}\\).\nThe number of combinations of \\(n\\) things taken \\(k\\) at a time:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\nThere are \\(\\frac{n!}{(n-k)!}\\) permutations of size \\(k\\).\n\nEach of those shares elements with \\(k! = k P_k\\).\nExample: \\(A, B, C, D \\quad {}_4C_3\\)\n\nPossible combinations:\n\n\\(A, B, C = A, C, B = B, A, C = B, C, A = C, A, B = C, B, A = 3!\\)\n\n\nDivide by \\(k!\\) to get the number of combinations.\n\n\n\n\nIf given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/04_notes.html#binomial-distribution",
    "href": "03_prob/04_notes.html#binomial-distribution",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\\(n\\) trials\nOutcome of each trial is “success” or “failure”\n\\(P(\\text{success}) = p\\) for each trial\nTrials are independent\n\n\nLet $X = $ # of successes\nThen \\(X \\sim \\text{Bin}(n, p)\\) (Distributed as Binomial)\nExample: White blood count\n\nLet $X = $ # of neutrophils out of 100 white blood cells\n\\(P(\\text{neutrophile}) = 0.6\\)\n\\(\\Rightarrow X \\sim \\text{Bin}(100, 0.6)\\)\n\nIf \\(X \\sim \\text{Bin}(n, p)\\) then\n\\[\nP(X = r) = \\binom{n}{r} p^r (1 - p)^{n - r}\n\\]\nExample: Suppose \\(X \\sim \\text{Bin}(3, 0.3)\\)\n\n\\(P(X = 2) = P(\\text{2 successes and 1 failure})\\)\n\\(P(SSF) = P(SF S) = P(F SS) = p^2 (1 - p)\\)\nSo \\(P(X = 2) = 3 \\, p^2 (1 - p) = \\binom{3}{2} p^2 (1 - p)\\)\n\nClaim: # of ways to order \\(r\\) successes and \\(n - r\\) failures is \\(\\binom{n}{r}\\)\n\nProof: Position \\(1, 2, \\dots, n\\)\nChoose \\(r\\) out of these to be \\(S\\), rest are \\(F\\)\n\nExample: $X = $ # of boys out of 5 children, \\(p = 0.51\\)\n\n\\(P(X = 2) = \\binom{5}{2} (0.51)^2 (0.49)^3 = 0.306\\)\n\n\\[\n\\binom{5}{2} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]\nCDF:\n\\[\nP(X \\leq x) = \\sum_{r=0}^{x} P(X = r) = \\sum_{r=0}^{x} \\binom{n}{r} p^r (1 - p)^{n - r}\n\\]\n\nNo simpler form.\n\nMean:\n\\[\nE(X) = \\sum_{r=0}^{n} r \\binom{n}{r} p^r (1 - p)^{n - r} = n \\, p\n\\]\n\nExpected # of successes $= $ # of trials $ P()$\n\nVariance:\n\\[\n\\text{Var}(X) = n \\, p (1 - p)\n\\]\n\n\\(n \\uparrow \\Rightarrow \\text{Var} \\uparrow\\)\n\\(p \\uparrow \\Rightarrow \\text{Var} \\uparrow\\)\nVariance is highest at \\(p = 0.5\\), smallest at \\(p = 0\\) or \\(1\\).\n\nBinomial functions in R:\n\ndbinom() = \\(P(X = r)\\)\npbinom() = \\(P(X \\leq x)\\)\nqbinom() = quantile\nrbinom() = random generation"
  },
  {
    "objectID": "03_prob/04_notes.html#poisson-distribution",
    "href": "03_prob/04_notes.html#poisson-distribution",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nCounts of rare events over some period of time or space.\nExample: # of typhoid cases in a year.\nExample: # of bacterial colonies on an agar plate.\nAssume:\n\nFor small time interval \\(\\Delta t\\), \\(P(\\text{success in } \\Delta t)\\) is about \\(\\lambda \\Delta t\\) (for some \\(\\lambda\\)).\n\\(P(\\text{more than 2 successes in } \\Delta t) \\approx 0\\)\nStationarity: \\(P(\\text{success})\\) about the same for all time intervals.\nIndependence: One success has no bearing on any other success.\n\nViolated, e.g., in epidemics.\n\n\nThen $X = $ # of “successes” in time \\(t\\)\n\n\\(X \\sim \\text{Pois}(\\mu)\\) such that \\(\\mu = \\lambda t\\)\n\n\\[\nP(X = k) = \\frac{e^{-\\mu} \\, \\mu^k}{k!}\n\\]\nNote:\n\nIf \\(X \\sim \\text{Pois}(\\mu)\\) over time \\(t\\) then \\(X \\sim \\text{Pois}(c \\mu)\\) over time \\(ct\\).\n\nExample:\n\n$X = $ # of typhoid deaths in 1 year\n\\(X \\sim \\text{Pois}(4.6)\\)\nLet $Y = $ # of typhoid deaths in half a year\n\\(Y \\sim \\text{Pois}\\left(\\frac{4.6}{2}\\right) = \\text{Pois}(2.3)\\)\n\nExample:\n\n$X = $ # of bacteria colonies in 100 cm²\n\\(X \\sim \\text{Pois}(2)\\)\n$Y = $ # of bacteria colonies in 1000 cm²\n\\(Y \\sim \\text{Pois}(20)\\)\n\nMean: If \\(X \\sim \\text{Pois}(\\mu)\\), \\(E(X) = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\mu\\)\nIf \\(X \\sim \\text{Pois}(\\lambda_1)\\) and \\(Y \\sim \\text{Pois}(\\lambda_2)\\) (and are independent), then \\(X + Y \\sim \\text{Pois}(\\lambda_1 + \\lambda_2)\\)\n\nNot generally true for other distributions (e.g., not for binomial).\n\nRelation to Binomial:\n\nIf \\(X \\sim \\text{Bin}(n, p)\\)\n\n\\(n\\) large \\((&gt; 100)\\)\n\\(p\\) small \\((&lt; 0.01)\\)\n\\(np\\) intermediate\n\nThen \\(X \\approx \\text{Pois}(np)\\)\n\n(Approximate)\n\n\nThis is used to justify Poisson in cases where we know \\(n\\) is large, but we don’t know it exactly.\nExample: $X = $ # of RNA molecules of a gene observed (on the order of 100)\n\nWe don’t know \\(n\\) but know it’s large.\nWe don’t know \\(p\\) but we know it’s small (because \\(X \\approx 100\\)).\nUse Poisson to model \\(X\\)!\n\nR functions\nExercises (4.24–4.29)\n\nof episodes for 1 child to have otitis media (ear disease) in 1 year is \\(\\text{Pois}(1.6)\\)\n\n4.24 What is the probability of getting 3 or more episodes in the first 2 years of life?\n\nSolution: $X = $ # in 2 years \\(\\sim \\text{Pois}(3.2)\\)\n\n\\[\n1 - \\text{ppois}(2, \\, \\text{lambda} = 3.2)\n\\] \\[\n= 0.6201\n\\]\n4.25 What is the probability of not getting any in the 1st year?\n\n$X = $ # in first year \\(\\sim \\text{Pois}(1.6)\\)\n\n\\[\n\\text{dpois}(x = 0, \\, \\text{lambda} = 1.6)\n\\] \\[\n= 0.2019\n\\]\n4.26 Probability two siblings will both have 3 or more episodes in the first year of life?\n\nAssumes independence.\n\n\\[\n= P(\\text{Sib 1 has } 2 \\text{ or more}) \\times P(\\text{Sib 2 has } 2 \\text{ or more})\n\\] \\[\n= 0.6201 \\times 0.6201 = 0.3845\n\\]\n\n4.27 Probability exactly 1 sibling will have 3 or more episodes (out of 2).\n\nLet $Y = $ # of siblings\n\n\\[\n  Y \\sim \\text{Bin}(2, 0.6201)\n  \\]\n\\[\n  P(Y = 1) = \\text{dbinom}(1, \\text{size} = 2, \\text{prob} = 0.6201)\n  \\]\n\\[\n  = 2 \\cdot 0.6201 \\cdot (1 - 0.6201)\n  \\]\n\\[\n  = 0.4712\n  \\]\n4.28 Probability neither will have 3 or more episodes in the first 2 years?\n\\[\n  (1 - 0.6201)^2 = 0.1443\n  \\]\n4.29 Expected number of siblings in a 2-sibling household who will have 3 or more episodes in the first two years.\n\\[\n  E(Y) = 2 \\cdot 0.6201 = 1.24\n  \\]"
  },
  {
    "objectID": "03_prob/05_notes.html",
    "href": "03_prob/05_notes.html",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "",
    "text": "A continuous random variable “takes on decimal values.”\nFor such random variables, the probability at any specific value is 0.\nExample:\n\n\\(\\Pr(\\text{a man is exactly } 6', 2.357921784123'') \\approx 0\\)\n\\(\\Pr(\\text{a man is exactly } 6'2'') \\approx 0\\)\nMen are a little above or a little below.\n\nBut, we know some regions are more likely than others.\n\n\\(\\Pr(5' \\leq X \\leq 7') &gt; \\Pr(0' \\leq X \\leq 1')\\)\n\nWe describe this intuition with a PDF.\nA Probability Density Function of a random variable \\(X\\) is a function \\(f\\) such that: \\[\n\\Pr(a \\leq X \\leq b) = \\text{area below curve between } a \\text{ and } b\n\\]\nThe CDF is again the \\(F(x) = \\Pr(X \\leq x)\\).\nExample: \\(X =\\) Serum triglyceride level\n\n\\(\\Pr(50 \\leq X \\leq 100)\\)\n\n\n\n\n\n\n\n\n\n\\(\\Pr(X \\leq 100)\\)\n\n\n\n\n\n\n\n\n\n\nExpected value: \\(\\mu\\), average \\(X\\) over many trials. \\[\n\\mu = E[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\\] where \\(f(x)\\) = density.\nVariance: Average squared distance. \\[\n\\sigma^2 = E\\left[(X - \\mu)^2\\right] = E(X^2) - \\mu^2\n\\] \\[\n\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx\n\\]\nMost common continuous distribution: Normal distribution\n\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right]\n\\]\nDensity depends on \\(\\sigma^2\\) (variance) and \\(\\mu\\) (mean).\n\nAlso, if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(E(X) = \\mu\\), \\(\\mathrm{Var}(X) = \\sigma^2\\).\nNormal distribution plot:\n\n\n\n\n\n\n\n\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different variances\n\n\n\n\n\n\n\n\n\nThe standard normal distribution is \\(N(0,1)\\).\nProperties:\n\n68–95–99.7 rule:\n\n68% of area within \\(\\pm 1\\sigma\\)\n95% of area within \\(\\pm 2\\sigma\\)\n99.7% of area within \\(\\pm 3\\sigma\\)\n\nSymmetric: \\(f(\\mu - x) = f(\\mu + x)\\)\n\\(\\mu =\\) median\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\)\nIf \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) are independent, then \\[\nZ = X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\n\\]\n\nWe denote PDF of standard normal by \\(\\phi(x)\\)\nThe CDF is \\(\\Phi(x) = \\Pr(X \\leq x)\\)\nExample:\n\nBlood Pressure \\(\\sim N(80, \\sqrt{144})\\)\nMild hypertension is \\(90 \\leq \\text{DBP} \\leq 100\\)\n\nUnits are in mmHg\n\nIndividuals are randomly sampled\nWhat is \\(\\Pr(\\text{mild hypertensive})\\)?\n\n\\[\\begin{align*}\n  \\Pr(90 \\leq X \\leq 100) &= \\Pr(X \\leq 100) - \\Pr(X &lt; 90)\\\\\n  &= \\texttt{pnorm(100, mean = 80, sd = sqrt(144))} - \\texttt{pnorm(90, mean = 80, sd = sqrt(144))}\\\\\n  &= 0.1545\n\\end{align*}\\]\nVisualizations of normal distribution regions XYZ IMAGE HERE\nExercise: Tree diameter \\(\\sim N(8, 2^2)\\) (in inches)\nWhat is the probability that the tree has diameter \\(&gt; 12\\) in?\nSolution:\n\\[\n1 - \\texttt{pnorm}(12, 8, 2) = 0.02275\n\\]\n\n\n\nIf \\(X_1, \\dots, X_n\\) are random variables and\n\\[\nL = \\sum_{i=1}^n c_i X_i \\quad \\text{for } c_i \\text{ constants (not r.v.s)}\n\\] then\n\\[\n\\mathbb{E}[L] = \\sum_{i=1}^n c_i \\mathbb{E}[X_i]\n\\] \\[\n\\mathrm{Var}(L) = \\sum_{i=1}^n c_i^2 \\mathrm{Var}(X_i)\n\\]\nIf the \\(X_i\\) are also normally distributed, then\n\\[\nL \\sim N(\\mathbb{E}[L], \\mathrm{Var}(L))\n\\]\nExample:\nLet \\(X\\) = serum creatinine level for Caucasian individual\nLet \\(Y\\) = serum creatinine level for Black individual\n\\[\nX \\sim N(1.3, 0.125), \\quad Y \\sim N(1.5, 0.25)\n\\]\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\[\nZ = \\frac{1}{2}X + \\frac{1}{2}Y \\Rightarrow Z \\sim N(1.4, 0.175)\n\\]\n\n\\(\\mathbb{E}(Z) = \\frac{1}{2}(1.3 + 1.5) = 1.4\\)\n\\(\\mathrm{Var}(Z) = \\frac{1}{4}(0.125 + 0.25) = 0.175\\)\n\nNormal Approximation to Binomial (rule of thumb):\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\) and \\(np(1-p) \\geq 5\\), then \\[\nX \\approx N(np, np(1-p))\n\\]\nLet \\(X \\sim \\mathrm{Bin}(n, p)\\), and let \\(Y \\sim N(np, np(1 - p))\\).\nThen with continuity correction: \\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\n\\]\nWe will use this for 2-sample binomial tests.\nWhy?\nLet \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials: \\[\nT_i =\n\\begin{cases}\n1 & \\text{w.p. } p \\\\\n0 & \\text{w.p. } 1 - p\n\\end{cases}\n\\]\nLet \\[\nX = T_1 + T_2 + \\dots + T_n = \\sum T_i\n\\]\nThe Central Limit Theorem says normal for large \\(n\\).\nNormal Approximation to Poisson\nIf \\(X \\sim \\mathrm{Poisson}(\\mu)\\), then\n\\(Y \\sim N(\\mu, \\mu)\\)\n\nRule of thumb: for \\(\\mu \\geq 10\\)\n\\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq Y \\leq b + \\frac{1}{2}\\right)\n\\]\n\n\n\n\nExercise 5.12 – 5.13 of Rosner\n\nOf men aged 30–34 who have smoked:\n\n\\(X\\) = number of years a man has smoked\n\\(Y\\) = number of years smoked by women in age group\n\n\n\\[\nX \\sim N(12.8, 5.1^2), \\quad Y \\sim N(9.3, 3.2^2)\n\\]\nQ1: What proportion of men have smoked for more than 20 years? Women?\n\nMen: \\[\n1 - \\texttt{pnorm}(20, \\text{mean} = 12.8, \\text{sd} = 5.1) = 0.07901\n\\]\nWomen: \\[\n1 - \\texttt{pnorm}(20, \\text{mean} = 9.3, \\text{sd} = 3.2) = 0.004133\n\\]\n\nXYZ IMAGE HERE\nExercise 5.126 – 5.130\nChristmas Bird Count is a holiday tradition in a boring part of Massachusetts.\n\n\n\nYear\n\\(x_i\\)\n\n\n\n\n2005\n76\n\n\n2006\n47\n\n\n2007\n63\n\n\n2008\n53\n\n\n2009\n62\n\n\n2010\n64\n\n\n2011\n67\n\n\n\n\\[\n\\sum x_i = 432, \\quad \\sum x_i^2 = 27,\\!717\n\\]\n\n\nWhat is the mean number of birds?\n\\[\n\\bar{x} = \\frac{432}{7} = 61.71\n\\]\nWhat is the standard deviation?\n\\[\n\\frac{1}{7}(27717) - \\left(\\frac{432}{7}\\right)^2 = 78.78\n\\]\n\\[\n\\mathrm{SD} = \\sqrt{78.78} = 8.876\n\\]\nSuppose number of birds is normal with same mean and SD as previous years.\nWhat is the probability of at least 60 birds? Apply continuity correction.\n\\[\n1 - \\texttt{pnorm}(59.5, \\text{mean} = 61.71, \\text{sd} = 8.876) = \\boxed{0.5983}\n\\]\nFind “normal range” \\((L, U)\\) (integers) such that:\n\n\\(L\\) = 5th percentile\n\n\\(U\\) = 95th percentile\n\n\\[\n\\texttt{qnorm}(c(0.05, 0.95), \\text{mean} = 61.71, \\text{sd} = 8.876) = (52.51, 70.91)\n\\]\n\nSo the normal range is 52 to 80\n\nWhat is the probability that \\(X \\geq U\\) at least once during a 10-year period?\n\\[\n\\Pr(X \\geq U) = 1 - \\texttt{pnorm}(79.5, \\text{mean} = 61.71, \\text{sd} = 8.876) = 0.02252\n\\]\nLet \\(Y\\) = number of years \\(\\geq U\\)\n\\(Y \\sim \\mathrm{Bin}(10, 0.02252)\\)\n\\[\n\\Pr(Y \\geq 1) = 1 - \\Pr(Y = 0) = 1 - \\texttt{dbinom}(0, 10, 0.02252) = \\boxed{0.2037}\n\\]"
  },
  {
    "objectID": "03_prob/05_notes_old.html",
    "href": "03_prob/05_notes_old.html",
    "title": "Chapter 5 Notes: Continuous Distributions",
    "section": "",
    "text": "A continuous random variable “takes on decimal values.”\nFor such random variables, the probability at any specific value is \\(0\\).\nExample:\n\n\\(P(\\text{a man is exactly 6'2.35792471613\"}) \\approx 0\\)?\n\\(P(\\text{a man is exactly 6'2\"}) \\approx 0\\)\nMen are a little above or a little below.\n\nHowever, we know some regions are more likely than others.\n\n\\(P(5' \\leq X \\leq 7') &gt; P(0' \\leq X \\leq 1')\\)\n\nWe describe this intuition with a PDF (Probability Density Function).\nA Probability Density Function of a random variable \\(X\\) is a function, \\(f\\), where:\n\n\\(P(a \\leq X \\leq b) = \\text{area below curve between } a \\text{ and } b\\)\n\nThe CDF (Cumulative Distribution Function) is again the \\(F(x) = P(X \\leq x)\\).\nExample: Let \\(X = \\text{Serum triglyceride level}\\)\n::: {.cell} ::: {.cell-output-display}  ::: :::\n::: {.cell} ::: {.cell-output-display}  ::: :::\nExpected value, \\(\\mu\\), the average \\(X\\) over many trials:\n\n\\(\\mu = \\int_{-\\infty}^{\\infty} x f(x) \\, dx \\quad \\text{where } f(x) = \\text{density}\\)\n\nVariance: Average squared distance\n\n\\(\\sigma^2 = E\\left((X - \\mu)^2\\right) = E(X^2) - \\mu^2\\)\n\\(\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx\\)\n\nMost common continuous distribution: Normal distribution\n\n\\(f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2} \\frac{(x - \\mu)^2}{\\sigma^2}\\right) \\quad \\text{if } X \\sim N(\\mu, \\sigma^2)\\)\nFunction of \\(\\sigma^2\\) (variance) at \\(\\mu\\) (mean)\nAlso, if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(E(X) = \\mu\\), \\(Var(X) = \\sigma^2\\)\n\nNormal distribution curve:\n\nA typical bell-shaped curve centered at \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n\nThe standard normal distribution is \\(N(0, 1)\\).\nProperties:\n\n68-95-99.7 rule:\n\n68% of area within \\(\\pm 1\\sigma\\)\n95% of area within \\(\\pm 2\\sigma\\)\n99.7% of area within \\(\\pm 3\\sigma\\)\n\nSymmetric: \\(f(\\mu - x) = f(\\mu + x)\\)\nMean = median = \\(\\mu\\)\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma} \\Rightarrow Z \\sim N(0, 1)\\)\nIf \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\), then \\(Z = X + Y\\) is also normally distributed:\n\n\\(Z \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\)\n\n\nWe denote the PDF of the standard normal by \\(\\phi(x)\\).\nThe CDF is \\(\\Phi(x) = P(X \\leq x)\\).\nExample: Blood Pressure is \\(N(80, 144)\\)\n\nMild hypertension is \\(90 \\leq DBP \\leq 100\\)\nDuring a random day, what is \\(P(\\text{mild hypertensive})\\)?\nSolution:\n\n\\(P(90 \\leq X \\leq 100)\\)\n\\(= P(X \\leq 100) - P(X \\leq 90)\\)\n\\(= \\text{pnorm}(100, \\text{mean} = 80, \\text{sd} = \\sqrt{144}) - \\text{pnorm}(90, \\text{mean} = 80, \\text{sd} = \\sqrt{144})\\)\n\\(\\approx 0.1545\\)\nXYZ Finish this figure:\n\n\n\n\n\n\n\n\n\n\n\nExample: Tree diameter \\(\\sim N(8, 2^2)\\) (in inches)\n\nWhat is the probability that a tree has a diameter &gt; 12 inches?\nSolution: \\(1 - \\text{pnorm}(12, 8, 2) \\approx 0.02275\\)\n\nIf \\(X_1, \\dots, X_n\\) are random variables and \\[ L = \\sum_{i=1}^n c_i X_i \\] for \\(c_i\\) constants (not random variables), then\n\n\\(E(L) = \\sum_{i=1}^n c_i E(X_i)\\)\n\\(\\text{Var}(L) = \\sum_{i=1}^n c_i^2 \\text{Var}(X_i)\\)\n\nIf the \\(X_i\\) are also normally distributed, then \\(L \\sim N(E(L), \\text{Var}(L))\\)\nExample:\n\n\\(X =\\) serum creatinine level for Caucasian individual\n\\(Y =\\) serum creatinine level for Black individual\n\\(X \\sim N(1.3, 0.25)\\)\n\\(Y \\sim N(1.5, 0.25)\\)\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\(Z = \\frac{1}{2} X + \\frac{1}{2} Y\\)\nThen \\(Z \\sim N(1.4, 0.125)\\)\nCalculations:\n\n\\(E(Z) = \\frac{1}{2}(1.3) + \\frac{1}{2}(1.5) = 1.4\\)\n\\(\\text{Var}(Z) = \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) = 0.125\\)\n\n\nNormal Approximation to Binomial (rule of thumb):\n\nIf \\(X \\sim \\text{Bin}(n, p)\\) and \\(np(1 - p) \\geq 5\\), then \\(X \\approx N(np, np(1 - p))\\)\n\nLet \\(X \\sim \\text{Bin}(n, p)\\), \\(Y \\sim N(np, np(1 - p))\\).\n\nThen \\(P(a \\leq X \\leq b) \\approx P\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\\) (continuity correction).\n\nWe will use this for 2-sample binomial tests.\nWhy? Let \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials.\n\n\\(T_i = \\begin{cases} 1 & \\text{with probability } p \\\\ 0 & \\text{with probability } 1 - p \\end{cases}\\)\n\\(X = T_1 + T_2 + \\dots + T_n = \\sum_{i=1}^n T_i\\)\n\nCentral Limit Theorem says normal for large \\(n\\).\nNormal Approximation to Poisson:\n\nIf \\(X \\sim \\text{Pois}(\\mu)\\), then \\(Y \\sim N(\\mu, \\mu)\\).\nRule of thumb: For \\(\\mu \\geq 10\\), \\(P(a \\leq X \\leq b) \\approx P(a - 1 \\leq Y \\leq b + 1)\\)\n\nExercise 5.12 – 5.13 of Rosner:\n\nOf men 30-34 who have smoked:\n\n$X = $ # years a man has smoked\n$Y = $ # of years smoked by women in age group\n\\(X \\sim N(12.8, 5.1^2)\\)\n\\(Y \\sim N(9.3, 3.2^2)\\)\n\nQ1: What proportion of men have smoked for more than 20 years? Women?\n\nMen: \\(1 - \\text{pnorm}(20, \\text{mean} = 12.8, \\text{sd} = 5.1) \\approx 0.0794\\)\n\n\n\n\n\n\n\n\n\n\n\nWomen: \\(1 - \\text{pnorm}(20, \\text{mean} = 9.3, \\text{sd} = 3.2) \\approx 0.0041\\)\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.12 – 5.13:\n\nThe Christmas Bird Count is a holiday tradition in a boring part of Massachusetts.\nData:\n\nYear and Number of Birds (\\(x_i\\)):\n\n2006: 76\n2007: 47\n2008: 63\n2009: 53\n2010: 62\n2011: 69\n2012: 62\n\n\\(\\sum x_i = 432\\)\n\\(\\sum x_i^2 = 27,717\\)\n\nQuestions:\n\nWhat is the mean number of birds?\n\n\\[ \\bar{x} = \\frac{\\sum x_i}{n} = \\frac{432}{7} \\approx 61.71 \\]\n\nWhat is the standard deviation?\n\nCalculate the variance: \\[ \\text{Variance} = \\frac{\\sum x_i^2}{n} - \\left(\\frac{\\sum x_i}{n}\\right)^2 \\] \\[ = \\frac{27,717}{7} - \\left(\\frac{432}{7}\\right)^2 \\] \\[ \\approx 78.78 \\]\nStandard deviation: \\[ \\text{SD} = \\sqrt{\\text{Variance}} = \\sqrt{78.78} \\approx 8.876 \\]\n\nSuppose the number of birds is normally distributed with the same mean and SD as parts 1 and 2. What is the probability of at least 60 birds? Apply the continuity correction.\n\n\\[ 1 - \\text{pnorm}(59.5, \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx 0.5983 \\]\n\nFind the “normal range” \\((L, U)\\) (integers) such that:\n\n\\(L\\) = 15th percentile\n\\(U\\) = 85th percentile\n\\[ \\text{qnorm}(c(0.15, 0.85), \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx (52.51, 70.91) \\]\nThus, 52 to 70 is the “normal range.”\n\nWhat is the probability of having a count \\(\\geq U\\) at least once during a 10-year period?\n\n\\[ P(X \\geq U) \\approx 1 - \\text{pnorm}(79.5, \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx 0.02252 \\]\nLet $Y = $ # of years \\(\\geq U \\sim \\text{Bin}(10, 0.02252)\\).\n\\[ P(Y \\geq 1) = 1 - P(Y = 0) = 1 - \\text{dbinom}(0, 10, 0.02252) \\]\n\\[ \\approx 0.2037 \\]"
  },
  {
    "objectID": "04_est/06_notes.html",
    "href": "04_est/06_notes.html",
    "title": "Chapter 6 Notes: Estimation",
    "section": "",
    "text": "XYZ IMAGE HERE\n\nProbability: \\(X \\sim N(80, 12)\\)\n\nAssumed known\n\nWhat is \\(\\Pr(X &gt; 90)\\)?\n\nInference: Observe \\(X_i = 81, 78, 77, 89, \\ldots\\)\n\nAssume \\(X_i \\sim N(\\mu, \\sigma^2)\\)\n\nWhat are \\(\\mu\\) and \\(\\sigma^2\\)?\n\nEstimation: Guess parameter values from data\n\nPoint estimation: A single number is your best guess\n\nE.g., estimate \\(\\mu\\) with \\(\\bar{X}\\)\n\nInterval estimation: Get a range of likely values of a parameter\n\nConfidence intervals\n\n\nHypothesis testing: How sure are we a parameter is different from some value?\n\nE.g., \\(\\mu \\ne 0\\)\n\n\nXYZ IMAGE HERE\n\nGroup we are interested in: reference / target / study population\n\nGroup we have data about: sample\n\nSampling (e.g., SRS)\n\nParameter: summary of population\n\n\\(\\mu\\), \\(\\sigma^2\\), \\(p\\)\n\nStatistic: summary of sample\n\n\\(\\bar{X}\\), \\(s^2\\), \\(\\hat{p}\\)\n\nSimplest way to get a sample is by a simple random sample\n\nEach unit has an equal chance of being in sample\n\nRandom selection (via SRS) is distinct from:\n\nRandom assignment: randomly assign units to different groups (e.g., treatment vs. control)\n\nRandom selection: results generalizable to target population\n\nBecause sample is similar to population in terms of demographic variables, etc.\n\nRandom assignment: allows for claims of causality\n\nBecause all possible confounders are equal in the groups\n\nRandomized Clinical Trial (RCT): Random assignment of treatment to compare them\nNo causal claims without random assignment\nExample: tobramycin and gentamicin are antibiotics.\n\nTobramycin is more aggressive and has more side effects.\n\nEarly studies were not randomized and showed tobramycin performed worse. Why?\n\nDoctors gave sicker patients tobramycin\n\n\nRandomization guarantees equal number of sicker and less sick in each group (on average)\nDouble blind: neither doctor nor patient know treatment\n\nGuards against placebo effect\n\nSingle blind: doctor knows\nUnblinded: both know\nsampling in R\nEstimate mean:\n\nSuppose \\(E(X_i) = \\mu\\)\n\nNot necessarily normal\n\nEstimate \\(\\mu\\) with \\(\\hat{\\mu} = \\bar{X} = \\frac{1}{n} \\sum X_i\\)\n\n\\(\\hat{\\mu}\\) is an estimate of \\(\\mu\\)\n\n\\(\\bar{X}\\) is the sample mean\n\n\nSampling distribution: Distribution of statistic across many (hypothetical) samples\n\nXYZ IMAGE HERE\n\nUsed to describe properties of statistics\nProperties of \\(\\bar{X}\\)\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a random sample with\n\\(E(X_i) = \\mu\\), \\(\\operatorname{Var}(X_i) = \\sigma^2\\)\nThen:\n\n\\(E(\\bar{X}) = \\mu\\) (unbiased)\n\\(\\operatorname{Var}(\\bar{X}) = \\sigma^2/n\\)\n\nMore precise for larger \\(n\\) (consistent)\n\n\\(\\bar{X} \\approx N(\\mu, \\sigma^2/n)\\) for large \\(n\\) even if \\(X_i\\) are not also normal\n\n(Central Limit Theorem)\n\n\n\nStandard error: standard deviation of a statistic\n\n\\(\\operatorname{SE}(\\bar{X}) = \\sigma / \\sqrt{n}\\)\n\nEstimated Standard Error (Standard Error): estimated standard deviation of a statistic\n\nBoth just mean estimated SE when they say SE, typically\n\n\\(\\operatorname{SE}(\\bar{X}) = s / \\sqrt{n}\\)\n\nTypically don’t know \\(\\sigma^2\\), so estimate it with \\(s^2\\)\n\n\n\n\n\n\n\n\n\nStatistic\n\\(X_i\\)\n\\(\\bar{X}\\)\n\n\n\n\nMean\n\\(\\mu\\)\n\\(\\mu\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\\(\\sigma^2 / n\\)\n\n\nEstimated Variance\n\\(s^2\\)\n\\(s^2 / n\\)\n\n\nDistribution\nUnknown\n\\(N(\\mu, \\sigma^2 / n)\\) (for large \\(n\\))\n\n\n\n\nExample: Mean birthweight is 112 oz with standard deviation 20.6.\nWhat is the probability the mean of 10 birthweights will be between 98 and 126?\n\n\\[\n\\bar{X} \\sim N\\left(112, \\left(\\frac{20.6}{\\sqrt{10}}\\right)^2\\right) = N(112, 6.514^2)\n\\]\nXYZ IMAGE HERE\n\\[\n= \\text{pnorm}(126,\\ 112,\\ 6.514) - \\text{pnorm}(98,\\ 112,\\ 6.514)\n\\]\n\\[\n= 0.9684\n\\]\n\nInterval Estimation\n\n\\(\\bar{X}\\) is not exactly equal to \\(\\mu\\)\nWant range of likely values of \\(\\mu\\)\nKnow \\(\\bar{X} \\sim N(\\mu, \\sigma^2 / n)\\) for large \\(n\\)\n\n\n\\[\n\\Pr(-1.96 \\leq \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\leq 1.96) \\approx 0.95\n\\]\n\\[\n\\Pr(-1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}) \\approx 0.95\n\\]\n\\[\n\\Pr(\\bar{X} - 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}) \\approx 0.95\n\\]\n\n\\(\\bar{X} \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}\\) captures \\(\\mu\\) with probability 0.95\n\n95% CI\n\nEstimate ± multiplier × standard error\n\nCommon CI format\n\nMore generally:\n\n\\[\n\\bar{X} \\pm q_{\\text{norm}}(1 - \\frac{\\alpha}{2}) \\cdot \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\n\\((1 - \\alpha) \\cdot 100\\%\\) CI for \\(\\mu\\)\n\nXYZ IMAGE HERE\n\nR Interpretation of CI\n\nThe above only works when \\(\\sigma^2\\) is known\n\n\\(\\sigma^2\\) is never known\n\n\\(\\frac{\\bar{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}\\) (not \\(N(0, 1)\\))\n\n\\(t\\)-distribution with \\(n - 1\\) degrees of freedom\n\nOnly an exact result when \\(X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\)\n\nBut \\(t\\)-distribution tends to work better in small samples even when \\(X_i\\) are not normal\n\nFor large \\(n\\), \\(t_{n-1} \\approx N(0, 1)\\), so CLT is OK\nBell-shaped, centered at 0\nAs \\(\\text{df} \\downarrow\\), extreme values more likely\nAs \\(\\text{df} \\uparrow\\), extreme values less likely\nUse \\(t\\) because of added variability from using \\(s^2\\) instead of \\(\\sigma^2\\)\n\n\n\nR code for t-distribution\n\nRosner uses notation \\(t_{\\text{df}, p}\\) for the \\(p\\) quantile of a \\(t_{\\text{df}}\\) distribution\n\n\\[\nt_{\\text{df}, p} = \\text{qt}(p, \\text{df})\n\\]\nXYZ IMAGE HERE\n\\[\n\\Pr\\left(-t_{n-1, 1 - \\alpha/2} \\leq \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} \\leq t_{n-1, 1 - \\alpha/2} \\right) = 1 - \\alpha\n\\]\n\\[\n\\Rightarrow \\Pr\\left(-t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\]\n\\[\n\\Rightarrow \\Pr\\left(\\bar{X} - t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\]\n\\[\n\\bar{X} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\n\\]\n\n\\((1 - \\alpha) \\cdot 100\\%\\) CI for \\(\\mu\\)\nExercise: \\(n = 10\\), \\(\\bar{X} = 116.9\\), \\(s = 21.70\\)\n\nCalculate 90%, 95%, 99% CIs\n\nSolution:\n\n\\(\\text{qt}(0.95,\\ 9) = 1.833\\)\n\\(\\text{qt}(0.975,\\ 9) = 2.262\\)\n\\(\\text{qt}(0.995,\\ 9) = 3.25\\)\n\n\n\\[\n116.9 \\pm 1.833 \\cdot \\frac{21.70}{\\sqrt{10}}\n\\]\n\\[\n116.9 \\pm 2.262 \\cdot \\frac{21.70}{\\sqrt{10}}\n\\]\n\\[\n116.9 \\pm 3.25 \\cdot \\frac{21.70}{\\sqrt{10}}\n\\]\n\nNote:\n\nCI level \\(\\uparrow\\) (so \\(\\alpha \\downarrow\\)) \\(\\Rightarrow\\) larger intervals\n\\(n \\uparrow\\) \\(\\Rightarrow\\) smaller intervals\n\\(s^2 \\uparrow\\) \\(\\Rightarrow\\) larger intervals\n\n\n\n\nBone Density Case Study\n\nEstimate variance:\n\nEstimate \\(\\sigma^2\\) with \\(s^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (X_i - \\bar{X})^2\\)\n\nWhy \\(n - 1\\)?\n\n\\[\n\\bar{X} = \\operatorname{argmin}_a \\sum (X_i - a)^2 \\quad \\text{(ideal)}\n\\]\n\\[\n\\Rightarrow \\frac{1}{n} \\sum (X_i - \\bar{X})^2 \\leq \\frac{1}{n} \\sum (X_i - \\mu)^2\n\\]\n\nThis is too small\n\nDividing by \\(n - 1\\) makes it bigger\n\n\\[\n\\frac{s^2}{\\sigma^2 / (n - 1)} \\sim \\chi^2_{n - 1}\n\\]\n\nChi-squared distribution with \\(n - 1\\) degrees of freedom\nProperties of chi-squared:\n\nSupport \\(\\geq 0\\)\n\\(E(\\chi^2_{\\text{df}}) = \\text{df}\\)\ndf \\(\\downarrow\\) \\(\\Rightarrow\\) thicker tails (extreme events happen more often)\n\nCan use this to get confidence intervals for \\(\\sigma^2\\)\nLet \\(\\chi^2_{\\text{df}, p}\\) = \\(p^\\text{th}\\) quantile of a \\(\\chi^2_{\\text{df}}\\) distribution\n\n\\[\n\\Pr\\left( \\chi^2_{n-1, \\alpha/2} \\leq \\frac{(n-1)s^2}{\\sigma^2} \\leq \\chi^2_{n-1, 1-\\alpha/2} \\right) = 1 - \\alpha\n\\]\n\\[\n\\Rightarrow \\Pr\\left( \\frac{(n - 1)s^2}{\\chi^2_{n-1, 1 - \\alpha/2}} \\leq \\sigma^2 \\leq \\frac{(n - 1)s^2}{\\chi^2_{n-1, \\alpha/2}} \\right) = 1 - \\alpha\n\\]\n\\[\n\\frac{(n - 1)s^2}{\\chi^2_{n-1, 1 - \\alpha/2}} \\leq \\sigma^2 \\leq \\frac{(n - 1)s^2}{\\chi^2_{n-1, \\alpha/2}}\n\\]\n\n\\(100(1 - \\alpha)\\%\\) CI\n\nXYZ IMAGE HERE\n\nNotes:\n\nLess often constructed than CI for \\(\\mu\\)\nVery sensitive to violations of normality\n\nCLT does not save you\n\n\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent Bernoulli trials s.t.:\n\n\\[\nX_i =\n\\begin{cases}\n1 & \\text{w.p. } p \\\\\n0 & \\text{w.p. } 1 - p\n\\end{cases}\n\\]\n\nThen \\(X = \\sum X_i\\) = number of 1’s out of \\(n\\) trials\n\n\\(X \\sim \\text{Bin}(n, p)\\)\n\nExample: Estimate prevalence of malignant melanoma\n\nSample \\(n = 5000\\) individuals.\n\n\\(X_i = 1\\) if has melanoma, \\(0\\) if no\n\nGoal: Observe \\(X_i\\), estimate \\(p\\)\n\n\\[\n\\hat{p} = \\frac{1}{n} \\sum X_i = \\frac{X}{n} = \\text{proportion of 1's}\n\\]\n\n\\(E(\\hat{p}) = E\\left(\\frac{X}{n}\\right) = \\frac{1}{n} E(X) = \\frac{1}{n} n p = p \\Rightarrow\\) unbiased\n\\(\\operatorname{Var}(\\hat{p}) = \\operatorname{Var}\\left(\\frac{X}{n}\\right) = \\frac{1}{n^2} \\operatorname{Var}(X) = \\frac{1}{n^2} n p(1 - p) = \\frac{p(1 - p)}{n}\\)\n\nMore precise for larger \\(n\\) \\(\\Rightarrow\\) consistent\n\nEstimated standard error:\n\n\\[\n\\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\]\n\nGoal: Interval estimate of \\(p\\)\nFor large \\(n\\), \\(\\hat{p} \\sim N\\left(p,\\ \\frac{p(1 - p)}{n} \\right)\\)\n\nRule of thumb: \\(n \\hat{p}(1 - \\hat{p}) \\geq 5\\)\n\nLet \\(z_p = \\text{qnorm}(p)\\) = \\(p^\\text{th}\\) quantile of \\(N(0, 1)\\)\n\n\\[\n\\frac{ \\hat{p} - p }{ \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} } } \\approx N(0, 1)\n\\]\n\\[\n\\Rightarrow \\Pr\\left( z_{1 - \\alpha/2} \\leq \\frac{ \\hat{p} - p }{ \\sqrt{ \\hat{p}(1 - \\hat{p}) / n } } \\leq z_{\\alpha/2} \\right) \\approx 0.95\n\\]\n\\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\]\n\nThis is a \\((1 - \\alpha) \\cdot 100\\%\\) CI for \\(p\\)\nExercise: 10,000 women, 400 have breast cancer.\n\nWhat is a 95% CI for breast cancer incidence?\n\nSolution:\n\n\\(\\hat{p} = 0.04\\)\n\n\\(n \\hat{p}(1 - \\hat{p}) = 384 &gt; 5\\)\n\n\\(z_{0.975} = \\text{qnorm}(0.975) = 1.96\\)\n\n\n\\[\n0.04 \\pm 1.96 \\cdot \\sqrt{ \\frac{0.04 \\cdot (1 - 0.04)}{10{,}000} } = (0.03616,\\ 0.04384)\n\\]\n\nWhat if \\(n\\) is small? Use an exact method\nFind \\(p_1\\) and \\(p_2\\) such that:\n\n\\[\n\\frac{\\alpha}{2} = \\text{pbinom}(x,\\ \\text{size} = n,\\ \\text{prob} = p_1)\n\\]\n\\[\n\\frac{\\alpha}{2} = 1 - \\text{pbinom}(x - 1,\\ \\text{size} = n,\\ \\text{prob} = p_2)\n\\]\nXYZ IMAGE HERE\n(One diagram shows tail under \\(p_2\\), the other under \\(p_1\\))\n\n\nBinomial R code\n\nPoisson Estimation\n\\(Y \\sim \\text{Poi}(\\mu)\\) where \\(\\mu = \\lambda T\\)\nGoal: Estimate \\(\\lambda\\)\n\n\\(\\lambda\\) = incidence rate per unit time\n\n\\(T\\) = time\n\nExample: Leukemia rate in Woburn, MA.\n\n12,000 residents, 10 years, 12 children got leukemia\n\nA common unit in biostatistics is a person-year\n\n1 person being followed for 1 year\n\nNormalizes by number of people in a study\n\nE.g., 12 cases out of 20 is a lot more than 12 out of 20000\n\nExample: Woburn study had 12,000 people × 10 years = 120,000 person-years\n\n\\[\n\\hat{\\lambda} = \\frac{X}{T}\n\\]\n\\[\nE(\\hat{\\lambda}) = E\\left( \\frac{X}{T} \\right) = \\frac{E(X)}{T} = \\frac{\\lambda T}{T} = \\lambda \\quad \\text{(unbiased)}\n\\]\n\nWoburn: \\(\\hat{\\lambda} = \\frac{12 \\text{ cases}}{120{,}000 \\text{ person-years}} = 0.0001 \\text{ cases per person-year}\\)\nCancer rates low, so useful to multiply by 100,000\n\n\\[\n0.0001 \\text{ cases/person-year} = 10 \\text{ cases / 100,000 person-years}\n\\]\n\nInterval uses same strategy as exact binomial case:\n\nFind \\(\\mu_1\\) such that \\(\\Pr(X \\geq x \\mid \\mu_1) = \\alpha/2\\)\nFind \\(\\mu_2\\) such that \\(\\Pr(X \\leq x \\mid \\mu_2) = \\alpha/2\\)\nInterval: \\((\\mu_1,\\ \\mu_2)\\)\n\nUses CDF of Poisson\n\n\nInterval for \\(\\lambda\\) is:\n\n\\[\n\\left( \\frac{\\mu_1}{T},\\ \\frac{\\mu_2}{T} \\right)\n\\]\n\nUse poisson.test() in R\nOne-sided CI (OK to skip)\n\nOnly interested in lower or upper bounds\n\nE.g., compare cancer treatment survival rate against baseline of 30%.\nWant lower bound on treatment effect to see if it’s better than baseline\n\n\nLet \\(p_1\\) such that \\(\\Pr(p &gt; p_1) = 1 - \\alpha\\)\n\nRandom lower bound\n\nOr, find \\(p_2\\) such that \\(\\Pr(p &lt; p_2) = 1 - \\alpha\\)\n\nE.g., upper bound on incidence rate of some treatment group\n\nSince \\(\\frac{\\hat{p} - p}{\\sqrt{ \\hat{p}(1 - \\hat{p}) / n }} \\sim N(0, 1)\\):\n\n\\[\n\\Pr\\left(-z_{1 - \\alpha} &lt; \\frac{\\hat{p} - p}{\\sqrt{ \\hat{p}(1 - \\hat{p}) / n }} \\right) = 1 - \\alpha\n\\]\n\\[\n\\Rightarrow \\Pr\\left(z_{1 - \\alpha} &gt; \\frac{\\hat{p} - p}{\\sqrt{ \\hat{p}(1 - \\hat{p}) / n }} \\right) = 1 - \\alpha\n\\]\n\\[\n\\Rightarrow \\Pr\\left(\\hat{p} + z_{1 - \\alpha} \\cdot \\sqrt{ \\hat{p}(1 - \\hat{p}) / n } &gt; p \\right) = 1 - \\alpha\n\\]\n\\[\n\\Rightarrow p_2 = \\hat{p} + z_{1 - \\alpha} \\cdot \\sqrt{ \\hat{p}(1 - \\hat{p}) / n }\n\\]\n\nSimilarly:\n\n\\[\np_1 = \\hat{p} - z_{1 - \\alpha} \\cdot \\sqrt{ \\hat{p}(1 - \\hat{p}) / n }\n\\]\nXYZ IMAGE HERE\n\nExample: 40 out of 100 patients survive a new cancer treatment.\nWhat is the lower bound on \\(\\Pr(\\text{survive})\\)?\n(Upper 1-sided 95% CI)\n\n\\[\n\\hat{p} = \\frac{40}{100} = 0.4\n\\]\n\\[\nz_{1 - \\alpha} = \\text{qnorm}(0.95) = 1.645 \\quad (\\alpha = 0.05)\n\\]\n\\[\n\\hat{p} - z_{1 - \\alpha} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\]\n\\[\n= 0.4 - 1.645 \\cdot \\sqrt{ \\frac{0.4 \\cdot 0.6}{100} } \\approx 0.319\n\\]"
  },
  {
    "objectID": "02_descriptive/02_notes.html#measures-of-spread",
    "href": "02_descriptive/02_notes.html#measures-of-spread",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "Measures of Spread",
    "text": "Measures of Spread\n\nSpread: How far apart numbers are.\nRange: \\(\\text{Max} - \\text{Min}\\) (sensitive to extreme values).\nInter-quartile Range (IQR): \\(75^{\\text{th}}\\) percentile - \\(25^{\\text{th}}\\) percentile.\n\\(p^{\\text{th}}\\) percentile = value \\(V_p\\) such that \\(p\\%\\) of points are at or below \\(V_p\\).\n\nExample: Median = \\(50^{\\text{th}}\\) percentile.\n\nQuantile: in units of proportions instead of percents.\n\n\\(0.75\\) quantile = \\(75^{\\text{th}}\\) percentile.\n\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\)\n\n\\(\\frac{1}{3}\\) quantile = \\(-4\\)\n\\(\\frac{2}{3}\\) quantile = \\(2\\)\n\\(1\\) quantile = \\(5\\)\n\nWhat about the \\(40^{\\text{th}}\\) percentile?\n\nUse some average of \\(-4\\) and \\(2\\), but definition varies.\nThe quantile() function in R has 9 different definitions of how this imputation works. See ?quantile.\n\nVariance: Average of squared deviations.\n\\[\n  s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\n  \\]\n\n\\(n-1\\) because we lose some information by estimating \\(\\bar{X}\\).\n\nStandard Deviation: Square root of variance.\n\\[\n  s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2}\n  \\]\n\nPuts this measure of spread on the same scale as units (e.g., \\(oz\\) instead of \\(oz^2\\)).\n\nLet \\(y_i = c_1 x_i + c_2\\), then \\(s^2(y) = c_1^2 s^2(x)\\) and \\(s(y) = c_1 s(x)\\)\n\nOnly scaling affects variance and standard deviation.\n\nWhy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: What is \\(s^2(y)\\) when \\(y_i = c_1 x_i\\)?\nR Notebook on Mean / Median / SD / Variance"
  },
  {
    "objectID": "02_descriptive/02_notes.html#properties-of-barx",
    "href": "02_descriptive/02_notes.html#properties-of-barx",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Suppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\n\n\nExerciseSolution\n\n\nWhat is the mean menstrual cycle deviation from 4 weeks?\n\n\n\\[\n3.92 - 4 = -0.08\n\\]"
  },
  {
    "objectID": "03_prob/03_notes.html#exercises-3.53---3.63-of-rosner",
    "href": "03_prob/03_notes.html#exercises-3.53---3.63-of-rosner",
    "title": "Chapter 3 Notes: Probability",
    "section": "Exercises 3.53 - 3.63 of Rosner",
    "text": "Exercises 3.53 - 3.63 of Rosner\n\nWord problemConvert to math notation\n\n\nThe familial aggregation of respiratory disease is a well-established clinical phenomenon. However, whether this aggregation is due to genetic or environmental factors or both is somewhat controversial. An investigator wishes to study a particular environmental factor, namely the relationship of cigarette-smoking habits in the parents to the presence or absence of asthma in their oldest child age 5 to 9 years living in the household (referred to below as their offspring). Suppose the investigator finds that (1) if both the mother and father are current smokers, then the probability of their offspring having asthma is .15; (2) if the mother is a current smoker and the father is not, then the probability of their offspring having asthma is .13; (3) if the father is a current smoker and the mother is not, then the probability of their offspring having asthma is .05; and (4) if neither parent is a current smoker, then the probability of their offspring having asthma is .04.\n\n\n\nDefine:\n\n\\(M^+ =\\) Mother smokes\n\\(F^+ =\\) Father smokes\n\\(O^+ =\\) Offspring has asthma\n\nGiven probabilities:\n\n\n\nScenario\n\\(P(O^+ | \\text{Scenario})\\)\n\n\n\n\n\\(M^+ \\cap F^+\\)\n0.15\n\n\n\\(M^+ \\cap F^-\\)\n0.13\n\n\n\\(M^- \\cap F^+\\)\n0.05\n\n\n\\(M^- \\cap F^-\\)\n0.04\n\n\n\n\n\n\n\n\nExercise 1.Solution\n\n\nSuppose the smoking habits of the parents are independent and the probability that the mother is a current smoker is .4, whereas the probability that the father is a current smoker is .5. What is the probability that both the father and mother are current smokers?\n\n\nWe want to calculate \\(P(M^+ \\cap F^+)\\) given: \\(P(M^+) = 0.4\\), \\(P(F^+) = 0.5\\), \\(M^+ \\perp F^+\\) \\[\nP(M^+ \\cap F^+) = P(M^+) \\cdot P(F^+) = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 2.Solution\n\n\nConsider the subgroup of families in which the mother is not a current smoker. What is the probability that the father is a current smoker among such families? How does this probability differ from that calculated in Problem 1?\n\n\nWe want to calculate \\(P(F^+ | M^-)\\). However, since \\(M^+\\) and \\(F^+\\) are independent, \\(P(F^+ | M^-) = P(F^+) = 0.5\\).\n\n\n\n\nExercise 3.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 4.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 5.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 6.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 7.Solution\n\n\n\n\n\n\n\n\n\n\nGiven:\n\n\\(P(M^+ | F^+) = 0.6\\)\n\\(P(M^+ | F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\nCalculate \\(P(F^+ \\cap M^-)\\):\n\n\n\nIs \\(F^+ \\perp M^+\\)?\n\n\nFind \\(P(O^+)\\):\n\n\nCalculate \\(P(F^+ | O^+)\\):\n\n\nCalculate \\(P(M^+ | O^+)\\):"
  },
  {
    "objectID": "03_prob/03_notes.html#exercises-3.53---3.59-of-rosner",
    "href": "03_prob/03_notes.html#exercises-3.53---3.59-of-rosner",
    "title": "Chapter 3 Notes: Probability",
    "section": "Exercises 3.53 - 3.59 of Rosner",
    "text": "Exercises 3.53 - 3.59 of Rosner\n\nExercise 0Solution\n\n\nThe familial aggregation of respiratory disease is a well-established clinical phenomenon. However, whether this aggregation is due to genetic or environmental factors or both is somewhat controversial. An investigator wishes to study a particular environmental factor, namely the relationship of cigarette-smoking habits in the parents to the presence or absence of asthma in their oldest child age 5 to 9 years living in the household (referred to below as their offspring). Suppose the investigator finds that (1) if both the mother and father are current smokers, then the probability of their offspring having asthma is 0.15; (2) if the mother is a current smoker and the father is not, then the probability of their offspring having asthma is 0.13; (3) if the father is a current smoker and the mother is not, then the probability of their offspring having asthma is 0.05; and (4) if neither parent is a current smoker, then the probability of their offspring having asthma is 0.04.\nConvert this word problem to mathematical notation amenable to analysis.\n\n\n\nDefine:\n\n\\(M^+ =\\) Mother smokes\n\\(F^+ =\\) Father smokes\n\\(O^+ =\\) Offspring has asthma\n\nGiven probabilities:\n\n\n\nScenario\n\\(P(O^+ | \\text{Scenario})\\)\n\n\n\n\n\\(M^+ \\cap F^+\\)\n0.15\n\n\n\\(M^+ \\cap F^-\\)\n0.13\n\n\n\\(M^- \\cap F^+\\)\n0.05\n\n\n\\(M^- \\cap F^-\\)\n0.04\n\n\n\n\n\n\n\n\nExercise 1Solution\n\n\nSuppose the smoking habits of the parents are independent and the probability that the mother is a current smoker is 0.4, whereas the probability that the father is a current smoker is 0.5. What is the probability that both the father and mother are current smokers?\n\n\nWe want to calculate \\(P(M^+ \\cap F^+)\\) given: \\(P(M^+) = 0.4\\), \\(P(F^+) = 0.5\\), \\(M^+ \\perp F^+\\) \\[\nP(M^+ \\cap F^+) = P(M^+) \\cdot P(F^+) = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 2Solution\n\n\nConsider the subgroup of families in which the mother is not a current smoker. What is the probability that the father is a current smoker among such families? How does this probability differ from that calculated in Problem 1?\n\n\nWe want to calculate \\(P(F^+ | M^-)\\). However, since \\(M^+\\) and \\(F^+\\) are independent, \\(P(F^+ | M^-) = P(F^+) = 0.5\\). This is just the probability that the father is a smoker, as opposed to part 1 where we calculated the probability that both father and mother are smokers.\n\n\n\nSuppose, alternatively, that if the father is a current smoker, then the probability that the mother is a current smoker is 0.6; whereas if the father is not a current smoker, then the probability that the mother is a current smoker is 0.2. Also assume that statements 1, 2, 3, and 4 above hold.\n\nExercise 3Solution\n\n\nIf the probability that the father is a current smoker is 0.5, what is the probability that the father is a current smoker and that the mother is not a current smoker?\n\n\nConverting the word problem to math notation, we have::\n\n\\(P(M^+ | F^+) = 0.6\\)\n\\(P(M^+ | F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\nOur goal is to calculate \\(P(F^+ \\cap M^-)\\).\nBy the complement rule, we have \\(P(M^- | F^+) = 1 - 0.6\\). Using this, we have:\n\n\\[\nP(F^+ \\cap M^-) = P(M^- | F^+) \\cdot P(F^+) = (1 - 0.6) \\cdot 0.5 = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 4Solution\n\n\nAre the current smoking habits of the father and the mother independent? Why or why not?\n\n\nThere are a couple ways to check this:\n\nSee if \\(P(M^+|F^+) = P(M^+|F^-) = P(M^+)\\) (value of \\(F\\) does not matter). Any inequality here indicates non-independence.\nSee if \\(P(M^+ \\cap F^+) = P(M^+)P(F^+)\\), or any other combination, such as \\(P(M^- \\cap F^+) = P(M^-)P(F^+)\\). An inequality here indicates non-independence.\n\nThe easiest way is just the first. We know that \\(P(M^+|F^+) =  0.6 \\neq 0.2 = P(M^+|F^-)\\). So no, they are not independent.\n\n\n\n\nExercise 5Solution\n\n\nUnder the assumptions made in Problems 3 and 4, find the unconditional probability that the offspring will have asthma.\n\n\nWe want to calculate \\(P(O^+)\\):\nUsing the law of total probability: \\[\\begin{align*}\nP(O^+) &= P(O^+ | M^+ \\cap F^+) P(M^+ \\cap F^+) + P(O^+ | M^+ \\cap F^-) P(M^+ \\cap F^-)\\\\\n&+ P(O^+ | M^- \\cap F^+) P(M^- \\cap F^+) + P(O^+ | M^- \\cap F^-) P(M^- \\cap F^-)\n\\end{align*}\\]\nWe were given:\n\n\\(P(O^+ | M^+ \\cap F^+) = 0.15\\),\n\\(P(O^+ | M^+ \\cap F^-) = 0.13\\),\n\\(P(O^+ | M^- \\cap F^+) = 0.05\\), and\n\\(P(O^+ | M^- \\cap F^-) = 0.04\\),\n\\(P(M^+|F^+) = 0.6\\)\n\\(P(M^+|F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\n\nWe have\n\n\\(P(M^+ \\cap F^+) = P(M^+|F^+)P(F^+) = 0.6 * 0.5 = 0.3\\)\n\\(P(M^+ \\cap F^-) = P(M^+|F^-)P(F^+) = 0.2 * 0.5 = 0.1\\)\n\\(P(M^- \\cap F^+) = P(M^-|F^+)P(F^+) = (1 - 0.6) * 0.5 = 0.2\\)\n\\(P(M^- \\cap F^-) = P(M^-|F^-)P(F^+) = (1 - 0.2) * 0.5 = 0.4\\)\n\nSubstituting the values: \\[\nP(O^+) = (0.15 \\cdot 0.3) + (0.13 \\cdot 0.1) + (0.05 \\cdot 0.2) + (0.04 \\cdot 0.4) = 0.084\n\\]\n\n\n\n\nExercise 6Solution\n\n\nSuppose a child has asthma. What is the posterior probability that the father is a current smoker?\n\n\nWe want to calculate \\(P(F^+ | O^+)\\). Using Bayes rule we have\n\\[\nP(F^+ | O^+) = \\frac{P(F^+ \\cap O^+)}{P(O^+)}\n\\] We already calculated \\(P(O^+) = 0.084\\) from Exercise 5. The hard part is \\(P(F^+ \\cap O^+)\\). But we can use the law of total probability to get that. \\[\\begin{align*}\nP(F^+ \\cap O^+) &= P(F^+ \\cap O^+ \\cap M^+) + P(F^+ \\cap O^+ \\cap M^-)\\\\\n&= P(O^+ | F^+ \\cap M^+)P(F^+ \\cap M^+) + P(O^+ | F^+ \\cap M^-)P(F^+ \\cap M^-)\\\\\n&= 0.15 \\cdot 0.3 + 0.05 \\cdot 0.2\\\\\n&= 0.055,\n\\end{align*}\\] where we used \\(P(F^+ \\cap M^+) = 0.3\\) and \\(P(F^+ \\cap M^-) = 0.2\\) from Exercise 5.\nSubstituting values we have \\[\nP(F^+ | O^+) = \\frac{0.055}{0.084} = 0.6548.\n\\]\n\n\n\n\nExercise 7Solution\n\n\nWhat is the posterior probability that the mother is a current smoker if the child has asthma?\n\n\nWe want \\(P(M^+ | O^+)\\). Using Bayes rule we have \\[\nP(M^+ | O^+) = \\frac{P(O^+ \\cap M^+)}{P(O^+)}\n\\] We can use the exact same strategy as in Exercise 6.\n\\[\\begin{align*}\nP(M^+ \\cap O^+) &= P(F^+ \\cap M^+ \\cap O^+) + P(F^- \\cap M^+ \\cap O^+) \\text{ (law of total probability)}\\\\\n&= P(O^+ | F^+ \\cap M^+) P(F^+ \\cap M^+) + P(O^+ | F^- \\cap M^+) P(F^- \\cap M^+) \\text{ (conditional probability)}\\\\\n&= 0.15 \\cdot 0.3 + 0.13 \\cdot 0.1 \\text{ (given/calculated previously)}\\\\\n&= 0.058\n\\end{align*}\\]\nSubstituting in values we have \\[\nP(M^+ | O^+) = \\frac{0.058}{0.084} = 0.6905.\n\\]"
  },
  {
    "objectID": "03_prob/04_notes.html#provided-distribution-in-r",
    "href": "03_prob/04_notes.html#provided-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "If given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/04_notes.html#binomial-distribution-in-r",
    "href": "03_prob/04_notes.html#binomial-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Binomial Distribution in R",
    "text": "Binomial Distribution in R\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nThe underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\n\n1 - pbinom(q = 74, size = 1500, prob = 0.05)\n\n[1] 0.5165\n\npbinom(q = 74, size = 1500, prob = 0.05, lower.tail = FALSE)\n\n[1] 0.5165\n\n\n\n\n\n\nExerciseSolution\n\n\nSuppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\nWhat is the expected number of such women (out of 100) that we would expect to die in the next year?\n\n\n\n1 - stats::pbinom(q = 4, size = 100, prob = 0.009)\n\n[1] 0.002191\n\n\nYes, very unusual.\n100 * 0.009 = 0.9. So about 1 woman out of 100."
  },
  {
    "objectID": "03_prob/04_notes.html#poisson-distribution-in-r",
    "href": "03_prob/04_notes.html#poisson-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Poisson Distribution in R",
    "text": "Poisson Distribution in R\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts."
  }
]