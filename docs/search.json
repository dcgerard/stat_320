[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Unless otherwise noted, all data are from Rosner (2015), downloaded from the book’s companion site.\nI did some cleaning (changing 9’s to NA’s, recoding numerics to informative characters, etc).\n\nbetacar\nHigh doses of beta-carotene in food have been linked to a reduced cancer risk in some observational studies. A study considered four beta-carotene capsule preparations: Solatene (30 mg), Roche (60 mg), and two from BASF (30 mg and 60 mg). To test their effectiveness in raising plasma-carotene levels, 23 volunteers were randomized to one of the four preparations, taking one pill every other day for 12 weeks. The primary endpoint was the plasma carotene level after prolonged ingestion.\n\nbetacar.csv\n\nPrepar: Preparation.\n\nPossible values: SOL, ROCHE, BASF-30, BASF-60.\n\nId: Subject #\nBase1lvl: 1st Baseline Level\nBase2lvl: 2nd Baseline Level\nWk6lvl: Week 6 Level\nWk8lvl: Week 8 Level\nWk10lvl: Week 10 Level\nWk12lvl: Week 12 Level\n\n\n\n\nbirthweight\nThe birthweights of 1000 consecutive infants born at Boston City Hospital, which serves a low-income population.\n\nbirthweight.csv\n\nid: ID\nweight: Birthweight (oz)\n\n\n\n\nblood\nData from a case-control study investigated various plasma risk factors for breast cancer. The women were matched approximately by age at the time of blood draw, fasting status, and, when possible, current postmenopausal hormone (PMH) use at the time of blood draw. Each matched set included one case and either one or two controls, although some sets are incomplete due to missing data. The matching variable is matchid.\n\nblood.csv\n\nId: ID\nmatchid: Matched ID\ncase: Case/control.\n\nPossible values: case, control.\n\ncurpmh: Current PMH use.\n\nPossible values: yes, no.\n\nageblood: Age at blood draw\nestradol: Estradiol\nestrone: Estrone\ntestost: Testosterone\nprolactn: Prolactin\n\n\n\n\nboneden\nA study in Australia examined the relationship between bone density and cigarette smoking in middle-aged female twins with different smoking histories. Forty-one pairs of twins visited a hospital in Victoria, Australia, where their bone density was measured. Participants also completed questionnaires providing information on their tobacco use, alcohol, coffee, and tea consumption, calcium intake from dairy products, menopausal and reproductive history, fracture history, use of oral contraceptives or estrogen replacement therapy, and physical activity levels. Tobacco consumption was measured in pack-years, with one pack-year defined as smoking one pack of cigarettes per day for one year.\n\nboneden.csv\n\nID: ID\nAge: Age (yrs)\nzyg: Twin type.\n\nPossible values: mz, dz\n\nTwin 1 Lighter Smoking Twin\n\nht1: Height (cm)\nwt1: Weight (kg)\ntea1: Tea (cups/week)\ncof1: Coffee (cups/week)\nalc1: Alcohol (drinks/week)\ncur1: Current Smoking (cigarettes/day)\nmen1: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr1: Pack-years smoking\nls1: Lumbar spine (g/cm\\(^2\\))\nfn1: Femoral neck (g/cm\\(^2\\))\nfs1: Femoral shaft (g/cm\\(^2\\))\n\nTwin 2 Heavier Smoking Twin\n\nht2: Height (cm)\nwt2: Weight (kg)\ntea2: Tea (cups/week)\ncof2: Coffee (cups/week)\nalc2: Alcohol (drinks/week)\ncur2: Current Smoking (cigarettes/day)\nmen2: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr2: Pack-years smoking\nls2: Lumbar spine (g/cm\\(^2\\))\nfn2: Femoral neck (g/cm\\(^2\\))\nfs2: Femoral shaft (g/cm\\(^2\\))\n\n\n\n\n\nbotox\nA study on patients with piriformis syndrome compared the effects of three types of injections: triamcinolone with lidocaine (TL), a placebo (Placebo), and Botox (Botox). The patients were randomly assigned to these groups in a 3:1:2 ratio and received injections directly into the piriformis muscle. They were evaluated at 2 weeks, 1 month, and monthly up to 17 months, though many missed visits. Improvement in pain was measured on a scale from 0% to 100% (higher means more improved). The study involved 69 patients, with one having the condition in both legs. The goal was to compare the efficacy between the groups, considering age, gender, and affected side as potential influencing factors.\n\nbotox.csv\n\nID: ID\ngroup:\n\nPossible values: TL, Placebo, Botox\n\nside: left (L), middle (M), or right (R).\ngender: male or female\nage: in years\npain0: pain score month 0\npain05: pain score month 0.5\npain1: pain score month 1\npain2: pain score month 2\npain3: pain score month 3\npain4: pain score month 4\npain5: pain score month 5\npain6: pain score month 6\npain7: pain score month 7\npain8: pain score month 8\npain9: pain score month 9\npain10: pain score month 10\npain11: pain score month 11\npain12: pain score month 12\npain13: pain score month 13\npain14: pain score month 14\npain15: pain score month 15\npain16: pain score month 16\npain17: pain score month 17\n\n\n\n\nbreast\nThe data set includes 1200 postmenopausal women from the NHS, free of cancer in 1990. Of these, 200 were using postmenopausal hormones (PMH) in 1990, and 1000 had never used them. The study aimed to link PMH use in 1990 to breast cancer incidence from 1990 to 2000. Fifty-three women developed breast cancer during this period. PMH use was categorized by current use and duration of use in 1990, with separate durations for estrogen and estrogen plus progesterone. Each woman has a return date for the 1990 questionnaire and a follow-up date, which is either the date of breast cancer diagnosis or the date of the last questionnaire by 2000. The file also includes data on other breast cancer risk factors as of 1990.\n\nbreast.csv\n\nId: ID\ncase: Whether a woman had breast cancer or not.\n\nPossible values: case,control\n\nage: age\nagemenar: age at menarche\nagemenop: age at menopause\nafb: age at first birth\nparity: parity\nbbd: Benign Breast disease.\n\nPossible values: yes, no\n\nfamhx: family history breast cancer.\n\nPossible values: yes, no\n\nbmi: BMI (kg/m**2)\nhgt: Height (inches)\nalcohol: Alcohol use (grams/day)\npmh: PMH status.\n\nPossible values: never user, current user\n\ndur3: Duration of Estrogen use (months)\ndur4: Duration of Estrogen + progesterone use (months)\ncsmk: Current Smoker.\n\nPossible values: yes, no\n\npsmk: Past smoker.\n\nPossible values: yes, no\n\nfoluptm: Months of follow up. Note: Some subjects provided no follow up after the 1990 questionnaire and foluptm = 0 for these people\n\n\n\n\ncholesterol\nCholesterol levels from 24 hospital employees who switched from a standard American diet to a vegetarian diet for one month. Their serum cholesterol was measured before and after the diet change.\n\ncholesterol.csv\n\nSubject: ID\nBefore: Serum-cholesterol levels before diet change (mg/dL)\nAfter: Serum-cholesterol levels after diet change (mg/dL)\nDifference: Difference in serum-cholesterol levels, Before - After, where positive numbers indicate a reduction in serum-cholesterol levels.\n\n\n\n\ncorneal\nFluoroquinolones, antibiotics for bacterial infections, are FDA-approved for systemic use. However, post-approval studies indicate a risk of peripheral neuropathy, leading to updated safety labeling.\nA small clinical trial tested the safety and effectiveness of two fluoroquinolone eye drops (drugs M and G) and a placebo (drug P) for bacterial eye infections. Ninety-three subjects were randomly assigned to three groups, each receiving one active drug and a placebo in opposite eyes. Participants used the drops four times daily for 10 days. The primary outcome was corneal sensitivity, measured in millimeters, with higher values indicating more normal sensitivity.\nCorneal sensitivity was assessed at baseline, 7 days, and 14 days, with measurements taken from the central cornea and four quadrants (superior, inferior, temporal, nasal).\n\ncorneal.csv\n\nid: ID\ntr: Treatment.\n\nPossible values: M, G, P\n\nc1: Central visit 1\ns1: Superior visit 1\ni1: Inferior Visit 1\nt1: Temporal visit 1\nn1: Nasal Visit 1\nc2: Central Visit 2(day 7)\ns2: Superior Visit 2\ni2: Inferior Visit 2\nt2: Temporal Visit 2\nn2: Nasal Visit 2\nc3: Central Visit 3(day 14)\ns3: Superior Visit 3\ni3: Inferior Visit 3\nt3: Temporal Visit 3\nn3: Nasal Visit 3\n\n\n\n\ndiabetes\nType I diabetes is common in children and requires regular insulin shots to prevent long-term complications like neurologic, vision, kidney issues, heart disease, and premature death.\nThe impact of diabetes control on childhood growth is less clear. To study this, adolescent boys aged 9−15 were examined about every 3 months. Each exam measured diabetes control using glycosylated hemoglobin (HgbA1c), where higher HgbA1c indicates poorer control (normal &lt;7.0). Age, height, and weight were also recorded. Data includes 94 boys over 910 visits.\nThe main question is the overall relationship between glycemic control and growth, primarily weight, for the entire group, not individual cases.\n\ndiabetes.csv\n\nID: ID\nmon_a1c: Month A1c\nday_a1c: Day A1c\nyr_a1c: Yr A1c\nage_yrs: Age in years\ngly_a1c: Hemoglobin A1c\nht_cm: Height in cm\nwt_kg: Weight in kg\n\n\n\n\near\nThis dataset includes 203 children with acute otitis media (OME) from a randomized clinical trial. Each child had OME in one or both ears and received a 14-day course of either cefaclor (CEF) or amoxicillin (AMO). Middle-ear status was assessed after the 14-day treatment.\n\near.csv\n\nId: ID\nClear: Clearance by 14 days,\n\nPossible values: yes, no\n\nAntibo: Antibiotic.\n\nPossible values: CEF, AMO\n\nAge: Age. Possible values: &lt;2 yrs, 2-5 yrs, 6+ yrs.\nEar: Ear,\n1 = 1st ear\n2 = 2nd ear\n\n\n\n\neff, nephro, and oto\nAminoglycoside antibiotics are crucial for treating severe gram-negative bacterial infections in hospitalized patients. Despite their toxicity and the development of new antibiotics, aminoglycosides remain widely used. Choosing the right aminoglycoside depends on the clinical situation, antimicrobial spectrum, cost, and side effects like nephrotoxicity and auditory toxicity. Numerous trials comparing these antibiotics have varied in design and conclusions, often lacking sufficient sample sizes to detect small differences.\nTo better understand their true effects, a meta-analysis of all randomized trials was conducted. This analysis included 45 trials from 1975 to 1985, comparing amikacin, gentamicin, netilmicin, sisomicin, and tobramycin. Data from 37 trials were suitable for comparison, focusing on efficacy, nephrotoxicity, and auditory toxicity. Efficacy was defined by bacterial or clinical response as reported by each trial, nephrotoxicity by the percentage of kidney-related toxic events reported, and auditory toxicity by the diffence in pre- and post-treatment audiograms. The data is organized into three sets: eff.csv, nephro.csv, and oto.csv, with separate records for each antibiotic and endpoint.\n\neff.csv\n\nName: Study name\nId: Study Number\nEndpnt: Endpoint, 1 = efficacy\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nCured: Number Cured\n\nnephro.csv\n\nname: Study name\nid: Study number\nEndpnt: Endpoint 2 = nephrotoxicity\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin\n\nSamp_sz: Sample size\nSide_eff: Number with side effects\n\noto.csv\n\nName: Study Name\nId: Study Number\nEndpnt: Endpoint.\n\nPossible values: efficacy, nephrotoxicity, ototoxicity.\n\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nSide_eff: Number with side effect\n\n\n\n\nendocrin\nThe data set contains split-sample plasma measurements of four hormones for each of five subjects, all from one laboratory.\n\nendocrin.csv\n\nSubject: Subject number\nReplicat: Replicate number\nEstrone: Estrone\nEstradol: Estradiol\nAndroste: Androstenedione\nTestost: Testosterone\n\n\n\n\nestradl\nObesity is common in American society and a risk factor for breast cancer in postmenopausal women, possibly due to increased estrogen levels, particularly serum estradiol. Researchers studied 151 African American and 60 Caucasian premenopausal women, measuring adiposity using body mass index (BMI) (a measure of overall adioposity) and waist-hip ratio (WHR) (a measure of abdominal adioposity). They also obtained a complete hormonal profile, including serum estradiol, and assessed other breast cancer risk factors: ethnicity, age, number of children, age at first birth, presence of children, and age at menarche.\n\nestradl.csv\n\nId: Identification number\nEstradl: Estradiol\nEthnic: Ethnicity.\n\nPossible values: African-American, Caucasian\n\nEntage: Age\nNumchild: Parity, number of children\nAgefbo: Age at 1st birth (=0 if numchild=0)\nAnykids: any children.\n\nPossible values: yes, no.\n\nAgemenar: age at menarche\nBMI: Body Mass Index\nWHR: waist-hip ratio\n\n\n\n\nestrogen\nThree distinct two-period crossover studies were conducted with different subject groups, measuring systolic and diastolic blood pressure. In Study 1, 0.625 mg of estrogen was compared with a placebo. Study 2 compared 1.25 mg of estrogen with a placebo. Study 3 compared 1.25 mg of estrogen with 0.625 mg of estrogen. Each active treatment period lasted for four weeks, with a two-week washout period between them.\n\nestrogen.csv\n\nId: ID\nstd_typ: Study type.\n\nPossible values: 0.625MG VS PLACEBO, 1.25MG VS PLACEBO, 1.25MG VS 0.625MG\n\nperiod: Period\ntrtgrp: Treatment.\n\nPossible values: PLACEBO, 0.625MG, 1.25MG\n\nsysd1r1: Systolic blood pressure day 1 reading 1\ndiasd1r1: Diastolic blood pressure day 1 reading 1\nsysd1r2: Systolic blood pressure day 1 reading 2\ndiasd1r2: Diastolic blood pressure day 1 reading 2\nsysd1r3: SYSTOLIC BP DAY 1 reading 3\ndiasd1r3: Diastolic blood pressure day 1 reading 3\nsysd2r1: Systolic blood pressure day 2 reading 1\ndiasd2r1: Diastolic blood pressure day 2 reading 1\nsysd2r2: Systolic blood pressure day 2 reading 2\ndiasd2r2: Diastolic blood pressure day 2 reading 2\nsysd2r3: Systolic blood pressure day 2 reading 3\ndiasd2r3: Diastolic blood pressure day 2 reading 3\nsysd3r1: Systolic blood pressure day 3 reading 1\ndiasd3r1: Diastolic blood pressure day 3 reading 1\nsysd3r2: Systolic blood pressure day 3 reading 2\ndiasd3r2: Diastolic blood pressure day 3 reading 2\nsysd3r3: Systolic blood pressure day 3 reading 3\ndiasd3r3: Diastolic blood pressure day 3 reading 3\n\n\n\n\nfev\nForced expiratory volume (FEV) is a measure of pulmonary function that quantifies the volume of air expelled in one second of sustained effort. This dataset includes FEV measurements from 1980 for 654 children aged 3 to 19 who participated in the Childhood Respiratory Disease (CRD) Study in East Boston, Massachusetts. The data are part of a longitudinal study aimed at tracking changes in pulmonary function over time in children.\n\nfev.csv\n\nId: ID number\nAge: Age (yrs)\nFEV: FEV (liters)\nHgt: Height (inches)\nSex: Sex.\n\nPossible values: female, male\n\nSmoke: Smoking Status.\n\nPossible values: non-current, current smoker.\n\n\n\n\n\nfield\nRetinitis pigmentosa (RP) is a hereditary eye disease that can lead to substantial vision loss or blindness. It has dominant, recessive, and sex-linked forms, with mutations in the rhodopsin (RHO) gene linked to dominant cases and RPGR gene mutations linked to sex-linked cases.\nThe data file field.csv contains visual field data for approximately 100 patients each from the RHO and RPGR groups. Visual field, measured in degrees\\(^2\\), indicates the area of vision. The dataset includes longitudinal data with varying follow-up times from a minimum of 3 years to a maximum of about 25-30 years. Measurements are provided separately for the right eye (OD) and the left eye (OS).\n\nfield.csv\n\nid: ID\ngroup: group.\n\nPossible values: RHO, RPGR\n\nage: age at visit (years)\ngender: gender. Note: all RPGR individuals have to be male.\n\nPossible values: male, female.\n\ndtvisit: date of visit (YYYY-MM-DD)\nfolowup: time from 1st visit in years\ntotfldod: total field area right eye (OD) in degrees\ntotfldos: total field area left eye (OS) in degrees\n\n\n\n\nheart\n\nheart.csv\n\nDiagnosis: Possible values:\n\nY1 = normal\nY2 = atrial septal defect without pulmonary stenosis or pulmonary hypertension\nY3 = ventricular septal defect with valvular pulmonary stenosis\nY4 = isolated pulmonary hypertension\nY5 = transposed great vessels\nY6 = ventricular septal defect without pulmonary hypertension\nY7 = ventricular septal defect with pulmonary hypertension\n\nPrevalence: Prevalence\nX1: age 1-20 years old\nX2: age&gt;20 years old\nX3: mild cyanosis\nX4: easy fatigue\nX5: chest pain\nX6: repeated respiratory infections\nX7: EKG axis more than 110\n\n\n\n\nhormone\nAn experiment was conducted to study the effects of avian pancreatic polypeptide (aPP), cholecystokinin (CCK), vasoactive intestinal peptide (VIP), and secretin on pancreatic and biliary secretions in laying hens. Researchers aimed to determine how these hormones affect the flow rates and pH values of these secretions.\nWhite Leghorn hens, aged 14-29 weeks, were surgically fitted with cannulas for collecting biliary and pancreatic secretions, and a jugular cannula for hormone infusion. Each hen underwent one trial per day, as long as her cannulas remained functional, leading to varying trial numbers per hen.\nEach trial started with a 20-minute saline infusion, followed by the collection of pancreatic and biliary secretions. The flow rates (in microliters per minute) and pH values were measured. This was followed by a 40-minute hormone infusion, with measurements repeated afterward.\nThe data set “hormone.csv” includes data for the four hormones and saline, with each trial recorded as one entry and 11 associated variables.\n\nhormone.csv\n\nID: ID of hen\nBilsecpr: Biliary secretion-pre (microliters per minute)\nBilphpr: Biliary pH-pre\nPansecpr: Pancreatic secretion-pre (microliters per minute)\nPanphpr: Pancreatic pH-pre\nDose: Dose of hormone\nBilsecpt: Biliary secretion-post (microliters per minute)\nBilphpt: Biliary pH-post\nPansecpt: Pancreatic secretion-post (microliters per minute)\nPanphpt: Pancreatic pH-post\nHormone: Hormone.\n\nPossible values: SAL, APP, CCK, SEC, VIP.\n\n\n\n\n\nhospital\nThese data are part of a larger data set gathered from individuals discharged from a specific Pennsylvania hospital. It was collected as part of a retrospective chart review focusing on antibiotic usage in hospitals.\n\nhospital.csv\n\nId: id no.\nDur_stay: Duration of hospital stay\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nTemp: First temperature following admission\nWBC: First WBC(x1000) following admission\nAntibio: Received antibiotic.\n\nPossible values: yes, no\n\nBact_cul: Received bacterial culture.\n\nPossible values: yes, no\n\nService: Service.\n\nPossible values: med, surg\n\n\n\n\n\ninfantbp\nResearchers investigated the link between high blood pressure and sodium intake by measuring infants’ responses to salt and sugar solutions. They measured the vigor of infants’ sucking (mean sucks per burst, MSB) when exposed to different solutions: water, 0.1 molar salt, 0.3 molar salt, and sugar. The responses were recorded over a series of periods using different stimuli: (i) nonnutritive sucking (ii) water, (iii) 5% sucrose + water, (iv) 15% sucrose + water, and (v) nonnutritive sucking.\n\ninfantbp.csv\n\nID: Infant ID\nMn_sbp: Mean Systolic Blood Pressure\nMn_dbp: Mean Diastolic Blood Pressure\nSalt Taste Variables\n\nMSB1slt: MSB-trial 1 water\nMSB2slt: MSB-trial 2 water\nMSB3slt: MSB-trial 3 0.1 molar salt + water\nMSB4slt: MSB-trial 4 0.1 molar salt + water\nMSB5slt: MSB-trial 5 water\nMSB6slt: MSB-trial 6 water\nMSB7slt: MSB-trial 7 0.3 molar salt + water\nMSB8slt: MSB-trial 8 0.3 molar salt + water\nMSB9slt: MSB-trial 9 water\nMSB10slt: MSB-trial 10 water\n\nSugar Taste Variables\n\nMSB1sug: MSB-trial 1 non-nutritive sucking\nMSB2sug: MSB-trial 2 water\nMSB3sug: MSB-trial 3 5% sucrose + water\nMSB4sug: MSB-trial 4 15% sucrose + water\nMSB5sug: MSB-trial 5 non-nutritive sucking\n\nNOTE: For MSB, 0 indicates the baby did not suck.\n\n\n\n\nlead\nA study examined the psychological and neurological effects of lead exposure on children near a lead smelter in El Paso, Texas. Blood lead levels were measured, categorizing 46 children with levels ≥ 40 μg/mL as the exposed group. Another 78 children with levels &lt; 40 μg/mL functioned as the control group. Key outcomes included finger–wrist taps (neurological function) and Wechsler full-scale IQ scores.\nOther behavioral effects of lead include hyperactivity. In this study, parents also rated their children’s hyperactivity on a scale from 0 (normal) to 3 (very hyperactive). Hyperactivity measures are available for 49 control children and 35 exposed children.\n\nlead.csv\n\nPatient information\n\nid: Identification number\nageyrs: Age in years\nsex: Sex.\n\nPossible values: male, female\n\n\nLead data\n\narea: Distance of esidence from smelter on august 1972. Possible values:\n\n0-1 Miles from smelter\n1-2.5 Miles\n2.5-4.1 Miles\n\nlead_grp: Blood lead level group. possible values:\n\ncontrol = Blood lead levels below 40 micrograms/100ml in both 1972 & 1973 (control group),\ncurrent exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in both 72 & 73 or a level greater than or equal to 40 in 73 alone (3 cases only) (currently exposed group),\nprevious exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in 72 and less than 40 in 73 (previously exposed group)\n\nGroup: group.\n\nPossible values: control, exposed\n\nld72: Blood lead values (micrograms/100ml) in 72\nld73: Blood lead values (micrograms/100ml) in 73\nfst2yrs: Did child live for 1st 2 years within 1 mile of smelter.\n\nPossible values: yes, no.\n\ntotyrs: Total number of years spent within 4.1 miles of smelter\n\nIQ Test Results\n\niqv_inf: INF - information subtest in WISC and WPPSI\niqv_comp: COMP - comprehension subtest in WISC and WPPSI\niqv_ar: AR - arithmetic subtest in WISC and WPPSI\niqv_ds: DS - digit span subtest(WISC) and sentence completion(WPPSI)\niqv_raw: V/RAW - raw score/verbal IQ\niqp_pc: PC - picture completion subtest in WISC and WPPSI\niqp_bd: BD - block design subtest in WISC and WPPSI\niqp_oa: OA - object assembly subtest(WISC), animal house subtest(WPPSI)\niqp_cod: COD - coding subtest(WISC), geometric design subtest(WPPSI)\niqp_raw: P/RAW - raw score/performance IQ (total of scores PC, BD, OA, & COD)\nhh_index: HH/INDEX - Hollingshead index of social status\niqv: IQV - verbal IQ\niqp: IQP - performance IQ\niqf: IQF - full scale IQ (not sum or average of IQV D IQP)\niq_type: Type of IQ test (WISC usually given to children \\(\\geq\\) 5 years and 1 month of age WPPSI usually given to children \\(\\leq\\) 5 years of age).\n\nPossible values: WISC, WPPSI\n\n\nSymptom data (as reported by parents)\n\npica: Pica.\n\nPossible values: yes, no.\n\ncolic: Colic.\n\nPossible values: yes, no.\n\nclumsi: Clumsiness.\n\nPossible values: yes, no.\n\nirrit: Irritability.\n\nPossible values: yes, no.\n\nconvul: Convulsions.\n\nPossible values: yes, no.\n\n\nNeurological test data\n\n_2plat_r: Number of taps for right hand in the 2-plate tapping test (number of taps in one 10 second trial)\n_2plat_l: Number OF taps for left hand in the 2-plate tapping test (number taps in one 10 second trial)\nvisrea_r: Visual reaction time right hand (milliseconds)\nvisrea_l: Visual reation time left hand (milliseconds)\naudrea_r: Auditory reaction time right hand (milliseconds)\naudrea_l: Auditory reaction time left hand (milliseconds)\nfwt_r: Finger-wrist tapping test right hand (number of taps in one 10 second trial)\nfwt_l: Finger-wrist tapping test left hand (#taps in one 10 second trial)\nhyperact: WWPS - Werry-Weiss-Peters scale for hyperactivity\n\n0 = no activity, \\(\\ldots\\), 4 = severly hyperactive (as reported by parents)\n\nmaxfwt: Finger-wrist tapping test in dominant hand (max of fwt_r,fwt_l)\n\n\n\n\n\nlvm\nThe Left Ventricular Mass Index (LVMI) measures the enlargement of the heart’s left side, expressed in gm/ht(m)^2.7. High LVMI values can predict future cardiovascular disease in children. A study investigated the relationship between LVMI levels and blood pressure categories in children and adolescents aged 10-18. Blood pressure was categorized as Normal (bpcat = normal, bp percentile &lt; 80%), Pre-Hypertensive (bpcat = pre-hypertensive, bp percentile ≥ 80% and &lt; 90%), or Hypertensive (bpcat = hypertensive, bp percentile ≥ 90%)..\n\nlvm.csv\n\nID: ID\nlvmht27: Left ventricular mass – height corrected = Left Ventricular Mass/Height(m)\\(^{2.7}\\), \\(g/m^{2.7}\\)\nbpcat: Blood pressure category.\n\nPossible values: normal, pre-hypertensive, hypertensive.\n\ngender: Bender.\n\nPossible values: male, female\n\nage: in years\nBMI: kg/m\\(^2\\)\n\n\n\n\nmice\nRetinitis pigmentosa (RP) is a hereditary eye condition causing night blindness and visual field loss, typically between ages 10 and 40. Some patients become legally blind by 30, while others retain central vision past 60. A specific gene linked RP has been identified whose transmision is autosomal dominant. The disease progression is tracked using electroretinogram (ERG), measuring retinal electrical activity, which decreases as RP advances, affecting routine activities like driving and walking at night.\nTo test if sunlight exposure harms RP patients, researchers introduced the RP gene into mice, creating “RP mice.” These mice were divided into light, dim, and dark lighting conditions from birth. A control group of normal mice was also subjected to similar conditions. ERG amplitudes (BAMP and AAMP) were measured at 15, 20, and 35 days of life for RP mice, and only BAMP was measured for normal mice.\n\nmice.csv\n\nId: ID\nGroup: GROUP.\n\nPossible values: RP, NORMAL\n\nTrtgrp: TREATMENT GROUP.\n\nPossible values: LIGHT, DIM, DARK.\n\nAge: AGE (days)\nB_amp: B AMP\nA_amp: A AMP\n\n\n\n\nnifed\nA clinical trial tested the effectiveness of nifedipine in reducing chest pain in hospitalized angina patients. The study lasted 14 days unless patients were withdrawn, discharged, or died. Patients were randomly assigned to receive either nifedipine or propranolol, starting at a standard dosage. If pain persisted or recurred, the dosage was increased in pre-specified steps. Patients in both groups could also receive nitrates as needed to control pain. The primary goal was to compare pain relief between nifedipine and propranolol, with a secondary goal of examining their effects on heart rate and blood pressure.\n\nnifed.csv\n\nId: ID\ntrtgrp: Treatment group,\n\nN = nifedipine\nP = placebo\n\nbashrtrt: Baseline heart rate immediately prior to randomization (beats/min)\nlv1hrtrt: Level 1 heart rate, Highest heart rate and systolic blood pressure at baseline and each level of therapy respectively (beats/min)\nlv2hrtrt: Level 2 heart rate (beats/min)\nlv3hrtrt: Level 3 heart rate (beats/min)\nbassys: Baseline systolic bp immediately prior to randomization (mm Hg)\nlv1sys: Level 1 systolic bp (mm Hg)\nlv2sys: Level 2 systolic bp (mm Hg)\nlv3sys: Level 3 systolic bp (mm Hg)\n\n\nMissing values indicate that either (a) the patient withdrew from the study prior to entering this level of therapy (b) the patient achieved pain relief prior to reaching this level or therapy, (c) the patient encountered this level of therapy, but this particular piece of data was missing.\n\n\npiriform\nA study evaluated the FAIR test (hip flexion, adduction, and internal rotation) for diagnosing piriformis syndrome (PS), which affects the piriformis muscle in the buttock, causing lumbar and sciatic pain. The test measures nerve-conduction velocity differences between an aggravating and a neutral posture, with higher scores indicating a greater likelihood of PS. Data from 142 participants without PS and 489 with PS (diagnosed clinically) are available. A FAIR test score of ≥ 1.86 ms is proposed to define a positive result. The FAIR test value, MAXCHG, is recorded in milliseconds.\n\npiriform.csv\n\nID: ID\npiriform: Piriformis Syndrome.\n\nPossible values: Negative, Positive\n\nsex: Sex.\n\nPossible values: male, female\n\nage: Age\nmaxchg: Max change between tibia and peroneal\n\n\n\n\nsexrat\nIt is often assumed that the gender distribution of consecutive children is independent. To test this hypothesis, birth records from the first five children in 51,868 families were analyzed. These data contain the frequency of how many times a pattern of child sexes showed up. E.g., MM (only two males) showed up 4400 times.\n\nsexrat.csv\n\nnm_chld: Number of children. For families with 5+ children, the sex of the first 5 children are listed. The number of children is given as 5 for such families.\nsx_1: Sex of 1st born\nsx_2: Sex of 2nd born\nsx_3: Sex of 3rd born\nsx_4: sex of 4th born\nsx_5: sex of 5th born\nsexchldn: Sex of all children. The sex of successive births is given. Thus, MMMF means that the first three children were males and the fourth child was a female. There were 484 such families.\nnum_fam: Number of families. Number of families with specific gender contribution of children\n\n\n\n\nsmoke\nA study was conducted among 234 individuals who wanted to quit smoking but had not yet done so. On the day they quit, their carbon monoxide (CO) levels were measured, and the time since their last cigarette was recorded. This CO level serves as an indicator of the number of cigarettes smoked daily before quitting but is influenced by the time since the last cigarette. Therefore, a “corrected CO level” was provided, adjusted for this time. Participant age, sex, and self-reported daily cigarette consumption were also recorded. The participants were followed for a year to determine the number of days they remained abstinent, ranging from 0 to 365 days.\n\nsmoke.csv\n\nID: ID number\nAge: age\nGender: Gender.\n\nPossible values: male, female\n\nCig_day: Cigarettes/day\nCO: Carbon monoxide (CO) (X 10)\nMin_last: Minutes elapsed since last cigarette\nLogCOadj: Log CO Adj * (X 1000). This variable represents adjusted carbon monoxide (CO) values. CO values were adjusted for minutes elapsed since last cigarette smoked using the formula Log 10 CO (Adjusted) = Log 10 CO - (-0.000638) X (Min - 80), where Min is the number of minutes elapsed since the last cigarette smoked.\nDay_abs: Days abstinent Those abstinent less than 1 day were given a value of zero.\n\n\n\n\nswiss\nThe Swiss Analgesic Study aimed to evaluate the impact of phenacetin-containing analgesics on kidney function and health. It involved 624 women from Basel, Switzerland, who had high phenacetin intake (study group) and 626 women with low or no phenacetin intake (control group). The study used urine N-acetyl-P-aminophenyl (NAPAP) levels to measure recent phenacetin use, dividing the study group into high-NAPAP and low-NAPAP subgroups. Both subgroups had higher NAPAP levels than the control group. The women were examined in 1967-1968 and again in 1969, 1970, 1971, 1972, 1975, and 1978, with kidney function assessed through various laboratory tests, including serum-creatinine levels.\n\nswiss.csv\n\nID: ID\nage: age (yrs)\ngroup: Group.\n\nPossible values: High NAPAP, Low NAPAP, control\n\ncreat_68: Serum Creatinine 1968 (mg/dL)\ncreat_69: Serum Creatinine 1969 (mg/dL)\ncreat_70: Serum Creatinine 1970 (mg/dL)\ncreat_71: Serum Creatinine 1971 (mg/dL)\ncreat_72: Serum Creatinine 1972 (mg/dL)\ncreat_75: Serum Creatinine 1975 (mg/dL)\ncreat_78: Serum Creatinine 1978 (mg/dL)\n\n\n\n\ntear\nA pilot study was conducted to evaluate an eye drop’s effectiveness in preventing dry eye, measured by tear breakup time (TBUT). Fourteen participants tested three protocols:\n\nProtocol A: No blinking for 3 seconds before placebo instillation.\nProtocol B: No blinking for 6 seconds before placebo instillation (standard protocol).\nProtocol C: No blinking for 10 seconds before placebo instillation.\n\nTBUT was recorded at baseline, immediately after, and at 5, 10, and 15 minutes post-instillation in a low-humidity environment. Each protocol was tested on the same participants, measuring both eyes with two replicates.\n\ntear.csv\n\nID: ID\nod3bas1: OD 3sec baseline 1\nod3bas2: OD 3 sec baseline 2\nod3im1: OD 3 sec immediately post 1\nod3im2: OD 3 sec immediately post 2\nod3pst51: OD 3 sec 5min post 1\nod3pst52: OD 3 sec 5min post 2\nod3pt101: OD 3 sec 10min post 1\nod3pt102: OD 3 sec 10min post 2\nod3pt151: OD 3 sec 15min post 1\nod3pt152: OD 3 sec 15min post 2\nos3bas1: OS 3sec baseline 1\nos3bas2: OS 3 sec baseline 2\nos3im1: OS 3 sec immediately post 1\nos3im2: OS 3 sec immediately post 2\nos3pst51: OS 3 sec 5min post 1\nos3pst52: OS 3 sec 5min post 2\nos3pt101: OS 3 sec 10min post 1\nos3pt102: OS 3 sec 10min post 2\nos3pt151: OS 3 sec 15min post 1\nos3pt152: OS 3 sec 15min post 2\nod6bas1: OD 6 sec baseline 1\nod6bas2: OD 6 sec baseline 2\nod6im1: OD 6 sec immediately post 1\nod6im2: OD 6 sec immediately post 2\nod6pst51: OD 6 sec 5min post 1\nod6pst52: OD 6 sec 5min post 2\nod6pt101: OD 6 sec 10min post 1\nod6pt102: OD 6 sec 10min post 2\nod6pt151: OD 6 sec 15min post 1\nod6pt152: OD 6 sec 15min post 2\nos6bas1: OS 6 sec baseline 1\nos6bas2: OS 6 sec baseline 2\nos6im1: OS 6 sec immediately post 1\nos6im2: OS 6 sec immediately post 2\nos6pst51: OS 6 sec 5min post 1\nos6pst52: OS 6 sec 5min post 2\nos6pt101: OS 6 sec 10min post 1\nos6pt102: OS 6 sec 10min post 2\nos6pt151: OS 6 sec 15min post 1\nos6pt152: OS 6 sec 15min post 2\nod10bas1: OD 10 sec baseline 1\nod10bas2: OD 10 sec baseline 2\nod10im1: OD 10 sec immediately post 1\nod10im2: OD 10 sec immediately post 2\nod10ps51: OD 10 sec 5min post 1\nod10ps52: OD 10 sec 5min post 2\nod10p101: OD 10 sec 10min post 1\nod10p102: OD 10 sec 10min post 2\nod10p151: OD 10 sec 15min post 1\nod10p152: OD 10 sec 15min post 2\nos10bas1: OS 10 sec baseline 1\nos10bas2: OS 10 sec baseline 2\nos10im1: OS 10 sec immediately post 1\nos10im2: OS 10 sec immediately post 2\nos10ps51: OS 10 sec 5min post 1\nos10ps52: OS 10 sec 5min post 2\nos10p101: OS 10 sec 10min post 1\nos10p102: OS 10 sec 10min post 2\nos10p151: OS 10 sec 15min post 1\nos10p152: OS 10 sec 15min post 2\n\n\n\n\ntemperat\nA student records temperatures at 20 sites within her house for 30 days each. She records the outside temperature and the weather condition.\n\ntemperat.csv\n\nDate: Date (MDY)\nOut_temp: Outside temerature (Degrees Fahrenheit)\nRoom: Room location\nIn_temp: Inside temperature (Degrees Fahrenheit)\nCor_fac: Correction factor added.\n\nPossible values: yes, no\n\nTyp_wea: Type of weather.\n\nPossible values: SUNNY, PARTLY CLOUDY, CLOUDY, RAINY, FOGGY\n\n\n\n\n\ntennis1\nA survey of tennis club members in the Boston area examined the occurrence of tennis elbow. Subjects reported anywhere from 0 to 8 episodes of tennis elbow. They were also asked about demographic factors and racquet characteristics.\n\ntennis1.csv\n\nId: ID\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nNum_epis: Number of episodes of tennis elbow\nTyp_last: Type of racquet used during last episode.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_last: Weight of racquet used during last episode.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_last: Material of racquet used during last episode.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_last: String type of racquet used during last episode.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\nTyp_curr: Type of racquet used currently.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_curr: Weight of racquet used currently.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_curr: Material of racquet used currently.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_curr: String type of racquet used currently.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\n\n\n\n\ntennis2\nTennis elbow is a painful condition common among tennis players. Treatments include rest, heat, and anti-inflammatory medications like Motrin (ibuprofen). A clinical trial with 87 participants compared the effectiveness of Motrin vs. placebo. Participants were randomly divided into two groups: Group A received Motrin for 3 weeks, followed by a 2-week washout period, and then placebo for 3 weeks; Group B received the treatments in reverse order. Pain levels were measured on a 1-6 scale (1 = worse, 6 = completely improved) at the end of each treatment and washout period. The comparison was made (i) during maximum activity, (ii) 12 hours following maximum activity, (iii) during the average day, and (iv) by overall impression of drug efficacy.\n\ntennis2.csv\n\nid: ID\nage: Age\nsex: Sex.\n\nPossible values: male, female\n\ndrg_ord: Drug order.\n\nPossible values: MOTRIN-PLACEBO, PLACEBO-MOTRIN\n\nPeriod 2 = Pain scores after the first active drug period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_2: During study period, pain during maximum activity vs baseline\n\n1 = Worse\n2 = Unchanged\n3 = Slightly improved (25%)\n4 = Moderately improved (50%)\n5 = Mostly improved (75%)\n6 = Completely improved\n\npain12_2: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_2: During the average day of study period pain vs. baseline (same code as painmx_2)\npainov_2: Overall impression of drug efficacy vs. baseline (same code as painmx_2)\n\nPeriod 3 = Pain scores after the washout period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_3: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_3: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_3: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_3: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\nPeriod 4 = Pain scores after the second active drug period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_4: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_4: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_4: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_4: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\n\n\n\n\nvalid\nThe food-frequency questionnaire (FFQ) is a common tool in dietary epidemiology to assess food consumption. It asks individuals to report their typical daily servings of over 100 food items from the past year, and a food-composition table calculates nutrient intakes. While FFQs are inexpensive, they are less accurate than diet records (DR), where participants document their weekly food intake, and a nutritionist calculates nutrient intakes. In a validation study, 173 nurses from the Nurses’ Health Study completed 4 weeks of diet records and an FFQ. Data for saturated fat, total fat, alcohol consumption, and caloric intake from both methods recorded here.\n\nvalid.csv\n\nId: ID number\nsfat_dr: Saturated fat-DR (g)\nsfat_ffq: Saturated fat-FFQ (g)\ntfat_dr: Total fat-DR (g)\ntfat_ffq: Total fat-FFQ (g)\nalco_dr: Alcohol consumption-DR (oz)\nalco_ffq: Alcohol consumption-FFQ (oz)\ncal_dr: Total calories-DR (K-cal)\ncal_ffq: Total calories-FFQ (K-cal)\n\n\n\n\nwales\nA study in South Wales investigated the hereditary factors of blood pressure in 623 individuals (propositii) over age 5 from two populations. The participants and their first-degree relatives had their blood pressure measured at home by one observer, with a baseline and three follow-up exams from the mid-1950s to the early 1960s. The dataset WALES.DAT includes familial blood pressure data from the Rhondda Fach and Vale of Glamorgan communities.\n\nwales.csv\n\nID: ID\nrecord:\nsex:\narea:\npropositus:\nsexofprop:\nrelationship:\ndoubler:\nsurveystat:\nmaritalstt:\nage:\nparity:\nheight:\nweight:\narmgrth:\ntriceps:\nsysbp:\ndiasbp:\npulse:\nalbumin:\nglucose:\nbacteria:\noccupation:\nhypertension:\ndiabetes:\npregnancy:\npet_fibroids:\nangina:\npmi:\ninter_claud:\n\n\n\n\n\n\n\n\n\n\nReferences\n\nRosner, B. 2015. Fundamentals of Biostatistics. 8th ed. Cengage Learning."
  },
  {
    "objectID": "01_r/01_r_intro.html",
    "href": "01_r/01_r_intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a statistical programming language designed to analyze data.\nThis is not an R course. But you need to know some tools to summarize/plot/model data.\nR is free, widely used, more generally applicable (beyond linear regression), and a useful tool for reproducibility. So this is what we will use.\nPython would have been a good choice too, but it is worse at basic stats (this is controversial)."
  },
  {
    "objectID": "01_r/01_r_intro.html#variables",
    "href": "01_r/01_r_intro.html#variables",
    "title": "Introduction to R",
    "section": "Variables",
    "text": "Variables\n\nA variable stores a value. You use the assignment operator “&lt;-” to assign values to variables. For example, we can assign the value of 10 to the variable x.\n\nx &lt;- 10\n\n\nIt is possible to use =, and I think there is nothing wrong with that. But for some reason the field has decided to only use &lt;-, so you should too.\n\nWhenever we use x later, it will use the value of 10\n\nx\n\n[1] 10\n\n\nThis is useful because you can reuse this value over and over again:\n\ny &lt;- 0\nx + y\nx * y\nx / y\nx - y\n\nTo assign a “string” (a fancy way to say a word) to x, put the string in quotes. For example, we can assign the value of \"Hello World\" to x.\n\nx &lt;- \"Hello World\"\nx\n\n[1] \"Hello World\""
  },
  {
    "objectID": "01_r/01_r_intro.html#functions",
    "href": "01_r/01_r_intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\n\nFunctions take objects (such as numbers or variables) as input and output new objects. Let’s look at a simple function that takes the log of a number:\n\nlog(x = 4, base = 2)\n\nThe inputs are called “arguments”. Generally, every function will be for the form:\n\nfunction_name(arg1 = val1, arg2 = val2, ...)\n\nIf you do not specify the name of the argument, R will assume you are assigning in their order.\n\nlog(4, 2)\n\nYou can change the order of the arguments if you specify them.\n\nlog(base = 2, x = 4)\n\nTo see the list of all possible arguments of a function, use the help() function:\n\nhelp(log)\n\nIn the help file, there are often default values for an argument. For example, the following indicates the the default value of base is exp(1).\n\nlog(x, base = exp(1))\n\nThis indicates that you can omit the base argument and R will assume that it should be exp(1).\n\nlog(x = 4, base = exp(1))\n\n[1] 1.386\n\nlog(x = 4)\n\n[1] 1.386\n\n\nIf an argument does not have a default, then it must be specified when calling a function.\nType this:\n\nlog(x = 4,\n\nThe “+” indicates that R is expecting more input (you forgot either a parentheses or a quotation mark). You can get back to the prompt by hitting the ESCAPE key."
  },
  {
    "objectID": "01_r/01_r_intro.html#useful-functions",
    "href": "01_r/01_r_intro.html#useful-functions",
    "title": "Introduction to R",
    "section": "Useful Functions",
    "text": "Useful Functions\n\nc() creates a vector (sequence of values)\n\ny &lt;- c(8, 1, 3, 4, 2)\ny\n\n[1] 8 1 3 4 2\n\n\nYou can perform vectorized operations on these vectors\n\ny + 2\n\n[1] 10  3  5  6  4\n\ny / 2\n\n[1] 4.0 0.5 1.5 2.0 1.0\n\ny - 2\n\n[1]  6 -1  1  2  0\n\n\nexp(): Exponentiation. This is the inverse of log().\n\nexp(10)\n\n[1] 22026\n\nlog(exp(10))\n\n[1] 10\n\n\nsqrt(): Square root\n\nsqrt(9)\n\n[1] 3\n\n\nmean(): The mean of a vector\n\nmean(y)\n\n[1] 3.6\n\n\nsd() The standard deviation of a vector\n\nsd(y)\n\n[1] 2.702\n\n\nsum(): Sum the values of a vector.\n\nsum(y)\n\n[1] 18\n\n\nseq(): Create a sequence of numbers\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nhead(): Show the first six values of an object.\n\n\nExerciseSolution\n\n\nCalculate this in R (hint: pi is \\(\\pi\\) in R) \\[\n\\frac{1}{\\sqrt{2\\pi}}e^{-1.3^2}\n\\]\n\n\n\nexp(-1.3^2) / sqrt(2 * pi)\n\n[1] 0.07361\n\n\n\n\n\n\nExerciseSolution\n\n\nCreate the following vector \\(x = (1, -2, 1.3)\\). What is the mean and standard deviation of this variable?\n\n\n\nx &lt;- c(1, -2, 1.3)\nmean(x)\n\n[1] 0.1\n\nsd(x)\n\n[1] 1.825\n\n\n\n\n\n\nExerciseSolution\n\n\nCreate a vector from 1 to 1000, take ths square root of each element, then sum them up.\n\n\n\nsum(sqrt(seq(1, 1000)))\n\n[1] 21097\n\n\n\n\n\n\nExerciseSolution\n\n\nWhat does the by argument do in seq()? Try it out.\n\n\nCreates increments of 2 instead of 1.\n\nseq(1, 10, by = 2)\n\n[1] 1 3 5 7 9"
  },
  {
    "objectID": "01_r/01_r_intro.html#r-packages",
    "href": "01_r/01_r_intro.html#r-packages",
    "title": "Introduction to R",
    "section": "R Packages",
    "text": "R Packages\n\nA package is a collection of functions that don’t come with R by default.\nThere are many many packages available. If you need to do any data analysis, there is probably an R package for it.\nUsing install.packages(), you can install packages that contain functions and datasets that are not available by default. Do this now with the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nYou will only need to install a package once per computer. Once it is installed you can gain access to all of the functions and datasets in a package by using the library() function.\n\nlibrary(tidyverse)\n\nYou will need to run library() at the start of every R session if you want to use the functions in a package.\nWhen I want to write the name of a function, I will write it like this()."
  },
  {
    "objectID": "01_r/01_r_intro.html#data-frames",
    "href": "01_r/01_r_intro.html#data-frames",
    "title": "Introduction to R",
    "section": "Data Frames",
    "text": "Data Frames\n\nThe fundamental unit object of data analysis is the data frame.\nA data frame has variables in the columns, and observations in the rows.\n \nR comes with a bunch of famous datasets in the form of a data frame. Such as the airquality dataset, which contains daily air quality measurements in New York from 1973.\n\ndata(\"airquality\")\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nYou can extract individual variables from a data frame using $\n\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6\n [19]  30  11   1  11   4  32  NA  NA  NA  23  45 115  37  NA  NA  NA  NA  NA\n [37]  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA\n [55]  NA  NA  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA\n [73]  10  27  NA   7  48  35  61  79  63  16  NA  NA  80 108  20  52  82  50\n [91]  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22\n[109]  59  23  31  44  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73\n[127]  91  47  32  20  23  21  24  44  21  28   9  13  46  18  13  24  16  13\n[145]  23  36   7  14  30  NA  14  18  20\n\n\nYou can explore these in a spreadsheet format using View() (note the capital “V”). Don’t ever have this in a file though, directly write it in the console.\n\nView(airquality)"
  },
  {
    "objectID": "01_r/01_r_intro.html#reading-in-data-frames",
    "href": "01_r/01_r_intro.html#reading-in-data-frames",
    "title": "Introduction to R",
    "section": "Reading in Data Frames",
    "text": "Reading in Data Frames\n\nMost datasets will nead to be loaded into R. To do so, we will use the {readr} package.\n\nlibrary(readr)\n\nThe only function I will require you to know from this package is read_csv(), which loads in data from a CSV file (“Comma-separated values”), a very popular format for storing data.\nIf you have the CSV file somewhere on your computer, then specify the path from the current working directory, and assign the data frame to a variable.\nFor other file formats, you need to use other functions, such as read_tsv(), read_table(), read_fwf(), etc. I will try to make sure read_csv() works for all datasets in this course.\nI will typicaly post course datasets at https://dcgerard.github.io/stat_320/data.html. You can load those data into R by pasting their URL’s into read_csv().\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nhead(lead)\n\n# A tibble: 6 × 40\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 29 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;\n\n\n\n\nExerciseSolution\n\n\nLoad in the birthweight data into R and print out the first six rows.\n\n\n\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nRows: 1000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): id, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(birthweight)\n\n# A tibble: 6 × 2\n     id weight\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     0    116\n2     1    124\n3     2    119\n4     3    100\n5     4    127\n6     5    103"
  },
  {
    "objectID": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "href": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "title": "Introduction to R",
    "section": "Basic Data Frame Manipulations",
    "text": "Basic Data Frame Manipulations\n\nYou will need to know just a few data frame manipulations, which we will perform using the {dplyr} package.\n\nlibrary(dplyr)\n\nThe first argument for {dplyr} functions is always the data frame you are modifying. The following arguments typically involve the columns of that data frame.\nUse the mutate() function from the {dplyr} package to make variable transformations.\n\nlead &lt;- mutate(lead, log_iqv_inf = log(iqv_inf))\nhead(lead)\n\n# A tibble: 6 × 41\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse glimpse() to get a brief look at the data frame.\n\nglimpse(lead)\n\nRows: 124\nColumns: 41\n$ id          &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112…\n$ area        &lt;chr&gt; \"2.5-4.1\", \"2.5-4.1\", \"2.5-4.1\", \"1-2.5\", \"0-1\", \"1-2.5\", …\n$ ageyrs      &lt;dbl&gt; 11.08, 9.42, 11.08, 6.92, 11.25, 6.50, 6.92, 15.00, 7.17, …\n$ sex         &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"f…\n$ iqv_inf     &lt;dbl&gt; 3, 7, 4, 4, 5, 5, 7, 3, 13, 7, 6, 11, 11, 6, 9, 4, 13, 4, …\n$ iqv_comp    &lt;dbl&gt; 4, 9, 9, 6, 4, 12, 9, 1, 10, 9, 10, 14, 12, 4, 11, 6, 17, …\n$ iqv_ar      &lt;dbl&gt; 3, 7, 5, 6, 8, 11, 10, 3, 14, 12, 6, 14, 8, 5, 11, 4, 13, …\n$ iqv_ds      &lt;dbl&gt; 5, 6, 3, 6, 5, 9, 7, 6, 13, 9, 7, 11, 8, 8, 9, 8, 14, 12, …\n$ iqv_raw     &lt;dbl&gt; 15, 29, 21, 22, 22, 37, 33, 13, 50, 37, 29, 50, 39, 23, 40…\n$ iqp_pc      &lt;dbl&gt; 10, 8, 10, 5, 5, 14, 10, 6, 8, 6, 6, 13, 8, 9, 14, 9, 16, …\n$ iqp_bd      &lt;dbl&gt; 8, 7, 7, 8, 10, 7, 8, 2, 15, 9, 8, 13, 9, 7, 17, 8, 16, 9,…\n$ iqp_oa      &lt;dbl&gt; 8, 10, 7, 5, 13, 7, 7, 3, 14, 12, 3, 15, 11, 6, 13, 13, 16…\n$ iqp_cod     &lt;dbl&gt; 5, 9, 20, 13, 12, 10, 16, 8, 9, 13, 9, 20, 12, 12, 16, 12,…\n$ iqp_raw     &lt;dbl&gt; 31, 34, 44, 31, 40, 38, 41, 19, 46, 40, 26, 61, 40, 34, 60…\n$ hh_index    &lt;dbl&gt; 77, 77, 30, 77, 62, 72, 54, 73, 22, 77, 63, 48, 48, 48, 48…\n$ iqv         &lt;dbl&gt; 61, 82, 70, 72, 72, 95, 89, 57, 116, 95, 82, 116, NA, 74, …\n$ iqp         &lt;dbl&gt; 85, 90, 107, 85, 100, 97, 101, 64, 111, 100, 76, 136, 100,…\n$ iqf         &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, …\n$ iq_type     &lt;chr&gt; \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"W…\n$ lead_grp    &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ Group       &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ ld72        &lt;dbl&gt; 25, 31, 30, 29, 2, 29, 25, 24, 24, 31, 21, 29, 32, 36, 30,…\n$ ld73        &lt;dbl&gt; 18, 28, 29, 30, 34, 25, 24, 15, 16, 24, 19, 27, 29, 32, 25…\n$ fst2yrs     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ totyrs      &lt;dbl&gt; 11, 6, 5, 5, 11, 6, 6, 15, 7, 7, 12, 10, 12, 12, 10, 10, 1…\n$ pica        &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ colic       &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ clumsi      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no…\n$ irrit       &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ convul      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ `_2plat_r`  &lt;dbl&gt; 16, 17, 16, 11, 17, 16, 10, 19, 15, 16, 17, 17, 15, 23, 19…\n$ `_2plar_l`  &lt;dbl&gt; 16, 16, 17, 9, 16, 14, 13, 14, 13, 11, 16, 17, 14, 21, 20,…\n$ visrea_r    &lt;dbl&gt; 36, 23, 20, 34, 26, 29, 29, 30, 31, 26, 19, 22, 19, 26, 17…\n$ visrea_l    &lt;dbl&gt; 38, 19, 24, 42, 34, 26, 29, 32, 28, 25, 19, 24, 17, 23, 16…\n$ audrea_r    &lt;dbl&gt; 27, 18, 16, 35, 31, 28, 30, 33, 31, 27, 16, 22, 18, 25, 17…\n$ audrea_l    &lt;dbl&gt; 25, 28, 17, 30, 33, 27, 27, 24, 29, 21, 19, 23, 20, 28, 16…\n$ fwt_r       &lt;dbl&gt; 72, 61, 46, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 44, 74…\n$ fwt_l       &lt;dbl&gt; 52, 48, 49, 41, 42, 35, 39, 58, 40, 37, 44, 48, 47, 53, 63…\n$ hyperact    &lt;dbl&gt; NA, 0, NA, 2, NA, 0, 0, NA, 0, 0, NA, 1, NA, NA, NA, 2, NA…\n$ maxfwt      &lt;dbl&gt; 72, 61, 49, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 53, 74…\n$ log_iqv_inf &lt;dbl&gt; 1.0986, 1.9459, 1.3863, 1.3863, 1.6094, 1.6094, 1.9459, 1.…\n\n\nUse View() to see a spreadsheet of the data frame (never put this in a Quarto file). Note the capital “V”.\n\nView(lead)\n\nUse rename() to rename variables.\n\nlead &lt;- rename(lead, ID = id)\nhead(lead)\n\n# A tibble: 6 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse filter() to remove rows.\n\nUse == to select rows based on equality\nUse &lt; and &gt; to select rows based on inequality\nUse &lt;= and &gt;= to select rows based on inequality/equality.\n\n\nfilter(lead, Group == \"control\")\n\n# A tibble: 78 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   101 2.5-…  11.1  male        3        4      3      5      15     10      8\n 2   102 2.5-…   9.42 male        7        9      7      6      29      8      7\n 3   103 2.5-…  11.1  male        4        9      5      3      21     10      7\n 4   104 1-2.5   6.92 male        4        6      6      6      22      5      8\n 5   105 0-1    11.2  male        5        4      8      5      22      5     10\n 6   106 1-2.5   6.5  male        5       12     11      9      37     14      7\n 7   107 2.5-…   6.92 male        7        9     10      7      33     10      8\n 8   108 0-1    15    fema…       3        1      3      6      13      6      2\n 9   109 1-2.5   7.17 fema…      13       10     14     13      50      8     15\n10   110 1-2.5   7.25 male        7        9     12      9      37      6      9\n# ℹ 68 more rows\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;, …\n\nfilter(lead, ageyrs &lt; 4)  \n\n# A tibble: 7 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   403 0-1      3.92 male        5        6     10      7      28     12      8\n2   406 1-2.5    3.75 male        9        4     10      7      30      8     11\n3   504 1-2.5    3.75 male        8        7      8      7      30      8      7\n4   505 1-2.5    3.75 fema…       6        5      6      3      20      8     12\n5   602 1-2.5    3.75 fema…       6       11      8     11      36     11     10\n6   606 2.5-4…   3.83 male       12        9     18      8      47     12     10\n7   607 0-1      3.92 male        8        4     11      1      24     13      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\nfilter(lead, ageyrs &gt; 4, ageyrs &lt; 5)\n\n# A tibble: 14 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   401 2.5-…   4.33 male        6       13     13      9      41     11     11\n 2   402 1-2.5   4.83 fema…       9        7     13      8      37     11     12\n 3   404 0-1     4.58 male        6        3      4      2      15     12      7\n 4   405 0-1     4.5  male        6        9      9     12      36      8      9\n 5   407 2.5-…   4.25 male        7        4      8      6      25     10     12\n 6   408 2.5-…   4.33 male        7        6      4      5      22      9      6\n 7   409 1-2.5   4.33 fema…       8        8     11     11      38     11      9\n 8   411 1-2.5   4.33 fema…       8        9     14      7      38     14      9\n 9   412 2.5-…   4.75 fema…       7        7     10      8      32     13     13\n10   414 0-1     4.5  male        6        3      7      2      18      6      8\n11   501 0-1     4.17 male       11        7      8      5      31     11     10\n12   502 2.5-…   4.58 male        7        7      9      3      26     10      3\n13   601 1-2.5   4.33 male        9        6      9      8      32      8      9\n14   604 1-2.5   4.58 male        7       10      6     12      35      9     11\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\n\n\nExerciseSolution\n\n\nThe birthweight data is in ounces. There are abour 28.3495 grams in an ounce. Create a new variable called weight_g that is the weight of the baby in grams.\n\n\n\nbirthweight &lt;- mutate(birthweight, weight_g = 28.3495 * weight)\nglimpse(birthweight)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the birthweight data, select just babies that are greater than or equal to 150 ounces.\n\n\n\nfilter(birthweight, weight &gt;= 150)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the lead data, select individuals who are both in the control group and are at least 15.\n\n\n\nfilter(lead, Group == \"control\", ageyrs &gt;= 15)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the lead data, rename ageyrs to just age\n\n\n\nrename(lead, age = ageyrs)"
  },
  {
    "objectID": "00_course_outline/00_math_prereqs.html",
    "href": "00_course_outline/00_math_prereqs.html",
    "title": "00 Math Prerequisites",
    "section": "",
    "text": "We do not emphasize the mathematics underlying statistics in this course. However, you should have some familiarity with pre-calculus topics. Here are some facts you should know off the top of your head:\n\nSummations\n\nCapital-sigma notation is useful for writing sums of many numbers/variables: \\[\\sum_{i = 1}^n x_i = x_1 + x_2 + \\cdots x_n\\]\nIf you sum a constant \\(n\\) times, you get \\(n\\) times that constant: \\[\\sum_{i = 1}^n a = an\\]\nYou can factor out multiplicative constants that don’t depend on the summing index: \\[\\sum_{i = 1}^n cx_i = c\\sum_{i = 1}^n x_i\\]\nThe order that you sum elements does not matter: \\[\\sum_{i = 1}^n (x_i + y_i) = \\sum_{i = 1}^n x_i + \\sum_{i = 1}^n y_i\\]\n\n\nExerciseSolution\n\n\nLet \\(X_1 = 5\\), \\(X_2 = 10\\), \\(X_3 = 1\\), \\(X_4 = 3\\)\n\nWhat is \\(\\sum_{i=1}^4 X_i\\)?\nWhat is \\(\\sum_{i=2}^3 X_i\\)?\nWhat is \\(\\sum_{i=2}^3 2X_i\\)?\nWhat is \\(\\sum_{i=1}^4 (X_i + 1)\\)?\nWhat is \\(\\sum_{i=1}^4 X_i^2\\)?\n\n\n\n\n\\(X_1 + X_2 + X_3 + X_4 = 5 + 10 + 1 + 3 = 19\\)\n\\(X_2 + X_3= 10 + 1 = 11\\)\n\\(2X_2 + 2X_3 = 2\\times 10 + 2\\times 1 = 22\\). You can also just multiply part 2 by 2 since \\(\\sum_{i=2}^3 2X_i = 2 \\sum_{i=2}^3 X_i\\).\n\\(\\sum_{i=1}^4 (X_i + 1) = \\sum_{i=1}^4 X_i + \\sum_{i=1}^4 1 = \\sum_{i=1}^4 X_i + 4 = 19 + 4 = 23\\)\n\\(X_1^2 + X_2^2 + X_3^2 + X_4^2 = 5^2 + 10^2 + 1^2 + 3^2 = 135\\)\n\n\n\n\n\n\nPowers, Exponentials, Logarithms\n\n\\(e^{ab} = {e^{a}}^{b} = {e^{b}}^{a}\\)\n\\(e^{a+b+c} = e^a(e^{b+c}) = e^ae^be^c\\)\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(a/b) = \\log(a) - \\log(b)\\)\n\\(x^ny^n = (xy)^n\\)"
  },
  {
    "objectID": "00_course_outline/00_overview.html",
    "href": "00_course_outline/00_overview.html",
    "title": "Course Overview",
    "section": "",
    "text": "Learning Objectives\n\nOverview of the course plus some reminders from STAT 202/203/204\nP-values/confidence intervals.\n\\(t\\)-tests for means in R.\nProportion tests in R.\n\n\n\nProbability and Distributions in R.\n\nDistribution: The possible values of a variable and how often it takes those values.\nA density describes the distribution of a quantitative variable. You can think of it as approximating a histogram. It is a curve where\n\nThe area under the curve between any two points is approximately the probability of being between those two points.\nThe total area under the curve is 1 (something must happen).\nThe curve is never negative (can’t have negative probabilities).\n\nThe density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nExercise: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?\nThe \\(t\\)-distribution shows up a lot in Statistics.\n\nIt is also bell-curved but has “thicker tails” (more extreme observations are more likely).\nIt is always centered at 0.\nIt only has one parameter, called the “degrees of freedom”, which determines how thick the tails are.\nSmaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.\nIf the degrees of freedom is large enough, the \\(t\\)-distribution is approximately the same as a normal distribution with mean 0 and variance 1.\n\n\\(t\\)-distributions with different degrees of freedom:\n\n\n\n\n\n\n\n\n\nDensity Function\n\ndt(x = -6, df = 2)\n\n[1] 0.004269\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation\n\nsamp &lt;- rt(n = 1000, df = 2)\nhead(samp)\n\n[1]  0.89857 -1.07176  0.09639  0.79371 -0.42428 -0.64561\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\npt(q = 2, df = 2)\n\n[1] 0.9082\n\n\n\n\n\n\n\n\n\n\n\nQuantile Function\n\nqt(p = 0.9082, df = 2)\n\n[1] 1.999\n\n\n\n\n\n\n\n\n\n\n\nThere are many other distributions implemented in R. To see the most common, run:\n\nhelp(\"Distributions\")\n\n\n\n\nAll of Statistics\n\nObservational/experimental Units: The people/places/things/animals/groups that we collect information about. Also known as “individuals” or “cases”. Sometimes I just say “units”.\nVariable: A property of the observational/experimental units.\n\nE.g.: height of a person, area of a country, marital status.\n\nValue: The specific level of a variable for an observational/experimental unit.\n\nE.g.: Bob is 5’11’’, China has an area of 3,705,407 square miles, Jane is divorced.\n\nQuantitative Variable: The variable takes on numerical values where arithmetic operations (plus/minus/divide/times) make sense.\n\nE.g.: height, weight, area, income.\nCounterexample: Phone numbers, social security numbers.\n\nCategorical Variable: The variable puts observational/experimental units into different groups/categories based on the values of that variable.\n\nE.g.: race/ethnicity, marital status, religion.\n\nBinary Variable: A categorical variable that takes on only two values.\n\nE.g.: dead/alive, treatment/control.\n\nPopulation: The collection of all observational units we are interested in.\nParameter: A numerical summary of the population.\n\nE.g.: Average height, proportion of people who are divorced, standard deviation of weight.\n\nSample: A subset of the population (some observational units, but not all of them).\nStatistic: A numeric summary of the sample.\n\nE.g.: Average height of the sample, proportion of people who are divorced in the sample, standard deviation of weight of a sample.\n\nGraphic:\n \nSampling Distribution: The distribution of a statistic over many hypothetical random samples from the population.\n \nAll of Statistics: We see a pattern in the sample.\n\nEstimation: Guess the pattern in the population based on the sample. Guess a parameter with a statistic. A statistic which is a guess for a parameter is called an estimate.\nHypothesis Testing: Ask if the pattern we see in the sample also exists in the population. Test if a parameter is some value.\nConfidence Intervals: Quantify our (un)certainty of the pattern in the population based on the sample. Provide a range of likely parameter values.\n\nWe will go through a lot of examples of this below\n\nlibrary(tidyverse)\nlibrary(broom)\n\nExercise: Read about the boneden data here What are the observational units? What are the variables? Which are quantitative and which are categorical?\nExercise: Read about the lead data here. What are the observational units? What are the variables? Which are quantitative and which are categorical?\n\n\n\nPattern: Mean is shifted (one quantitative variable)\n\nExample: The boneden data explores the difference in bone density between a heavier and a lighter smoking twin.\nObservational Units: The twins.\nPopulation: All twins where one smokes more than the other.\nSample: The 41 twins in our study.\nVariable: The difference in lumbar spine density (in g/cm2) between the twins. We derived this quantitative variable by subtracting one density from another.\n\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\n\nPattern: Use a histogram/boxplot to visualize the shift from 0.\n\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(bins = 20, fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = 0, lty = 2) +\n  xlab(\"Difference in Bone Density\")\n\n\n\n\n\n\n\n\nGraphic:\n \nParameter of interest: Mean difference in bone density for all twins.\nEstimate: Use sample mean\n\nboneden %&gt;%\n  summarize(meandiff = mean(ls_diff))\n\n# A tibble: 1 × 1\n  meandiff\n     &lt;dbl&gt;\n1   0.0359\n\n\n0.03585 is our “best guess” for the parameter, but it is almost certainly not the value of the parameter (since we didn’t measure everyone).\nHypothesis Testing:\n\nWe are interested in if the mean difference is different from 0.\nTwo possibilities:\n\nNull Hypothesis: Mean is not different from 0, we just happened by chance to get twins that had some difference in density.\nAlternative Hypothesis: Mean is different from 0.\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a \\(p\\)-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\np-value: the probability that you would see data as or more supportive of the alternative hypothesis than what you saw assuming that the null hypothesis is true.\n\nGraphic:\n \nThe distribution of possible null sample means is given by statistical theory. Specifically, the \\(t\\)-statistic (mean divided by the standard deviation of the sampling distribution of the mean) has a \\(t\\) distribution with \\(n - 1\\) degrees of freedom (\\(n\\) is the sample size). It works as long as your data aren’t too skewed or if you have a large enough sample size.\nFunction: t.test()\n\ntout &lt;- t.test(ls_diff ~ 1, data = boneden)\ntout\n\n\n  One Sample t-test\n\ndata:  ls_diff\nt = 2.6, df = 40, p-value = 0.01\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.007986 0.063721\nsample estimates:\nmean of x \n  0.03585 \n\n\nThe tidy() function from the broom package will format the output of common procedures to a convenient data frame.\n\ntdf &lt;- tidy(tout)\ntdf$estimate\n\nmean of x \n  0.03585 \n\ntdf$p.value\n\n[1] 0.01299\n\n\nWe often want a range of “likely” values. These are called confidence intervals. t.test() will return these confidence intervals, giving lowest and highest likely values for the mean difference in bone density:\n\ntdf$conf.low\n\n[1] 0.007986\n\ntdf$conf.high\n\n[1] 0.06372\n\n\nInterpreting confidence intervals:\n\nCORRECT: We used a procedure that would capture the true parameter in 95% of repeated samples.\nCORRECT: Prior to sampling, the probability of capturing the true parameter is 0.95.\nWRONG: After sampling, the probability of capturing the true parameter is 0.95.\n\nBecause after sampling the parameter is either in the interval or it’s not. We just don’t know which.\n\nWRONG: 95% of twins have bone density differences within the bounds of the 95% confidence interval.\n\nBecause confidence intervals are statements about parameters, not observational units or statistics.\n\n\nGraphic:\n \nIntuition: Statistical theory tells us that the sample mean will be within (approximately) 2 standard deviations of the population mean in 95% of repeated samples. This is two standard deviations of the sampling distribution of the sample mean, not two standard deviations of the sample. So we just add and subtract (approximately) two standard deviations of the sampling distribution from the sample mean.\nExercise: The birthweight data available here contains the birthweights (in ounces) of 1000 newborns born in a Boston area hospital. Wikipedia says the average birthweight for individuals of European and African descent is 123 ounces. Does this Boston hospital have the same mean as what Wikipedia says? Explain.\n\n\n\nPattern: Means of two groups are different (one quantitative, one binary)\n\nExample: IQ differences between children with high levels of lead (exposed) and those with low levels of lead (control).\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead |&gt;\n  select(Group, iqf) |&gt;\n  glimpse()\n\nRows: 124\nColumns: 2\n$ Group &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"control\"…\n$ iqf   &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, 118, 8…\n\n\nObservational Units: Children\nPopulation: All children\nSample: The 120 children for whom we have both lead and IQ measurements.\nVariables: The lead level group (binary/categorical) and the full scale IQ (quantitative).\nPattern: Use a boxplot to see if the groups differ.\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nParameter of interest: Difference in mean IQ levels between the control and exposed groups.\nEstimate: The difference in mean IQ between the two groups in our sample.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(meaniq = mean(iqf, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  Group   meaniq\n  &lt;chr&gt;    &lt;dbl&gt;\n1 control   92.6\n2 exposed   88.0\n\n92.55 - 88.02\n\n[1] 4.53\n\n\nThe control group is about 4.5 points higher on average\nHypothesis Test:\n\nWe want to know if the difference in the mean IQ in the two groups is actually different.\nTwo possibilities:\n\nNull Hypothesis: The mean IQs are the same in the two groups. We just happened by chance to get a lower IQ lead group and a higher IQ control group.\nAlternative Hypothesis: The mean IQ are different in the two groups.\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\n\n\nGraphic:\n \nThe distribution of possible null sample means comes from statistical theory. The t-statistic has a \\(t\\) distribution with a complicated degrees of freedom.\nFunction: t.test(). The quantitative variable goes to the left of the tilde and the binary variable goes to the right of the tilde.\n\ntout &lt;- t.test(iqf ~ Group, data = lead)\ntdf &lt;- tidy(tout)\ntdf$estimate\n\n[1] 4.532\n\ntdf$p.value\n\n[1] 0.07966\n\n\nt.test() also returns a 95% confidence interval for the difference in means. This has the exact same interpretation as in the previous section.\n\nc(tdf$conf.low, tdf$conf.high)\n\n[1] -0.5448  9.6094\n\n\nAssumptions (in decreasing order of importance):\n\nIndependence: conditional on group, IQ of one child doesn’t give us any information on the IQs of any other children (reasonable).\nApproximate normality: The distribution of IQ’s is bell-curved in group. Doesn’t matter for moderate-large sample sizes because of the central limit theorem.\n\nExercise: Is there a difference between control and exposed groups when it comes to the finger-wrist tapping test in the dominant hand (maxfwt).\n\n\n\nPattern: Proportion is shifted (one binary variable).\n\nThe exposed individuals were mostly males. There were 30 males and 16 females. In the US, about 51.22% of all births are boys. Are boys more likely to be recruited to the study than girls?\n\nlead |&gt;\n  filter(Group == \"exposed\") |&gt;\n  group_by(sex) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female    16\n2 male      30\n\n\nObservational Units: U.S. children exposed to lead\nPopulation: All U.S. children exposed to lead\nSample: The 46 children in our sample who were exposed to lead.\nVariable: Sex (male/female)\nPattern: Calculate sample proportion.\n\n30 / 46\n\n[1] 0.6522\n\n\nParameter of interest: Proportion of children exposed to lead who are boys\nEstimate with sample proportion, 0.6522\nHypothesis Testing:\n\nWe are interested in if our sample had some bias in selecting more boys.\nTwo possibilities:\n\nNull Hypothesis: Probability of a boy being included in the sample is 0.5122. We just happened by chance to get a lot more boys.\nAlternative Hypothesis: There is bias and the probability of a boy being in the sample is greater than for a girl(because boys are more likely to be exposed to lead, or because they were more likely to be recruited to the study).\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If this probability is high, we don’t conclude anything.\n\nGraphic:\n \nThe distribution of possible null sample proportions comes from statistical theory. The number of successes has a binomial distribution with success probability 0.5122 and size parameter equal to the sample size. The sample proportion is the number successes divided by the sample size.\nFunction: prop.test() (when you have a large number of both successes and failures) or binom.test() (for any number of successes and failures).\n\nbout &lt;- tidy(binom.test(x = 30, n = 46, p = 0.5122))\nbout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0757    0.498     0.786\n\n\n\npout &lt;- tidy(prop.test(x = 30, n = 46, p = 0.5122))\npout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0798    0.497     0.782\n\n\nExercise: Is there a sex bias for the control group?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 320: Biostatistics",
    "section": "",
    "text": "Syllabus\nPractice Problems\n\n00 R\n\nIntroduction to R\nPlotting in R\nDocuments in R\n\n01 Overview\n\nMath Prereqs\nCourse Overview\n\n02 Descriptive Statistics\n\nChapter 2 Notes (Descriptive Statistics)\n\n03 Probability\n\nChapter 3 Notes (Probability)\n\nConfusion Matrix\n\nChapter 4 Notes (Discrete Probability Distributions)\nChapter 5 Notes (Continuous Probability Distributions)\n\n04 Estimation\n\nChapter 6 Notes (Estimation)\n\nRandom Selection/Assignment\nCentral Limit Theorem Illustration\nCI Interpretation\nt-distribution\nBone Density Case Study\nchi-squared distribution\nEstimating Binomial Proportion\n\n\n05 Testing\n\nChapter 7 Notes (One Sample Hypothesis Tests)\n\nOne Sample \\(t\\)-Tests in R\nPower Calculations in R\nOne Sample Binomial Tests in R\nOne Sample Binomial Power Calculations\n\nChapter 8 Notes (Two Sample Hypothesis Tests)\n\nTwo Sample \\(t\\)-Tests in R\n\n\n06 Nonparametric Methods\n\nChapter 9 Notes (Nonparametric Methods)\n\nOne Sample Inference in R\nTwo Sample Inference in R\n\n\n07 Categorical Tests\n\nChapter 10 Notes (Categorical Methods)\n\n2x2 Contingency Tables in R\nMcNemar’s Test in R\nLarger Contingency Tables in R\nCohen’s Kappa in R\n\n\nNext stop:\n\nRegression\n\n\nHandwritten Notes:\n\nChapter 2 Notes (Descriptive Statistics)\nChapter 3 Notes (Probability)\nChapter 4 Notes (Discrete Probability Distributions)\nChapter 5 Notes (Continuous Probability Distributions)\nChapter 6 Notes (Estimation)\nChapter 7 Notes (One Sample Hypothesis Tests)\nChapter 8 Notes (Two Sample Hypothesis Tests)\nChapter 9 Notes (Nonparametric Methods)\nChapter 10 Notes (Categorical Methods)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STAT 320 - Biostatistics",
    "section": "",
    "text": "Instructor: Dr. David Gerard\nEmail: dgerard@american.edu\nOffice: DMTI 106E\n\n\nQ1 Learning Outcomes:\n \n\nStudents will solve quantitative problems including approaches that go beyond memorized procedures.\nStudents will demonstrate an understanding of mathematical relationships from multiple perspectives, such as functions from graphical, verbal, numerical, and analytic points of view.\n\n\n\nQ2 Learning Outcomes:\n \n\nTranslate real-world questions or intellectual inquiries into quantitative frameworks.\nSelect and apply appropriate quantitative methods or reasoning.\nDraw appropriate insights from the application of a quantitative framework.\nExplain quantitative reasoning and insights using appropriate forms of representation so that others could replicate the findings.\n\n\n\nCourse Description\nSTAT-320 is an introduction to the statistical methodology commonly used in public health, medical, and biological studies. This course emphasizes working with data and communicating statistical ideas. A breadth of topics will be covered including: study design, tests of significance, confidence intervals, t-procedures, chi-square and Fisher’s exact test, linear regression, logistic regression, analysis of variance, nonparametric methods, and more advanced topics as time permits. The R computer program will be used to conduct analyses.\nThe major focus for this course is the ideas behind, and the methods for, drawing conclusions about a population from a sample. At the end of this course you will be expected to identify the major concepts related to statistical reasoning and to statistical inferences for drawing such conclusions, recognize how these concepts are used in disciplines related to health and medicine, and implement the methods yourself in statistical analyses using the methods covered. In particular, you are expected to be able to (1) identify the appropriate statistical model or models for a given analysis, (2) write the model in the correct notation, (3) implement the model in the R software package on a given set of data, (4) interpret the output in the context of the study, (5) diagnose model deficiences, (6) suggest improvements to the model if necessary, and (7) summarize the results of the analysis. Work will be a balance between understanding the concepts underlying a method, implementation of the method, and interpretation of the results.\n\n\nRequired Text\n\nRosner, B. (2016) Fundamentals of Biostatistics, Eighth Edition. Brooks/Cole, Boston, MA, USA.\n\n\nThere will be occasional readings from other sources, such as journal articles, for class discussion or for homework assignments. These will be posted in Canvas or links will be given to find these online.\n\n\n\nGrading\n\n\n\n\n\nAssignment\nPercent\n\n\n\n\nHomeworks\n20%\n\n\nParticipation\n20%\n\n\nExams 1, 2, and 3\n60%\n\n\n\n\n\n\n\n\n\nParticipation:\n\nShow up to class. Stay off your phones. Engage with the in-class exercises. You don’t need to complete or turn in the exercises—just make a genuine effort to try them.\nParticipation points will only be deducted under the following circumstances:\n\nYou miss many classes without explanation. Occasional absences are fine. I will take attendance most days, and if you start missing a lot of class, I’ll send you warnings about the impact on your grade before deducting any points.\nYou’re not making a good-faith effort on in-class exercises. For example, if you’re clearly working on something else, I’ll make a note and begin sending you warnings before deducting points. Again, you just need to try the exercises—you don’t need to complete them perfectly.\nYou engage in behavior that is clearly disrespectful to me or your classmates. This includes things like repeated interruptions or dismissive comments. Our classroom is a space for learning, where everyone is respected and discourse remains civil and scholarly.\n\n\nExams\n\nExams are not officially cumulative, but since statistical concepts build on one another, they are effectively cumulative.\nYou may bring one handwritten reference sheet (8.5’’ × 11’’, both sides). Typed sheets are not allowed.\nNo other resources are permitted.\n\nIf you touch your calculator, phone, computer, smartwatch, smart glasses, or any similar device during the exam, it will result in an automatic fail for the course.\n\nI will drop your lowest exam score. Because of this:\n\nNo make-up exams will be offered. If you miss an exam, it will count as your drop.\nYou may not leave the room during the exam unless you are turning it in.\n\nIf you leave mid-exam (even for an emergency), it will count as your drop.\nIf you are unable to remain in the room for the full 1 hour and 15 minutes, please contact ASAC to request an official accommodation.\n\nIf you miss two exams, you should consider withdrawing from the course. The last day to withdraw is October 31, 2025.\n\n\nHomeworks\n\nHomework assignments are designed to reinforce your understanding of course concepts and to help you prepare for the exams. You are permitted to use generative AI tools (e.g., ChatGPT) to assist with homework. However, I strongly recommend using such tools only after you have made a sincere effort to solve the problems on your own, such as checking your work or seeking clarification.\nEducational research (e.g. https://doi.org/10.1177/1529100612453266) consistently shows that actively working through practice problems is among the most effective ways to learn quantitative material. In contrast, passively reading solutions—whether written by others or generated by AI—offers minimal learning benefit. If you rely primarily on AI-generated solutions, you may not be adequately prepared for the exams.\nTo allow some flexibility, your lowest homework score will be dropped.\n\n\nUsual grade cutoffs will be used:\n\n\n\n\n\nGrade\nLower\nUpper\n\n\n\n\nA\n93\n100\n\n\nA-\n90\n92\n\n\nB+\n88\n89\n\n\nB\n83\n87\n\n\nB-\n80\n82\n\n\nC+\n78\n79\n\n\nC\n73\n77\n\n\nC-\n70\n72\n\n\nD\n60\n69\n\n\nF\n0\n59\n\n\n\n\n\nIndividual assignments will not be curved. However, at the discretion of the instructor, the overall course grade at the end of the semester may be curved.\n\n\nStudy Recommendations\nIf you are interested, the study techniques in https://doi.org/10.1177/1529100612453266 are listed as\n\nHigh Utility:\n\nPractice testing\n\nSelf-testing or taking practice tests over to-be-learned material\n\nDistributed practice\n\nImplementing a schedule of practice that spreads out study activities over time\n\n\nModerate Utility:\n\nElaborative interrogation:\n\nGenerating an explanation for why an explicitly stated fact or concept is true\n\nSelf-explanation:\n\nExplaining how new information is related to known information, or explaining steps taken during problem solving\n\nInterleaved practice\n\nImplementing a schedule of practice that mixes different kinds of problems, or a schedule of study that mixes different kinds of material, within a single study session\n\n\nLow Utility:\n\nRereading\n\nRestudying text material again after an initial reading\n\nSummarization\n\nWriting summaries (of various lengths) of to-be-learned texts\n\nHighlighting\n\nMarking potentially important portions of to-be-learned materials while reading\n\nThe keyword mnemonic\n\nUsing keywords and mental imagery to associate verbal materials\n\nImagery use for text learning\n\nAttempting to form mental images of text materials while reading or listening\n\n\n\n\n\nLate Work Policy\n\nAll assignments must be submitted on the day they are due.\nEach student will have two three-day extensions, where you can turn in the assignment on Thursday by end-of-day.\nPlease just let me know ahead of time that you will be using one of your two extensions.\nPlease do not tell me why you need the extension. Any reason is a fine reason.\nAny homeworks not submitted by the due date will receive a grade of 0.\n\n\n\nImportant Dates\n\n09/01: Labor Day (no classes or office hours).\n09/29: (tentative): Exam 1 (Chapters 1 through 5)\n10/30: (tentative): Exam 2 (Chapters 6 though 8)\n10/31: Last day to withdraw.\n11/24: Classes meat online via Zoom (or a recorded lecture). No in-person class.\n11/27: Thanksgiving break (no classes or office hours).\nTBD (sometime between 12/08 and 12/12): Exam 3 (Chapters 9 through 11).\n\n\n\nComputing and Software\nWe will use the R computing language to complete some assignment questions. R is free and may be downloaded from the R website (http://cran.r-project.org/). In addition, I highly recommend you interface with R through the free RStudio IDE (https://www.rstudio.com/). R and RStudio are also available on computers in the Anderson Computing Complex in addition to various labs across campus. R Studio may also be run from your web browser using American University’s Virtual Applications System. Please see me during office hours if you have questions regarding R.\n\n\nData\nData sets for homeworks assignments and examples from the textbook are available on the Data page. Almost all of these are cleaned versions of the data from the book’s companion website.\n\n\nAcademic Integrity\n\nStandards of academic conduct are set forth in the university’s Academic Integrity Code. By registering for this course, students have acknowledged their awareness of the Academic Integrity Code and they are obliged to become familiar with their rights and responsibilities as defined by the Code. Violations of the Academic Integrity Code will not be treated lightly and disciplinary action will be taken should violations occur. This includes cheating, fabrication, and plagiarism.\nI expect you to work with others and me, and I expect you to use online resources as you work on your assignments. However, your submissions must be composed of your own thoughts, coding, and words. You should be able to explain your work on assignments/projects and your rationale. Based on your explanation (or lack thereof), I may modify your grade.\nYou can use generative AI (e.g. ChatGPT, CoPilot, etc) on the homeworks if you want. But\n\nThese are your only study exercises for the exams. So I wouldn’t use AI to do them for me except to check my work after I am done.\nYou are still expected to “own” all of your responses. I reserve the right to ask you to explain any of your solutions. If you write something weird or too advanced in the homework, I’ll call you in and ask you questions about it. Based on your explanation (or lack thereof), I may modify your grade.\n\nNo resources are allowed for the exam except the 1 page (8.5’’ by 11’’) handwritten cheat sheet (and a pen or pencil, of course). If you touch your phone/computer/smart watch/smart glasses/etc during the exam then that is an automatic fail for the course.\nAll solutions that I provide are under my copyright. These solutions are for personal use only and may not be distributed to anyone else. Giving these solutions to others, including other students or posting them on the internet, is a violation of my copyright and a violation of the student code of conduct.\n\n\n\nSharing Course Content:\nStudents are not permitted to make visual or audio recordings (including livestreams) of lectures or any class-related content or use any type of recording device unless prior permission from the instructor is obtained and there are no objections from any student in the class. If permission is granted, only students registered in the course may use or share recordings and any electronic copies of course materials (e.g., PowerPoints, formulas, lecture notes, and any discussions – online or otherwise). Use is limited to educational purposes even after the end of the course. Exceptions will be made for students who present a signed Letter of Accommodation from the Academic Support and Access Center. Further details are available from the ASAC website.\n\n\nUse of Student Work\nThe professor will use academic work that you complete for educational purposes in this course during this semester. Your registration and continued enrollment constitute your consent.\n\n\nSyllabus Change Policy\nThis syllabus is a guide for the course and is subject to change with advanced notice. These changes may come via email or Canvas. Make sure to check Canvas and your university-supplied email regularly. You are accountable for all such communications."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#learning-objectives",
    "href": "00_course_outline/00_course_outline.html#learning-objectives",
    "title": "Course Outline for Stat 320",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nThree aspects of Statistics\nPopulation/Sample"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics",
    "href": "00_course_outline/00_course_outline.html#statistics",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nData — Numerical or qualitative descriptions of people/places/things that we want to study."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-1",
    "href": "00_course_outline/00_course_outline.html#statistics-1",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nSome examples\n\nLead Exposure\n\nData: Retrospective study measuring lead exposure in children along with variou outcome variables like IQ score, different measures of neurological function, and hyperactivity assessmenets.\nQuestion: What are the neurological and behavioral effects of lead exposure in young children?\n\nSmoking and bone density\n\nData: Pairs of twins, one of whom is a lighter smoker and one of whome is a heavier smoker. Different bone density measures were taken.\nQuestion: Do individuals who smoke have lower densities?"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-2",
    "href": "00_course_outline/00_course_outline.html#statistics-2",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nThree aspects:\n\nData Design\nData Description\nData Inference — informed by Probability"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-design",
    "href": "00_course_outline/00_course_outline.html#data-design",
    "title": "Course Outline for Stat 320",
    "section": "Data Design",
    "text": "Data Design\nWhere do we get data?\n\nWhat is the proper way to collect data?\nWhen can we claim a causal connection between variables? (e.g. Does smoking lead to lower bone density? Does lead lead to increased neurological and behavioral problems?)\nWhat are some sources of bias (unwanted systematic tendencies in the data collection)?\nOnly touched on in this course."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-description",
    "href": "00_course_outline/00_course_outline.html#data-description",
    "title": "Course Outline for Stat 320",
    "section": "Data Description",
    "text": "Data Description\nHow do we describe the data we have?\n\nNumerical summaries — use numbers to describe the data.\nGraphical summaries — use pictures to describe the data.\nExploratory data analysis — play with the data to get a “feel” for it.\nLots of R.\nFirst week of the semester."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-inference-probability",
    "href": "00_course_outline/00_course_outline.html#data-inference-probability",
    "title": "Course Outline for Stat 320",
    "section": "Data Inference (Probability)",
    "text": "Data Inference (Probability)\nHow can we tell if our conclusions from the exploratory data analysis are real?\n\nLast thirteen weeks of the semester.\nProbability — subdiscipline of Mathematics that provides a foundation for modeling chance events.\nInference — describing a population (probabilistically) by using information from sample."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#population",
    "href": "00_course_outline/00_course_outline.html#population",
    "title": "Course Outline for Stat 320",
    "section": "Population",
    "text": "Population\nStatisticians (among others) are interested in characteristics of a large group of people/countries/objects\n\nCharacterize/describe neurological and behavioral health of young children\nCharacterize/describe bone health of smokers.\nCharacterize/describe the effectiveness of a drug on a all adults.\n\nA population is a group of individuals/objects/locations for which you want information."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#sample",
    "href": "00_course_outline/00_course_outline.html#sample",
    "title": "Course Outline for Stat 320",
    "section": "Sample",
    "text": "Sample\nIt is usually expensive/impossible to measure characteristics of every case in a population.\nA sample is a subgroup of individuals/objects/locations of the population.\n\nMeasure lead intake and different measures of neurological and behavioral health in a sample of 124 children.\nFind a group of 41 twins who have different smoking behaviors and compare their bone densities."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference",
    "href": "00_course_outline/00_course_outline.html#inference",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\nFrom the sample, describe the population using probability.\n\nWe have strong evidence that lighter smoking twins have heavier lumbar spine bone density than heavier smoking twins (pair \\(t\\)-test \\(p = 0.006494\\)). The corresponding 95% confidence interval for the difference in bone density is 0.00799 g/cm2 0.06372 g/cm2. Since the twins are not a random sample, the generalizability of this result depends on how representative the twins are of the general population of interest. Since this is an observational study and not an experiment, the statistics alone cannot make a claim for causality — such a claim would have to depend on other arguments."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference-1",
    "href": "00_course_outline/00_course_outline.html#inference-1",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\n\nIn this class, we will learn how to formulate such statements and interpret them."
  },
  {
    "objectID": "01_r/01_quarto.html",
    "href": "01_r/01_quarto.html",
    "title": "Introduction to Quarto ",
    "section": "",
    "text": "Quarto  is a file format that is a combination of plain text and R code.\nLots of great educational material is available at https://quarto.org/\nYou write code and commentary of code in one file. You may then compile (RStudio calls this “rendering”) the Quarto  file to many different kinds of output: pdf (including beamer presentations), html (including various presentation formats), Word, PowerPoint, etc.\nQuarto  is useful for:\n\nCommunication of statistical results.\nCollaborating with other data scientists.\nUsing it as a modern lab notebook to do data science.\n\nQuarto  can also make “literate programming” documents for python, Julia, JavaScript, etc…\n\n\n\n\nInstall Quarto  via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/\n\n\n\n\n\n\nOpen up a new Quarto  file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto  file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto  contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto  document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto  file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto :\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: today\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_quarto.html#getting-statrted",
    "href": "01_r/01_quarto.html#getting-statrted",
    "title": "Introduction to Quarto ",
    "section": "",
    "text": "Install Quarto  via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/"
  },
  {
    "objectID": "01_r/01_quarto.html#playing-with-quarto",
    "href": "01_r/01_quarto.html#playing-with-quarto",
    "title": "Introduction to Quarto ",
    "section": "",
    "text": "Open up a new Quarto  file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto  file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto  contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto  document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto  file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto :\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: today\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_ggplot.html",
    "href": "01_r/01_ggplot.html",
    "title": "R Graphics with {ggplot2}",
    "section": "",
    "text": "Basic plotting in R using the {ggplot2} package."
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous",
    "href": "01_r/01_ggplot.html#continuous",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous",
    "text": "Continuous\n\nHistogram:\n\nVariable should be on the \\(x\\)-axis.\nUse the geom_histogram() function.\n\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nMake the bin lines black and the fill white, and change the number of bins.\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\")\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nLoad in the boneden data (see here for a description) and make a histogram of lumbar spine density for the lighter smoking twin with 20 bins. Make the bins red.\n\n\n\nlibrary(readr)\nlibrary(ggplot2)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nRows: 41 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): zyg, men1, men2\ndbl (22): ID, age, ht1, wt1, tea1, cof1, alc1, cur1, pyr1, ls1, fn1, fs1, ht...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(data = boneden, mapping = aes(x = ls1)) +\n  geom_histogram(bins = 20, fill = \"red\")"
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete",
    "href": "01_r/01_ggplot.html#discrete",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete",
    "text": "Discrete\n\nBarplot:\n\nPut the variable on the \\(x\\)-axis.\nUse geom_bar().\n\n\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nWhat variables from the lead data are appropriately plotted using a bar plot? Plot a couple of them.\n\n\n\nggplot(data = lead, mapping = aes(x = sex)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = area)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = lead_grp)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = Group)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = fst2yrs)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = iq_type)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = pica)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = colic)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = clumsi)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = irrit)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = convul)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = maxfwt)) +\n  geom_bar()"
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "href": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous X, Continuous Y",
    "text": "Continuous X, Continuous Y\n\nScatterplot:\n\nSay what variables should be on the \\(x\\)- and \\(y\\)-axes.\nUse geom_point().\n\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point()\n\n\n\n\n\n\n\n\nJitter points to account for overlaying points.\n\nUse geom_jitter() instead of geom_point().\n\n\nggplot(data = lead, mapping = aes(x = totyrs, y = hyperact)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nAdd a Loess Smoother by adding geom_smooth().\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nUsing the boneden data, make a scatterplot exploring the association between the lumbar spine densities for the two twin types (smoking status).\n\n\n\nggplot(data = boneden, mapping = aes(x = ls1, y = ls2)) +\n  geom_point()"
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "href": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete X, Continuous Y",
    "text": "Discrete X, Continuous Y\n\nBoxplot\n\nPlace one variable on \\(x\\)-axis and other on \\(y\\)-axis.\nTypically, but not always, continuous goes on \\(y\\)-axis.\nUse geom_boxplot().\n\n\nggplot(data = lead, mapping = aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nUsing the boneden data, first calculate the difference in lumbar spine densities between the two twins (you’ll need to use mutate() here). Then plot this difference versus zygosity (monozygotic versus dizygotic).\n\n\n\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\nggplot(data = boneden, mapping = aes(x = zyg, y = ls_diff)) +\n  geom_boxplot()"
  },
  {
    "objectID": "01_r/01_figures/formatting.html",
    "href": "01_r/01_figures/formatting.html",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#text-formatting",
    "href": "01_r/01_figures/formatting.html#text-formatting",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#headings",
    "href": "01_r/01_figures/formatting.html#headings",
    "title": "1st Level Header",
    "section": "Headings",
    "text": "Headings"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#nd-level-header",
    "href": "01_r/01_figures/formatting.html#nd-level-header",
    "title": "1st Level Header",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#lists",
    "href": "01_r/01_figures/formatting.html#lists",
    "title": "1st Level Header",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output."
  },
  {
    "objectID": "01_r/01_figures/formatting.html#links-and-images",
    "href": "01_r/01_figures/formatting.html#links-and-images",
    "title": "1st Level Header",
    "section": "Links and images",
    "text": "Links and images\nhttp://example.com\nlinked phrase\n\n\n\noptional caption text"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#tables",
    "href": "01_r/01_figures/formatting.html#tables",
    "title": "1st Level Header",
    "section": "Tables",
    "text": "Tables\n\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell\nContent Cell\n\n\nContent Cell\nContent Cell"
  },
  {
    "objectID": "02_descriptive/02_descriptive.html",
    "href": "02_descriptive/02_descriptive.html",
    "title": "Summary Statistics in R",
    "section": "",
    "text": "We’ll use the lead data as an example. Read about it here.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nYou calculate the summary statistics (mean/median/quantiles/variance/standard deviation) all within a summarize() call.\n\nsummarize(\n  lead, \n  Mean = mean(iqf, na.rm = TRUE), \n  Min = min(iqf, na.rm = TRUE),\n  Q25 = quantile(iqf, probs = 0.25, na.rm = TRUE),\n  Med = median(iqf, na.rm = TRUE), \n  Q75 = quantile(iqf, probs = 0.75, na.rm = TRUE),\n  Max = max(iqf, na.rm = TRUE),\n  Var = var(iqf, na.rm = TRUE),\n  SD = sd(iqf, na.rm = TRUE)\n)\n\n# A tibble: 1 × 8\n   Mean   Min   Q25   Med   Q75   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  90.8    46    80  89.5  98.5   141  212.  14.6\n\n\nThe values on the left of = are the names of the summaries and are up to you.\nThe values on the right of = are the function calls for the summaries.\n\nmean(): the arithmetic mean.\nmin(): the minimum. Same as quantile(x, probs = 0)\nquantile(): the quantiles. You specify which quantile with the probs argument.\nmedian(): the median. Same as quantile(x, probs = 0.5)\nmax(): the maximum. Same as quantile(x, probs = 1)\nvar(): the sample variance.\nsd(): the sample standard deviation.\n\nI have the na.rm = TRUE argument because there are some children who did not have a iqf score. These are “missing” and encoded with NA. If you do not provide that argument, R doesn’t know what those values are and so returns an NA or errors.\n\nsummarize(\n  lead, \n  Mean = mean(iqf), \n  Min = min(iqf),\n  # Q25 = quantile(iqf, probs = 0.25), # errors\n  Med = median(iqf), \n  # Q75 = quantile(iqf, probs = 0.75), # errors\n  Max = max(iqf),\n  Var = var(iqf),\n  SD = sd(iqf)\n)\n\n# A tibble: 1 × 6\n   Mean   Min   Med   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    NA    NA    NA    NA    NA    NA\n\n\nYou can also apply these functions on vectors that you extract from the data frame.\n\nvar(lead$iqf, na.rm = TRUE)\n\n[1] 212.3\n\n\nLet’s demonstrate some properties. Variance is invariant to shift\n\nvar(lead$iqf + 10000, na.rm = TRUE)\n\n[1] 212.3\n\n\nbut scales with the square of the multiplicative factor\n\nvar(10 * lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n10^2 * var(lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n\nThe standard deviation scales with the multiplicative factor because it is the square root of the variance.\n\nsd(10 * lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n10 * sd(lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n\nThe mean and quantiles shift and scale with the additive and multiplicative factors.\n\nmean(lead$iqf * 10 + 20, na.rm = TRUE)\n\n[1] 928.2\n\n10 * mean(lead$iqf, na.rm = TRUE) + 20\n\n[1] 928.2\n\nquantile(lead$iqf * 10 + 20, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)\n\n 25%  50%  75% \n 820  915 1005 \n\n10 * quantile(lead$iqf, probs = c(0.25, 0.5, 0.75), na.rm = TRUE) + 20\n\n 25%  50%  75% \n 820  915 1005 \n\n\nExercise: Calculate the mean and median of the birthweight data. What is the more appropriate measure of center?\nYou can calculate grouped summaries (a summary for each group) by grouping the data first.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(\n    Mean = mean(iqf, na.rm = TRUE),\n    SD = sd(iqf, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 3\n  Group    Mean    SD\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 control  92.6  15.7\n2 exposed  88.0  12.2\n\n\nGroup summaries are where the power of descriptive statistics really comes into play. Here, we see that the exposed group has a lower IQ on average than the control group. Whether this is real signal will have to be answered via a formal hypothesis test. But the descriptive statistics gives us some initial information on the data.\nExercise: What about different lead groups? Calculate descriptive statistics for the different lead groups."
  },
  {
    "objectID": "hw/hw_ch2/hw_ch2.html",
    "href": "hw/hw_ch2/hw_ch2.html",
    "title": "Homework 01L Chapters 1 and 2",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives: - Chapter 2 of Rosner - Descriptive Statistics - Graphics - R\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: &lt; https://www.adobe.com/acrobat/online/merge-pdf.html&gt;\nUse the tidyverse way to answer these questions. If you use the base R way, that’s fine, but I’ll call you in and have you show me that you really know how the base R way works by doing some practice problems in front of me.\n\n\nQuestion 1: Cardiovascular Disease\nRead in the data using the read_csv() function, which you can read about here: https://dcgerard.github.io/stat_320/data.html#lvm\n\nlibrary(tidyverse)\nlvm &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lvm.csv\")\n\n\nUse R to print what variables are in lvm?\n\n\nnames(lvm)\n\n[1] \"ID\"      \"lvmht27\" \"bpcat\"   \"gender\"  \"age\"     \"BMI\"    \n\n\n\nWhat is the arithmetic mean of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(Mean = mean(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat             Mean\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      34.1\n2 normal            29.3\n3 pre-hypertensive  33.8\n\n\n\nIs it appropriate to use the arithmetic mean (by blood pressure group) for LVMI, or should we have used the median? Explain your reasoning and justify with an appropriate box plot.\n\n\nggplot(lvm, aes(x = bpcat, y = lvmht27)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n## Since the distributions are mostly symmetric, and there \n## do not appear to be any major outliers, it looks like\n## the sample mean is an appropriate measure of center.\n\n\nWhat does the boxplot from the previous questiontell you?\n\n\n## The center of group 3 is larger than group 2 than group 1. \n## But groups 1 and 3 are more spread out than group 2. All \n## groups appear to have roughly symmetric distributions.\n\n\nWhat is the standard deviation of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(SD = sd(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat               SD\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      8.56\n2 normal            6.66\n3 pre-hypertensive  5.75\n\n\n\nDoes there appear to be any association between age and lvmht27? Use an appropriate plot to make your case.\n\n\nggplot(lvm, aes(x = age, y = lvmht27)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x)\n\n\n\n\n\n\n\n## Maybe a weak positive association.\n\n\n\nValidity Data\nRead about the validity study on the food frequency questionnaire here: https://dcgerard.github.io/stat_320/data.html#valid\n\nThe data are available at &lt;&gt;. Load these data into R as the valid data frame.\n\n\nvalid &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/valid.csv\")\n\nRows: 173 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Id, sfat_dr, sfat_ffq, tfat_dr, tfat_ffq, alco_dr, alco_ffq, cal_dr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nUse descriptive statistics to relate nutrient intake for the DR and FFQ. Do you think the FFQ is a reasonably ac- curate approximation to the DR? Why or why not?\nA frequently used method for quantifying dietary intake is in the form of quintiles. Compute quintiles for each nutri- ent and each method of recording, and relate the nutrient composition for DR and FFQ using the quintile scale. (That is, how does the quintile category based on DR relate to the quintile category based on FFQ for the same individual?) Do you get the same impression about the concordance between DR and FFQ using quintiles as in the previous problem, in which raw (ungrouped) nutrient intake is considered?\nIn nutritional epidemiology, it is customary to assess nutrient intake in relation to total caloric intake. One measure used to accomplish this is nutrient density, which is defined as 100% × (caloric intake of a nutrient/total caloric intake). For fat consumption, 1 g of fat is equivalent to 9 calories. Compute the nutrient density for total fat for the DR and FFQ.\n\n\nvalid |&gt;\n  mutate(nd_dr = tfat_dr * 9 / cal_dr * 100,\n         nd_ffq = tfat_ffq * 9 / cal_ffq * 100) -&gt;\n  valid\n\n\nObtain appropriate descriptive statistics for nutrient density for both DR and FFQ. How do they compare?\n\n\nRelate the nutrient density for total fat for the DR versus the FFQ using the quintile approach in Problem 2.28. Is the concordance between total fat for DR and FFQ stronger, weaker, or the same when total fat is expressed in terms of nutrient density as opposed to raw nutrient?"
  },
  {
    "objectID": "03_prob/03_prob.html",
    "href": "03_prob/03_prob.html",
    "title": "Probability Definitions and Properties",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution"
  },
  {
    "objectID": "03_prob/03_prob_discrete.html",
    "href": "03_prob/03_prob_discrete.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts."
  },
  {
    "objectID": "03_prob/03_confusion.html",
    "href": "03_prob/03_confusion.html",
    "title": "Confusion Matrix",
    "section": "",
    "text": "The below is a simplified version of the Confusion matrix table from Wikipedia that emphasizes the terminology more common to biostatistics (e.g. sensitivity/specificity/\\(PV^+\\)/\\(PV^-\\))\n\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\nTest Positive \\(T^+\\)\n\n\nTest Negative \\(T^-\\)\n\n\n\n\n\n\n\n\nTruth\n\n\n\nPositive \\(D^+\\)\n\n\nTrue Positive (TP)\n\n\nFalse Negative (FN)  (Type II Error)\n\n\nSensitivity  (True Positive Rate, Recall, Power)  \\(\\frac{TP}{D^+}\\)\n\n\nFalse Negative Rate  (Type II Error Rate)  \\(\\frac{FN}{D^+}\\)\n\n\n\n\nNegative \\(D^-\\)\n\n\nFalse Positive (FP)  (Type I Error)\n\n\nTrue Negative (TN)\n\n\nFalse Positive Rate  (Type I Error Rate)  \\(\\frac{FP}{D^-}\\)\n\n\nSpecificity (True Negative Rate)  \\(\\frac{TN}{D^-}\\)\n\n\n\n\n\n\nPrevalence  \\(\\frac{D^+}{D^+ + D^-}\\)\n\n\nPositive Predictive Value  (Precision)  \\(PV^+ = \\frac{TP}{T^+}\\)\n\n\nFalse Omission Rate  \\(\\frac{FN}{T^-}\\)\n\n\n\n\n\n\n\n\n\n\nFalse Discovery Rate  \\(\\frac{FP}{T^+}\\)\n\n\nNegative Predictive Value  \\(PV^- = \\frac{TN}{T^-}\\)"
  },
  {
    "objectID": "03_prob/03_prob_cont.html",
    "href": "03_prob/03_prob_cont.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "The density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] -1.0742  0.3714  1.6070  0.2678  0.4236 -0.8769\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2"
  },
  {
    "objectID": "hw/hw_ch5/hw_ch5.html",
    "href": "hw/hw_ch5/hw_ch5.html",
    "title": "Homework, Chapter 4",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives:\n\nChapter 5 of Rosner\nNormal Distribution\n\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: https://www.adobe.com/acrobat/online/merge-pdf.html\nI will only grade a randomly chosen subset of these questions. Please complete all of them, since you don’t know which ones I will grade.\n\n\nBlood Chemistry\nIn pharmacologic research a variety of clinical chemistry measurements are routinely monitored closely for evidence of side effects of the medication under study. Suppose typical blood-glucose levels are normally distributed, with mean = 90 mg/dL and standard deviation = 38 mg/dL.\n\nIf the normal range is 65−120 mg/dL, then what percentage of values will fall in the normal range?\n\n\n# X ~ N(90, 38^2)\npnorm(q = 120, mean = 90, sd = 38) - pnorm(q = 65, mean = 90, sd = 38)\n\n[1] 0.5298\n\n\n\nIn some studies only values at least 1.5 times as high as the upper limit of normal are identified as abnormal. What percentage of values would fall in this range?\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 0.008932\n\n\n\nAnswer Problem 2 for values 2.0 times the upper limit of normal.\n\n\n# Calculate upper limit\nu &lt;- 120 * 2\nu\n\n[1] 240\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 3.951e-05\n\n\n\nFrequently, tests that yield abnormal results are re- peated for confirmation. What is the probability that for a normal person a test will be at least 1.5 times as high as the upper limit of normal on two separate occasions? Assume the two tests are independent.\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\np1 &lt;- pnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\np1^2\n\n[1] 7.978e-05\n\n\n\nSuppose that in a pharmacologic study involving 6000 patients, 75 patients have blood-glucose levels at least 1.5 times the upper limit of normal on one occasion. What is the probability that this result could be due to chance? Assume the patient tests are independent.\n\n\n## Y = number of patients above 1.5 times upper limit\n## Y ~ Binom(6000, p1)\n## Want Pr(Y &gt;= 75)\n1 - pbinom(q = 74, size = 6000, prob = p1)\n\n[1] 0.003166\n\n## Only a 0.3% chance, so very unlikely\n\n\n\nOrthopedics\nA study was conducted of a diagnostic test (the FAIR test, i.e., hip flexion, adduction, and internal rotation) used to identify people with piriformis syndrome (PS), a pelvic condition that involves malfunction of the piriformis muscle (a deep buttock muscle), which often causes lumbar and buttock pain with sciatica (pain radiating down the leg) [7]. The FAIR test is based on nerve-conduction velocity and is expressed as a difference score (nerve-conduction velocity in an aggravating posture minus nerve-conduction velocity in a neutral posture). It is felt that the larger the FAIR test score, the more likely a participant will be to have PS. Data are given in the Data Set PIRIFORM.DAT for 142 participants without PS (piriform = 1) and 489 participants with PS (piriform = 2) for whom the diagnosis of PS was based on clinical criteria. The FAIR test value is called MAXCHG and is in milliseconds (ms). A cutoff point of ≥ 1.86 ms on the FAIR test is proposed to define a positive test.\n\nWhat is the sensitivity of the test for this cutoff point?\nWhat is the specificity of the test for this cutoff point?\nSuppose that 70% of the participants who are referred to an orthopedist who specializes in PS will actually have the condition. If a test score of ≥ 1.86 ms is obtained for a par- ticipant, then what is the probability that the person has PS?\nThe criterion of ≥ 1.86 ms to define a positive test is arbitrary. Using different cutoff points to define positivity, obtain the ROC curve for the FAIR test. What is the area under the ROC curve? What does it mean in this context?\nDo you think the distribution of FAIR test scores within a group is normally distributed? Why or why not?"
  },
  {
    "objectID": "04_est/04_sample.html",
    "href": "04_est/04_sample.html",
    "title": "Random Sampling",
    "section": "",
    "text": "Before doing sampling, make sure you set the seed so that you have reproducible results. E.g., this makes it so that your “random selection” is the same every time you first run the seed.\n\nset.seed(3574927)\n\nIf you are having trouble coming up with a random seed, you could NIST’s Interoperable Randomness Beacons API, which generates a new truly random number every 60 seconds. You would only run this once and copy the resulting number in your script. You would not include this script anywhere in any code you have.\n\n## HTTP request for a random number\nlibrary(httr2)\nreqout &lt;- request(base_url = \"https://beacon.nist.gov/beacon/2.0/pulse/last\") |&gt;\n  req_perform()\n\n## A random value represented as a 64-byte (512 bits) hex string\nhex &lt;- resp_body_json(reqout)$pulse$localRandomValue \n\n## select only first few digits to make number small. You can increase this.\nhexsmall &lt;- substr(hex, start = 1, stop = 6) \n\n## convert to an integer. This is your seed.\nstrtoi(x = hexsmall, base = 16) \n\nYou generate ID’s for with seq() or :\n\n# 1:100\nidvec &lt;- seq(from = 1, to = 20)\nidvec\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nYou can sample (without replacement) from this vector with sample()\n\nsample(x = idvec, size = 5)\n\n[1]  6 18 11  1  3\n\n\nIf you don’t give an argument for size, then sample will randomly permute the values.\n\nsample(x = idvec)\n\n [1]  1  5  6 16 13 18 19 17 20 12  7  8  3  2 14 11  4 10  9 15\n\n\nThis is useful for random assignment.\nYou should generally also randomize order, but if you need to see the group ID’s in an easier to read format, use sort().\n\nsample(x = idvec, size = 5) |&gt;\n  sort()\n\n[1]  4 10 12 13 15\n\n\nIf you are doing random assignment, you have a data frame of individuals. E.g., from the birthweight data.\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nThen you create a column with the number of groups via rep(), and then randomly permute it with sample(). E.g., suppose we want three groups:\n\nbirthweight |&gt;\n  mutate(group = rep(1:3, length.out = n())) |&gt; ## choose groups of equal size\n  mutate(group = sample(group)) ## randomly assign\n\n# A tibble: 1,000 × 3\n      id weight group\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1     0    116     2\n 2     1    124     1\n 3     2    119     2\n 4     3    100     1\n 5     4    127     3\n 6     5    103     2\n 7     6    140     1\n 8     7     82     3\n 9     8    107     3\n10     9    132     1\n# ℹ 990 more rows\n\n\n\n\nExerciseSolution\n\n\nRandomly assign 400 individuals (with IDs 1 through 400) into two groups, \"treatment\" and \"control\".\n\n\n\ntibble(id = 1:400) |&gt;\n  mutate(group = rep(c(\"treatmnet\", \"control\"), length.out = n())) |&gt; \n  mutate(group = sample(group))\n\n# A tibble: 400 × 2\n      id group    \n   &lt;int&gt; &lt;chr&gt;    \n 1     1 control  \n 2     2 control  \n 3     3 treatmnet\n 4     4 control  \n 5     5 treatmnet\n 6     6 control  \n 7     7 control  \n 8     8 control  \n 9     9 treatmnet\n10    10 treatmnet\n# ℹ 390 more rows\n\n\n\n\n\n\nExerciseSolution\n\n\nThe treatment is way more expensive than the control, so randomly assign only 100 to \"treatment\" and 300 to \"control\".\n\n\n\ntibble(id = 1:400) |&gt;\n  mutate(group = c(rep(\"treatment\", 100), rep(\"control\", 300))) |&gt; \n  mutate(group = sample(group))\n\n# A tibble: 400 × 2\n      id group    \n   &lt;int&gt; &lt;chr&gt;    \n 1     1 control  \n 2     2 treatment\n 3     3 control  \n 4     4 treatment\n 5     5 control  \n 6     6 treatment\n 7     7 control  \n 8     8 control  \n 9     9 control  \n10    10 treatment\n# ℹ 390 more rows"
  },
  {
    "objectID": "04_est/04_clt.html",
    "href": "04_est/04_clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Consider the birthweight data:\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\nggplot(birthweight, aes(x = weight)) +\n  geom_histogram(bins = 30, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nLet’s take samples of size \\(n\\) = 1, 5, 10, 20, 50, 100 from this distribution with replacement. For each sample size, we college 10000 repeat samples, each time calculating the sample mean \\(\\bar{X}\\), to obtain \\(\\bar{X}_1,\\bar{X}_2,\\ldots,\\bar{X}_{10000}\\). Below are histograms of these \\(\\bar{X}\\)’s\nThe distribution of the \\(\\bar{X}\\)’s has smaller and smaller variance as the sample size increases since \\(\\mathrm{var}(\\bar{X}) = \\sigma^2/n\\).\n \nThe distribution of the \\(\\bar{X}\\)’s gets closer to a normal as the sample size increases. Though it’s already sufficiently normal for most purposes once \\(n = 10\\).\n \nThe true mean \\(\\mu\\) is the vertical dashed red line. You see that the distribution of the sample mean has a mean of \\(\\mu\\), \\(\\mathrm{E}[\\bar{X}] = \\mu\\)."
  },
  {
    "objectID": "04_est/04_t.html",
    "href": "04_est/04_t.html",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_t.html#learning-objectives",
    "href": "04_est/04_t.html#learning-objectives",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_boneden.html",
    "href": "04_est/04_boneden.html",
    "title": "Bone Density Case Study",
    "section": "",
    "text": "Twin study with \\(n=41\\) where one smoked more than the other. Bone density was measured in both twins. More detail here.\n\nlibrary(tidyverse)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nLet’s explore if lumbar spine bone density differed between the twins. First, we’ll calculate the difference in density between the twins:\n\nboneden |&gt;\n  mutate(diff = ls1 - ls2) -&gt; #ls1 = lighter smoking, ls2 = heavy smoking\n  boneden \n\nLet’s plot the data\n\nggplot(boneden, aes(x = diff)) +\n  geom_histogram(bins = 7, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the difference\n\nboneden |&gt;\n  summarize(xbar = mean(diff), s = sd(diff), n = n())\n\n# A tibble: 1 × 3\n    xbar      s     n\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 0.0359 0.0883    41\n\n\nWe can use this to get a 95% confidence interval for the mean difference in lumbar spine bone density between heavy and light smoking twins.\nThe standard error is\n\n0.08829 / sqrt(41)\n\n[1] 0.01379\n\n\nThe appropriate quantile of the t-distribution is\n\nqt(p = 0.975, df = 41 - 1)\n\n[1] 2.021\n\n\nSo, the 95% confidence interval is\n\n0.03585 - 2.021 * 0.01379 # lower\n\n[1] 0.00798\n\n0.03585 + 2.021 * 0.01379 # upper\n\n[1] 0.06372\n\n\nSince the lower bound of the 95% CI is above 0, we can be fairly confident that the true mean difference is greater than 0. That is, we are pretty sure that the lighter smoking twin has heavier bone density. We will formalize what “pretty sure” means in Chapter 7.\n\n\nReal Way\n\nIt would be crazy to do the above calculations, by hand, every time. For this class, I’ll occasionally ask you do that to solidify your understanding. But in real data analysis we use code to automate inference.\nWe will use the {broom} package to summarize inference output.\n\nlibrary(broom)\n\nYou calculate an interval for a mean using t.test() using one of two ways:\n\n## tout &lt;- t.test(boneden$diff)\ntout &lt;- t.test(diff ~ 1, data = boneden)\n\nYou get a summary of the output with broom::tidy()\n\ntidy(tout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359  0.00799    0.0637\n\n\nYou can change the level with the conf.level argument in t.test()\n\nt.test(diff ~ 1, data = boneden, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359   0.0126    0.0591\n\n\n\n\nExerciseSolution\n\n\nCalculate an 80% confidence interval for the mean birth weight of newborns using the birth weight data that you can download from here: https://dcgerard.github.io/stat_320/data/birthweight.csv. Do this both “by hand” (after calculating the summary statistics and t-quantile) and using R’s automated functions.\n\n\nFirst, we load in the data\n\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nThe “by hand” way is of the form \\(\\bar{x} \\pm t_{n-1,1-\\alpha/2}s/\\sqrt{n}\\):\n\nn &lt;- nrow(birthweight)\nmult &lt;- qt(p = 0.9, df = n - 1)\nxbar &lt;- mean(birthweight$weight)\nse &lt;- sd(birthweight$weight) / sqrt(n)\nxbar - mult * se\n\n[1] 105.6\n\nxbar + mult * se\n\n[1] 108.1\n\n\nThis is implemented in t.test():\n\nt.test(weight ~ 1, data = birthweight, conf.level = 0.8) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     107.     106.      108."
  },
  {
    "objectID": "04_est/04_chi2.html",
    "href": "04_est/04_chi2.html",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/04_chi2.html#learning-objectives",
    "href": "04_est/04_chi2.html#learning-objectives",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/binom.html",
    "href": "04_est/binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\nExercise: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?"
  },
  {
    "objectID": "04_est/04_binom.html",
    "href": "04_est/04_binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\n\n\nExerciseSolution\n\n\nOf 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?\n\n\nWe will start with the Wald solution by hand. This is of the form \\(\\hat{p} \\pm z_{1-\\alpha/2}\\mathrm{SE}(\\hat{p})\\):\n\nn &lt;- 10\nx &lt;- 6\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.9)\nphat - z * se\n\n[1] 0.4015\n\nphat + z * se\n\n[1] 0.7985\n\n\nR’s prop.test() give’s Wilson intervals, not Wald intervals:\n\nprop.test(x = x, n = n, conf.level = 0.8) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.6    0.356     0.809\n\n\nThe exact method is done via binom.test():\n\nbinom.test(x = x, n = n, conf.level = 0.8) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.6    0.354     0.812\n\n\nYes it matters. We should use the binomial approach since 10 * 0.6 * 0.4 = 2.4 &lt; 5"
  },
  {
    "objectID": "05_tests/05_ttest.html",
    "href": "05_tests/05_ttest.html",
    "title": "One Sample t-Tests in R",
    "section": "",
    "text": "Suppose we know the average birthweight in America is 110 oz. We are curious if the babies in a Boston area hospital have a different birthweight. Let \\(X_i\\) be the birthweight of the \\(i\\) Boston baby, then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\) and are independent. We want to test\n\\[\\begin{align}\nH_0: \\mu &= 110\\\\\nH_1: \\mu &\\neq 110\n\\end{align}\\]\nLet’s first read in the data on the \\(n = 1000\\) babies:\n\nlibrary(tidyverse)\nlibrary(broom)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nWe then use t.test() to run the \\(t\\)-test. The arguments are:\n\nformula: a formula object (generated with a tilde ~).\n\nWe put the name of the variable we are exploring to the left of the tilde.\nWe put the number 1 to the right of the tilde.\n\ndata: the name of the data frame containing the variable.\nmu: The null value. The default is 0 since this is the most common test.\nalternative: We use the default \"two.sided\", since our alternative hypothesis is of the form parameter \\(\\neq\\) value.\n\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110)\n\nWe then use broom::tidy() to get a summary of the \\(t\\)-test output.\n\nbout &lt;- tidy(tout)\nbout\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     107.     -3.32 0.000920       999     105.      109. One Samp… two.sided  \n\n\nWe can manually verify these results (though, you wouldn’t do this step in real life):\n\nxbar &lt;- mean(birthweight$weight)\ns &lt;- sd(birthweight$weight)\nn &lt;- length(birthweight$weight)\nmu0 &lt;- 110\ntstat &lt;- (xbar - mu0) / (s / sqrt(n))\npval &lt;- 2 * pt(-abs(tstat), df = n - 1)\ntstat\n\n[1] -3.324\n\npval\n\n[1] 0.0009204\n\n\nIf instead we had the alternative of \\(H_1: \\mu &lt; \\mu_0\\), then we would use the alternative = \"less\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"less\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.0004602\n\n\nIf instead we had the alternative of \\(H_1: \\mu &gt; \\mu_0\\), then we would use the alternative = \"greater\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"greater\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.9995\n\n\n\nExerciseSolution\n\n\nConsider the lead data that you can read about here and download from https://dcgerard.github.io/stat_320/data/lead.csv. IQ tests are designed to have a mean of 100. Use iqf to test if the control group has an average IQ value of 100. Separately test if the exposed group has an average IQ less than 100. State the hypotheses, test results, and conclusions.\n\n\nLet’s load in the lead data\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nWe’ll filter for just the control group\n\nlead |&gt;\n  filter(Group == \"control\", !is.na(iqf)) -&gt;\n  df_control\n\nLet \\(X_i\\) be the IQ of the \\(i\\)th control individual. We assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 100\\) versus \\(H_1: \\mu \\neq 100\\). We can run this test using t.test():\n\nt.test(iqf ~ 1, data = df_control, mu = 100) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     92.6     -4.08 0.000113        73     88.9      96.2 One Samp… two.sided  \n\n\nThe \\(p\\)-value is 0.0001128, so we have very strong evidence that mean IQ in the control group is not 100\nWe’ll now filter for the exposed group\n\nlead |&gt;\n  filter(Group == \"exposed\", !is.na(iqf)) -&gt;\n  df_exposed\n\nLet \\(X_i\\) be the IQ of the \\(i\\)th exposed individual. We assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 100\\) versus \\(H_1: \\mu &lt; 100\\). We can run this test using t.test():\n\nt.test(iqf ~ 1, data = df_exposed, mu = 100, alternative = \"less\") |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic     p.value parameter conf.low conf.high method alternative\n     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      \n1     88.0     -6.66     1.65e-8        45     -Inf      91.0 One S… less       \n\n\nThe \\(p\\)-value is 1.7e-08, so we again have very strong evidence that mu &lt; 100."
  },
  {
    "objectID": "05_tests/05_power.html",
    "href": "05_tests/05_power.html",
    "title": "Power Calculations",
    "section": "",
    "text": "For \\(t\\)-methods, you use power.t.test() to calculate do power and smaple size calculations. It takes as input four of the following:\n\nn: The sample size \\(n\\)\ndelta: The effect size (difference in means) \\(|\\mu_1 - \\mu_0|\\)\nsd: The standard deviation of the data \\(\\sigma\\)\nsig.level: The signficicance level \\(\\alpha\\)\npower: The power \\(1 - \\beta\\).\n\nYou must put values for exactly four of the above. The fifth should be NULL and the function will return the fifth value.\nOther inputs are for the type of test:\n\ntype: Use \"one.sample\" for one-sample \\(t\\)-tests and \"two.sample\" for two-sample \\(t\\)-tests.\nalternative: Either \"two.sided\" or \"one.sided\".\n\nSuppose we plan on running a study with 100 participants of low socioeconomic status (SES). The mean birthweight in america is 120 oz. A pilot study suggested that the average birthweight of low SES babies is 115 oz with a standard deviation of 24 oz. What is the power of a test with a significance level of 0.05?\n\npower.t.test(\n  n = 100, \n  delta = 5, \n  sd = 24, \n  sig.level = 0.05, \n  type = \"one.sample\", \n  alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 5\n             sd = 24\n      sig.level = 0.05\n          power = 0.6643\n    alternative = one.sided\n\n\nLet’s compare that power to the \\(z\\)-test calculation from Rosner\n\npnorm(qnorm(0.05) + sqrt(100) * 5 / 24)\n\n[1] 0.6695\n\n\nIt’s a little different because Rosner uses \\(z\\)-methods instead of \\(t\\)-methods for power and sample size calculations. But it’s not too different to be practically important, especially since power and sample size calculations are mostly just wild educated guesses.\n\n\nExerciseSolution\n\n\nWhat sample size would be needed for a power of at least 0.8?\n\n\n\npower.t.test(\n  power = 0.8, \n  delta = 5, \n  sd = 24, \n  sig.level = 0.05, \n  type = \"one.sample\", \n  alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 143.8\n          delta = 5\n             sd = 24\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\n\nWe need a sample size of at least 144.\n\n\n\n\nExerciseSolution\n\n\nA new drug is proposed to prevent glaucoma among people with high intraocular pressure (IOP). A pilot study is conducted with 10 individuals. After 1 month of using the drug, their IOP decreases by 5 mm HG with a standard deviation of 10 mm HG. What is the sample size needed to achieve 90% power for a two-sided test with significance level of 0.05.\n\n\n\npower.t.test(\n  delta = 5, \n  sd = 10,\n  sig.level = 0.05,\n  power = 0.9,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 44\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\nWe need a smaple size of at least 44."
  },
  {
    "objectID": "05_tests/05_binom.html#example",
    "href": "05_tests/05_binom.html#example",
    "title": "One Sample Binomial Tests in R",
    "section": "Example",
    "text": "Example\nSuppose \\(x\\) = 5 and \\(n\\) = 8.\n\nx &lt;- 5\nn &lt;- 8\nbinom.test(x = x, n = n)\n\n\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 5, number of trials = 8, p-value = 0.7\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2449 0.9148\nsample estimates:\nprobability of success \n                 0.625"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound",
    "href": "05_tests/05_binom.html#finding-lower-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-1",
    "href": "05_tests/05_binom.html#finding-lower-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-2",
    "href": "05_tests/05_binom.html#finding-lower-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-3",
    "href": "05_tests/05_binom.html#finding-lower-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-4",
    "href": "05_tests/05_binom.html#finding-lower-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-5",
    "href": "05_tests/05_binom.html#finding-lower-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-6",
    "href": "05_tests/05_binom.html#finding-lower-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-7",
    "href": "05_tests/05_binom.html#finding-lower-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound",
    "href": "05_tests/05_binom.html#finding-upper-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-1",
    "href": "05_tests/05_binom.html#finding-upper-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-2",
    "href": "05_tests/05_binom.html#finding-upper-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-3",
    "href": "05_tests/05_binom.html#finding-upper-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-4",
    "href": "05_tests/05_binom.html#finding-upper-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-5",
    "href": "05_tests/05_binom.html#finding-upper-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-6",
    "href": "05_tests/05_binom.html#finding-upper-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-7",
    "href": "05_tests/05_binom.html#finding-upper-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval",
    "href": "05_tests/05_binom.html#final-confidence-interval",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(np_1\\)\nRight: \\(np_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval-1",
    "href": "05_tests/05_binom.html#final-confidence-interval-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(p_1\\)\nRight: \\(p_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html",
    "href": "05_tests/05_binom.html",
    "title": "One Sample Binomial Tests in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nWe have \\[\nX \\sim \\mathrm{Binom}(n,p)\n\\] and we are testing\n\n\\(H_0\\): \\(p = p_0\\)\n\\(H_1\\): \\(p \\neq p_0\\) or \\(p &gt; p_0\\) or \\(p &lt; p_0\\)\n\n\n\nApproximate Approach\n\nWe use the central limit theorem. If (rule-of-thumb) \\(np_0(1-p_0) \\geq 5\\) then \\[\nX \\sim N(p_0, p_0(1-p_0)/n)\n\\] and we calculate the tail probabilities of (for \\(\\hat{p} = X/n\\)) \\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\\]\nThis is done via prop.test(), which you can feed into broom::tidy()\n\nx: the observed number of successes\nn: The total number of trials\np: The null value of the success probability\nalternative: Either \"two.sided\", \"less\", or \"greater\"\n\nSuppose that about 20% of women who are trying to concieve take 12 months or more to get pregnant, which we will call infertility. Researchers are interested in if a genetic marker is associated with infertility. Of 40 women with this marker, 10 were infertile. Is this marker associated with infertility?\n\n\\(X \\sim \\mathrm{Binom}(40, p)\\)\n\\(H_0\\): \\(p = 0.2\\)\n\\(H_1\\): \\(p &gt; 0.2\\)\n\n\npout &lt;- prop.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(pout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.277\n\n\nSince the \\(p\\)-value is 0.2766, we don’t have evidence that the marker is associated with infertility.\nWe can do this calculation by hand (but you would never do this):\n\nphat &lt;- 10 / 40 \np0 &lt;- 0.2\nz &lt;- (phat - p0 - 1/80) / sqrt(p0 * (1 - p0) / 40) ## continuity correction\npnorm(q = z, lower.tail = FALSE)\n\n[1] 0.2766\n\n\n\n\n\nExact Approach\n\nThe exact approach calculates the probability under the null of being as more supportive of the alternative as the data we observed.\nE.g., for our infertility example, we would calculate \\(\\mathrm{Pr}(X \\geq 10 | p = 0.2)\\)\n\n1 - pbinom(q = 9, size = 40, prob = 0.2)\n\n[1] 0.2682\n\n\nThis procedure is implemented in the binom.test() function, which has the same inputs as prop.test().\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.268\n\n\nWhen the alternative is 2-sided, \\(H_1 \\neq p_0\\), R is a little different than the procedure Rosner proposes. It calculates the sums the probabilities under the null of all \\(X\\) that are less probable than our observed \\(x\\)\n\\[\n\\sum_{k \\text{ s.t. } Pr(k) \\leq Pr(x)}\\binom{n}{k} p_0^k(1-p_0)^{n-k}\n\\]\nIn the infertility example, this would be\n\nprob &lt;- dbinom(x = 0:40, size = 40, prob = 0.2)\nsum(prob[prob &lt;= dbinom(x = 10, size = 40, prob = 0.2)])\n\n[1] 0.4296\n\n\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2)\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.430\n\n\n\n\nExerciseSolution\n\n\nOut of 13 deaths at a nuclear facility among men aged 55-64, 5 of them were due to cancer. The proportion of deaths caused by cancer in that group in the greater population is 0.2. Is there more cancer deaths in this nuclear facility? Use both the normal and exact approaches.\n\n\nWe are testing \\(H_0: p = 0.2\\) versus \\(H_1: p &gt; 0.2\\) assuming that \\(X \\sim \\mathrm{Binom}(13,p)\\), where \\(X\\) is the number who had cancer.\nThe normal approach is done via prop.test():\n\nprop.test(x = 5, n = 13, p = 0.2, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\nWarning in prop.test(x = 5, n = 13, p = 0.2, alternative = \"greater\"):\nChi-squared approximation may be incorrect\n\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0939\n\n\nThe exact approach is done via binom.test():\n\nbinom.test(x = 5, n = 13, p = 0.2, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0991\n\n\nWe should be using the exact approach by our rule-of-thumb being less than 5:\n\n0.2 * (1 - 0.2) * 13\n\n[1] 2.08\n\n\nThe \\(p\\)-value is about 0.1, so no real evidence of more cancer deaths."
  },
  {
    "objectID": "05_tests/05_binom_power.html",
    "href": "05_tests/05_binom_power.html",
    "title": "Power Calculations for Binomial Tests",
    "section": "",
    "text": "There are no base R functions that do power and sample size calculations. But I created one for you based on Equations 7.32 and 7.33 from Rosner.\n\n#' Power/sample size calculation of 1-sample proportion test\n#' \n#' Uses central limit theorem, so make sure `p0 * (1 - p0) * n &gt;= 5`\n#' \n#' Exactly one of `n`, `power`, `p0`, `p1`, or `alpha` needs to be `NULL`.\n#' \n#' @param n The sample size\n#' @param power The power\n#' @param p0 The null proportion\n#' @param p1 The alternative proportion\n#' @param alpha The significance level\n#' @param TOL Tolerance level\n#' \n#' @author David Gerard\n#' \n#' @examples\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' b1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' \n#' ## two p1's\n#' b1power(n = 500, power = 0.9, p0 = 0.02, p1 = NULL, alpha = 0.05)\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.00406, alpha = 0.05)$power\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.044, alpha = 0.05)$power\nb1power &lt;- function(\n    n = NULL, \n    power = NULL,\n    p0 = NULL,\n    p1 = NULL, \n    alpha = 0.05,\n    TOL = 1e-6) {\n  \n  if (is.null(n) + is.null(power) + is.null(p0) + is.null(p1) + is.null(alpha) != 1) {\n    stop(\"exactly one of n, power, p0, p1, and alpha need to be NULL\")\n  }\n  \n  oout &lt;- list(n = n, power = power, p0 = p0, p1 = p1, alpha = alpha)\n  \n  pfun &lt;- function(n, p0, p1, alpha) {\n    za2 &lt;- stats::qnorm(alpha / 2)\n    stats::pnorm(sqrt(p0 * (1 - p0) / (p1 * (1 - p1))) * (za2 +\n                   abs(p0 - p1) * sqrt(n) / sqrt(p0 * (1 - p0))))\n  }\n  \n  if (is.null(power)) {\n    oout$power &lt;- pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)\n  } else if (is.null(n)) {\n    z1a2 &lt;- stats::qnorm(1 - alpha / 2)\n    zp &lt;- stats::qnorm(power)\n    oout$n &lt;- p0 * (1 - p0) * (z1a2 + zp * sqrt(p1 * (1 - p1) / (p0 * (1 - p0))))^2 / (p1 - p0)^2\n    oout$n &lt;- ceiling(oout$n)\n  } else if (is.null(p0)) {\n    rp0 &lt;- function(p0) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp0(p0 = TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp0, interval = c(TOL, p1))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp0(p0 = 1 - TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp0, interval = c(p1, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p0 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(p1)) {\n    rp1 &lt;- function(p1) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp1(p1 = TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp1, interval = c(TOL, p0))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp1(p1 = 1 - TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp1, interval = c(p0, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p1 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(alpha)) {\n    ralpha &lt;- function(alpha) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    rout &lt;- stats::uniroot(f = ralpha, interval = c(TOL, 1-TOL))\n    oout$alpha &lt;- rout$root\n  }\n  \n  if (any(oout$p0[!is.na(oout$p0)] * (1 - oout$p0[!is.na(oout$p0)]) * oout$n &lt; 5)) {\n    warning(\"too small sample size\")\n  }\n  return(oout)\n}\n\n\nAssumes the sample size is large enough to use the central limit theorem (\\(np_0(1-p_0) \\geq 5\\)).\nSuppose we wish to test the hypothesis that women with a sister history of breast cancer are at higher risk of developing breast cancer themselves. Suppose the prevalence rate of breast cancer is 2% among 50 to 54 year-old US women, whereas it is 5% among women with a sister history. We wish to interval 500 women 50 to 54 years old with a sistory history of the disease. What is the power of such a study assuming that we conduct a two-sided test with \\(\\alpha = 0.05\\)?\n\n# 0.9655\nb1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 500\n\n$power\n[1] 0.9655\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05\n\n\nHow many women should we interview in the study proposed to achieve 90% power?\n\n# 341\nb1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 341\n\n$power\n[1] 0.9\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05\n\n\nThere is also the {pwr} package that you can try out. It uses different equations from Rosner, so you need to do a funky pre-calculation for the effect size. The numbers are slightly different too:\n\nlibrary(pwr)\nh &lt;- ES.h(p1 = 0.05, p2 = 0.02)\npwr.p.test(h = h, n = 500, sig.level = 0.05)\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1672\n              n = 500\n      sig.level = 0.05\n          power = 0.9624\n    alternative = two.sided\n\n\n\nh &lt;- ES.h(p1 = 0.05, p2 = 0.02)\npwr.p.test(h = h, sig.level = 0.05, power = 0.9)\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1672\n              n = 375.7\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided"
  },
  {
    "objectID": "05_tests/two_sample_t.html",
    "href": "05_tests/two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women containt eh systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\nExercise: A study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)"
  },
  {
    "objectID": "05_tests/05_two_sample_t.html",
    "href": "05_tests/05_two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women contain the systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\n\n\nExerciseSolution\n\n\nA study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)\n\n\n\nLet \\(X_i\\) be the difference in volume in the \\(i\\)th twin. Then we assume \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 0\\) versus \\(H_1: \\mu \\neq 0\\). In R, we run this test using:\n\nt.test(x = sc_df$Affected, y = sc_df$Unaffected, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1   -0.199     -3.23 0.00606        14   -0.331   -0.0667 Paired t-… two.sided  \n\n\nWe have strong evidence of a difference in mean volume between the twin types (\\(p\\) = 0.00602, n = 15). We estimate that the unaffected twin has on average 0.2 cm\\(^3\\) larger hippocampus than the affected twin (95% CI 0.0667 cm\\(^3\\) to 0.3306 cm\\(^3\\) larger).\nBy hand,w e first calculate the difference between the two twins:\n\nsc_df &lt;- mutate(sc_df, diff = Affected - Unaffected)\n\nWe now need the summary statistics\n\nxbar &lt;- mean(sc_df$diff)\ns &lt;- sd(sc_df$diff)\nn &lt;- nrow(sc_df)\n\nThe \\(t\\)-statistic is \\(\\frac{\\bar{x}}{s/\\sqrt{n}}\\)\n\nt &lt;- xbar / (s / sqrt(n))\nt\n\n[1] -3.229\n\n\nWe calculate the \\(p\\)-value by two times the tail area:\n\np_value &lt;- 2 * pt(-abs(t), df = n - 1)\np_value\n\n[1] 0.006062\n\n\nThe 95% confidence interval is \\(\\bar{x} \\pm t_{1-\\alpha/2,n-1}s/\\sqrt{n}\\)\n\nlower &lt;- xbar - qt(p = 0.975, df = n - 1) * s / sqrt(n)\nupper &lt;- xbar + qt(p = 0.975, df = n - 1) * s / sqrt(n)\nc(lower, upper)\n\n[1] -0.3306 -0.0667\n\n\n\n\n\n\n\nUnpaired (Equal Variance)\n\nConsider the lead data that you can read about here.\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\n\nWe are interested in if the exposed and control groups have the same mean full scale IQ. Let’s explore the data\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet \\(X_i\\) be the \\(i\\)th IQ score in the control group, let \\(Y_i\\) be the \\(i\\)th IQ score in the exposed group.\nThen we assume that \\(X_i \\sim \\mathrm{N}(\\mu_1, \\sigma^2)\\) and \\(Y_i \\sim \\mathrm{N}(\\mu_2, \\sigma^2)\\), and that all observations are independent.\nWe use t.test() to run a two-sample \\(t\\)-test.\n\nThe quantitative variable is to the left of the tilde ~\nThe variable encoding the two groups is to the right of the tilde\nIf we assume equal variances in each group, we set var.equal = TRUE\n\n\nt.test(iqf ~ Group, data = lead, var.equal = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.67  0.0977       118   -0.845      9.91\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWe can verify this result manually (you would never do this in real life, but you might on an exam).\n\n## Get summary statistics of the two groups\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(mean = mean(iqf), var = var(iqf), n = n()) -&gt;\n  sumdf\nxbar &lt;- sumdf$mean[[1]]\nybar &lt;- sumdf$mean[[2]]\ns2x &lt;- sumdf$var[[1]]\ns2y &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## Calculate pooled sample standard deviation\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\n\n## Calculate t-statistic\ntstat &lt;- (xbar - ybar) / (s * sqrt(1 / n1 + 1 / n2))\n\n## compare to t distribution with n1 + n2 - 2 df\npval &lt;- 2 * pt(-abs(tstat), df = n1 + n2 - 2)\n\n## Get confidence intervals\nlower &lt;- (xbar - ybar) - qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nupper &lt;- (xbar - ybar) + qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\n\nc(pval = pval, lower = lower, upper = upper)\n\n    pval    lower    upper \n 0.09772 -0.84454  9.90917 \n\n\n\n\nExerciseSolution\n\n\nIs there any difference between exposed and control groups when it comes to the finger-wrist tapping test in dominant hand (maxfwt)? Assume equal variances.\n\n\nWe read in the data\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nIt appears that the exposed group might have a lower mean tapping value than the control group. But it is unclear.\n\nlead |&gt;\n  filter(!is.na(maxfwt)) |&gt;\n  ggplot(aes(x = Group, y = maxfwt)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe run the test using t.test().\n\nt.test(maxfwt ~ Group, data = lead, var.equal = TRUE) |&gt; \n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     7.01      54.4      47.4      2.68 0.00872        97     1.81      12.2\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nYes, we have strong evidence of a difference between groups (p = 0.005). We estimate that the control group taps about 7 more times on average (95% CI of 1.8 to 12.2 more times).\n\n\n\n\nExerciseSolution\n\n\nA sample of eight 35- to 39-year-old non-pregnant, premenoposaul OC users have a mean systoolic blood pressure (SBP) of 132.82 mm Hg and a sample standard deviation of 15.34 mm Hg. A different sample of 21 non-pregnant, premenopausal, non-OC users have a mean SBP of 127.44 mm Hg and a sample standard deviation of 18.23 mm Hg. What can be said about the underlying mean difference in blood pressure between the two groups? Provide a measure of how sure we are that there is a difference, and provide some interval estimate for this difference. Assume equal variances.\n\n\nWe plug in these summary statistics:\n\nxbar &lt;- 132.82\ns2x &lt;- 15.34^2\nn1 &lt;- 8\n\nybar &lt;- 127.44\ns2y &lt;- 18.23^2\nn2 &lt;- 21\n\nLet’s calculate the pooled sample standard deviation \\(s_p = \\sqrt{\\frac{(n_1-1)s_x^2 + (n_2 - 1) s_y^2}{n_1 + n_2 - 2}}\\)\n\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\n\nNow the \\(t\\)-statistic \\(t = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{1/n_1 + 1/n_2}}\\)\n\ntstat &lt;- (xbar - ybar) / (s * sqrt(1 / n1 + 1 / n2))\n\nThe \\(p\\)-value of the test is area in both tails\n\npval &lt;- 2 * pt(-abs(tstat), df = n1 + n2 - 2)\npval\n\n[1] 0.4664\n\n\nThe confidence interval:\n\nlower &lt;- (xbar - ybar) - qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nupper &lt;- (xbar - ybar) + qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nc(lower = lower, upper = upper)\n\n lower  upper \n-9.561 20.321 \n\n\nWe don’t have evidence that the non-OC and OC using groups differ in mean systolic blood pressure (\\(p\\)-value = 0.46664). The difference in mean blood pressure between groups is estimated to be 5.38 (95% CI of -9.561 to 20.321).\n\n\n\n\n\nTest for Equal Variance\n\nSuppose we have \\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y_i \\sim N(\\mu_2,\\sigma_2^2)\\).\nIt is possible to test \\(H_0: \\sigma_1 = \\sigma_2\\) versus \\(H_1: \\sigma_1 \\neq \\sigma_2\\).\nFolks don’t typically do this test because:\n\nIt is very sensitive to the normality assumption. Conversely, the \\(t\\)-test is not.\nThe \\(t\\)-test with equal variances is relatively robust to violations in the equal variance assumption.\nFolks typically just use the unequal variances \\(t\\)-test below.\n\nBut if your boss asks you run such a test, use var.test().\n\nvar.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\nMultiple parameters; naming those columns num.df and den.df.\n\n\n# A tibble: 1 × 9\n  estimate num.df den.df statistic p.value conf.low conf.high method alternative\n     &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      \n1     1.65     73     45      1.65  0.0719    0.955      2.76 F tes… two.sided  \n\n\nThis test is based on the ratio of the variances \\(s_1^2 / s_2^2\\). Under the null, this follows a \\(F\\)-distribution with \\(n_1 - 1\\) numerator degrees of freedom and \\(n_2 - 1\\) denominator degrees of freedom.\n\nlead |&gt;\n  filter(!is.na(iqf)) |&gt;\n  group_by(Group) |&gt;\n  summarize(var = var(iqf), n = n()) -&gt;\n  sumdf\nvar1 &lt;- sumdf$var[[1]]\nvar2 &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## for fstat &gt; 1\nfstat1 &lt;- var1 / var2\n2 * pf(q = fstat1, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)\n\n[1] 0.07194\n\n## For fstat &lt; 1\nfstat2 &lt;- var2 / var1\n2 * pf(q = fstat2, df1 = n2 - 1, df2 = n1 - 1, lower.tail = TRUE)\n\n[1] 0.07194\n\n\n\n\n\nUnpaired (Unequal Variance)\n\nWhen you don’t want to assume equal variances (typically the case), just use the default settings of t.test() that has var.equal = FALSE.\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\nt.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.77  0.0797      112.   -0.545      9.61\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nDon’t bother memorizing Satterthwaite’s approximation for the degrees of freedom. Just do this in the computer.\n\n\nExerciseSolution\n\n\nIs there a difference in finger tapping between groups? Don’t assume equal variances.\n\n\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(maxfwt))\nt.test(maxfwt ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     7.01      54.4      47.4      2.61  0.0113      65.0     1.64      12.4\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nSome evidence of a difference (p = 0.01125). We estimate that the control group has 7 more taps on average (95% CI of 1.6 to 12.3 more taps on average).\n\n\n\n\n\nPower and Sample Size Calculations in Two-sample \\(t\\)-tests\n\nUse power.t.test().\nIn the two-sample case, n means the sample size per group. It assumes the sample sizes are equal, so the total sample size is 2 * n.\nIt also assumes the standard deviations are equal, so you need to use a pooled estimate of the standard deviation.\nIf you need more precise power or sample size calculations, then those exist.\n\nBut I think these calculations are so much guess work that the error of assuming equal sample sizes is less than the error of the wild guesses you are giving it.\n\nE.g., suppose we have the OC user exercise above as a pilot experiment.\n\nxbar &lt;- 132.82\ns2x &lt;- 15.34^2\nn1 &lt;- 8\n\nybar &lt;- 127.44\ns2y &lt;- 18.23^2\nn2 &lt;- 21\n\nLet’s calculate a pooled estimate of the variance, and we will assume that is the true variance for the power calculations.\n\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\ns\n\n[1] 17.53\n\n\nLet’s suppose we want a power of 0.8. Then the sample size we would need is 168 per group:\n\npower.t.test(\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  power = 0.8, \n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 167.6\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf a researcher can only afford \\(n = 100\\) per gropu, then the power calculation would be 0.58:\n\npower.t.test(\n  n = 100,\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.5793\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nExerciseSolution\n\n\nSuppose a new drug is proposed to lower intraocular pressure (IOP) among people with glaucoma. It is anticipated that mean IOP will drop by 8 mm Hg after 1 month with the new drug. The comparison group will get the standard drug, which is anticipated to have a mean drop in IOP of 5 mm Hg after 1 month. It is expected that the sd of change within each group will be 10 mm Hg. How many subjects need to be enrolled to achieve 90% power if an equal sample size is planned within each group and a two-sided test with \\(\\alpha\\) = 0.05 will be used?\n\n\n\npower.t.test(delta = 3, sd = 10, sig.level = 0.05, power = 0.9)\n\n\n     Two-sample t test power calculation \n\n              n = 234.5\n          delta = 3\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n235 subjects per group"
  },
  {
    "objectID": "06_nonparametric/06_wilcoxin_1.html",
    "href": "06_nonparametric/06_wilcoxin_1.html",
    "title": "One-sample Nonparametric Tests",
    "section": "",
    "text": "We will load the boneden data, that you can read about here.\n\nlibrary(tidyverse)\nlibrary(broom)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nRecall that this is a twin study with a heavier smoking twin and a lighter smoking twin. We are interested in the difference in bone density between these pairs of twins.\n\nboneden |&gt;\n  mutate(ls_diff = ls1 - ls2) -&gt;\n  boneden\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 10)\n\n\n\n\n\n\n\n\n\n\nSign test\n\nWe might not be willing to assume a normal distribution for these data. An alternative is the sign test, which tests whether the number of positive values is greater than expected by chance.\nThe sign test is the exact same thing as the binomial test using the number of positive values as the data and \\(p = 1/2\\) as the null.\nFirst, we calculate the number of positive values and the total number of values.\n\nboneden |&gt;\n  summarize(x = sum(ls_diff &gt; 0), n = sum(ls_diff != 0))\n\n# A tibble: 1 × 2\n      x     n\n  &lt;int&gt; &lt;int&gt;\n1    28    41\n\n\nWe can now do a normal approximation proportion test, or an exact binomial test on these data\n\nprop.test(x = 28, n = 41, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.683      4.78  0.0288         1    0.518     0.814 1-sample … two.sided  \n\n\n\nbinom.test(x = 28, n = 41, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.683        28  0.0275        41    0.519     0.819 Exact bin… two.sided  \n\n\nSo we have some evidence that there is a difference in median bone density between the two twins (\\(p \\approx 0.028\\)).\n\n\n\nWilcoxin signed-rank test\n\nIf we are willing to at least assume that the data are symmetric, we can gain some power by doing the Wilcoxin signed-rank test\n\nwilcox.test(ls_diff ~ 1, data = boneden, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      618.  0.0156 Wilcoxon signed rank test with continuity corre… two.sided  \n\n\nI set exact = FALSE because the exact test as implemented in R cannot handle ties.\nThe idea of the Wilcoxin signed-rank test is that if you rank the observations by the magnitude (absolute value), then the rank sum of the negative numbers should be about the same as the rank sum of the positive numbers.\n\nset.seed(77)\n## Draw data from a distribution with median 0\nx &lt;- rnorm(1000) \n## Calculate ranks of absolue values\nr &lt;- rank(abs(x))\n## Mean rank of positive numbers\nsum(r[x &gt; 0])\n\n[1] 256612\n\n## Mean rank of negative numbers\nsum(r[x &lt; 0])\n\n[1] 243888\n\n\nIf the median is positive, you would expect the magnitude of the positive numbers to be larger than the maginude of the negative numbers.\n\nset.seed(77)\n## Draw data from a distribution with median 1\nx &lt;- rnorm(1000, mean = 1) \n## Calculate ranks of absolue values\nr &lt;- rank(abs(x))\n## Mean rank of positive numbers\nsum(r[x &gt; 0])\n\n[1] 465908\n\n## Mean rank of negative numbers\nsum(r[x &lt; 0])\n\n[1] 34592\n\n\nA visualization: The true distribution is the curve, symmetric about 0. A sample of 10 individuals is the rug plot. The ranks of the magnitudes are above the rug plot. The red numbers are the rank sums of the positive and negative values.\n\n\n\n\n\n\n\n\n\nSame as before, but the true distribution is symmetric about 1.\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nDo a sign test, Wilcoxin signed-rank test, and a paired \\(t\\)-test for the difference in femoral neck density between the two twins. Which do you think is more appropriate?\n\n\nWe’ll first calculate the difference between bone densities:\n\nboneden |&gt;\n  mutate(fn_diff = fn1 - fn2) -&gt;\n  boneden\n\nWe’ll run an exact sign test using the number of positive and negative differences:\n\nboneden |&gt;\n  summarize(x = sum(fn_diff &gt; 0), n = sum(fn_diff &gt; 0 | fn_diff &lt; 0))\n\n# A tibble: 1 × 2\n      x     n\n  &lt;int&gt; &lt;int&gt;\n1    22    39\n\nbinom.test(x = 22, n = 39) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.564        22   0.522        39    0.396     0.722 Exact bin… two.sided  \n\n\nThe Wilcoxin signed rank test is done via wilcox.test().\n\nwilcox.test(x = boneden$fn_diff, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      404.   0.856 Wilcoxon signed rank test with continuity corre… two.sided  \n\n\nThe t-test is done via t.test().\n\nt.test(fn_diff ~ 1, data = boneden) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1 0.000732    0.0503   0.960        40  -0.0287    0.0301 One Sampl… two.sided  \n\n\nWe’ll do a histogram and a QQ-plot to check symmetry and normality.\n\nggplot(boneden, aes(x = fn_diff)) +\n  geom_histogram(bins = 10, color = \"black\", fill = \"white\")\n\n\n\n\n\n\n\nggplot(boneden, aes(sample = fn_diff)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nIt looks extremely symmetric and normal, so the t-test is probably the best. All the p-values are large, so we see no evidence of a difference between twin types in femerol neck bone density.\n\n\n\n\nExerciseSolution\n\n\nWhat is the signed-rank sum from these data. Do it by hand then check your work using R.\n\nx &lt;- c(11, 39, 75, 60, 66, -28, 55, 61, -69, -7)\n\n\n\nOrdering by magnitude, we have: -7, 11, -28, 39, 55, 60, 61, 66, -69, 75\nSo the ranks of the positive numbers are 2, 4, 5, 6, 7, 8, 10\nAdding these up we get 42\nWe can check in R via\n\nsum(rank(abs(x))[x &gt; 0])\n\n[1] 42"
  },
  {
    "objectID": "06_nonparametric/06_wilcoxin_2.html",
    "href": "06_nonparametric/06_wilcoxin_2.html",
    "title": "Two-sample Nonparametric Methods",
    "section": "",
    "text": "Consider the study on the effects of lead, described here.\n\nlibrary(tidyverse)\nlibrary(broom)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\n\nWilcoxin Rank-sum test (AKA Mann-Whitney \\(U\\) test)\n\nThe Wilcoxin rank-sum test is the nonparametric version of the two-sample \\(t\\)-test.\nThe null is that the distributions of the two samples are the same. The alternative is that they are the same except shifted. That is,\n\n\\(X_i \\sim F_1\\) for \\(i = 1,\\ldots,n_1\\) and \\(Y_j \\sim F_2\\) for \\(j = 1,\\ldots,n_2\\). Here, \\(F_1\\) and \\(F_2\\) are the CDF’s of samples 1 and 2, respectively.\n\\(H_0\\): \\(F_1 = F_1\\)\n\\(H_1\\): \\(F_1(x) = F_2(x + \\Delta)\\) for some \\(\\Delta \\neq 0\\).\n\nThis shift interpretation is only valid if the distributions are about the same but shifted (checkable using histograms). If the distributions vary wildly, the hypothesis test is actually\n\n\\(H_0\\): \\(P(X &gt; Y) = P(Y &gt; X)\\)\n\\(H_1\\): \\(P(X &gt; Y) \\neq P(Y &gt; X)\\)\n\nThe idea of this test is that the rank-sums should be about the same on average (if \\(n_1 = n_2\\))\n\nset.seed(68)\ndf &lt;- tibble(\n  value = c(rnorm(20), rnorm(20)),\n  group = c(rep(1, 20), rep(2, 20))\n)\ndf |&gt;\n  mutate(rank = rank(value)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1   413\n2     2   407\n\n\nBut, if the two distributions differ by some location shift, then the distribution shifted up will have higher ranks on average, and so a larger rank sum.\n\nset.seed(68)\ndf &lt;- tibble(\n  value = c(rnorm(20), rnorm(20, mean = 1)),\n  group = c(rep(1, 20), rep(2, 20))\n)\ndf |&gt;\n  mutate(rank = rank(value)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1   303\n2     2   517\n\n\nVisualization: If the two distributions are the same, the average rankings should be about the same (when \\(n_1 = n_2\\)). Sample of 5 individuals from each group (rug plot) from distribution whose PDF is plotted. The ranks are the numbers above the rug plots. The rank sum of each sample is in red.\n\n\n\n\n\n\n\n\n\nVisualization: If one distribution is shifted, the rankings for the shifted to the right distribution will in general be larger than expected than if the distributions were the same.\n\n\n\n\n\n\n\n\n\nWe can evaluate if the exposed and the control groups have different distributions of IQF\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nUse wilcox.text() to run a Wilcoxin rank-sum test.\n\nResponse variable is to the left of the tilde ~\nGrouping variable is to the right of the tilde\ndata: The data frame that containst hte variables\n\n\nwilcox.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     1996.   0.114 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\nThe \\(p\\)-value above was pretty large, so we don’t have evidence of a difference of IQ between the two groups.\n\n\nExerciseSolution\n\n\nCalculate the rank sum statistic using these data. First by hand and then using R.\n\ndf &lt;- tibble(\n  group = c(1, 1, 1, 2, 2, 2),\n  val = c(80, -36, -83, 63, 79, 93)\n)\n\n\n\nBy hand, we have the ranks are: (5, 2, 1, 3, 4, 6)\nWe add up the first three to get 8\nWe can also do this in R:\n\ndf |&gt;\n  mutate(rank = rank(val)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     8\n2     2    13\n\n\n\n\n\n\nExercise*Solution\n\n\nThe Wilcoxin rank-sum test is most often used for ordinal data. E.g. consider the Werry-Weiss-Peters scale for hyperactivity (as reported by parents), which goes from 0 for no hyperactivity to 4 for severe hyperactivity. Is there a difference in hyperactivity between the exposed and control groups? Do an EDA and then answer with a formal hypothesis test.\n\n\nLooks like exposed group is a little higher in the scale on average.\n\nlead |&gt;\n  filter(!is.na(hyperact)) |&gt;\n  group_by(Group, hyperact) |&gt;\n  summarize(n = n()) |&gt;\n  ungroup() |&gt;\n  group_by(Group) |&gt;\n  mutate(prop = n / sum(n))\n\n`summarise()` has grouped output by 'Group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Group [2]\n  Group   hyperact     n   prop\n  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1 control        0    24 0.490 \n2 control        1    20 0.408 \n3 control        2     3 0.0612\n4 control        3     2 0.0408\n5 exposed        0    15 0.429 \n6 exposed        1    14 0.4   \n7 exposed        2     5 0.143 \n8 exposed        3     1 0.0286\n\n\nLet’s run the Wilcoxin rank sum test:\n\nwilcox.test(hyperact ~ Group, data = lead, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      784.   0.465 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\nP-value for group differences is 0.4649, so we don’t have evidence of any group differences.\n\n\n\n\n\nPermutation Tests\n\nThe the distribution of the two groups are indeed the exact same, then hypothetically we could arbitrarily choose which individuals belong to which group and the distribution of the rank sum should be the same.\nThis is the idea of the permutation test.\nYou generate a null distribution via:\n\nRandomly assign group labels to individuals\nCalculate the rank-sum statistic\nRepeat 1 and 2 many many times.\n\nIf the null is true, then our observed rank sum statistic should be about the same as the rank sum statistics from this null distribution. So we calculate a \\(p\\)-value by seeing how extreme our observed rank sum statistic is.\nYou can randomly assign labels by randomly permuting them with sample().\n\nset.seed(1)\nlead |&gt;\n  filter(!is.na(iqf)) |&gt; ## always remove NA's first\n  mutate(rank = rank(iqf)) |&gt;\n  select(iqf, rank, Group) -&gt;\n  lead_sub\nlead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  head()\n\n# A tibble: 6 × 4\n    iqf  rank Group   new_group\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    \n1    70   4   control exposed  \n2    85  40   control control  \n3    86  46   control control  \n4    76  15.5 control control  \n5    84  36   control exposed  \n6    96  81.5 control control  \n\n\nSo one iteration of generating the null distribution would be to first choose each individual’s group, the calculate the rank sum statistic.\n\nlead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  group_by(new_group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nsumdf$rsum[[1]]\n\n[1] 4622\n\n\nYou can replicate this process with replicate().\n\nrout &lt;- replicate(n = 1000, expr = {\n  lead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  group_by(new_group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nsumdf$rsum[[1]]\n})\n\nOur observed rank-sum statistic is\n\nlead_sub |&gt; \n  group_by(Group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nrobs &lt;- sumdf$rsum[[1]]\nrobs\n\n[1] 4770\n\n\nOur observed rank-sum statistic is a little rare:\n\ndata.frame(rsum = rout) |&gt;\n  ggplot(aes(x = rsum)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\") +\n  geom_vline(xintercept = robs, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nWe can quantify how rare by seeing how many null rank sum statistics are at or above our observed statistic. This is our permutation test \\(p\\)-value\n\n2 * mean(robs &lt;= rout)\n\n[1] 0.12\n\n\nFor large \\(n\\), this will be about the same as the normality approximation from the Wilcoxin rank-sum test:\n\nwilcox.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     1996.   0.114 Wilcoxon rank sum test with continuity correcti… two.sided"
  },
  {
    "objectID": "07_cat/07_binom.html",
    "href": "07_cat/07_binom.html",
    "title": "Categorical Tests",
    "section": "",
    "text": "library(broom)\n\n\nTwo-sample Binomial Test\n\nWe have the following 2x2 contingency table from a study comparing age of a mother at her first birth against breast cancer status. The hypothesis is that women who have their first births later in life are at higher risks of breast cancer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge at First Birth\n\nStatus\n\nTotal\n\n\nCase\nControl\n\n\n\n\n≥30\n683\n1498\n2181\n\n\n≤29\n2537\n8747\n11284\n\n\nTotal\n3220\n10245\n13465\n\n\n\n\n\n\n\nLet \\(n_1 = 3220\\) and \\(n_2 = 10245\\) be the sample sizes among case and control women, respectively.\nLet \\(x_1 = 683\\) and \\(x_2 = 1498\\) be the number of women older than 30 at first birth for case and control women, respectively.\nOur model is the \\(X_1 \\sim \\text{Binom}(n_1, p_1)\\) and \\(X_2 \\sim \\text{Binom}(n_2, p_2)\\).\nOur hypotheses are\n\n\\(H_0\\): \\(p_1 = p_2\\)\n\\(H_1\\): \\(p_1 \\neq p_2\\)\n\nYou can run this test in R via\n\nprop.test(x = c(683, 1498), n = c(3220, 10245)) |&gt;\n  tidy()\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic  p.value parameter conf.low conf.high method    \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1     0.212     0.146      77.9 1.09e-18         1   0.0500    0.0818 2-sample …\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nIf we did this manually (which you will only do for exams and homeworks, not in real life), we first calculate the estimated proportions\n\np1hat &lt;- 683 / 3220\np2hat &lt;- 1498 / 10245\n\nWe then calculate the pooled estimated proportion, which is our estimate if the null is true\n\nphat &lt;- (683 + 1498) / (3220 + 10245)\n\nOur test statistic (I’m skipping the continuity correction)\n\nz &lt;- (p1hat - p2hat) / sqrt((1 / 3220 + 1 / 10245) * phat * (1 - phat))\nz\n\n[1] 8.853\n\n\nAnd our \\(p\\)-value compares this to the standard normal\n\n2 * pnorm(-abs(z))\n\n[1] 8.545e-19\n\n\n\n\nExerciseSolution\n\n\nA study looked at the effects of oral contraceptive (OC) use on heart disease in women 40 to 44 years of age. The researchers found that among 5000 current OC users at baseline, 13 women developed a myocardial infarction (MI) over a 3-year period, whereas among 10,000 never-OC users, 7 developed an MI over a 3-year period. Assess the statistical significance of the results. Do this both “by hand” and using prop.test(). State your results.\n\n\nLet \\(X_1\\) be the number of OC users that have MI (out of \\(n_1 = 5000\\)). Let \\(X_2\\) be the number of never-OC users that have MI (out of \\(n_1 = 10000\\)). Then \\(X_1 \\sim \\mathrm{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(n_2,p_2)\\). We are testing \\(H_0: p_1 = p_2\\) versus \\(H_1: p_1 \\neq p_2\\).\nUsing prop.test() we get:\n\nprop.test(x = c(13, 7), n = c(5000, 10000)) |&gt;\n  tidy()\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic p.value parameter conf.low conf.high method     \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1    0.0026    0.0007      7.67 0.00563         1 0.000246   0.00355 2-sample t…\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nWe calculate \\(\\hat{p}_1\\), \\(\\hat{p}_2\\), and \\(\\hat{p}\\)\n\np1hat &lt;- 13 / 5000\np2hat &lt;- 7 / 10000\nphat &lt;- (13 + 7) / (5000 + 10000)\n\nThe \\(z\\) statistic is\n\nz &lt;- (p1hat - p2hat) / sqrt((1 / 5000 + 1 / 10000) * phat * (1 - phat))\n\nThe two-sided \\(p\\)-value is:\n\n2 * pnorm(-abs(z))\n\n[1] 0.002646\n\n\nWith continuity correction, this becomes:\n\nz &lt;- (p1hat - p2hat - 1/10000 - 1/20000) / sqrt((1 / 5000 + 1 / 10000) * phat * (1 - phat))\n2 * pnorm(-abs(z))\n\n[1] 0.005626\n\n\nWe have strong evidence that there is a difference in MI rates between OC users and non-OC users (p = 0.005626).\n\n\n\n\n\nContingency Table Perspective\n\nIn this study design, we collected case women and control women, and measured their age.\nIf we would have run the test accidentally assuming that we had collected younger and older women, and measured their cancer status, then it turns out that we would have gotten the exact same \\(p\\)-value.\n\nprop.test(x = c(683, 1498), n = c(3220, 10245))$p.value\n\n[1] 1.092e-18\n\nprop.test(x = c(683, 2537), n = c(2181, 11284))$p.value\n\n[1] 1.092e-18\n\n\nYou can consider the binomial test, then, as a test for association between two variables that each are binary (categorical with only two categories).\nTo run the equivalent test of association using a contingency table, first create it using matrix():\n\ntab &lt;- matrix(c(683, 1498, 2537, 8747), nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(Age = c(\"≥30\", \"≤29\"), Status = c(\"Case\", \"Control\"))\ntab\n\n     Status\nAge   Case Control\n  ≥30  683    1498\n  ≤29 2537    8747\n\n\nThe dimnames() code above sets the row names (first) and the column names (second) via a list object.\nThen just plug it into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                                           \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                            \n1      77.9 1.09e-18         1 Pearson's Chi-squared test with Yates' continuit…\n\n\n\n\nExerciseSolution\n\n\nFrom OC exercise above, insert these data into a 2x2 contingency table. Then run a chi-squared test for homogeneity. Verify that your results are the same as above.\n\n\n\ntab &lt;- matrix(c(13, 5000 - 13, 7, 10000 - 7), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(OC = c(\"Yes\", \"No\"), MI = c(\"Yes\", \"No\"))\ntab\n\n     MI\nOC    Yes   No\n  Yes  13 4987\n  No    7 9993\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1      7.67 0.00563         1 Pearson's Chi-squared test with Yates' continuity…\n\n\np-value is the same as above\n\n\n\n\n\nFisher’s Exact Test\n\nThe above methods are only valid for large \\(n\\) (expected counts at least 5 in every cell).\nIf this is not a valid assumption, then you can use fisher.test() to run an exact test (controls Type I error for all \\(n\\), not just large \\(n\\)).\nThe syntax is the exact same as chisq.test()\n\ntab\n\n     MI\nOC    Yes   No\n  Yes  13 4987\n  No    7 9993\n\nfisher.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1     3.72 0.00400     1.38      11.0 Fisher's Exact Test for Count… two.sided  \n\n\nFor large \\(n\\), the chi-squared and Fisher tests will provide about the same values. So why use chisq.test()? Sometimes, approximate methods can be better. But my opinion is that the stated benefits are minor compared to the benefit of controlling type I error exactly. So I would always use the Fisher test.\n\n\nExerciseSolution\n\n\nResearchers collected information on salt diet versus cardiovascular death. Run a Fisher’s exact test using the below table to evaluate if diet is associated with cardiovascular death. How does it compare to the chi-squared test?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCause of death\n\nType of diet\n\nTotal\n\n\nHigh salt\nLow salt\n\n\n\n\nNon-CVD\n2\n23\n25\n\n\nCVD\n5\n30\n35\n\n\nTotal\n7\n53\n60\n\n\n\n\n\n\n\n\n\n\ntab &lt;- matrix(c(2, 23, 5, 30), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(cause = c(\"Non-CVD\", \"CVD\"), diet = c(\"High salt\", \"Low salt\"))\ntab\n\n         diet\ncause     High salt Low salt\n  Non-CVD         2       23\n  CVD             5       30\n\nfisher.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1    0.527   0.688   0.0463      3.58 Fisher's Exact Test for Count… two.sided  \n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1     0.116   0.734         1 Pearson's Chi-squared test with Yates' continuity…\n\n\nSame conclusion. No evidence of an association. But p-value is smaller in Fisher’s exact test."
  },
  {
    "objectID": "07_cat/07_mcnemar.html",
    "href": "07_cat/07_mcnemar.html",
    "title": "McNemar’s Test",
    "section": "",
    "text": "library(broom)\n\n\nContingency Table Approach\n\nWomen were matched into pairs based on age and clinical characteristics. One woman of each pair was given treatment A and the other treatment B. The doctors then followed the women to see which survived and which died within 5 years.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\nB\n\nTotal\n\n\nSurvived\nDied\n\n\n\n\nSurvived\n510\n16\n526\n\n\nDied\n5\n90\n95\n\n\nTotal\n515\n106\n621\n\n\n\n\n\n\n\nIf we have a 2x2 contingency taable with matched pairs as the sampling unit, we can put it into R using matrix(), as with other contingency tables.\n\ntab &lt;- matrix(c(510, 16, 5, 90), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(A = c(\"Survived\", \"Died\"), B = c(\"Survived\", \"Died\"))\ntab\n\n          B\nA          Survived Died\n  Survived      510   16\n  Died            5   90\n\n\nTo run McNemar’s test, we can run mcnemar.test() for the large-sample approach\n\nmcnemar.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      4.76  0.0291         1 McNemar's Chi-squared test with continuity correc…\n\n\nThis is the exact same as running prop.test() on the discordant pairs.\n\nprop.test(x = 16, n = 5 + 16, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.762      4.76  0.0291         1    0.525     0.909 1-sample … two.sided  \n\n\nFor an exact test, we can run binom.test() on the discordant pairs.\n\nbinom.test(x = 16, n = 5 + 16, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.762        16  0.0266        21    0.528     0.918 Exact bin… two.sided  \n\n\nThis is best for small sample sizes.\nRule of thumb: 20 or more discordant pairs is enough for McNemar’s test. Fewer than that and use the binomial method.\nBut for this sample size, asymptotic approaches are fine.\n\n\n\nRaw Data Approach\n\nNow suppose we don’t have a 2x2 contingency table of pairs, but just two binary variables.\nE.g., a mall device and a trained observer assess if a person is hypertensive. The data are as follows\n\ndf &lt;- data.frame(\n  mall = c(\"-\", \"-\", \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"-\", \n           \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"-\"),\n  trained = c(\"-\", \"-\", \"-\", \"+\", \"-\", \"-\", \"-\", \"+\", \"+\", \"-\", \n              \"-\", \"-\", \"-\", \"-\", \"+\", \"-\", \"-\", \"-\", \"-\", \"-\")\n)\ndf\n\n   mall trained\n1     -       -\n2     -       -\n3     +       -\n4     +       +\n5     -       -\n6     +       -\n7     -       -\n8     +       +\n9     +       +\n10    -       -\n11    +       -\n12    +       -\n13    -       -\n14    +       -\n15    -       +\n16    +       -\n17    +       -\n18    -       -\n19    -       -\n20    -       -\n\n\nYou can create the contingency table with table() and then run mcnemar.test().\n\ntab &lt;- table(df$mall, df$trained)\ntab\n\n\n    - +\n  - 9 1\n  + 7 3\n\nmcnemar.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      3.12  0.0771         1 McNemar's Chi-squared test with continuity correc…\n\n\nOr run the exact test, which you should here since there are 8 discordant pairs, which is less than our rule of thumb of 20.\n\nbinom.test(x = 7, n = 7 + 1) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.875         7  0.0703         8    0.473     0.997 Exact bin… two.sided  \n\n\nAn alternative to first creating the contingency table is to just put each variable in mcnemar.test() (as long as the sample size is large enough.\n\nmcnemar.test(x = df$mall, y = df$trained) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      3.12  0.0771         1 McNemar's Chi-squared test with continuity correc…\n\n\n\n\nExercise (from Rosner):Solution\n\n\nA twin design is used to study age-related macular degeneration (AMD), a common eye disease of the elderly that results in substantial losses in vision. Suppose we contact 66 twinships in which one twin has AMD and the other twin does not. The twins are given a dietary questionnaire to report their usual diet. We find that in 10 twinships the AMD twin takes multivitamin supplements and the normal twin does not. In 8 twinships the normal twin takes multivitamin supplements and the AMD twin does not. In 3 twinships both twins take multivitamin supplements, and in 45 twinships neither twin takes multivitamin supplements.\n\nWhat test can be used to assess whether there is an association between AMD and taking multivitamin supplements?\nAre AMD and taking multivitamin supplements significantly associated based on these data?\n\n\n\n\ntab &lt;- matrix(c(3, 10, 8, 45), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(AMD = c(\"Yes\", \"No\"), normal = c(\"Yes\", \"No\"))\ntab\n\n     normal\nAMD   Yes No\n  Yes   3 10\n  No    8 45\n\n\nThis has 18 discordant pairs, so we should use the exact method.\n\nbinom.test(x = 10, n = 10 + 8, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.556        10   0.815        18    0.308     0.785 Exact bin… two.sided  \n\n\np-value = 0.8145, so no evidence of an association"
  },
  {
    "objectID": "07_cat/07_larger.html",
    "href": "07_cat/07_larger.html",
    "title": "Larger Contingency Tables",
    "section": "",
    "text": "library(broom)\n\n\nSuppose we measure case and control status (for breast cancer) for various ages at first birth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge at First Birth\n\nAge at First Birth\n\nTotal\n\n\n&lt;20\n20-24\n25-29\n30-34\n≥35\n\n\n\n\nCase\n320\n1206\n1011\n463\n220\n3220\n\n\nControl\n1422\n4432\n2893\n1092\n406\n10245\n\n\nTotal\n1742\n5638\n3904\n1555\n626\n13465\n\n\n\n\n\n\n\nAs in the 2x2 case, we use matrix() to insert the data. Just change the nrow and ncol arguments to represent the number of rows and columns\n\ntab &lt;- matrix(\n  c(320, 1206, 1011, 463, 220,\n    1422, 4432, 2893, 1092, 406),\n  nrow = 2, ncol = 5, byrow = TRUE)\ndimnames(tab) &lt;- list(status = c(\"Case\", \"Control\"),\n                      age = c(\"&lt;20\", \"20-24\", \"25-29\", \"30-34\", \"≥35\"))\ntab\n\n         age\nstatus     &lt;20 20-24 25-29 30-34 ≥35\n  Case     320  1206  1011   463 220\n  Control 1422  4432  2893  1092 406\n\n\nYou then just plug this into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                    \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      130. 3.30e-27         4 Pearson's Chi-squared test\n\n\nAs in the 2x2 case, you can generate a contingency table from raw data using table().\nE.g., from the lead data, suppose that we are interested in testing if there is an association between lead_grp and sex.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nWe can create this contingency table by table()\n\ntab &lt;- table(lead$sex, lead$lead_grp)\ntab\n\n\n         control current exposed previous exposed\n  female      32               7                9\n  male        46              17               13\n\n\nAnd we can plug this into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.14   0.565         2 Pearson's Chi-squared test\n\n\nAlternatively, we could plug the two variables under consideration from the raw data frame directly into chisq.test().\n\nchisq.test(x = lead$sex, y = lead$lead_grp) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.14   0.565         2 Pearson's Chi-squared test\n\n\n\n\nExercise (from Rosner)Solution\n\n\nWe are interested in studying the relationship between the prevalence of hypertension in adolescents and ethnic group, where hypertension is defined as being above the 90th percentile for a child’s age, sex, and height, based on national norms.\n\nSuppose that 8 of 100 Caucasian adolescent girls, 12 out of 95 African-American adolescent girls, and 10 of 90 Hispanic adolescent girls are above the 90th percentile for blood pressure. What test can be used to assess whether there is an association between adolescent hypertension and ethnic group?\nImplement this test and report a \\(p\\)-value.\n\n\n\n\nObviously, the chi-squared test for homogeneity, since that is the only test we talked about in these notes.\nLet’s create this table:\n\ntab &lt;- matrix(c(8, 100 - 8, 12, 95 - 12, 10, 90 - 10), nrow = 2, byrow = FALSE)\ndimnames(tab) &lt;- list(Status = c(\"hypertensive\", \"normotensive\"),\n                      Ethnicity = c(\"Caucasion\", \"African-American\", \"Hispanic\"))\ntab\n\n              Ethnicity\nStatus         Caucasion African-American Hispanic\n  hypertensive         8               12       10\n  normotensive        92               83       80\n\n\nLet’s run the \\(\\chi^2\\) test:\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.16   0.561         2 Pearson's Chi-squared test\n\n\nWe have no evidence of an association (p = 0.5606)."
  },
  {
    "objectID": "07_cat/07_kappa.html",
    "href": "07_cat/07_kappa.html",
    "title": "Cohen’s Kappa in R",
    "section": "",
    "text": "Some women ate beef and wrote down in two repeat surveys how much beef they ate. We are interested in how reliable this survey is. The data look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey 1\n\nSurvey 2\n\nTotal\n\n\n≤1 Serving/Week\n&gt;1 Serving/Week\n\n\n\n\n≤1 Serving/Week\n136\n92\n228\n\n\n&gt;1 Serving/Week\n69\n240\n309\n\n\nTotal\n205\n332\n537\n\n\n\n\n\n\n\nThere is no base R function to calculate Cohen’s kappa (though there are some third party packages). I made a function that will do it for you:\n\n#' @param tab The 2x2 contingency table\n#' \n#' @return A list with the following elements\n#' \\itemize{\n#'   \\item{kappa: Cohen's kappa}\n#'   \\item{se: (estimated) standard error}\n#'   \\item{z: test statistic}\n#'   \\item{p_value: upper one-sided p-value against the null of kappa = 0}\n#' }\n#' \n#' @author David Gerard\ncohen_kappa &lt;- function(tab) {\n  stopifnot(nrow(tab) == ncol(tab))\n  n &lt;- sum(tab)\n  po &lt;- sum(diag(tab)) / n\n  a &lt;- rowSums(tab) / n\n  b &lt;- colSums(tab) / n\n  pe &lt;- sum(a * b)\n  kappa &lt;- (po - pe) / (1 - pe)\n  se &lt;- sqrt((pe + pe^2 - sum(a * b * (a + b))) / (n * (1 - pe)^2))\n  z &lt;- kappa / se\n  p_value &lt;- stats::pnorm(z, lower.tail = FALSE)\n  return(list(kappa = kappa, se = se, z = z, p_value = p_value))\n}\n\nFirst, put in contingency table in R as before\n\ntab &lt;- matrix(c(136, 92, 69, 240), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(survey1 = c(\"low\", \"high\"), survey2 = c(\"low\", \"high\"))\ntab\n\n       survey2\nsurvey1 low high\n   low  136   92\n   high  69  240\n\n\nThen use this function I wrote:\n\ncohen_kappa(tab = tab)\n\n$kappa\n[1] 0.3782\n\n$se\n[1] 0.04298\n\n$z\n[1] 8.799\n\n$p_value\n[1] 6.921e-19"
  },
  {
    "objectID": "02_descriptive/02_notes.html",
    "href": "02_descriptive/02_notes.html",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Observe \\(X_1, X_2, \\dots, X_n\\)\n\nSample of numeric values\nSubscript indexes the units\n\nExample: \\(X_i =\\) Birthweight for baby \\(i\\)\nMeasure of location = center of a sample (statistic) or a population (parameter)\nArithmetic Mean \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} (X_1 + X_2 + \\dots + X_n)\n\\]\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\) \\[\n  \\sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 5 + (-4)\n  \\]\n\\[\n  \\sum_{i=1}^2 X_i = X_1 + X_2 = 2 + 5\n  \\]\n\\[\n  \\sum_{i=2}^2 X_i = X_2 = 5\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{3} \\sum_{i=1}^3 X_i = \\frac{1}{3} (2 + 5 - 4) = \\frac{3}{3} = 1\n  \\]\n\\(\\bar{X}\\) is sensitive to extreme observations.\nExample with extreme value:\n\\[\n  X_4 = 3997\n  \\]\n\\[\n  \\frac{1}{4} \\sum_{i=1}^4 X_i = \\frac{1}{4} (2 + 5 - 4 + 3997) = \\frac{4000}{4} = 1000\n  \\]\nMedian\n\nFor \\(n\\) odd \\(\\Rightarrow \\left(\\frac{n+1}{2}\\right)\\)th largest observation.\nFor \\(n\\) even \\(\\Rightarrow\\) Average of \\(\\left(\\frac{n}{2}\\right)\\)th and \\(\\left(\\frac{n}{2} + 1\\right)\\)th largest observations.\n\nExample:\n\n\\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4 \\Rightarrow -4, 2, 5\\)\n\\[\n\\text{Median}(X) = 2\n\\]\nIf \\(X_4 = 3997\\)\n\\[\n\\text{Median}(X) = \\frac{2 + 5}{2} = 3.5\n\\]\n\nIf distribution is symmetric, \\(\\text{median}(X) \\approx \\bar{X}\\).\nMean follows the skew of distribution (dashed is median, dotted is mean):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse mean if total is important.\nUse median if lots of skew.\nA mode is a frequently occurring value.\nTypes of Modalities:\n\nUnimodal:\n\n\n\n\n\n\n\n\n\nBimodal:\n\n\n\n\n\n\n\n\n\nTrimodal:\n\n\n\n\n\n\n\n\n\n\nThe mode is typically not used as a real measure of center but rather as a way to describe distribution.\n\n\n\n\nSuppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\n\n\nExerciseSolution\n\n\nWhat is the mean menstrual cycle deviation from 4 weeks?\n\n\n\\[\n3.92 - 4 = -0.08\n\\]"
  },
  {
    "objectID": "02_descriptive/02_notes.html#arithmetic-mean",
    "href": "02_descriptive/02_notes.html#arithmetic-mean",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "The arithmetic mean ( {X} ) is defined as:\n[ {X} = _{i=1}^n X_i = (X_1 + X_2 + + X_n) ]\n\n\nGiven ( X_1 = 2 ), ( X_2 = 3 ), ( X_3 = 4 ):\n\nSum: ( _{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 3 + 4 )\nMean:\n\n[ {X} = _{i=1}^3 X_i = (2 + 3 + 4) = = 3 ]\nThe arithmetic mean ( {X} ) is sensitive to extreme observations.\n\n\n\nIf ( X_3 = 3997 ):\n[ {X} = _{i=1}^3 X_i = (2 + 3 + 3997) = = 1334 ]\nAs shown, the mean increases significantly due to the extreme value."
  },
  {
    "objectID": "02_descriptive/02_notes.html#mean-and-distribution-skew",
    "href": "02_descriptive/02_notes.html#mean-and-distribution-skew",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "Mean and Distribution Skew",
    "text": "Mean and Distribution Skew\n\nThe mean is affected by the skew of the distribution.\n\n\nRight Skew\nIn a right-skewed distribution:\n\n( &gt; )\nExample: Years of oral contraception use.\n\n\nlibrary(ggplot2)\nset.seed(123)\nright_skew &lt;- data.frame(value = rexp(1000, rate = 0.5))\n\nggplot(right_skew, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(value)), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nLeft Skew\nIn a left-skewed distribution:\nMean&lt;MedianMean&lt;Median\nExample: Relative humidity in summer in Ohio.\n\nset.seed(123)\nleft_skew &lt;- data.frame(value = -rexp(1000, rate = 0.5) + 5)\n\nggplot(left_skew, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(value)), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Left-Skewed Distribution\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nChoosing Mean or Median\nUse the mean if the total is important.\nUse the median if there is a lot of skew."
  },
  {
    "objectID": "02_descriptive/02_notes.html#measures-of-location",
    "href": "02_descriptive/02_notes.html#measures-of-location",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Observe \\(X_1, X_2, \\dots, X_n\\)\n\nSample of numeric values\nSubscript indexes the units\n\nExample: \\(X_i =\\) Birthweight for baby \\(i\\)\nMeasure of location = center of a sample (statistic) or a population (parameter)\nArithmetic Mean \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} (X_1 + X_2 + \\dots + X_n)\n\\]\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\) \\[\n  \\sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 5 + (-4)\n  \\]\n\\[\n  \\sum_{i=1}^2 X_i = X_1 + X_2 = 2 + 5\n  \\]\n\\[\n  \\sum_{i=2}^2 X_i = X_2 = 5\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{3} \\sum_{i=1}^3 X_i = \\frac{1}{3} (2 + 5 - 4) = \\frac{3}{3} = 1\n  \\]\n\\(\\bar{X}\\) is sensitive to extreme observations.\nExample with extreme value:\n\\[\n  X_4 = 3997\n  \\]\n\\[\n  \\frac{1}{4} \\sum_{i=1}^4 X_i = \\frac{1}{4} (2 + 5 - 4 + 3997) = \\frac{4000}{4} = 1000\n  \\]\nMedian\n\nFor \\(n\\) odd \\(\\Rightarrow \\left(\\frac{n+1}{2}\\right)\\)th largest observation.\nFor \\(n\\) even \\(\\Rightarrow\\) Average of \\(\\left(\\frac{n}{2}\\right)\\)th and \\(\\left(\\frac{n}{2} + 1\\right)\\)th largest observations.\n\nExample:\n\n\\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4 \\Rightarrow -4, 2, 5\\)\n\\[\n\\text{Median}(X) = 2\n\\]\nIf \\(X_4 = 3997\\)\n\\[\n\\text{Median}(X) = \\frac{2 + 5}{2} = 3.5\n\\]\n\nIf distribution is symmetric, \\(\\text{median}(X) \\approx \\bar{X}\\).\nMean follows the skew of distribution (dashed is median, dotted is mean):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse mean if total is important.\nUse median if lots of skew.\nA mode is a frequently occurring value.\nTypes of Modalities:\n\nUnimodal:\n\n\n\n\n\n\n\n\n\nBimodal:\n\n\n\n\n\n\n\n\n\nTrimodal:\n\n\n\n\n\n\n\n\n\n\nThe mode is typically not used as a real measure of center but rather as a way to describe distribution.\n\n\n\n\nSuppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\nExercise: What is the mean menstrual cycle deviation from 4 weeks?"
  },
  {
    "objectID": "03_prob/03_notes.html",
    "href": "03_prob/03_notes.html",
    "title": "Chapter 3 Notes: Probability",
    "section": "",
    "text": "Sample Space: Set of all possible outcomes.\nEvent: Any set of outcomes (subset of sample space).\nProbability of Event: Frequency of the event over a large number of trials.\nExample: Tuberculin skin test to detect tuberculosis\n\n\n\nOutcome\nProb\n\n\n\n\nPositive\n0.1\n\n\nNegative\n0.7\n\n\nUncertain\n0.2\n\n\n\nSample Space: \\(\\{ \\text{Positive}, \\text{Negative}, \\text{Uncertain} \\}\\)\nPossible Events:\n\n\n\nEvent\nProb\n\n\n\n\n\\(\\{ \\text{Positive} \\}\\)\n0.1\n\n\n\\(\\{ \\text{Negative} \\}\\)\n0.7\n\n\n\\(\\{ \\text{Uncertain} \\}\\)\n0.2\n\n\n\\(\\{ \\text{Positive, Negative} \\}\\)\n0.8\n\n\n\\(\\{ \\text{Positive, Uncertain} \\}\\)\n0.3\n\n\n\\(\\{ \\text{Negative, Uncertain} \\}\\)\n0.9\n\n\n\\(\\{ \\text{Positive, Negative, Uncertain} \\}\\)\n1\n\n\n\nNotation:\n\n\\(P(E) =\\) Probability of event \\(E\\)\nE.g., if \\(E = \\{\\text{Positive, Negative}\\}\\), then \\(P(E) = 0.8\\)\n\nTwo events are mutually exclusive if they cannot both happen at the same time.\nExample:\n\n\\(E_1 = \\{\\text{Positive, Negative}\\}\\)\n\\(E_2 = \\{\\text{Uncertain}\\}\\)\n\\(E_3 = \\{\\text{Negative, Uncertain}\\}\\)\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\\(E_1\\) and \\(E_3\\) can both happen if the outcome is “Negative.”\n\\(E_2\\) and \\(E_3\\) can both happen if the outcome is “Uncertain.”"
  },
  {
    "objectID": "03_prob/04_notes.html",
    "href": "03_prob/04_notes.html",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "A random variable assigns numbers to outcomes in the sample space.\nExample:\n\nNumber of children with retinitis pigmentosa\nNumber of individuals with leukemia\netc…\n\nBasically, an event that is a number.\nDiscrete random variable: Can count them (but may be infinite).\n\nTypically \\(0, 1, 2, 3, \\dots\\)\n\nContinuous random variable: Cannot count them.\n\nTypically some interval \\((-\\infty, \\infty)\\), \\([0,1]\\), etc.\n\nDenote random variables with capital letters \\(X, Y, Z\\), etc.\n\n\n\n\n\n\n\nTipProbability Mass Function (PMF)\n\n\n\nAssigns a probability to a possible value \\(r\\). Denote this probability by \\(P(X = r)\\).\n\n\n\nA PMF is a function of \\(r\\), not \\(X\\). \\(X\\) is used to denote the random variable.\nHypertensive Example: Let \\(X =\\) number of patients in a trial of 4 who have improved blood pressure.\n\n\\(P(X = 0) = 0.008\\)\n\\(P(X = 1) = 0.076\\)\n\\(P(X = 2) = 0.265\\)\n\\(P(X = 3) = 0.411\\)\n\\(P(X = 4) = 0.240\\)\n\n\\(0 \\leq P(X = r) \\leq 1\\) for all \\(r\\)\n\\(\\sum_r P(X = r) = 1\\)\n\nSum over all possible \\(r\\) is 1.\n\n\n\n\n\n\n\n\nTipExpected Value or Mean\n\n\n\nMeasure of center of a PMF.\n\\[\nE(X) = \\sum_r r \\cdot P(X = r) = \\mu\n\\] - Again, summing over all possible \\(r\\).\n\n\n\nHypertensive Example:\n\\[\nE(X) = 0 \\cdot 0.008 + 1 \\cdot 0.076 + 2 \\cdot 0.265 + 3 \\cdot 0.411 + 4 \\cdot 0.240 = 2.8\n\\]\nCan be interpreted as the average value of \\(X\\) across many trials.\nNote: \\(\\mu\\) is a population parameter, not a statistic, which is a function of observed data.\nExample: Across some trials, we might see:\n\n\n\n\\(x\\)\nfrequency\n\n\n\n\n0\n0\n\n\n1\n9\n\n\n2\n24\n\n\n3\n48\n\n\n4\n19\n\n\n\n\\[\n\\bar{X} = 0 \\cdot 0 + 1 \\cdot \\frac{9}{100} + 2 \\cdot \\frac{24}{100} + 3 \\cdot \\frac{48}{100} + 4 \\cdot \\frac{19}{100} = 2.77\n\\]\n\n\n\n\n\n\n\nTipVariance\n\n\n\nMeasure of spread.\n\\[\n  \\text{Var}(X) = \\sum_r (r - \\mu)^2 \\, P(X = r) = \\sigma^2\n  \\]\n\n\n\n\\(SD(X) = \\sqrt{\\sigma^2} = \\sigma\\)\nNote: \\(\\text{Var}(X) = E(X^2) - E(X)^2 = \\sum_r r^2 \\, P(X = r) - \\left(\\sum_r r \\, P(X = r)\\right)^2\\)\nLarger \\(\\sigma\\) means more variable.\n\n\n\n\n\n\n\nTipCumulative Distribution Function (CDF)\n\n\n\n\\[\nF(x) = P(X \\leq x) = \\text{Probability } X \\text{ is less than or equal to } x\n\\]\n\n\n\nExample: Hypergeometric distribution\n\n\\(F(0) = 0.008\\)\n\\(F(1) = 0.008 + 0.076\\)\n\\(F(2) = 0.008 + 0.076 + 0.265\\)\n\\(F(3) = 0.008 + 0.076 + 0.265 + 0.411\\)\n\\(F(4) = 0.008 + 0.076 + 0.265 + 0.411 + 0.240\\)\n\nUseful for probability calculation:\n\\[\nP(1 \\leq X \\leq 3) = P(X \\leq 3) - P(X \\leq 0)\n\\]\n\n\nExerciseSolution\n\n\n\\(X =\\) number of boys in a family of 4\n\n\n\n\\(r\\)\n\\(P(X = r)\\)\n\n\n\n\n0\n\\(\\frac{1}{16}\\)\n\n\n1\n\\(\\frac{1}{4}\\)\n\n\n2\n\\(\\frac{3}{8}\\)\n\n\n3\n\\(\\frac{1}{4}\\)\n\n\n4\n\\(\\frac{1}{16}\\)\n\n\n\nCalculate \\(E(X)\\), \\(SD(X)\\), \\(F(X)\\)\n\n\n\n\\(E[X] = 0 \\cdot \\frac{1}{16} + 1 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{4} + 4 \\cdot \\frac{1}{16} = 2\\)\n\\(E[X^2] = 0^2 \\cdot \\frac{1}{16} + 1^2 \\cdot \\frac{1}{4} + 2^2 \\cdot \\frac{3}{8} + 3^2 \\cdot \\frac{1}{4} + 4^2 \\cdot \\frac{1}{16} = 5\\)\n\\(Var(X) = E[X^2] - E[X]^2 = 5 - 2^2 = 1\\)\n\\(SD(X) = \\sqrt{Var(X)} = \\sqrt{1} = 1\\)\nCDF:\n\n\\(F(0) = \\frac{1}{16}\\)\n\\(F(1) = \\frac{1}{16} + \\frac{1}{4}\\)\n\\(F(2) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8}\\)\n\\(F(3) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8} + \\frac{1}{4}\\)\n\\(F(4) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8} + \\frac{1}{4} + \\frac{1}{16}\\)\n\n\n\n\n\n\nSome distributions are seen in real data over and over again:\n\nBinomial: count out of \\(n\\)\nPoisson: count during some time interval, or count in some area of space.\n\nThese have specific PDFs/CDFs.\nWe need to know about permutations/combinations to understand them.\nNumber of permutations of \\(n\\) things taken \\(k\\) times:\n\\[\n{}_nP_k = n(n-1)(n-2) \\dots (n-k+1) = \\frac{n!}{(n-k)!}\n\\]\nExample: Individuals \\(= A, B, C, D\\)\n\\[\n{}_4P_2 = \\frac{4!}{2!} = \\frac{4 \\cdot 3}{1} = 12\n\\]\nPossible permutations:\n\n\\(A, B\\)\n\\(A, C\\)\n\\(A, D\\)\n\\(B, A\\)\n\\(B, C\\)\n\\(B, D\\)\n\\(C, A\\)\n\\(C, B\\)\n\\(C, D\\)\n\\(D, A\\)\n\\(D, B\\)\n\\(D, C\\)\n\n\\({}_4P_3\\) Example:\n\nTree diagram shows all possible arrangements of \\(A, B, C, D\\) taken 3 at a time:\n\n \nWhat if order does not matter? For example, \\(\\{A, B, C\\} = \\{C, B, A\\}\\).\nThe number of combinations of \\(n\\) things taken \\(k\\) at a time:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\nThere are \\(\\frac{n!}{(n-k)!}\\) permutations of size \\(k\\).\n\nEach of those shares elements with \\(k! = {}_kP_k\\) other permutations.\nExample: \\(A, B, C, D \\quad {}_4C_3\\)\n\nPossible combinations:\n\n\\(A, B, C = A, C, B = B, A, C = B, C, A = C, A, B = C, B, A = 3!\\)\n\n\nDivide by \\(k!\\) to get the number of combinations.\n\n\n\n\nIf given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\npmf\n\n# A tibble: 5 × 2\n      r    pr\n  &lt;int&gt; &lt;dbl&gt;\n1     0 0.008\n2     1 0.076\n3     2 0.265\n4     3 0.411\n5     4 0.24 \n\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/04_notes.html#binomial-distribution",
    "href": "03_prob/04_notes.html#binomial-distribution",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\\(n\\) trials\nOutcome of each trial is “success” or “failure”\n\\(P(\\text{success}) = p\\) for each trial\nTrials are independent\n\n\nLet $X = $ # of successes\nThen \\(X \\sim \\text{Bin}(n, p)\\) (Distributed as Binomial)\nExample: White blood count\n\nLet $X = $ # of neutrophils out of 100 white blood cells\n\\(P(\\text{neutrophile}) = 0.6\\)\n\\(\\Rightarrow X \\sim \\text{Bin}(100, 0.6)\\)\n\nIf \\(X \\sim \\text{Bin}(n, p)\\) then\n\\[\nP(X = r) = \\binom{n}{r} p^r (1 - p)^{n - r}\n\\]\nExample: Suppose \\(X \\sim \\text{Bin}(3, 0.3)\\)\n\n\\(P(X = 2) = P(\\text{2 successes and 1 failure})\\)\n\\(P(SSF) = P(SF S) = P(F SS) = p^2 (1 - p)\\)\nSo \\(P(X = 2) = 3 \\, p^2 (1 - p) = \\binom{3}{2} p^2 (1 - p)\\)\n\nClaim: # of ways to order \\(r\\) successes and \\(n - r\\) failures is \\(\\binom{n}{r}\\)\n\nProof: Position \\(1, 2, \\dots, n\\)\nChoose \\(r\\) out of these to be \\(S\\), rest are \\(F\\)\n\nExample: $X = $ # of boys out of 5 children, \\(p = 0.51\\)\n\n\\(P(X = 2) = \\binom{5}{2} (0.51)^2 (0.49)^3 = 0.306\\)\n\n\\[\n\\binom{5}{2} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]\nCDF:\n\\[\nP(X \\leq x) = \\sum_{r=0}^{x} P(X = r) = \\sum_{r=0}^{x} \\binom{n}{r} p^r (1 - p)^{n - r}\n\\]\n\nNo simpler form.\n\nMean:\n\\[\nE(X) = \\sum_{r=0}^{n} r \\binom{n}{r} p^r (1 - p)^{n - r} = n \\, p\n\\]\n\nExpected # of successes $= $ # of trials $ P()$\n\nVariance:\n\\[\n\\text{Var}(X) = n \\, p (1 - p)\n\\]\n\n\\(n \\uparrow \\Rightarrow \\text{Var} \\uparrow\\)\n\\(p \\uparrow \\Rightarrow \\text{Var} \\uparrow\\)\nVariance is highest at \\(p = 0.5\\), smallest at \\(p = 0\\) or \\(1\\).\n\nBinomial functions in R:\n\ndbinom() = \\(P(X = r)\\)\npbinom() = \\(P(X \\leq x)\\)\nqbinom() = quantile\nrbinom() = random generation"
  },
  {
    "objectID": "03_prob/04_notes.html#poisson-distribution",
    "href": "03_prob/04_notes.html#poisson-distribution",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nCounts of rare events over some period of time or space.\nExample: # of typhoid cases in a year.\nExample: # of bacterial colonies on an agar plate.\nAssume:\n\nFor small time interval \\(\\Delta t\\), \\(P(\\text{success in } \\Delta t)\\) is about \\(\\lambda \\Delta t\\) (for some \\(\\lambda\\)).\n\\(P(\\text{more than 2 successes in } \\Delta t) \\approx 0\\)\nStationarity: \\(P(\\text{success})\\) about the same for all time intervals.\nIndependence: One success has no bearing on any other success.\n\nViolated, e.g., in epidemics.\n\n\nThen $X = $ # of “successes” in time \\(t\\)\n\n\\(X \\sim \\text{Pois}(\\mu)\\) such that \\(\\mu = \\lambda t\\)\n\n\\[\nP(X = k) = \\frac{e^{-\\mu} \\, \\mu^k}{k!}\n\\]\nNote:\n\nIf \\(X \\sim \\text{Pois}(\\mu)\\) over time \\(t\\) then \\(X \\sim \\text{Pois}(c \\mu)\\) over time \\(ct\\).\n\nExample:\n\n$X = $ # of typhoid deaths in 1 year\n\\(X \\sim \\text{Pois}(4.6)\\)\nLet $Y = $ # of typhoid deaths in half a year\n\\(Y \\sim \\text{Pois}\\left(\\frac{4.6}{2}\\right) = \\text{Pois}(2.3)\\)\n\nExample:\n\n$X = $ # of bacteria colonies in 100 cm²\n\\(X \\sim \\text{Pois}(2)\\)\n$Y = $ # of bacteria colonies in 1000 cm²\n\\(Y \\sim \\text{Pois}(20)\\)\n\nMean: If \\(X \\sim \\text{Pois}(\\mu)\\), \\(E(X) = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\mu\\)\nIf \\(X \\sim \\text{Pois}(\\lambda_1)\\) and \\(Y \\sim \\text{Pois}(\\lambda_2)\\) (and are independent), then \\(X + Y \\sim \\text{Pois}(\\lambda_1 + \\lambda_2)\\)\n\nNot generally true for other distributions (e.g., not for binomial).\n\nRelation to Binomial:\n\nIf \\(X \\sim \\text{Bin}(n, p)\\)\n\n\\(n\\) large \\((&gt; 100)\\)\n\\(p\\) small \\((&lt; 0.01)\\)\n\\(np\\) intermediate\n\nThen \\(X \\approx \\text{Pois}(np)\\)\n\n(Approximate)\n\n\nThis is used to justify Poisson in cases where we know \\(n\\) is large, but we don’t know it exactly.\nExample: $X = $ # of RNA molecules of a gene observed (on the order of 100)\n\nWe don’t know \\(n\\) but know it’s large.\nWe don’t know \\(p\\) but we know it’s small (because \\(X \\approx 100\\)).\nUse Poisson to model \\(X\\)!\n\nR functions\nExercises (4.24–4.29)\n\nof episodes for 1 child to have otitis media (ear disease) in 1 year is \\(\\text{Pois}(1.6)\\)\n\n4.24 What is the probability of getting 3 or more episodes in the first 2 years of life?\n\nSolution: $X = $ # in 2 years \\(\\sim \\text{Pois}(3.2)\\)\n\n\\[\n1 - \\text{ppois}(2, \\, \\text{lambda} = 3.2)\n\\] \\[\n= 0.6201\n\\]\n4.25 What is the probability of not getting any in the 1st year?\n\n$X = $ # in first year \\(\\sim \\text{Pois}(1.6)\\)\n\n\\[\n\\text{dpois}(x = 0, \\, \\text{lambda} = 1.6)\n\\] \\[\n= 0.2019\n\\]\n4.26 Probability two siblings will both have 3 or more episodes in the first year of life?\n\nAssumes independence.\n\n\\[\n= P(\\text{Sib 1 has } 2 \\text{ or more}) \\times P(\\text{Sib 2 has } 2 \\text{ or more})\n\\] \\[\n= 0.6201 \\times 0.6201 = 0.3845\n\\]\n\n4.27 Probability exactly 1 sibling will have 3 or more episodes (out of 2).\n\nLet $Y = $ # of siblings\n\n\\[\n  Y \\sim \\text{Bin}(2, 0.6201)\n  \\]\n\\[\n  P(Y = 1) = \\text{dbinom}(1, \\text{size} = 2, \\text{prob} = 0.6201)\n  \\]\n\\[\n  = 2 \\cdot 0.6201 \\cdot (1 - 0.6201)\n  \\]\n\\[\n  = 0.4712\n  \\]\n4.28 Probability neither will have 3 or more episodes in the first 2 years?\n\\[\n  (1 - 0.6201)^2 = 0.1443\n  \\]\n4.29 Expected number of siblings in a 2-sibling household who will have 3 or more episodes in the first two years.\n\\[\n  E(Y) = 2 \\cdot 0.6201 = 1.24\n  \\]"
  },
  {
    "objectID": "03_prob/05_notes.html",
    "href": "03_prob/05_notes.html",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "",
    "text": "Continous Distributions and the Normal\n\nA continuous random variable “takes on decimal values.”\nFor such random variables, the probability at any specific value is 0.\nExample:\n\nObviously, \\(\\Pr(\\text{a man is exactly } 6', 2.357921784123'') \\approx 0\\)\nBy the same logic, \\(\\Pr(\\text{a man is exactly } 6'2'') \\approx 0\\)\nMen are always a little above or a little below.\n\nBut, we know some regions are more likely than others.\n\n\\(\\Pr(5' \\leq X \\leq 7') &gt; \\Pr(0' \\leq X \\leq 1')\\)\n\nWe describe this intuition with a PDF.\n\n\n\n\n\n\n\nTipProbability Density Function\n\n\n\nA Probability Density Function (PDF) of a random variable \\(X\\) is a function \\(f\\) such that: \\[\n\\Pr(a \\leq X \\leq b) = \\text{area below curve between } a \\text{ and } b\n\\]\n\n\n\n\n\n\n\n\nTipCumulative Distribution Function\n\n\n\nThe Cumulative Distribution Function (CDF) is a function \\(F(x) = \\Pr(X \\leq x)\\).\n\n\n\nExample: \\(X =\\) Serum triglyceride level\n\n\\(\\Pr(50 \\leq X \\leq 100)\\)\n\n\n\n\n\n\n\n\n\n\\(\\Pr(X \\leq 100)\\)\n\n\n\n\n\n\n\n\n\n\nExpected value: \\(\\mu\\), average \\(X\\) over many trials. \\[\n\\mu = \\mathrm{E}[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\\] where \\(f(x)\\) = density.\nVariance: Average squared distance. \\[\n\\sigma^2 = \\mathrm{E}\\left[(X - \\mu)^2\\right] = \\mathrm{E}(X^2) - \\mu^2\n\\] \\[\n\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx\n\\]\nMost common continuous distribution: Normal distribution\n\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right]\n\\]\nDensity depends on \\(\\sigma^2\\) (variance) and \\(\\mu\\) (mean).\n\nAlso, if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(\\mathrm{E}(X) = \\mu\\), \\(\\mathrm{Var}(X) = \\sigma^2\\).\n\n\n\n\n\n\n\n\n\n\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different variances\n\n\n\n\n\n\n\n\n\nThe standard normal distribution is \\(N(0,1)\\).\nProperties:\n\n68–95–99.7 rule:\n\n68% of area within \\(\\pm 1\\sigma\\)\n95% of area within \\(\\pm 2\\sigma\\)\n99.7% of area within \\(\\pm 3\\sigma\\)\n\nSymmetric: \\(f(\\mu - x) = f(\\mu + x)\\)\n\\(\\mu =\\) median\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\)\nIf \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) are independent, then \\[\nZ = X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\n\\]\n\nWe denote PDF of standard normal by \\(\\phi(x)\\)\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is \\(\\Phi(x) = \\Pr(X \\leq x)\\)\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nset.seed(1)\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\nBlood Pressure \\(\\sim N(80, 144)\\)\nMild hypertension is \\(90 \\leq \\text{DBP} \\leq 100\\)\n\nUnits are in mmHg\n\nIndividuals are randomly sampled\nWhat is \\(\\Pr(\\text{mild hypertensive})\\)?\n\n\n\\(\\Pr(90 \\leq X \\leq 100)\\)  =   =  -  =\\(\\Pr(X \\leq 100) - \\Pr(X &lt; 90)\\) =pnorm(100, mean = 80, sd = sqrt(144))} - \\texttt{pnorm(90, mean = 80, sd = sqrt(144)) = 0.1545\n\n\n\nExerciseSolution\n\n\nTree diameter \\(\\sim N(8, 2^2)\\) (in inches). What is the probability that the tree has diameter \\(&gt; 12\\) in?\n\n\n\n1 - pnorm(q = 12, mean = 8, sd = 2)\n\n[1] 0.02275\n\n\n\n\n\n\nIf \\(X_1, \\dots, X_n\\) are random variables and\n\\[\nL = \\sum_{i=1}^n c_i X_i \\quad \\text{for } c_i \\text{ constants (not r.v.s)}\n\\] then\n\\[\n\\mathrm{E}[L] = \\sum_{i=1}^n c_i \\mathrm{E}[X_i]\n\\]\nIf the \\(X_i\\)’s are also independent, then \\[\n\\mathrm{Var}(L) = \\sum_{i=1}^n c_i^2 \\mathrm{Var}(X_i)\n\\]\nIf the \\(X_i\\) are also normally distributed, then\n\\[\nL \\sim N(\\mathrm{E}[L], \\mathrm{Var}(L))\n\\]\nExample:\nLet \\(X\\) = serum creatinine level for a Caucasian individual\nLet \\(Y\\) = serum creatinine level for a Black individual\nAssume: \\[\nX \\sim N(1.3, 0.25), \\quad Y \\sim N(1.5, 0.25)\n\\]\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\[\nZ = \\frac{1}{2}X + \\frac{1}{2}Y \\Rightarrow Z \\sim N(1.4, 0.125)\n\\]\n\n\\(\\mathrm{E}(Z) = \\frac{1}{2}(1.3 + 1.5) = 1.4\\)\n\\(\\mathrm{Var}(Z) = \\frac{1}{4}(0.25 + 0.25) = 0.125\\)\n\n\n\nNormal Approximation to Binomial\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\) and \\(np(1-p) \\geq 5\\) (rule of thumb), then \\[\n  X \\approx N(np, np(1-p))\n  \\]\n\nLet \\(X \\sim \\mathrm{Bin}(n, p)\\), and let \\(Y \\sim N(np, np(1 - p))\\).\nThen with continuity correction: \\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\n\\]\nMore generally, the continuity correction says that, if you are approximating a discrete random variable \\(X\\) with a normal distribution \\(Y\\), then\n\n\\(P(X \\geq a) \\approx P(Y &gt; a - 1/2)\\)\n\\(P(X &gt; a) \\approx P(Y &gt; a + 1/2)\\)\n\\(P(X \\leq b) \\approx P(Y &lt; b + 1/2)\\)\n\\(P(X &lt; b) \\approx P(Y &lt; b - 1/2)\\)\n\\(P(a \\leq X \\leq b) \\approx P(a - 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a &lt; X \\leq b) \\approx P(a + 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a \\leq X &lt; b) \\approx P(a - 1/2 &lt; Y &lt; b - 1/2)\\)\n\\(P(a &lt; X &lt; b) \\approx P(a + 1/2 &lt; Y &lt; b - 1/2)\\)\n\nLet’s demonstrate the continuity correction\n\np &lt;- 0.5\nn &lt;- 20\nmu &lt;- n * p\nsig &lt;- sqrt(n * p * (1 - p))\na &lt;- 8\nb &lt;- 12\n\npbinom(q = b, size = n, prob = p)\n\n[1] 0.8684\n\npnorm(q = b, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.8145\n\npnorm(q = b + 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.8682\n\npbinom(q = b, size = n, prob = p) - pbinom(q = a - 1, size = n, prob = p)\n\n[1] 0.7368\n\npnorm(q = b, mean = mu, sd = sig) - pnorm(q = a, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.6289\n\npnorm(q = b + 1/2, mean = mu, sd = sig) - pnorm(q = a - 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.7364\n\n\nWe will use the normal approximation for 2-sample binomial tests.\nWhy does the normal approximation work?\nLet \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials: \\[\nT_i =\n\\begin{cases}\n1 & \\text{w.p. } p \\\\\n0 & \\text{w.p. } 1 - p\n\\end{cases}\n\\]\nLet \\[\nX = T_1 + T_2 + \\dots + T_n = \\sum T_i\n\\]\nThe Central Limit Theorem says \\(X\\) is normal for large \\(n\\).\n\n\n\nNormal Approximation to Poisson\nIf \\(X \\sim \\mathrm{Poisson}(\\mu)\\), \\(Y \\sim N(\\mu, \\mu)\\), and \\(\\mu \\geq 10\\) (rule of thumb), then\n\\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq Y \\leq b + \\frac{1}{2}\\right)\n\\]\n\n\n\nExercise 5.12 – 5.13 of Rosner\nThe duration of cigarette smoking has been linked to many diseases, including lung cancer and various forms of heart disease. Suppose we know that among men ages 30−34 who have ever smoked, the mean number of years they smoked is 12.8 with a standard deviation of 5.1 years. For women in this age group, the mean number of years they smoked is 9.3 with a standard deviation of 3.2.\n\nExercise 5.12Solution\n\n\nAssuming that the duration of smoking is normally distributed, what proportion of men in this age group have smoked for more than 20 years?\n\n\nLet \\(X\\) be the number of years a randomly sampled smoking man has smoked. Then \\(X \\sim N(12.8, 5.1^2)\\). We want \\(P(X &gt; 20)\\) which we can get via pnorm().\n\n\n\n\n\n\n\n\n\n\n1 - pnorm(q = 20, mean = 12.8, sd = 5.1)\n\n[1] 0.07901\n\n\n\n\n\n\nExercise 5.11Solution\n\n\nAnswer Problem 5.12 for women.\n\n\nLet \\(Y\\) be the number of years a randomly sampled smoking woman has smoked. Then \\(Y \\sim N(9.3, 3.2^2)\\). We want \\(P(Y &gt; 20)\\).\n\n1 - pnorm(q = 20, mean = 9.3, sd = 3.2)\n\n[1] 0.0004133\n\n\n\n\n\n\n\nExercise 5.126 – 5.130 of Rosner\nThe Christmas Bird Count (CBC) is an annual tradition in Lexington, Massachusetts. A group of volunteers counts the number of birds of different species over a 1-day period. Each year, there are approximately 30–35 hours of observation time split among multiple volunteers. The following counts were obtained for the Northern Cardinal (or cardinal, in brief) for the period 2005–2011.\n\n\n\nYear\n\\(x_i\\)\n\n\n\n\n2005\n76\n\n\n2006\n47\n\n\n2007\n63\n\n\n2008\n53\n\n\n2009\n62\n\n\n2010\n64\n\n\n2011\n67\n\n\n\nNote that: \\[\n\\sum_{i=1}^7 x_i = 432, \\quad \\sum_{i=1}^7 x_i^2 = 27,212\n\\]\n\nExercise 5.126Solution\n\n\nWhat is the mean number of cardinal birds per year observed from 2005 to 2011?\n\n\nThe mean is \\[\n\\bar{x} = \\frac{1}{7}\\sum_{i=1}^7 x_i = \\frac{1}{7}432 = 61.71\n\\]\n\n\n\n\nExercise 5.127HintSolution\n\n\nWhat is the standard deviation (sd) of the number of cardinal birds observed?\n\n\nUse the sample version of \\(\\mathrm{Var}(X) = \\mathrm{E}(X^2) - \\mathrm{E}(X)^2\\)\n\n\n\\[\\begin{align*}\ns^2(x) &= \\frac{1}{7}\\sum_{i=1}^7 x_i^2 - \\bar{x}^2\\\\\n&= \\frac{1}{7}27,212 - 61.71^2\\\\\n&= 79.3\n\\end{align*}\\]\nThus, \\[\ns(x) = \\sqrt{s^2(x)} = \\sqrt{79.3} = 8.9\n\\]\nIf you don’t round until the very end, you get:\n\nsqrt(27212 / 7 - (432/7)^2)\n\n[1] 8.876\n\n\n\n\n\nSuppose we assume that the distribution of the number of cardinal birds observed per year is normally distributed and that the true mean and sd are the same as the sample mean and sd calculated in Problems 5.126 and 5.127.\n\nExercise 5.128Solution\n\n\nWhat is the probability of observing at least 60 cardinal birds in 2012?\nTry to apply a continuity correction to get a more accurate answer.\n\n\nLet \\(X\\) be the numer of cardinal birds observed in a given year. Then we assume that, approximately \\(X \\sim N(61.71, 8.876^2)\\). We want \\(P(X &gt;= 60)\\).\nIt’s fine if you wrote\n\n1 - pnorm(q = 60, mean = 61.71, sd = 8.876)\n\n[1] 0.5764\n\n\n\nplt_norm(mu = 61.71, sig = 8.876, lb = 61.71)\n\n\n\n\n\n\n\n\nApplying the continuity correction strategy, a more accurate probability would be to subtract 1/2 from 60 first.\n\n1 - pnorm(q = 60 - 0.5, mean = 61.71, sd = 8.876)\n\n[1] 0.5983\n\n\n\n\n\nThe observers wish to identify a normal range for the number of cardinal birds observed per year. The normal range will be defined as the interval (L, U), where L is the largest integer \\(\\leq\\) 15th percentile and U is the smallest integer \\(\\geq\\) 85th percentile .\n\nExercise 5.129HintSolution\n\n\nIf we make the same assumptions as in Problem 5.128, then what is L? What is U?\n\n\nUse qnorm()\n\n\nL is 52:\n\nqnorm(p = 0.15, mean = 61.71, sd = 8.876)\n\n[1] 52.51\n\n\nU is 71.\n\nqnorm(p = 0.85, mean = 61.71, sd = 8.876)\n\n[1] 70.91\n\n\n\n\n\n\nExercise 5.130HintSolution\n\n\nWhat is the probability that the number of cardinal birds will be \\(\\geq\\) U at least once on Christmas day during the 10-year period 2012–2021? Make the same assumptions as in Problem 5.128.\n\n\nUse the binomial distribution with success probability determined by \\(P(X \\geq 71)\\), where \\(X\\) is the number of birds in a year.\n\n\nIf \\(X\\) is the number of birds in a year, then \\(X \\sim N(61.71, 8.876^2)\\). Since U = 71, \\(P(X \\geq U)\\) is\n\n1 - pnorm(q = 70.5, mean = 61.71, sd = 8.876)\n\n[1] 0.161\n\n\nLet \\(Y\\) be the number of years, out of 10, that have at least U birds. Then \\(Y \\sim \\mathrm{Binom}(10, 0.161)\\). We want \\(P(Y \\geq 1)\\).\n\n1 - pbinom(q = 0, size = 10, prob = 0.161)\n\n[1] 0.8272"
  },
  {
    "objectID": "03_prob/05_notes_old.html",
    "href": "03_prob/05_notes_old.html",
    "title": "Chapter 5 Notes: Continuous Distributions",
    "section": "",
    "text": "A continuous random variable “takes on decimal values.”\nFor such random variables, the probability at any specific value is \\(0\\).\nExample:\n\n\\(P(\\text{a man is exactly 6'2.35792471613\"}) \\approx 0\\)?\n\\(P(\\text{a man is exactly 6'2\"}) \\approx 0\\)\nMen are a little above or a little below.\n\nHowever, we know some regions are more likely than others.\n\n\\(P(5' \\leq X \\leq 7') &gt; P(0' \\leq X \\leq 1')\\)\n\nWe describe this intuition with a PDF (Probability Density Function).\nA Probability Density Function of a random variable \\(X\\) is a function, \\(f\\), where:\n\n\\(P(a \\leq X \\leq b) = \\text{area below curve between } a \\text{ and } b\\)\n\nThe CDF (Cumulative Distribution Function) is again the \\(F(x) = P(X \\leq x)\\).\nExample: Let \\(X = \\text{Serum triglyceride level}\\)\n::: {.cell} ::: {.cell-output-display}  ::: :::\n::: {.cell} ::: {.cell-output-display}  ::: :::\nExpected value, \\(\\mu\\), the average \\(X\\) over many trials:\n\n\\(\\mu = \\int_{-\\infty}^{\\infty} x f(x) \\, dx \\quad \\text{where } f(x) = \\text{density}\\)\n\nVariance: Average squared distance\n\n\\(\\sigma^2 = E\\left((X - \\mu)^2\\right) = E(X^2) - \\mu^2\\)\n\\(\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx\\)\n\nMost common continuous distribution: Normal distribution\n\n\\(f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2} \\frac{(x - \\mu)^2}{\\sigma^2}\\right) \\quad \\text{if } X \\sim N(\\mu, \\sigma^2)\\)\nFunction of \\(\\sigma^2\\) (variance) at \\(\\mu\\) (mean)\nAlso, if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(E(X) = \\mu\\), \\(Var(X) = \\sigma^2\\)\n\nNormal distribution curve:\n\nA typical bell-shaped curve centered at \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n\nThe standard normal distribution is \\(N(0, 1)\\).\nProperties:\n\n68-95-99.7 rule:\n\n68% of area within \\(\\pm 1\\sigma\\)\n95% of area within \\(\\pm 2\\sigma\\)\n99.7% of area within \\(\\pm 3\\sigma\\)\n\nSymmetric: \\(f(\\mu - x) = f(\\mu + x)\\)\nMean = median = \\(\\mu\\)\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma} \\Rightarrow Z \\sim N(0, 1)\\)\nIf \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\), then \\(Z = X + Y\\) is also normally distributed:\n\n\\(Z \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\)\n\n\nWe denote the PDF of the standard normal by \\(\\phi(x)\\).\nThe CDF is \\(\\Phi(x) = P(X \\leq x)\\).\nExample: Blood Pressure is \\(N(80, 144)\\)\n\nMild hypertension is \\(90 \\leq DBP \\leq 100\\)\nDuring a random day, what is \\(P(\\text{mild hypertensive})\\)?\nSolution:\n\n\\(P(90 \\leq X \\leq 100)\\)\n\\(= P(X \\leq 100) - P(X \\leq 90)\\)\n\\(= \\text{pnorm}(100, \\text{mean} = 80, \\text{sd} = \\sqrt{144}) - \\text{pnorm}(90, \\text{mean} = 80, \\text{sd} = \\sqrt{144})\\)\n\\(\\approx 0.1545\\)\nXYZ Finish this figure:\n\n\n\n\n\n\n\n\n\n\n\nExample: Tree diameter \\(\\sim N(8, 2^2)\\) (in inches)\n\nWhat is the probability that a tree has a diameter &gt; 12 inches?\nSolution: \\(1 - \\text{pnorm}(12, 8, 2) \\approx 0.02275\\)\n\nIf \\(X_1, \\dots, X_n\\) are random variables and \\[ L = \\sum_{i=1}^n c_i X_i \\] for \\(c_i\\) constants (not random variables), then\n\n\\(E(L) = \\sum_{i=1}^n c_i E(X_i)\\)\n\\(\\text{Var}(L) = \\sum_{i=1}^n c_i^2 \\text{Var}(X_i)\\)\n\nIf the \\(X_i\\) are also normally distributed, then \\(L \\sim N(E(L), \\text{Var}(L))\\)\nExample:\n\n\\(X =\\) serum creatinine level for Caucasian individual\n\\(Y =\\) serum creatinine level for Black individual\n\\(X \\sim N(1.3, 0.25)\\)\n\\(Y \\sim N(1.5, 0.25)\\)\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\(Z = \\frac{1}{2} X + \\frac{1}{2} Y\\)\nThen \\(Z \\sim N(1.4, 0.125)\\)\nCalculations:\n\n\\(E(Z) = \\frac{1}{2}(1.3) + \\frac{1}{2}(1.5) = 1.4\\)\n\\(\\text{Var}(Z) = \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) = 0.125\\)\n\n\nNormal Approximation to Binomial (rule of thumb):\n\nIf \\(X \\sim \\text{Bin}(n, p)\\) and \\(np(1 - p) \\geq 5\\), then \\(X \\approx N(np, np(1 - p))\\)\n\nLet \\(X \\sim \\text{Bin}(n, p)\\), \\(Y \\sim N(np, np(1 - p))\\).\n\nThen \\(P(a \\leq X \\leq b) \\approx P\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\\) (continuity correction).\n\nWe will use this for 2-sample binomial tests.\nWhy? Let \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials.\n\n\\(T_i = \\begin{cases} 1 & \\text{with probability } p \\\\ 0 & \\text{with probability } 1 - p \\end{cases}\\)\n\\(X = T_1 + T_2 + \\dots + T_n = \\sum_{i=1}^n T_i\\)\n\nCentral Limit Theorem says normal for large \\(n\\).\nNormal Approximation to Poisson:\n\nIf \\(X \\sim \\text{Pois}(\\mu)\\), then \\(Y \\sim N(\\mu, \\mu)\\).\nRule of thumb: For \\(\\mu \\geq 10\\), \\(P(a \\leq X \\leq b) \\approx P(a - 1 \\leq Y \\leq b + 1)\\)\n\nExercise 5.12 – 5.13 of Rosner:\n\nOf men 30-34 who have smoked:\n\n$X = $ # years a man has smoked\n$Y = $ # of years smoked by women in age group\n\\(X \\sim N(12.8, 5.1^2)\\)\n\\(Y \\sim N(9.3, 3.2^2)\\)\n\nQ1: What proportion of men have smoked for more than 20 years? Women?\n\nMen: \\(1 - \\text{pnorm}(20, \\text{mean} = 12.8, \\text{sd} = 5.1) \\approx 0.0794\\)\n\n\n\n\n\n\n\n\n\n\n\nWomen: \\(1 - \\text{pnorm}(20, \\text{mean} = 9.3, \\text{sd} = 3.2) \\approx 0.0041\\)\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.12 – 5.13:\n\nThe Christmas Bird Count is a holiday tradition in a boring part of Massachusetts.\nData:\n\nYear and Number of Birds (\\(x_i\\)):\n\n2006: 76\n2007: 47\n2008: 63\n2009: 53\n2010: 62\n2011: 69\n2012: 62\n\n\\(\\sum x_i = 432\\)\n\\(\\sum x_i^2 = 27,717\\)\n\nQuestions:\n\nWhat is the mean number of birds?\n\n\\[ \\bar{x} = \\frac{\\sum x_i}{n} = \\frac{432}{7} \\approx 61.71 \\]\n\nWhat is the standard deviation?\n\nCalculate the variance: \\[ \\text{Variance} = \\frac{\\sum x_i^2}{n} - \\left(\\frac{\\sum x_i}{n}\\right)^2 \\] \\[ = \\frac{27,717}{7} - \\left(\\frac{432}{7}\\right)^2 \\] \\[ \\approx 78.78 \\]\nStandard deviation: \\[ \\text{SD} = \\sqrt{\\text{Variance}} = \\sqrt{78.78} \\approx 8.876 \\]\n\nSuppose the number of birds is normally distributed with the same mean and SD as parts 1 and 2. What is the probability of at least 60 birds? Apply the continuity correction.\n\n\\[ 1 - \\text{pnorm}(59.5, \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx 0.5983 \\]\n\nFind the “normal range” \\((L, U)\\) (integers) such that:\n\n\\(L\\) = 15th percentile\n\\(U\\) = 85th percentile\n\\[ \\text{qnorm}(c(0.15, 0.85), \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx (52.51, 70.91) \\]\nThus, 52 to 70 is the “normal range.”\n\nWhat is the probability of having a count \\(\\geq U\\) at least once during a 10-year period?\n\n\\[ P(X \\geq U) \\approx 1 - \\text{pnorm}(79.5, \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx 0.02252 \\]\nLet $Y = $ # of years \\(\\geq U \\sim \\text{Bin}(10, 0.02252)\\).\n\\[ P(Y \\geq 1) = 1 - P(Y = 0) = 1 - \\text{dbinom}(0, 10, 0.02252) \\]\n\\[ \\approx 0.2037 \\]"
  },
  {
    "objectID": "04_est/06_notes.html",
    "href": "04_est/06_notes.html",
    "title": "Chapter 6 Notes: Estimation",
    "section": "",
    "text": "stateDiagram-v2\n    state \"Data Generating Process\" as Process\n    Process --&gt; Data: Probability\n    Data --&gt; Process: Inference\n\n\n\n\n\n\n\n\nProbability (Chapters 3, 4, and 5):\n\nWe know that \\(X \\sim N(80, 12)\\)\n\nWhat is \\(\\Pr(X &gt; 90)\\)?\n\nInference (Chapters 6 through 14):\n\nWe observe \\(X_i = 81, 78, 77, 89, \\ldots\\)\n\nWe assume \\(X_i \\sim N(\\mu, \\sigma^2)\\)\n\nBut the values of \\(\\mu\\) and \\(\\sigma^2\\) are unknown.\n\nWhat are \\(\\mu\\) and \\(\\sigma^2\\)?\n\nEstimation: Guess parameter values from data\n\nPoint estimation: A single number is your best guess\n\nE.g., estimate \\(\\mu\\) with \\(\\bar{X}\\)\n\nInterval estimation: Get a range of likely values of a parameter\n\nE.g., confidence intervals\n\n\nHypothesis testing: How sure are we a parameter is different from some value?\n\nE.g., \\(H_0: \\mu = 0\\)\n\n\n \n\n\n\n\n\n\nTipReference / Target / Study Population\n\n\n\nGroup we are interested in\n\n\n\n\n\n\n\n\nTipSample\n\n\n\nGroup we have data about\n\n\n\n\n\n\n\n\nTipParameter\n\n\n\nNumeric summary of population\nE.g.: \\(\\mu\\), \\(\\sigma^2\\), \\(p\\)\n\n\n\n\n\n\n\n\nTipStatistic\n\n\n\nNumeric summary of sample\nE.g. \\(\\bar{X}\\), \\(s^2\\), \\(\\hat{p}\\)\n\n\n\nSimplest way to get a sample is by a simple random sample\n\nEach unit has an equal chance of being in sample\n\nRandom selection (e.g., via SRS) is distinct from Random Assignment.\nRandom assignment: randomly assign units to different groups (e.g., treatment vs. control)\nRandom selection: results generalizable to target population\n\nBecause sample is similar to population in terms of demographic variables\n\nRandom assignment: allows for claims of causality\n\nBecause all possible confounders are equal (on average) in the groups\n\nRandomized Clinical Trial (RCT): Random assignment of treatments to compare them.\nNo causal claims without random assignment\nExample: tobramycin and gentamicin are antibiotics.\n\nTobramycin is more aggressive and has more side effects.\n\nEarly studies were not randomized and showed tobramycin performed worse. Why?\n\nDoctors gave sicker patients tobramycin\n\nRandomization guarantees equal number of sicker and less sick in each group (on average)\n\nDouble blind: neither doctor nor patient know treatment\n\nGuards against placebo effect\n\nSingle blind: doctor knows\nUnblinded: both know\n\n\nSampling in R"
  },
  {
    "objectID": "02_descriptive/02_notes.html#measures-of-spread",
    "href": "02_descriptive/02_notes.html#measures-of-spread",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "Measures of Spread",
    "text": "Measures of Spread\n\nSpread: How far apart numbers are.\nRange: \\(\\text{Max} - \\text{Min}\\) (sensitive to extreme values).\nInter-quartile Range (IQR): \\(75^{\\text{th}}\\) percentile - \\(25^{\\text{th}}\\) percentile.\n\\(p^{\\text{th}}\\) percentile = value \\(V_p\\) such that \\(p\\%\\) of points are at or below \\(V_p\\).\n\nExample: Median = \\(50^{\\text{th}}\\) percentile.\n\nQuantile: in units of proportions instead of percents.\n\n\\(0.75\\) quantile = \\(75^{\\text{th}}\\) percentile.\n\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\)\n\n\\(\\frac{1}{3}\\) quantile = \\(-4\\)\n\\(\\frac{2}{3}\\) quantile = \\(2\\)\n\\(1\\) quantile = \\(5\\)\n\nWhat about the \\(40^{\\text{th}}\\) percentile?\n\nUse some average of \\(-4\\) and \\(2\\), but definition varies.\nThe quantile() function in R has 9 different definitions of how this imputation works. See ?quantile.\n\nVariance: Average of squared deviations.\n\\[\n  s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\n  \\]\n\n\\(n-1\\) because we lose some information by estimating \\(\\bar{X}\\).\n\nStandard Deviation: Square root of variance.\n\\[\n  s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2}\n  \\]\n\nPuts this measure of spread on the same scale as units (e.g., \\(oz\\) instead of \\(oz^2\\)).\n\nLet \\(y_i = c_1 x_i + c_2\\), then \\(s^2(y) = c_1^2 s^2(x)\\) and \\(s(y) = c_1 s(x)\\)\n\nOnly scaling affects variance and standard deviation.\n\nWhy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: What is \\(s^2(y)\\) when \\(y_i = c_1 x_i\\)?\nR Notebook on Mean / Median / SD / Variance"
  },
  {
    "objectID": "02_descriptive/02_notes.html#properties-of-barx",
    "href": "02_descriptive/02_notes.html#properties-of-barx",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Suppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\n\n\nExerciseSolution\n\n\nWhat is the mean menstrual cycle deviation from 4 weeks?\n\n\n\\[\n3.92 - 4 = -0.08\n\\]"
  },
  {
    "objectID": "03_prob/03_notes.html#exercises-3.53---3.63-of-rosner",
    "href": "03_prob/03_notes.html#exercises-3.53---3.63-of-rosner",
    "title": "Chapter 3 Notes: Probability",
    "section": "Exercises 3.53 - 3.63 of Rosner",
    "text": "Exercises 3.53 - 3.63 of Rosner\n\nWord problemConvert to math notation\n\n\nThe familial aggregation of respiratory disease is a well-established clinical phenomenon. However, whether this aggregation is due to genetic or environmental factors or both is somewhat controversial. An investigator wishes to study a particular environmental factor, namely the relationship of cigarette-smoking habits in the parents to the presence or absence of asthma in their oldest child age 5 to 9 years living in the household (referred to below as their offspring). Suppose the investigator finds that (1) if both the mother and father are current smokers, then the probability of their offspring having asthma is .15; (2) if the mother is a current smoker and the father is not, then the probability of their offspring having asthma is .13; (3) if the father is a current smoker and the mother is not, then the probability of their offspring having asthma is .05; and (4) if neither parent is a current smoker, then the probability of their offspring having asthma is .04.\n\n\n\nDefine:\n\n\\(M^+ =\\) Mother smokes\n\\(F^+ =\\) Father smokes\n\\(O^+ =\\) Offspring has asthma\n\nGiven probabilities:\n\n\n\nScenario\n\\(P(O^+ | \\text{Scenario})\\)\n\n\n\n\n\\(M^+ \\cap F^+\\)\n0.15\n\n\n\\(M^+ \\cap F^-\\)\n0.13\n\n\n\\(M^- \\cap F^+\\)\n0.05\n\n\n\\(M^- \\cap F^-\\)\n0.04\n\n\n\n\n\n\n\n\nExercise 1.Solution\n\n\nSuppose the smoking habits of the parents are independent and the probability that the mother is a current smoker is .4, whereas the probability that the father is a current smoker is .5. What is the probability that both the father and mother are current smokers?\n\n\nWe want to calculate \\(P(M^+ \\cap F^+)\\) given: \\(P(M^+) = 0.4\\), \\(P(F^+) = 0.5\\), \\(M^+ \\perp F^+\\) \\[\nP(M^+ \\cap F^+) = P(M^+) \\cdot P(F^+) = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 2.Solution\n\n\nConsider the subgroup of families in which the mother is not a current smoker. What is the probability that the father is a current smoker among such families? How does this probability differ from that calculated in Problem 1?\n\n\nWe want to calculate \\(P(F^+ | M^-)\\). However, since \\(M^+\\) and \\(F^+\\) are independent, \\(P(F^+ | M^-) = P(F^+) = 0.5\\).\n\n\n\n\nExercise 3.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 4.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 5.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 6.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 7.Solution\n\n\n\n\n\n\n\n\n\n\nGiven:\n\n\\(P(M^+ | F^+) = 0.6\\)\n\\(P(M^+ | F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\nCalculate \\(P(F^+ \\cap M^-)\\):\n\n\n\nIs \\(F^+ \\perp M^+\\)?\n\n\nFind \\(P(O^+)\\):\n\n\nCalculate \\(P(F^+ | O^+)\\):\n\n\nCalculate \\(P(M^+ | O^+)\\):"
  },
  {
    "objectID": "03_prob/03_notes.html#exercises-3.53---3.59-of-rosner",
    "href": "03_prob/03_notes.html#exercises-3.53---3.59-of-rosner",
    "title": "Chapter 3 Notes: Probability",
    "section": "Exercises 3.53 - 3.59 of Rosner",
    "text": "Exercises 3.53 - 3.59 of Rosner\n\nExercise 0Solution\n\n\nThe familial aggregation of respiratory disease is a well-established clinical phenomenon. However, whether this aggregation is due to genetic or environmental factors or both is somewhat controversial. An investigator wishes to study a particular environmental factor, namely the relationship of cigarette-smoking habits in the parents to the presence or absence of asthma in their oldest child age 5 to 9 years living in the household (referred to below as their offspring). Suppose the investigator finds that (1) if both the mother and father are current smokers, then the probability of their offspring having asthma is 0.15; (2) if the mother is a current smoker and the father is not, then the probability of their offspring having asthma is 0.13; (3) if the father is a current smoker and the mother is not, then the probability of their offspring having asthma is 0.05; and (4) if neither parent is a current smoker, then the probability of their offspring having asthma is 0.04.\nConvert this word problem to mathematical notation amenable to analysis.\n\n\n\nDefine:\n\n\\(M^+ =\\) Mother smokes\n\\(F^+ =\\) Father smokes\n\\(O^+ =\\) Offspring has asthma\n\nGiven probabilities:\n\n\n\nScenario\n\\(P(O^+ | \\text{Scenario})\\)\n\n\n\n\n\\(M^+ \\cap F^+\\)\n0.15\n\n\n\\(M^+ \\cap F^-\\)\n0.13\n\n\n\\(M^- \\cap F^+\\)\n0.05\n\n\n\\(M^- \\cap F^-\\)\n0.04\n\n\n\n\n\n\n\n\nExercise 1HintSolution\n\n\nSuppose the smoking habits of the parents are independent and the probability that the mother is a current smoker is 0.4, whereas the probability that the father is a current smoker is 0.5. What is the probability that both the father and mother are current smokers?\n\n\nUse the “and” rule.\n\n\nWe want to calculate \\(P(M^+ \\cap F^+)\\) given: \\(P(M^+) = 0.4\\), \\(P(F^+) = 0.5\\), \\(M^+ \\perp F^+\\) \\[\nP(M^+ \\cap F^+) = P(M^+) \\cdot P(F^+) = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 2HintSolution\n\n\nConsider the subgroup of families in which the mother is not a current smoker. What is the probability that the father is a current smoker among such families? How does this probability differ from that calculated in Problem 1?\n\n\nUse independence.\n\n\nWe want to calculate \\(P(F^+ | M^-)\\). However, since \\(M^+\\) and \\(F^+\\) are independent, \\(P(F^+ | M^-) = P(F^+) = 0.5\\). This is just the probability that the father is a smoker, as opposed to part 1 where we calculated the probability that both father and mother are smokers.\n\n\n\nSuppose, alternatively, that if the father is a current smoker, then the probability that the mother is a current smoker is 0.6; whereas if the father is not a current smoker, then the probability that the mother is a current smoker is 0.2. Also assume that statements 1, 2, 3, and 4 above hold.\n\nExercise 3HintSolution\n\n\nIf the probability that the father is a current smoker is 0.5, what is the probability that the father is a current smoker and that the mother is not a current smoker?\n\n\nUse \\(P(F^+ \\cap M^-) = P(M^- | F^+) \\cdot P(F^+)\\).\n\n\nConverting the word problem to math notation, we have::\n\n\\(P(M^+ | F^+) = 0.6\\)\n\\(P(M^+ | F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\nOur goal is to calculate \\(P(F^+ \\cap M^-)\\).\nBy the complement rule, we have \\(P(M^- | F^+) = 1 - 0.6\\). Using this, we have:\n\n\\[\nP(F^+ \\cap M^-) = P(M^- | F^+) \\cdot P(F^+) = (1 - 0.6) \\cdot 0.5 = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 4HintSolution\n\n\nAre the current smoking habits of the father and the mother independent? Why or why not?\n\n\nWhat is \\(P(M^+|F^+)\\)? What is \\(P(M^+|F^-)\\)?\n\n\nThere are a couple ways to check this:\n\nSee if \\(P(M^+|F^+) = P(M^+|F^-) = P(M^+)\\) (value of \\(F\\) does not matter). Any inequality here indicates non-independence.\nSee if \\(P(M^+ \\cap F^+) = P(M^+)P(F^+)\\), or any other combination, such as \\(P(M^- \\cap F^+) = P(M^-)P(F^+)\\). An inequality here indicates non-independence.\n\nThe easiest way is just the first. We know that \\(P(M^+|F^+) =  0.6 \\neq 0.2 = P(M^+|F^-)\\). So no, they are not independent.\n\n\n\n\nExercise 5HintSolution\n\n\nUnder the assumptions made in Problems 3 and 4, find the unconditional probability that the offspring will have asthma.\n\n\nUse the law of total probability. You’ll be using \\(P(O^+ | M^+ \\cap F^+)\\), \\(P(O^+ | M^+ \\cap F^-)\\), \\(P(O^+ | M^- \\cap F^+)\\), \\(P(O^+ | M^- \\cap F^-)\\), \\(P(M^+ \\cap F^+)\\), \\(P(M^+ \\cap F^-)\\), \\(P(M^- \\cap F^+)\\), and \\(P(M^- \\cap F^-)\\).\n\n\nWe want to calculate \\(P(O^+)\\):\nUsing the law of total probability: \\[\\begin{align*}\nP(O^+) &= P(O^+ | M^+ \\cap F^+) P(M^+ \\cap F^+) + P(O^+ | M^+ \\cap F^-) P(M^+ \\cap F^-)\\\\\n&+ P(O^+ | M^- \\cap F^+) P(M^- \\cap F^+) + P(O^+ | M^- \\cap F^-) P(M^- \\cap F^-)\n\\end{align*}\\]\nWe were given:\n\n\\(P(O^+ | M^+ \\cap F^+) = 0.15\\),\n\\(P(O^+ | M^+ \\cap F^-) = 0.13\\),\n\\(P(O^+ | M^- \\cap F^+) = 0.05\\), and\n\\(P(O^+ | M^- \\cap F^-) = 0.04\\),\n\\(P(M^+|F^+) = 0.6\\)\n\\(P(M^+|F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\n\nWe have\n\n\\(P(M^+ \\cap F^+) = P(M^+|F^+)P(F^+) = 0.6 * 0.5 = 0.3\\)\n\\(P(M^+ \\cap F^-) = P(M^+|F^-)P(F^+) = 0.2 * 0.5 = 0.1\\)\n\\(P(M^- \\cap F^+) = P(M^-|F^+)P(F^+) = (1 - 0.6) * 0.5 = 0.2\\)\n\\(P(M^- \\cap F^-) = P(M^-|F^-)P(F^+) = (1 - 0.2) * 0.5 = 0.4\\)\n\nSubstituting the values: \\[\nP(O^+) = (0.15 \\cdot 0.3) + (0.13 \\cdot 0.1) + (0.05 \\cdot 0.2) + (0.04 \\cdot 0.4) = 0.084\n\\]\n\n\n\n\nExercise 6HintSolution\n\n\nSuppose a child has asthma. What is the posterior probability that the father is a current smoker?\n\n\nUse Bayes rule.\nAlso note that \\(P(O^+ \\cap F^+) = P(O^+ \\cap F^+ \\cap M^+) + P(O^+ \\cap F^+ \\cap M^-)\\).\n\n\nWe want to calculate \\(P(F^+ | O^+)\\). Using Bayes rule we have\n\\[\nP(F^+ | O^+) = \\frac{P(F^+ \\cap O^+)}{P(O^+)}\n\\] We already calculated \\(P(O^+) = 0.084\\) from Exercise 5. The hard part is \\(P(F^+ \\cap O^+)\\). But we can use the law of total probability to get that. \\[\\begin{align*}\nP(F^+ \\cap O^+) &= P(F^+ \\cap O^+ \\cap M^+) + P(F^+ \\cap O^+ \\cap M^-)\\\\\n&= P(O^+ | F^+ \\cap M^+)P(F^+ \\cap M^+) + P(O^+ | F^+ \\cap M^-)P(F^+ \\cap M^-)\\\\\n&= 0.15 \\cdot 0.3 + 0.05 \\cdot 0.2\\\\\n&= 0.055,\n\\end{align*}\\] where we used \\(P(F^+ \\cap M^+) = 0.3\\) and \\(P(F^+ \\cap M^-) = 0.2\\) from Exercise 5.\nSubstituting values we have \\[\nP(F^+ | O^+) = \\frac{0.055}{0.084} = 0.6548.\n\\]\n\n\n\n\nExercise 7HintSolution\n\n\nWhat is the posterior probability that the mother is a current smoker if the child has asthma?\n\n\nUse the same strategy as in Exercise 6.\n\n\nWe want \\(P(M^+ | O^+)\\). Using Bayes rule we have \\[\nP(M^+ | O^+) = \\frac{P(O^+ \\cap M^+)}{P(O^+)}\n\\] We can use the exact same strategy as in Exercise 6.\n\\[\\begin{align*}\nP(M^+ \\cap O^+) &= P(F^+ \\cap M^+ \\cap O^+) + P(F^- \\cap M^+ \\cap O^+) \\text{ (law of total probability)}\\\\\n&= P(O^+ | F^+ \\cap M^+) P(F^+ \\cap M^+) + P(O^+ | F^- \\cap M^+) P(F^- \\cap M^+) \\text{ (conditional probability)}\\\\\n&= 0.15 \\cdot 0.3 + 0.13 \\cdot 0.1 \\text{ (given/calculated previously)}\\\\\n&= 0.058\n\\end{align*}\\]\nSubstituting in values we have \\[\nP(M^+ | O^+) = \\frac{0.058}{0.084} = 0.6905.\n\\]"
  },
  {
    "objectID": "03_prob/04_notes.html#provided-distribution-in-r",
    "href": "03_prob/04_notes.html#provided-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "If given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/04_notes.html#binomial-distribution-in-r",
    "href": "03_prob/04_notes.html#binomial-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Binomial Distribution in R",
    "text": "Binomial Distribution in R\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\nThe probability of seeing 4 or fewer neutrophiles out of 10 white blood cells is\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nThe underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\n\n1 - pbinom(q = 74, size = 1500, prob = 0.05)\n\n[1] 0.5165\n\n\nor\n\npbinom(q = 74, size = 1500, prob = 0.05, lower.tail = FALSE)\n\n[1] 0.5165\n\n\n\n\n\n\nExerciseSolution\n\n\nSuppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\nWhat is the expected number of such women (out of 100) that we would expect to die in the next year?\n\n\n\n1 - stats::pbinom(q = 4, size = 100, prob = 0.009)\n\n[1] 0.002191\n\n\nYes, very unusual.\n100 * 0.009 = 0.9. So about 1 woman out of 100."
  },
  {
    "objectID": "03_prob/04_notes.html#poisson-distribution-in-r",
    "href": "03_prob/04_notes.html#poisson-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Poisson Distribution in R",
    "text": "Poisson Distribution in R\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts.\nIt is also useful if you want to add two Binomial counts together. If \\(X \\sim \\mathrm{Binom}(n_1, p_1)\\) and \\(Y \\sim \\mathrm{Binom}(n_2, p_2)\\), suppose we want to calculated the distribution of \\(Z = X + Y\\).\n\nTo do this exactly, we need to use a discrete linear convolution, which is annoying.\n\n(unless \\(p_1 = p_2\\))\n\nBut if we can approximate \\(X \\sim \\mathrm{Poi}(n_1p_1)\\) and \\(Y \\sim \\mathrm{Poi}(n_2p_2)\\), then \\(Z \\sim \\mathrm{Poi}(n_1p_1 + n_1p_2)\\), which is super nice.\nE.g. Population 1 has 100 individuals and each individual has a 0.002 probability of getting a disease. Population 2 has 200 individuals and each individual has a 0.004 probability of getting the disease. What is the distribution of the total number of folks who get the disease?\n\nIt’s about Poisson(0.2 + 0.8) = Poisson(1)."
  },
  {
    "objectID": "03_prob/04_notes.html#provided-distributions-in-r",
    "href": "03_prob/04_notes.html#provided-distributions-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "If given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\npmf\n\n# A tibble: 5 × 2\n      r    pr\n  &lt;int&gt; &lt;dbl&gt;\n1     0 0.008\n2     1 0.076\n3     2 0.265\n4     3 0.411\n5     4 0.24 \n\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/05_notes.html#solution-3",
    "href": "03_prob/05_notes.html#solution-3",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "Solution",
    "text": "Solution\nThe mean is \\[\n\\bar{x} = \\frac{1}{7}\\sum_{i=1}^7 x_i = \\frac{1}{7}432 = 61.71\n\\] :::\n\nExercise 5.127HintSolution\n\n\nWhat is the standard deviation (sd) of the number of cardinal birds observed?\n\n\nUse the sample version of \\(\\mathrm{Var}(X) = \\mathrm{E}(X^2) - \\mathrm{E}(X)^2\\)\n\n\n\\[\\begin{align*}\ns^2(x) &= \\frac{1}{7}\\sum_{i=1}^7 x_i^2 - \\bar{x}^2\\\\\n&= \\frac{1}{7}27,212 - 61.71^2\\\\\n&= 79.3\n\\end{align*}\\]\nThus, \\[\ns(x) = \\sqrt{s^2(x)} = \\sqrt{79.3} = 8.9\n\\]\nIf you don’t round until the very end, you get:\n\nsqrt(27212 / 7 - (432/7)^2)\n\n[1] 8.876\n\n\n\n\n\nSuppose we assume that the distribution of the number of cardinal birds observed per year is normally distributed and that the true mean and sd are the same as the sample mean and sd calculated in Problems 5.126 and 5.127.\n\nExercise 5.128Solution\n\n\nWhat is the probability of observing at least 60 cardinal birds in 2012? (Hint: Apply a continuity correction where appropriate.)\n\n\n\n\n\n\nThe observers wish to identify a normal range for the number of cardinal birds observed per year. The normal range will be defined as the interval (L, U), where L is the largest integer ≤ 15th percentile and U is the smallest integer \\(\\geq\\) 85th percentile .\n\nExercise 5.129Solution\n\n\nIf we make the same assumptions as in Problem 5.128, then what is L? What is U?\n\n\n\n\n\n\n\nExercise 5.130Solution\n\n\nWhat is the probability that the number of cardinal birds will be \\(\\geq\\) U at least once on Christmas day during the 10-year period 2012–2021? (Hint: Make the same assumptions as in Problem 5.128.)\n\n\n\n\n\n\n\nWhat is the mean number of birds?\n\\[\n\\bar{x} = \\frac{432}{7} = 61.71\n\\]\nWhat is the standard deviation?\n\\[\n\\frac{1}{7}(27717) - \\left(\\frac{432}{7}\\right)^2 = 78.78\n\\]\n\\[\n\\mathrm{SD} = \\sqrt{78.78} = 8.876\n\\]\nSuppose number of birds is normal with same mean and SD as previous years.\nWhat is the probability of at least 60 birds? Apply continuity correction.\n\\[\n1 - \\texttt{pnorm}(59.5, \\text{mean} = 61.71, \\text{sd} = 8.876) = \\boxed{0.5983}\n\\]\nFind “normal range” \\((L, U)\\) (integers) such that:\n\n\\(L\\) = 5th percentile\n\n\\(U\\) = 95th percentile\n\n\\[\n\\texttt{qnorm}(c(0.05, 0.95), \\text{mean} = 61.71, \\text{sd} = 8.876) = (52.51, 70.91)\n\\]\n\nSo the normal range is 52 to 80\n\nWhat is the probability that \\(X \\geq U\\) at least once during a 10-year period?\n\\[\n\\Pr(X \\geq U) = 1 - \\texttt{pnorm}(79.5, \\text{mean} = 61.71, \\text{sd} = 8.876) = 0.02252\n\\]\nLet \\(Y\\) = number of years \\(\\geq U\\)\n\\(Y \\sim \\mathrm{Bin}(10, 0.02252)\\)\n\\[\n\\Pr(Y \\geq 1) = 1 - \\Pr(Y = 0) = 1 - \\texttt{dbinom}(0, 10, 0.02252) = \\boxed{0.2037}\n\\]"
  },
  {
    "objectID": "03_prob/05_notes.html#normal-distribution-in-r",
    "href": "03_prob/05_notes.html#normal-distribution-in-r",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "",
    "text": "Density Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nset.seed(1)\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nTree diameter \\(\\sim N(8, 2^2)\\) (in inches). What is the probability that the tree has diameter \\(&gt; 12\\) in?\n\n\n\n1 - pnorm(q = 12, mean = 8, sd = 2)\n\n[1] 0.02275\n\n\n\n\n\n\nIf \\(X_1, \\dots, X_n\\) are random variables and\n\\[\nL = \\sum_{i=1}^n c_i X_i \\quad \\text{for } c_i \\text{ constants (not r.v.s)}\n\\] then\n\\[\n\\mathrm{E}[L] = \\sum_{i=1}^n c_i \\mathrm{E}[X_i]\n\\]\nIf the \\(X_i\\)’s are also independent, then \\[\n\\mathrm{Var}(L) = \\sum_{i=1}^n c_i^2 \\mathrm{Var}(X_i)\n\\]\nIf the \\(X_i\\) are also normally distributed, then\n\\[\nL \\sim N(\\mathrm{E}[L], \\mathrm{Var}(L))\n\\]\nExample:\nLet \\(X\\) = serum creatinine level for a Caucasian individual\nLet \\(Y\\) = serum creatinine level for a Black individual\nAssume: \\[\nX \\sim N(1.3, 0.25), \\quad Y \\sim N(1.5, 0.25)\n\\]\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\[\nZ = \\frac{1}{2}X + \\frac{1}{2}Y \\Rightarrow Z \\sim N(1.4, 0.125)\n\\]\n\n\\(\\mathrm{E}(Z) = \\frac{1}{2}(1.3 + 1.5) = 1.4\\)\n\\(\\mathrm{Var}(Z) = \\frac{1}{4}(0.25 + 0.25) = 0.125\\)\n\n\n\n\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\) and \\(np(1-p) \\geq 5\\) (rule of thumb), then \\[\n  X \\approx N(np, np(1-p))\n  \\]\n\nLet \\(X \\sim \\mathrm{Bin}(n, p)\\), and let \\(Y \\sim N(np, np(1 - p))\\).\nThen with continuity correction: \\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\n\\]\nMore generally, the continuity correction says that, if you are approximating a discrete random variable \\(X\\) with a normal distribution \\(Y\\), then\n\n\\(P(X \\geq a) \\approx P(Y &gt; a - 1/2)\\)\n\\(P(X &gt; a) \\approx P(Y &gt; a + 1/2)\\)\n\\(P(X \\leq b) \\approx P(Y &lt; b + 1/2)\\)\n\\(P(X &lt; b) \\approx P(Y &lt; b - 1/2)\\)\n\\(P(a \\leq X \\leq b) \\approx P(a - 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a &lt; X \\leq b) \\approx P(a + 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a \\leq X &lt; b) \\approx P(a - 1/2 &lt; Y &lt; b - 1/2)\\)\n\\(P(a &lt; X &lt; b) \\approx P(a + 1/2 &lt; Y &lt; b - 1/2)\\)\n\nLet’s demonstrate the continuity correction\n\np &lt;- 0.5\nn &lt;- 20\nmu &lt;- n * p\nsig &lt;- sqrt(n * p * (1 - p))\na &lt;- 8\nb &lt;- 12\n\npbinom(q = b, size = n, prob = p)\n\n[1] 0.8684\n\npnorm(q = b, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.8145\n\npnorm(q = b + 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.8682\n\npbinom(q = b, size = n, prob = p) - pbinom(q = a - 1, size = n, prob = p)\n\n[1] 0.7368\n\npnorm(q = b, mean = mu, sd = sig) - pnorm(q = a, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.6289\n\npnorm(q = b + 1/2, mean = mu, sd = sig) - pnorm(q = a - 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.7364\n\n\nWe will use the normal approximation for 2-sample binomial tests.\nWhy does the normal approximation work?\nLet \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials: \\[\nT_i =\n\\begin{cases}\n1 & \\text{w.p. } p \\\\\n0 & \\text{w.p. } 1 - p\n\\end{cases}\n\\]\nLet \\[\nX = T_1 + T_2 + \\dots + T_n = \\sum T_i\n\\]\nThe Central Limit Theorem says \\(X\\) is normal for large \\(n\\).\n\n\n\n\nIf \\(X \\sim \\mathrm{Poisson}(\\mu)\\), \\(Y \\sim N(\\mu, \\mu)\\), and \\(\\mu \\geq 10\\) (rule of thumb), then\n\\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq Y \\leq b + \\frac{1}{2}\\right)\n\\]"
  },
  {
    "objectID": "04_est/06_notes.html#when-variance-is-not-known",
    "href": "04_est/06_notes.html#when-variance-is-not-known",
    "title": "Chapter 6 Notes: Estimation",
    "section": "When Variance is Not Known",
    "text": "When Variance is Not Known\n\nThe above only works when \\(\\sigma^2\\) is known\n\n\\(\\sigma^2\\) is never known\n\n\\(\\frac{\\bar{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}\\) (not \\(N(0, 1)\\))\n\n\\(t\\)-distribution with \\(n - 1\\) degrees of freedom\n\nOnly an exact result when \\(X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\)\n\nBut \\(t\\)-distribution tends to work better in small samples even when \\(X_i\\) are not normal\n\nFor large \\(n\\), \\(t_{n-1} \\approx N(0, 1)\\), so CLT is OK\nBell-shaped, centered at 0\nAs \\(\\text{df} \\downarrow\\), extreme values more likely\nAs \\(\\text{df} \\uparrow\\), extreme values less likely\nUse \\(t\\) because of added variability from using \\(s^2\\) instead of \\(\\sigma^2\\)\n\n\nR Code for t-distribution\n\n\nRosner uses notation \\(t_{\\text{df}, p}\\) for the \\(p\\) quantile of a \\(t_{\\text{df}}\\) distribution\n\n\\[\nt_{\\text{df}, p} = \\texttt{qt(p, df)}\n\\]\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n&\\Pr\\left(-t_{n-1, 1 - \\alpha/2} \\leq \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} \\leq t_{n-1, 1 - \\alpha/2} \\right) = 1 - \\alpha\\\\\n&\\Leftrightarrow \\Pr\\left(-t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\right) = 1 - \\alpha\\\\\n&\\Leftrightarrow \\Pr\\left(\\bar{X} - t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\\]\n\nThus, the following is a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma^2\\) is not known: \\[\\begin{align}\n&\\bar{X} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\text{ or, equivalently}\\\\\n&\\bar{X} \\pm \\texttt{qt(1 - }\\alpha/2\\texttt{, n-1)} \\cdot \\frac{s}{\\sqrt{n}}\n\\end{align}\\]\n\n\nExerciseHintSolution\n\n\nSuppose \\(n = 10\\), \\(\\bar{X} = 116.9\\), \\(s = 21.70\\). Calculate 90%, 95%, 99% CIs\n\n\nThe degrees of freedom of the appropriate \\(t\\) distribution is \\(10 - 1 = 9\\).\nE.g., for a 90% CI, \\(\\alpha = 1 - 0.9 = 0.1\\), and you’ll need the \\(1 - \\alpha/2 = 1 - 0.1/2 = 0.95\\) quantile of that \\(t\\) distribution.\n\n\nSince \\(n = 10\\), the degrees of freedom of the appropriate \\(t\\) distribution is \\(10 - 1 = 9\\)., For 90% CI’s, \\(\\alpha = 0.1\\) and the appropriate quantile is\n\nqt(p = 1 - 0.1/2, df = 9)\n\n[1] 1.833\n\n\nWe can thus get a 90% CI by\n\n116.9 - 1.833 * 21.70 / sqrt(10)\n\n[1] 104.3\n\n116.9 + 1.833 * 21.70 / sqrt(10)\n\n[1] 129.5\n\n\nSimilarly, for a 95% CI we have\n\n116.9 - qt(p = 1 - 0.05/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 101.4\n\n116.9 + qt(p = 1 - 0.05/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 132.4\n\n\nAnd, for a 99% CI we have\n\n116.9 - qt(p = 1 - 0.01/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 94.6\n\n116.9 + qt(p = 1 - 0.01/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 139.2\n\n\n\n\n\n\nNote:\n\nCI level \\(\\uparrow\\) (so \\(\\alpha \\downarrow\\)) \\(\\Rightarrow\\) larger intervals\n\\(n \\uparrow\\) \\(\\Rightarrow\\) smaller intervals\n\\(s^2 \\uparrow\\) \\(\\Rightarrow\\) larger intervals\n\n\n\nBone Density Case Study"
  },
  {
    "objectID": "05_tests/07_notes.html",
    "href": "05_tests/07_notes.html",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "",
    "text": "Two hypotheses\n\n\\(H_0\\): Null\n\nBaseline, to be disproved\n\n\n\\(H_1\\): Alternative\n\nOpposite of null (contradicts \\(H_0\\))\n\n\nExample: Want to test if low socioeconomic status (SES) mothers have babies with lower birthweight\n\nKnow: national average is 120 oz\n\nFor \\(n = 10\\) low SES mothers, observe\n\n\\(\\bar{X} = 115\\) oz, \\(S = 24\\) oz\n\n\nLet \\(\\mu\\) = average birthweight of low SES mothers\n\n\\(H_0\\): \\(\\mu = 120\\)\n\n\\(H_1\\): \\(\\mu &lt; 120\\)\n\n\nWe use data (e.g., \\(\\bar{X}\\) and \\(s^2\\)) to make a decision (\\(H_0\\) vs. \\(H_1\\))\n\n\\[\n\\begin{array}{c|cc}\n& H_0 \\text{ true} & H_1 \\text{ true} \\\\\n\\hline\n\\text{Fail to Reject } H_0 & \\text{True negative} & \\text{Type II error} \\\\\n\\text{Reject } H_0 & \\text{Type I error} & \\text{True positive} \\\\\n\\end{array}\n\\]\n\nExample: If the truth is \\(\\mu = 120\\) (\\(H_0\\) is true) but we say \\(\\mu &lt; 120\\) (reject \\(H_0\\)), this is a Type I error\nTypically,\n\nNull Hypothesis: parameter = some value\n\nAlternative Hypothesis: One of:\n\nparameter ≠ value,\nparameter &gt; value,\nparameter &lt; value,\n\n\n\n\n\n\n\n\n\nTipType I error rate\n\n\n\nProbability of a Type I error \\[\\begin{align*}\n\\Pr(\\text{Reject } H_0 \\mid H_0 \\text{ True}) &= \\text{significance level} \\\\\n&= \\alpha\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nTipType II error rate\n\n\n\nProbability of a Type II error\n\\[\\begin{align*}\n\\Pr(\\text{Fail to reject } H_0 \\mid H_1 \\text{ True}) &= \\beta\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nTipPower\n\n\n\nProbability of correctly rejecting the null. \\[\\begin{align*}\n\\Pr(\\text{Reject } H_0 \\mid H_1 \\text{ True})\n&= 1 - \\Pr(\\text{Fail to reject } H_0 \\mid H_1 \\text{ True}) \\\\\n&= 1 - \\beta\n\\end{align*}\\]\n\n\n\nThese probabilities are all in terms of repeated samples.\n\n\nExerciseSolution:\n\n\nThere is a new pain relief drug for osteoarthritis (OA).\n\n50 OA patients take drug; measure \\(X\\) = % decline in pain level.\n\nReported:\n\nIf \\(X &gt; 0\\): less pain\nIf \\(X &lt; 0\\): more pain\n\n\nWe want to test if the drug is effective (average \\(X &gt; 0\\))\n\n\nWhat hypotheses are being tested?\n\nWhat do Type I error, Type II error, and power mean?\n\n\n\n\nLet \\(\\mu\\) = mean % decline in pain\n\\(H_0\\): \\(\\mu = 0\\)\n\\(H_1\\): \\(\\mu &gt; 0\\)\nType I: Say drug reduces pain on average, but it does not\n\nType II: Say drug does not reduce pain when it does\n\nPower: Probability we say it works when it truly works\n\n\n\n\n\nThe goal of a good test is to make both \\(\\alpha\\) and \\(\\beta\\) as small as possible\nHowever, typically, as \\(\\alpha \\downarrow\\), \\(\\beta \\uparrow\\) and as \\(\\beta \\downarrow\\), \\(\\alpha \\uparrow\\).\n\nThese are conflicting criteria\n\nCommon strategy: Fix \\(\\alpha = 0.05\\) (or similar), and use a test that has small \\(\\beta\\)\nAll hypothesis testing asks: How weird is our data if \\(H_0\\) were true?\n\n \n\nIf data are very weird, reject \\(H_0\\)\nIf data are not very weird, fail to reject \\(H_0\\)"
  },
  {
    "objectID": "05_tests/07_notes.html#power-calculations",
    "href": "05_tests/07_notes.html#power-calculations",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Power Calculations",
    "text": "Power Calculations\n\nPower = Probability of correctly rejecting the null:\n\\[\n\\text{Power} = \\Pr(\\text{Reject } H_0 \\mid H_1) = 1 - \\beta\n\\]\nIn experiments/surveys, it is common to guess power prior to collecting data:\n\nCalculate \\(n\\) needed\nSee what effect sizes we can detect\nEstimate likelihood our study will be successful\n\nTo do this, we need:\n\nGuess of effect size \\(\\mu_1 - \\mu_0\\) (from pilot study or wild guess)\nGuess of standard deviation \\(\\sigma\\)\nSample size \\(n\\)\nSignificance level \\(\\alpha\\) (provided by researcher)\n\nIf \\(\\sigma\\) is known, use z-test instead of t-test to calculate power\n\n\\[\\begin{align*}\nH_0 &: \\mu = \\mu_0 \\\\\nH_1 &: \\mu &lt; \\mu_0\n\\end{align*}\\]\n\nReject \\(H_0\\) if:\n\\[\n\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha\n\\]\n(This follows a standard normal under \\(H_0\\))\n\n\nPower calculation under \\(\\mu = \\mu_1\\):\n\\[\\begin{align*}\n\\Pr(\\text{Reject } H_0 \\mid \\mu = \\mu_1)\n&= \\Pr\\left( \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha \\ \\Big| \\ \\mu = \\mu_1 \\right) \\\\\n&= \\Pr\\left( \\frac{\\bar{X} - \\mu_1 + \\mu_1 - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha \\right)\n\\end{align*}\\]\nContinuing from the previous derivation:\n\\[\\begin{align*}\n\\Pr\\left( \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha \\ \\Big| \\ \\mu = \\mu_1 \\right)\n&= \\Pr\\left( \\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} &lt; \\frac{\\mu_0 - \\mu_1}{\\sigma/\\sqrt{n}} + z_\\alpha \\right) \\\\\n&= \\Phi\\left( \\frac{\\mu_0 - \\mu_1}{\\sigma/\\sqrt{n}} + z_\\alpha \\right)\n\\end{align*}\\]\n\n\\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution\n\nThis gives us an estimate of power\n\nRewriting:\n\\[\n\\text{Power} = \\Phi\\left( z_\\alpha + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma} \\right)\n\\]\nPlot: \\(\\Phi(x)\\) is an S-shaped curve ranging from 0 to 1\nXYZ IMAGE HERE\n(Standard normal CDF)\n\n\nSummary of how parameters affect power:\n\n\n\n\n\n\n\n\n\nParameter\nChange\nEffect on Power\nReason\n\n\n\n\n\\(\\alpha\\)\n↓\nPower ↓\nMore stringent test\n\n\n\\(n\\)\n↓\nPower ↓\nLess data\n\n\n\\(|\\mu_0 - \\mu_1|\\)\n↓\nPower ↓\nSmaller effect size\n\n\n\\(\\sigma\\)\n↓\nPower ↑\nMore precise measurements\n\n\n\nXYZ IMAGE HERE\n(Overlapping normal curves showing the distribution of \\(\\bar{X}\\) under \\(H_0\\) and \\(H_1\\), with \\(\\alpha\\) and power labeled)\n\nLeft curve: distribution under \\(\\mu = \\mu_0\\)\n\nRight curve: distribution under \\(\\mu = \\mu_1\\)\n\nArea to the left of the critical value under the \\(H_1\\) curve is power = \\(1 - \\beta\\)\n\n\n\nExample:\nPilot study using 10 individuals gave:\n\\(\\bar{X} = -5\\), \\(s = 10\\)\nAssume \\(\\mu = 0\\) is the null (no effect)\nNow propose a new study with \\(n = 30\\)\nAt \\(\\alpha = 0.05\\), what is the power?\n\\[\\begin{align*}\n\\mu_1 &= -5 \\\\\n\\sigma &= 10 \\\\\nn &= 30 \\\\\n\\alpha &= 0.05 \\Rightarrow z_\\alpha = -1.645 \\quad (\\text{i.e., } \\texttt{qnorm(0.05)})\n\\end{align*}\\]\nCompute power: \\[\\begin{align*}\n\\Phi\\left(-1.645 + \\frac{0 - (-5)}{10/\\sqrt{30}} \\right)\n&= \\Phi(1.094) \\\\\n&= \\texttt{pnorm(1.094)} \\approx 0.863\n\\end{align*}\\]\n\n\\(\\Rightarrow\\) Estimated power is 0.863\n\n\n\nExercise:\nMean birthweight in the US is 120 oz.\nWhat is the power to detect low birthweight in a study with:\n- \\(n = 100\\)\n- \\(\\mu_1 = 115\\)\n- \\(\\alpha = 0.05\\)\n- \\(\\sigma = 24\\)\n\\[\\begin{align*}\n\\text{Power} &= \\Phi\\left(-1.645 + \\frac{120 - 115}{24 / \\sqrt{100}} \\right) \\\\\n&= \\Phi(0.438) = \\texttt{pnorm(0.438)} \\approx 0.669\n\\end{align*}\\]\n\n\nFor right-tailed test:\nIf\n\\[\\begin{align*}\nH_0 &: \\mu = \\mu_0 \\\\\nH_1 &: \\mu &gt; \\mu_0\n\\end{align*}\\]\nThen\n\\[\n\\text{Power} = \\Phi\\left( z_\\alpha + \\frac{\\sqrt{n}(\\mu_1 - \\mu_0)}{\\sigma} \\right)\n\\]\n(Just swap \\(\\mu_0\\) and \\(\\mu_1\\) from the left-tailed version.)\n\n\nFor two-tailed test:\nIf\n\\[\\begin{align*}\nH_0 &: \\mu = \\mu_0 \\\\\nH_1 &: \\mu \\ne \\mu_0\n\\end{align*}\\]\nThen\n\\[\n\\text{Power} = \\Phi\\left( z_{\\alpha/2} + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma} \\right)\n+ \\Phi\\left( z_{\\alpha/2} + \\frac{\\sqrt{n}(\\mu_1 - \\mu_0)}{\\sigma} \\right)\n\\]\n\nAdd both tails\n\nUse \\(\\alpha/2\\) critical value"
  },
  {
    "objectID": "05_tests/07_notes.html#sample-size-calculation",
    "href": "05_tests/07_notes.html#sample-size-calculation",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Sample Size Calculation",
    "text": "Sample Size Calculation\n\nTypically, you want a power of at least 0.8\n\nSo, what \\(n\\) will give us power of 0.8?\n\n\nAssume:\n\nGuess of effect size: \\(\\mu_1 - \\mu_0\\) (from pilot study or wild guess)\n\nGuess of standard deviation \\(\\sigma\\)\n\nDesired power = \\(1 - \\beta\\) (chosen by researcher)\n\nSignificance level \\(\\alpha\\)\n\n\n\nOne-sided test (e.g., \\(H_0\\!: \\mu = \\mu_0\\) vs. \\(H_1\\!: \\mu &lt; \\mu_0\\)):\nStart from power formula: \\[\n1 - \\beta = \\Phi\\left( z_\\alpha + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma} \\right)\n\\]\nNow solve for \\(n\\) (either numerically or algebraically):\nTake inverse: \\[\n\\Phi^{-1}(1 - \\beta) = z_\\alpha + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma}\n\\]\nSolve: \\[\n\\sqrt{n} = \\frac{\\Phi^{-1}(1 - \\beta) - z_\\alpha}{(\\mu_0 - \\mu_1)/\\sigma}\n\\]\nSquare both sides: \\[\nn = \\frac{(\\Phi^{-1}(1 - \\beta) - z_\\alpha)^2 \\cdot \\sigma^2}{(\\mu_0 - \\mu_1)^2}\n\\]\n\n\nAlternate form (for \\(H_0\\!: \\mu = \\mu_0\\) vs. \\(H_1\\!: \\mu &gt; \\mu_0\\)):\n\\[\nn = \\frac{(z_{1-\\beta} + z_{1-\\alpha})^2 \\cdot \\sigma^2}{(\\mu_1 - \\mu_0)^2}\n\\]\n\n\nExercise: What is the effect on \\(n\\) of increasing:\n\n\\(\\sigma\\)\n\n\\(\\beta\\)\n\n\\(\\alpha\\)\n\n\\(|\\mu_0 - \\mu_1|\\)\n\n\nEffects:\n\n\\(\\sigma \\uparrow \\ \\Rightarrow\\ n \\uparrow\\)\n(Less precise measurements, so need more data)\n\\(\\beta \\uparrow \\ \\Rightarrow\\ 1 - \\beta \\downarrow \\ \\Rightarrow\\ z_{1 - \\beta} \\downarrow \\ \\Rightarrow\\ n \\downarrow\\)\n(Less power needed)\n\\(\\alpha \\uparrow \\ \\Rightarrow\\ z_{1 - \\alpha} \\downarrow \\ \\Rightarrow\\ n \\downarrow\\)\n(Less stringent test)\n\\(|\\mu_0 - \\mu_1| \\uparrow \\ \\Rightarrow\\ n \\downarrow\\)\n(Larger effect sizes are easier to detect)\n\nXYZ IMAGE HERE\n(Standard normal with \\(\\alpha\\), \\(\\beta\\), \\(z_{1-\\alpha}\\), and \\(z_{1-\\beta}\\) labeled)\n\n\n\nFor two-sided test:\nIf \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu \\ne \\mu_0\\),\nsolve for \\(n\\) using:\n\\[\nn = \\frac{\\sigma^2 (z_{1-\\beta} + z_{1 - \\alpha/2})^2}{(\\mu_0 - \\mu_1)^2}\n\\]\n(The only difference from one-sided test is the use of \\(z_{1 - \\alpha/2}\\) instead of \\(z_{1 - \\alpha}\\).)\n\nPower Calculations in R\n\nSkip Section 7.8"
  },
  {
    "objectID": "05_tests/07_notes.html#one-sample-inference-for-binomial",
    "href": "05_tests/07_notes.html#one-sample-inference-for-binomial",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "One-Sample Inference for Binomial",
    "text": "One-Sample Inference for Binomial\n\nExample:\nPrevalence of breast cancer is 2%.\nOf 10,000 women whose mothers had breast cancer,\n400 of them got breast cancer in their lives.\nAre they at higher risk?\n\nLet \\(X\\) = number of those 10,000 who got breast cancer\n\n\\(X \\sim \\text{Binom}(10{,}000,\\ p)\\)\n\n\\[\\begin{align*}\nH_0 &: p = 0.02 \\\\\nH_1 &: p &gt; 0.02\n\\end{align*}\\]\n\n\nUse normal approximation:\n\n\\(\\hat{p} = \\frac{x}{n} \\sim N\\left(p_0,\\ \\frac{p_0(1 - p_0)}{n}\\right)\\) under \\(H_0\\)\n\nThen: \\[\n\\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} \\sim N(0, 1)\n\\]\nXYZ IMAGE HERE\n(Normal curve with right tail labeled as p-value)\n\n\np-value:\n\\[\n\\text{p-value} = 1 - \\Phi\\left( \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} \\right)\n\\]\n\nIf\n\\[\\begin{align*}\nH_0 &: p = p_0 \\\\\nH_1 &: p \\ne p_0\n\\end{align*}\\]\nThen calculate area in both tails:\n\nXYZ IMAGE HERE\n(Symmetric normal curve with shaded tails, showing two-sided test)\n\nTwo-tailed p-value: \\[\n\\text{p-value} = 2 \\cdot \\Phi\\left( -\\left| \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} \\right| \\right)\n\\]\n\n\n\nContinuity correction:\n\nSubtract \\(1/n\\) in the numerator for upper-tail tests\n\nAdd \\(1/n\\) in the numerator for lower-tail tests\n\n\n\nExample: Back to breast cancer example\n\\[\\begin{align*}\n\\hat{p} &= \\frac{400.5}{10{,}000} = 0.04005 \\\\\nz &= \\frac{0.04005 - 0.02}{\\sqrt{0.02(1 - 0.02)/10{,}000}} \\approx 14.32 \\\\\n\\text{p-value} &= \\Phi(-14.32) \\approx 8.4 \\cdot 10^{-47} \\ll 0.0001\n\\end{align*}\\]\n\n\\(\\Rightarrow\\) Very highly significant"
  },
  {
    "objectID": "05_tests/07_notes.html#exact-test",
    "href": "05_tests/07_notes.html#exact-test",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Exact Test",
    "text": "Exact Test\n\nWe are still assuming that \\(X \\sim \\text{Bin}(n, p)\\)\n\nHypotheses:\n\\(H_0: p = p_0\\)\n\\(H_1: p \\ne p_0\\), or \\(p &gt; p_0\\), or \\(p &lt; p_0\\)\nThe normal method only works for large \\(n\\)\nRule of thumb: You can use the normal method if \\[\nn p_0(1 - p_0) \\ge 5\n\\]\nIf \\(n\\) is not large enough, use an exact test\n\nControls for \\(\\alpha\\) for all \\(n\\), not just large \\(n\\)\n\nUnder the null \\(X \\sim \\mathrm{Binom}(n, p_0)\\). We we observe \\(x\\) (lower-case \\(x\\)), then just calculate the probability from the binomial PMF of seeing values as weird or weirder.\n\\(H_1: p &lt; p_0\\): Calculate \\(\\Pr(X \\leq x \\mid p = p_0)\\) with pbinom(q = x, size = n, prob = p0)\nE.g., \\(X = 3\\), \\(H_0: p_0 = 0.5\\), \\(H_1: p &lt; 0.5\\). This the the PMF of \\(X\\) under \\(H_0\\) and the sum of the orange bars is the \\(p\\)-value.\n\n\n\n\n\n\n\n\n\n\\(H_1: p &gt; p_0\\): Calculate \\(\\Pr(X \\leq x \\mid p = p_0)\\) with 1 - pbinom(q = x - 1, size = n, prob = p0)\nE.g., \\(X = 3\\), \\(H_0: p_0 = 0.5\\), \\(H_1: p_1 &gt; 0.5\\). This the the PMF of \\(X\\) under \\(H_0\\) and the sum of the orange bars is the \\(p\\)-value.\n\n\n\n\n\n\n\n\n\n\\(H_1: p \\neq p_0\\): 2 times the smaller of \\(\\Pr(X \\leq x \\mid p = p_0)\\) or \\(\\Pr(X \\geq x \\mid p = p_0)\\).\nE.g., \\(X = 3\\), \\(H_0: p_0 = 0.5\\), \\(H_1: p &lt; 0.5\\). This the the PMF of \\(X\\) under \\(H_0\\) and the two times the sum of the orange bars is the \\(p\\)-value.\n\n\n\n\n\n\n\n\n\nThe exact \\(p\\)-value formula for \\(H_1: p \\neq p_0\\) is \\[\n\\text{p-value} =\n\\begin{cases}\n2 \\sum_{k = 0}^{x} \\binom{n}{k} p_0^k (1 - p_0)^{n - k} & \\text{ if } x \\leq np_0\\\\\n2 \\sum_{k = x}^{n} \\binom{n}{k} p_0^k (1 - p_0)^{n - k} & \\text{ if } x &gt; np_0\n\\end{cases}\n\\]\nFor \\(H_1: p &lt; p_0\\): \\[\n\\text{p-value} = \\sum_{k = 0}^{x} \\binom{n}{k} p_0^k (1 - p_0)^{n - k}\n\\]\nFor \\(H_1: p &gt; p_0\\): \\[\n\\text{p-value} = \\sum_{k = x}^{n} \\binom{n}{k} p_0^k (1 - p_0)^{n - k}\n\\]\nExample: In a nuclear facility, there were 13 deaths, of which 5 were caused by cancer. Cancer causes 20% of deaths in this age group. Are there actually more cancer deaths than would be expected?\nLet \\(X\\) be the number of deaths caused by cancer out of 13. Then \\(X \\sim \\text{Binom}(13, p)\\). We are testing \\(H_0: p = 0.2\\) versus \\(H_1: p &gt; 0.2\\). The \\(p\\)-value is the probability of seeing at least 5 cancer deaths (out of 13) if indeed \\(p = 0.2\\).\n\n1 - pbinom(q = 4, size = 13, prob = 0.2)\n\n[1] 0.09913\n\n\nThis is a pretty large \\(p\\)-value, so we do not have evidence that there are more cancer deaths than expected.\n\n\nBinomial Tests in R"
  },
  {
    "objectID": "05_tests/07_notes.html#power-calculations-for-binomial-tests",
    "href": "05_tests/07_notes.html#power-calculations-for-binomial-tests",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Power Calculations for Binomial Tests",
    "text": "Power Calculations for Binomial Tests\nWe want: \\[\n\\text{Power} = \\Pr(\\text{Reject } H_0 \\mid H_1)\n\\]\n\nNeed:\n\nTrue alternative value: \\(p_1\\)\n\nSample size: \\(n\\)\n\nSignificance level: \\(\\alpha\\)\n\nWe skip the exact distribution and use a normal approximation.\nXYZ IMAGE HERE\n(Overlapping normal distributions under \\(H_0: p = p_0\\) and \\(H_1: p = p_1\\) with shaded rejection region and labeled \\(\\alpha/2\\), \\(\\beta\\))\n\n\nUnder \\(H_0: p \\ne p_0\\), power is approximated as:\n\\[\n\\text{Power} = \\Phi\\left(\n\\frac{p_0 - p_1}{\\sqrt{p_1(1 - p_1)/n}}\n\\cdot z_{\\alpha/2}\n+ \\frac{|p_0 - p_1| \\cdot \\sqrt{n}}{\\sqrt{p_1(1 - p_1)}}\n\\right)\n\\]\n\n\nSummary of effects on power:\n\n\n\n\n\n\n\n\n\nParameter\nChange\nEffect on Power\nReason\n\n\n\n\n\\(n\\)\n↑\nPower ↑\nMore data\n\n\n\\(\\alpha\\)\n↑\nPower ↑\nLess stringent test\n\n\n\\(|p_0 - p_1|\\)\n↑\nPower ↑\nMore signal"
  },
  {
    "objectID": "05_tests/07_notes.html#sample-size-calculation-binomial",
    "href": "05_tests/07_notes.html#sample-size-calculation-binomial",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Sample Size Calculation (Binomial)",
    "text": "Sample Size Calculation (Binomial)\n\nNeed:\n\nTrue alternative \\(p_1\\)\n\nDesired power \\(1 - \\beta\\)\n\nSignificance level \\(\\alpha\\)\n\nWe set: \\[\n1 - \\beta = f(n,\\ p_1,\\ \\alpha)\n\\]\nThen solve for \\(n\\)\n\nPower Calculations in R"
  },
  {
    "objectID": "05_tests/07_notes.html#poisson-test",
    "href": "05_tests/07_notes.html#poisson-test",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Poisson Test",
    "text": "Poisson Test\n\nLet \\(X \\sim \\text{Poisson}(\\mu)\\)\n\n\nHypotheses:\n\n\\(H_0: \\mu = \\mu_0\\)\n\n\\(H_1\\): \\(\\mu \\ne \\mu_0\\), or \\(\\mu &gt; \\mu_0\\), or \\(\\mu &lt; \\mu_0\\)\n\n\n\np-value (two-sided):\nLet \\(x\\) be the observed count. Then:\n\\[\n\\text{p-value} = \\sum_{h : \\Pr(h) \\le \\Pr(x)} \\Pr(h)\n= \\sum_{h : \\Pr(h) \\le \\Pr(x)} \\frac{e^{-\\mu_0} \\mu_0^h}{h!}\n\\]\nXYZ IMAGE HERE\n(PMF showing observed \\(x\\) and shaded tails for exact p-value)\n\n\nExample:\n8446 rubber workers, ages 40–84\n4 deaths due to Hodgkin’s disease\nExpected deaths (based on U.S. rates): 3.3\nLet \\(X\\) = # with Hodgkin’s\nAssume \\(X \\sim \\text{Poisson}(\\mu)\\)\n\\[\\begin{align*}\nH_0 &: \\mu = 3.3 \\\\\nH_1 &: \\mu \\ne 3.3\n\\end{align*}\\]\nR code:\n\npoisson.test(x = 4, r = 3.3)\n\n\n    Exact Poisson test\n\ndata:  4 time base: 1\nnumber of events = 4, time base = 1, p-value = 0.6\nalternative hypothesis: true event rate is not equal to 3.3\n95 percent confidence interval:\n  1.09 10.24\nsample estimates:\nevent rate \n         4 \n\n\nResult:\n\\[\n\\text{p-value} = 0.578\n\\]\n\n\\(\\Rightarrow\\) No evidence of elevated risk\n\n\n\nNote:\n\nBinomial test is not appropriate here\nbecause each individual has a different \\(p_i\\).\nThe null hypothesis was based on the expected \\(p_i\\) for each individual."
  },
  {
    "objectID": "05_tests/07_notes.html#standardized-mortality-ratio-smr",
    "href": "05_tests/07_notes.html#standardized-mortality-ratio-smr",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Standardized Mortality Ratio (SMR)",
    "text": "Standardized Mortality Ratio (SMR)\n\\[\n\\text{SMR} = \\frac{\\text{Observed}}{\\text{Expected}} \\times 100\\%\n\\]\n\n\\(\\text{SMR} &gt; 100\\%\\) \\(\\Rightarrow\\) more deaths than expected in the study population\n\n\\(\\text{SMR} &lt; 100\\%\\) \\(\\Rightarrow\\) fewer deaths\n\n\\(\\text{SMR} = 100\\%\\) \\(\\Rightarrow\\) same number of deaths as expected\n\n\nExample:\nObserved = 4, Expected = 3.3\n\\[\n\\text{SMR} = \\frac{4}{3.3} \\times 100\\% \\approx 121\\%\n\\]"
  },
  {
    "objectID": "05_tests/07_notes.html#exercises-7.17.8",
    "href": "05_tests/07_notes.html#exercises-7.17.8",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Exercises 7.1–7.8",
    "text": "Exercises 7.1–7.8\n\n12 patients\nAfter 24 hours, mean serum creatinine is \\(1.2 \\text{ mg/dL}\\)\n\nGeneral population: \\(\\mu = 1\\), \\(\\sigma = 0.4\\)\n\n\n(1) Known \\(\\sigma\\)\n\n\\(H_0\\): \\(\\mu = 1\\)\n\n\\(H_1\\): \\(\\mu \\ne 1\\)\n\n\\[\nz = \\frac{1.2 - 1}{0.4 / \\sqrt{12}} = 1.732\n\\]\nUnder \\(H_0\\), \\(z \\sim N(0, 1)\\):\n\\[\n2 \\cdot \\text{pnorm}(-1.73) = 0.08326\n\\]\nFail to reject \\(H_0\\).\n\n\n(2.1) If instead \\(s = 0.4\\), \\(\\sigma\\) unknown\n\n\\(t = 1.73 \\sim t_{11}\\)\n\n\\[\n2 \\cdot \\text{pt}(-1.73, \\text{df} = 11) = 0.1112\n\\]\n\\(p\\)-value = 0.6376\nFail to reject \\(H_0\\).\n\n\n(2.2) \\(s = 0.6\\), \\(\\sigma\\) unknown\n\\[\nt = \\frac{1.2 - 1}{0.6 / \\sqrt{12}} = 1.155\n\\]\n\\[\n2 \\cdot \\text{pt}(-1.155, \\text{df} = 11) = 0.2777\n\\]\nFail to reject \\(H_0\\).\n\n\n7.4\n\\[\n1.2 \\pm t_{11, 0.975} \\cdot \\frac{0.6}{\\sqrt{12}} \\\\\nt_{0.975, \\text{df}=11} = 2.201 \\Rightarrow \\text{CI} = (0.82, 1.58)\n\\]\n\n\n7.5\n\n1 is included in the 99% CI \\(\\Rightarrow\\) \\(p\\)-value &gt; 0.05\n\n\n\n7.6\n\\[\n\\text{pt}(-1.52, \\text{df}=6) = 0.179\n\\]\n\n\n7.7\n\\[\n1 - \\text{pt}(2.5, \\text{df}=36) = 0.008557\n\\]\n\n\n7.8\n\\[\n\\text{qt}(0.1, \\text{df}=54) = -1.297\n\\]\n\n\n7.52–7.53\n\n\\(n = 200\\)\n\\(X\\) = number who develop cancer\nobserved \\(x = 4\\)\n\\(X \\sim \\text{Binom}(200, p)\\)\n\n\\(H_0\\): \\(p = 0.01\\) vs \\(H_1\\): \\(p \\ne 0.01\\)\n\nCheck:\n\\[\nnp_0(1 - p_0) = 0.01 \\cdot 0.99 \\cdot 200 = 1.98 &lt; 5\n\\]\n→ Use exact binomial test.\nR code:\n\nbinom.test(x = 4, n = 200, p = 0.01)\n\n\n    Exact binomial test\n\ndata:  4 and 200\nnumber of successes = 4, number of trials = 200, p-value = 0.1\nalternative hypothesis: true probability of success is not equal to 0.01\n95 percent confidence interval:\n 0.005476 0.050414\nsample estimates:\nprobability of success \n                  0.02 \n\n\n\n\nManual p-value computation:\n\\[\n\\text{p-value} = 2 \\cdot \\sum_{k=4}^{200} \\binom{200}{k} (0.01)^k (0.99)^{200-k}\n\\]\nR shortcut:\n\n2 * dbinom(3, size = 200, prob = 0.01)\n\n[1] 0.3627\n\n\nResult:\n\\[\n\\text{p-value} \\approx 0.2839\n\\]\n\nR output \\(p\\)-value ≈ 0.142\n\n\n\n7.54\n\n5-year period\n\\(X = 20\\), \\(n = 200\\)\n5-year incidence rate is \\(0.05\\)\n\nHypotheses: - \\(H_0\\): \\(p = 0.05\\) - \\(H_1\\): \\(p \\ne 0.05\\)\nSince \\(0.05 \\cdot 0.95 \\cdot 200 = 9.5\\), we can use the normal approximation.\n\\[\nz = \\frac{0.1 - 0.05}{\\sqrt{0.05 \\cdot 0.95 / 200}} = 3.082\n\\]\n\\[\n\\text{p-value} = 2(1 - \\text{pnorm}(3.082)) = 0.002055\n\\]\n→ Reject \\(H_0\\)\n\n\n7.55\nCompute 95% CI for \\(p\\):\n\\[\n\\hat{p} \\pm z_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\quad \\text{where } z_{0.975} = 1.96\n\\]\n\\[\n0.1 \\pm 1.96 \\cdot \\sqrt{\\frac{0.1 \\cdot 0.9}{200}} = (0.05842, 0.1416)\n\\]\nR code:\n\nprop.test(x = 20, n = 200, p = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  20 out of 200, null probability 0.05\nX-squared = 9.5, df = 1, p-value = 0.002\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.06366 0.15230\nsample estimates:\n  p \n0.1"
  },
  {
    "objectID": "05_tests/07_notes.html#two-sided-tests-for-the-mean",
    "href": "05_tests/07_notes.html#two-sided-tests-for-the-mean",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Two-sided Tests for the Mean",
    "text": "Two-sided Tests for the Mean\n\nSometimes Usually, our alternative is just that the mean is not equal to the null value.\n\nE.g., average birthweights for low SES families are different from the population average.\n\nWhat constitutes “more weird” in this case is the area in both tails.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantTwo-tailed Test\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\)\nHypotheses:\n\n\\(H_0\\): \\(\\mu = \\mu_0\\)\n\n\\(H_1\\): \\(\\mu \\ne \\mu_0\\)\n\nTest statistic:\n\\[\nt = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}}\n\\]\nTwo-tailed p-value:\n\\[\n\\text{p-value} = 2 \\cdot \\texttt{pt}(-|t|,\\ n-1)\n\\]\nCritical value technique: Reject \\(H_0\\) if\n\\[\n|t| &gt; \\texttt{qt}\\left(1 - \\frac{\\alpha}{2},\\ n-1\\right)\n\\]\n\n\n\nExercise:Solution\n\n\nCholesterol levels of US women are 190 mg/dL on average.\nWant to compare cholesterol levels of recent Asian immigrants.\n100 female Asian immigrants:\n- \\(\\bar{X} = 181.52\\) mg/dL\n- \\(s = 40\\) mg/dL\n\n\n\\[\\begin{align*}\nH_0 &: \\mu = 190 \\\\\nH_1 &: \\mu \\ne 190 \\\\\nt &= \\frac{181.52 - 190}{40 / \\sqrt{100}} = -2.12 \\\\\n\\text{p-value} &= 2 \\cdot pt(-2.12,\\ 99) \\approx 0.037\n\\end{align*}\\]\n\n\n\n\nUse two-sided tests by default, unless only one direction is clearly of interest\n\ne.g., drug efficacy\n\nBecause this is an introductory class, there will be a lot of one-sided tests, to evaluate your understanding. These are typically rare in practice.\n\n\nOne-sample t-tests in R"
  },
  {
    "objectID": "05_tests/07_notes.html#confidence-intervals-and-hypothesis-tests",
    "href": "05_tests/07_notes.html#confidence-intervals-and-hypothesis-tests",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nSuppose \\(X_1, X_2, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\) and\n\n\\(H_0 : \\mu = \\mu_0\\)\n\\(H_1 : \\mu \\ne \\mu_0\\)\n\nThen, we reject \\(H_0\\) at level \\(\\alpha\\) if and only if the \\(100(1 - \\alpha)\\%\\) confidence interval does not contain \\(\\mu_0\\)\n\nI.e., if \\(\\mu_0\\) is outside the \\((1-\\alpha)100\\%\\) confidence interval, then reject \\(H_0\\) at level \\(\\alpha\\) and the \\(p\\)-value is less than \\(\\alpha\\)\nGeneral result: All tests correspond to some confidence interval, and vice versa\nSo, a \\(100(1 - \\alpha)\\%\\) CI contains all values of \\(\\mu_0\\) that would fail to reject \\(H_0: \\mu = \\mu_0\\)\nOne-sided confidence intervalss correspond to one-sided tests"
  },
  {
    "objectID": "05_tests/07_notes.html#sample-size-for-z-tests",
    "href": "05_tests/07_notes.html#sample-size-for-z-tests",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Sample Size for z-tests",
    "text": "Sample Size for z-tests\nSuppose \\(H_0: \\mu = \\mu_0\\).\n\nIf \\(H_1: \\mu &gt; \\mu_0\\) or \\(H_1: \\mu &lt; \\mu_0\\) then \\[\nn = \\frac{(z_{1-\\beta} + z_{1-\\alpha})^2\\sigma^2}{(\\mu_0 - \\mu_1)^2}\n\\]\nIf \\(H_1: \\mu \\neq \\mu_0\\) then \\[\nn = \\frac{ (z_{1-\\beta} + z_{1 - \\alpha/2})^2\\sigma^2}{(\\mu_0 - \\mu_1)^2}\n\\]"
  },
  {
    "objectID": "05_tests/08_notes.html",
    "href": "05_tests/08_notes.html",
    "title": "Chapter 8: Two-sample Inference",
    "section": "",
    "text": "Paired t-test\n\nCompare 2 populations where parameters are not known.\nA paired sample is where observations in each population are matched.\n \nExample: Twin study where one twin smokes more than the other.\n\nPopulation 1: lighter smoking twins\nPopulation 2: heavier smoking twins\nMatched pair: each pair of twins\n\nExample: We measure blood pressure on the same individual at 2 time points\n\nPopulation 1: pre oral contraceptive (OC)\nPopulation 2: post OC\nMatched pair: the same individual\n\nThis second example is one of a longitudinal study, where we follow the same people over time\nLet\n\n\\(X_i \\sim N(\\mu_i, \\sigma^2)\\)\n\nFor example, pre-OC\n\n\n\\(Y_i \\sim N(\\mu_i + \\Delta, \\sigma^2)\\)\n\nFor example, post-OC\n\n\nHypotheses:\n\\(H_0\\): \\(\\Delta = 0\\)\n\\(H_1\\): \\(\\Delta \\ne 0\\)\nThis tests if there is a difference between populations while allowing each pair to have their own baseline mean \\(\\mu_i\\).\nDefine differences:\n\\(D_i = Y_i - X_i \\sim N(\\Delta, \\sigma_D^2)\\)\nVariance of differences:\n\\(\\sigma_D^2 = \\text{var}(X) + \\text{var}(Y) - 2\\ \\text{cov}(X, Y)\\)\n\nBut this is a nuisance parameter, so just call it \\(\\sigma_D^2\\)\n\nSo, just use a one-sample t-test on \\(D_i\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nA paired t-test is just a one-sample t-test on differences.\n\n\n\n\n\n\n\n\nTipPaired t-test\n\n\n\nInput: \\(X_1,X_2,\\ldots,X_n\\) and \\(Y_1,Y_2,\\ldots,Y_n\\) where \\(X_i\\) and \\(Y_i\\) are matched pairs.\n\nWe assume \\(X_i \\sim N(\\mu_i, \\sigma_x^2)\\) and \\(Y_i \\sim N(\\mu_i + \\Delta, \\sigma_y^2)\\).\nWe test:\n\n\\(H_0: \\Delta = d_0\\)\n\\(H_A: \\Delta \\neq d_0\\) or \\(\\Delta &gt; d_0\\) or \\(\\Delta &lt; d_0\\).\nwhere \\(d_0\\) = null mean difference between populatoins (e.g. 0)\n\nWe calculate:\n\n\\(D_i = X_i - Y_i\\)\n\\(s_D\\) = standard deviation of \\(D_i\\)’s\n\n\\(\\bar{D}\\) = mean of \\(D_i\\)’s\n\nThe \\(t\\) statistic follows a \\(t_{n-1}\\) distribution if \\(H_0\\) is true: \\[\nt = \\frac{\\bar{D} - d_0}{s_D / \\sqrt{n}}\n\\]\n\\(H_1: \\Delta \\neq d_0\\): \\[\n\\text{p-value} = 2\\texttt{pt}(-|t|, n-1)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\Delta &gt; d_0\\): \\[\n\\text{p-value} = 1 - \\texttt{pt}(t, n-1)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\Delta &lt; d_0\\): \\[\n\\text{p-value} = \\texttt{pt}(t, n-1)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha) \\cdot 100\\%\\) confidence interval for \\(\\Delta\\) is: \\[\n\\bar{D} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s_D}{\\sqrt{n}}\n\\]\n\n\n\n\nPaired t-tests in R\n\n\n\nTwo-sample t-tests with equal variances\n\nMore commonly, studies have 2 independent samples.\nExample:\n\nCollect one group of OC users\n\nCollect a separate group of non-OC users\n\nCross-sectional study: data collected at one point in time (units under different conditions)\nAssume: \\[\nX_1, X_2, \\ldots, X_{n_1} \\overset{\\text{iid}}{\\sim} N(\\mu_1, \\sigma_1^2)\n\\] \\[\nY_1, Y_2, \\ldots, Y_{n_2} \\overset{\\text{iid}}{\\sim} N(\\mu_2, \\sigma_2^2)\n\\]\nNote: different sample sizes are possible, and the observations are not paired.\nHypotheses:\n\n\\(H_0\\): \\(\\mu_1 = \\mu_2\\)\n\\(H_1\\): \\(\\mu_1 \\ne \\mu_2\\), or \\(\\mu_1 &lt; \\mu_2\\), or \\(\\mu_1 &gt; \\mu_2\\)\n\nFor now, assume \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\)\n\nAssumes the two populations have the same variability, which is often not valid.\nWe will relax this later\n\nWe observe:\n\n\\(\\bar{X} =\\) mean of \\(X_i\\)’s\n\n\\(\\bar{Y} =\\) mean of \\(Y_i\\)’s\n\n\\(s_1^2 =\\) sample variance of \\(X_i\\)’s\n\n\\(s_2^2 =\\) sample variance of \\(Y_i\\)’s\n\n\\(n_1 =\\) sample size 1\n\n\\(n_2 =\\) sample size 2\n\nConsider \\(\\bar{X} - \\bar{Y}\\)\n\\[\nE[\\bar{X} - \\bar{Y}] = E[\\bar{X}] - E[\\bar{Y}] = \\mu_1 - \\mu_2\n\\]\n\nEquals 0 under \\(H_0\\), not 0 under \\(H_1\\)\n\nVariance of difference:\n\\[\\begin{align*}\n\\text{Var}(\\bar{X} - \\bar{Y}) &= \\text{Var}(\\bar{X}) + \\text{Var}(\\bar{Y}) - 2\\,\\text{Cov}(\\bar{X}, \\bar{Y})\\\\\n&= \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2} \\\\\n&= \\sigma^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\n\\end{align*}\\] (Covariance term is 0 due to independence)\nTherefore, by properties of the normal distribution: \\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\right)\n\\]\nIf \\(\\sigma^2\\) were known, then could base our test on the distribution of the mean divided by the standard deviation:\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\n\\]\nWe could then compare this statistic to a \\(N(0,1)\\) distribution to get p-value.\nHowever, \\(\\sigma^2\\) is never known in practice, so we need to estimate it.\n\n\n\n\n\n\n\nTipPooled Sample Variance\n\n\n\n\nAssuming \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\), then we estimate \\(\\sigma^2\\) with the pooled sample variance:\n\n\\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\n\n\nThe pooled sample variance can equivalently be written as:\n\\[\ns^2 = \\frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \\frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2\n\\]\n\nThis should show you that higher weight goes to the sample with larger \\(n\\)\n\nOur test statistic becomes:\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nThis follows a \\(t_{n_1 + n_2 - 2}\\) distribution only if \\(H_0\\) is true.\n\nIt follows something else if \\(H_1\\) is true.\n\n\n\n\n\n\n\n\nImportantTwo-sample t-test with Equal Variances\n\n\n\nInput: \\(X_1, X_2, \\ldots, X_{n_1}\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) (sample sizes might be different)\n\nWe assume \\(X_i \\sim N(\\mu_1, \\sigma^2)\\) and \\(Y_i \\sim N(\\mu_2, \\sigma^2)\\).\n\nEqual variances, possibly different means\n\nWe test:\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(H_A: \\mu_1 \\neq \\mu_2\\) or \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\).\n\nWe calculate:\n\n\\(\\bar{X}\\), \\(\\bar{Y}\\), and the pooled sample variance \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nThe \\(t\\) statistic follows a \\(t_{n_1 + n_2 - 2}\\) distribution if \\(H_0\\) is true: \\[\nt = \\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t_{n_1 + n_2 - 2}\n\\]\n\\(H_1: \\mu_1 \\neq \\mu_2\\): \\[\n\\text{p-value} = 2\\texttt{pt}(-|t|, n_1 + n_2 - 2)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &gt; \\mu_2\\): \\[\n\\text{p-value} = 1 - \\texttt{pt}(t, n_1 + n_2 - 2)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &lt; \\mu_2\\): \\[\n\\text{p-value} = \\texttt{pt}(t, n_1 + n_2 - 2)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha)100\\%\\) confidence interval is\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\n\n\n\n\nTwo-sample t-test in R, equal variance\n\n\n\nTest for Equal Variances\n\nBecause of the equal variance assumption above, folks have developed statistical tests for whether the variances are indeed equal.\nLet: \\[\nX_i \\sim N(\\mu_1, \\sigma_1^2)\n\\]\n\\[\nY_i \\sim N(\\mu_2, \\sigma_2^2)\n\\]\nHypotheses:\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\n\\(H_1\\): \\(\\sigma_1^2 \\ne \\sigma_2^2\\)\n\nThe test is based on \\(s_1^2\\) and \\(s_2^2\\).\n\nIf they are very different, this provides evidence that \\(\\sigma_1^2 \\neq \\sigma_2^2\\).\n\nNobody does this in real life because:\n\nVery sensitive to non-normality. In contrast, the \\(t\\)-test is not sensitive because of the CLT\nThe equal variance \\(t\\)-test is robust to violations in equal variance assumption.\nNobody assumes equal variances anyway because they all use Welch’s  2-sample \\(t\\)-test (§8.7)\n\nIf your boss asks you to test for equal variances, use var.test()\n\n\nTest for equal variance in R\n\n\n\nTwo-sample t-test with unequal variances\n\nWe’ll now relax the equal variance assumption\n\nThis is a mathy way to say that we won’t assume equal variances.\n\nOur approach is called Welch’s  t-test\nAlways use this unless you know for sure that the variances are equal.\nLet:\n\n\\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) with sample size \\(n_1\\)\n\n\\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\) with sample size \\(n_2\\)\n\nThen:\n\n\\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2,\\ \\frac{1}{n_1} \\sigma_1^2 + \\frac{1}{n_2} \\sigma_2^2\\right)\n\\]\n\nThe test statistic is the mean divided by the estimated standard error\n\\[\nt = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}}\n\\]\nThis is approximately \\(t_\\nu\\) if \\(H_0\\) is true\nThe degrees of freedom \\(\\nu\\) for the null distribution is a weird thing called the Satterthwaite approximation:\n\\[\n\\nu = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1 - 1} + \\frac{(s_2^2/n_2)^2}{n_2 - 1}}\n\\]\n\nYou don’t need to remember this.\nThis \\(\\nu\\) is just to make the \\(t_\\nu\\) distribution as close as possible to the actual null distribution of test statistic.\n\n\n\n\n\n\n\n\nImportantTwo-sample t-test with Equal Variances\n\n\n\nInput: \\(X_1, X_2, \\ldots, X_{n_1}\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) (sample sizes might be different)\n\nWe assume \\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\).\n\nPossibly unequal variances, possibly different means\n\nWe test:\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(H_A: \\mu_1 \\neq \\mu_2\\) or \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\).\n\nWe calculate:\n\n\\(\\bar{X}\\), \\(\\bar{Y}\\), \\(s_1^2\\), and \\(s_2^2\\)\n\nThe \\(t\\) statistic follows a \\(t_{\\nu}\\) distribution if \\(H_0\\) is true: \\[\nt = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}}\n\\]\n\\(\\nu\\) is derived from Satterthwaite’s Equation.\n\\(H_1: \\mu_1 \\neq \\mu_2\\): \\[\n\\text{p-value} = 2\\texttt{pt}(-|t|, \\nu)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &gt; \\mu_2\\): \\[\n\\text{p-value} = 1 - \\texttt{pt}(t, \\nu)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &lt; \\mu_2\\): \\[\n\\text{p-value} = \\texttt{pt}(t, \\nu)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha)100\\%\\) confidence interval is\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{\\nu,\\ 1 - \\alpha/2} \\cdot \\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}\n\\]\n\n\n\n\nTwo-sample t-test in R, equal variance\n\n\n\nSample Size and Power for 2-Sample t-tests\n\nIdea: Given\n\n\\(\\mu_1 - \\mu_2\\) (effect size)\n\n\\(\\sigma_1^2\\) (variance of sample 1)\n\n\\(\\sigma_2^2\\) (variance of sample 2)\n\n\\(\\alpha\\) (significance level)\n\n\\(n_1\\) (sample size 1)\n\n\\(n_2\\) (sample size 2)\n\nThen we can calculate power \\(1 - \\beta\\) using similar methods as before.\nTo get \\(n_1\\), \\(n_2\\), assume \\(n_2 = k n_1\\) for known \\(k\\)\n\nE.g., we know we have equal sample sizes, or we know group 1 will have twice as many folks, etc.\nTotal sample size: \\(n_1 + k n_1 = n_1 \\left(1 + k\\right)\\)\nYou then have an equation of the form \\(g(n_1) = 1-\\beta\\), where \\(g(\\cdot)\\) is a function that gets the power given a known sample size.\nTo get the sample size given power, you can solve for \\(n_1\\).\n\n\n\nPower in R\n\nSkip Section 8.10\n\n\nAssumptions of t-methods\nThere are three assumptions, in decreasing order of importance (first is most important):\n\nIndependence\n\nCheck by thinking about sampling design\n\nDid you measure units in clusters (e.g. all from the same family)\nDid you measure the same units over time\n\nIf violated, use more complicated methods\n\nAnova, multiple linear regression, longitudinal approaches\n\n\nEqual variance\n\nNot required for Welch’s  t-test\n\nJust use Welch’s  t-test\n\nNormality\n\nViolated if there are outliers or skew\nCheck using:\n\nHistograms\nBoxplots\nQQ-plots (more on this later)\n\nOnly a big deal if \\(n\\) is small (e.g., \\(&lt; 50\\)) and there are lots of skew/outliers\n\n\n\nNote: Need normality within each group, not for the pooled or marginal distribution.\n\n\n\n\n\n\n\n\n\n\nMarginal distribution is not normal → that’s okay\n\nCheck by histograms and Q–Q plots in each group.\n\n\nIf normality is violated:\n\nIf all values are \\(&gt; 0\\), try logging the \\(X_i\\)’s\nIf all values are \\(\\geq 0\\), try taking the square root of the \\(X_i\\)’s\nRemove outliers and report both results\nUse a nonparametric method (see Chapter 9)\n\n\n\n\nExercises 8.3–8.11\nThe mean ±1 sd of ln [calcium intake (mg)] among 25 females, 12 to 14 years of age, below the poverty level is 6.56 ± 0.64. Similarly, the mean ± 1 sd of ln [calcium intake (mg)] among 40 females, 12 to 14 years of age, above the poverty level is 6.80 ± 0.76.\n\nExercise 8.3Solution\n\n\nWhat is the appropriate procedure to test for a significant difference in means between the two groups?\n\n\nA two-sample t-test. Equal variance would probably be OK, but why not just use Welch ?\nI think Rosner has you run a test for equal variance to determine if we assume equal variances or not. But don’t do this. In real life, just run Welch .\nBut it’s easier for me to test the equal variance approach. So in subsequent problems, assume the variances are equal.\n\n\n\n\nExercise 8.4HintSolution\n\n\nImplement the procedure in Problem 8.3 using the critical-value method at significance level 0.1.\n\n\n\n\\(\\bar{X} = 6.56\\)\n\\(\\bar{Y} = 6.80\\)\n\\(s_1^2 = 0.64^2 = 0.4096\\)\n\\(s_2^2 = 0.76^2 - 0.5776\\)\n\\(n_1 = 25\\)\n\\(n_2 = 40\\)\n\nFirst calculate the pooled sample variance.\n\n\nWe assume\n\n\\(X_1,\\ldots,X_{25} \\sim N(\\mu_1,\\sigma^2)\\)\n\\(Y_1,\\ldots,Y_{40} \\sim N(\\mu_2,\\sigma^2)\\)\n\nWe are testing\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(H_1: \\mu_1 \\neq \\mu_2\\)\n\nWe need the the pooled sample variance:\n\n((25 - 1) * 0.64^2 + (40 - 1) * 0.76^2) / (25 + 40 - 2)\n\n[1] 0.5136\n\n\nThe \\(t\\)-statistic is then\n\n(6.56 - 6.8) / (sqrt(.5136) * sqrt(1 / 25 + 1/40))\n\n[1] -1.314\n\n\nWe compare this to a \\(t\\) distribution with 25 + 40 - 2 = 63 degrees of freedom. We calculate the \\(1-\\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the \\(t\\) distribution (it’s \\(\\alpha/2\\) because it’s a two-sided test) to get\n\nqt(p = 0.95, df = 63)\n\n[1] 1.669\n\n\nSince \\(|-1.314| = 1.314 &lt; 1.669\\), we fail to reject the null hypothesis at significance level 0.1 and conclude that we do not have evidence to say that females above and below the poverty level have different mean calcium intakes.\n\n\n\n\nExercise 8.5HintSolution\n\n\nWhat is the p-value corresponding to your answer to Problem 8.4?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe compare the t-statistic of -1.314 for a \\(t\\) distribution with 25 + 40 - 2 = 63 degrees of freedom. Since this is a two-sided test, we need the area in both tails.\n\n2 * pt(-1.314, df = 63)\n\n[1] 0.1936\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.6HintSolution\n\n\nCompute a 95% CI for the difference in means between the two groups.\n\n\nEstimate \\(\\pm\\) Multiplier \\(\\times\\) Standard Error\nWe already calculate the pooled sample variance. You need the \\(1 - \\alpha/2\\) quantile of the \\(t\\) distribution with \\(n_1 + n_2 - 2\\) degrees of freedom, which you can get with qt().\n\n\nWe calculate, for \\(\\alpha = 0.05\\): \\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nWe already calculated the pooled sample variance as \\(s^2 = 0.5136\\). Thus We also have that the 1 - 0.05/2 = 0.975 quantile of the t distribution with 25 + 40 -2 = 63 degrees of freedom is\n\nqt(0.975, df = 63)\n\n[1] 1.998\n\n\nThus, our interval is:\n\n6.56 - 6.8 - 1.998 * sqrt(0.5136) * sqrt(1/25 + 1/40)\n\n[1] -0.6051\n\n6.56 - 6.8 + 1.998 * sqrt(0.5136) * sqrt(1/25 + 1/40)\n\n[1] 0.1251\n\n\n\n\n\n\nExercise 8.7HintSolution\n\n\nSuppose an equal number of 12- to 14-year-old girls below and above the poverty level are recruited to study differences in calcium intake. How many girls should be recruited to have an 80% chance of detecting a significant difference using a two-sided test with α = .05?\n\n\nUse power.t.test(). Use the estimates above for your effect size and standard deviation.\n\n\nWe’ll use the estimates above as our “wild guess” for the effect size \\[\n\\bar{X} - \\bar{Y} = 6.56 - 6.8 = -0.24\n\\] and the estimated standard deviation comes from the pooled sample variance \\[\n\\sqrt{0.5136} = 0.7167\n\\]\n\npower.t.test(\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  power = 0.8,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 141\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThat means we need at least 141 individual per group.\n\n\n\n\nExercise 8.8HintSolution\n\n\nAnswer Problem 8.7 if a one-sided rather than a two-sided test is used.\n\n\nIt’s just one change in power.t.test() from 8.7.\n\n\n\npower.t.test(\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  power = 0.8,\n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 111\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nSo we need at least 111 individuals.\n\n\n\n\nExercise 8.10HintSolution\n\n\nSuppose 50 girls above the poverty level and 50 girls below the poverty level are recruited for the study. How much power will the study have of finding a significant difference using a two-sided test with α = .05, assuming that the population parameters are the same as the sample estimates in Problem 8.2?\n\n\nJust give power.t.test() n instead of power.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.3813\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power will be 0.3813.\n\n\n\n\nExercise 8.11HintSolution\n\n\nAnswer Problem 8.10 assuming a one-sided rather than a two-sided test is used.\n\n\nJust change one thing from power.t.test() in problem 8.10.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.5071\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nThe power will be 0.5071."
  },
  {
    "objectID": "05_tests/08_notes.html#paired-t-test",
    "href": "05_tests/08_notes.html#paired-t-test",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Paired t-test",
    "text": "Paired t-test\n\nLet\n\\(X_i \\sim N(\\mu_i, \\sigma^2)\\) for example, pre-OC\n\\(Y_i \\sim N(\\mu_i + \\Delta, \\sigma^2)\\) for example, post-OC\nHypotheses:\n\\(H_0\\): \\(\\Delta = 0\\)\n\\(H_1\\): \\(\\Delta \\ne 0\\)\nThis tests if there is a difference between populations while allowing each pair to have their own baseline mean \\(\\mu_i\\).\nDefine differences:\n\\(D_i = Y_i - X_i \\sim N(\\Delta, \\sigma_D^2)\\)\nVariance of differences:\n\\(\\sigma_D^2 = \\text{var}(X) + \\text{var}(Y) - 2\\ \\text{cov}(X, Y)\\)\n\nBut nuisance parameters, so just call it \\(\\sigma_D^2\\)\n\nSo, just use a one-sample t-test on \\(D_i\\)\nA paired t-test is just a one-sample t-test on differences.\n\n\\[\nt = \\frac{\\bar{D} - d_0}{s_D / \\sqrt{n}} \\sim t_{n-1}\n\\]\nXYZ IMAGE HERE\n\n\\(d_0\\) = null value\n\n\\(s_D\\) = SD of \\(D_i\\)’s\n\n\\(\\bar{D}\\) = mean of \\(D_i\\)’s\nGet \\((1 - \\alpha) \\cdot 100\\%\\) confidence interval by\n\n\\[\n\\bar{D} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s_D}{\\sqrt{n}}\n\\]\nwhere\n\\(t_{n-1, 1 - \\alpha/2}\\) is the \\(t\\)-quantile with df = \\(n - 1\\)\n(typically \\(\\alpha = 0.05\\))"
  },
  {
    "objectID": "05_tests/08_notes.html#paired-t-tests-independent-samples",
    "href": "05_tests/08_notes.html#paired-t-tests-independent-samples",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Paired t-tests ≠ independent samples",
    "text": "Paired t-tests ≠ independent samples\n\nMore common studies have 2 independent samples.\nExample:\n\nCollect one group of OC users\n\nCollect a separate group of non-OC users\n\nCross-sectional study: data collected at one point in time (units under different conditions)\n\n\\[\nX_1, X_2, \\ldots, X_{n_1} \\overset{\\text{iid}}{\\sim} N(\\mu_1, \\sigma_1^2)\n\\]\n\\[\nY_1, Y_2, \\ldots, Y_{n_2} \\overset{\\text{iid}}{\\sim} N(\\mu_2, \\sigma_2^2)\n\\]\n\nDifferent sample sizes possible, not paired.\nHypotheses:\n\n\\(H_0\\): \\(\\mu_1 = \\mu_2\\)\n\\(H_1\\): \\(\\mu_1 \\ne \\mu_2\\), or \\(\\mu_1 &lt; \\mu_2\\), or \\(\\mu_1 &gt; \\mu_2\\)\n\nFor now, assume \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\) (i.e., equal/pooled variance)\nWe observe:\n\n\\(\\bar{X} =\\) mean of \\(X_i\\)’s\n\n\\(\\bar{Y} =\\) mean of \\(Y_i\\)’s\n\n\\(s_1^2 =\\) sample variance of \\(X_i\\)’s\n\n\\(s_2^2 =\\) sample variance of \\(Y_i\\)’s\n\n\\(n_1 =\\) sample size 1\n\n\\(n_2 =\\) sample size 2\n\nConsider \\(\\bar{X} - \\bar{Y}\\)\n\n\\[\n\\mathbb{E}[\\bar{X} - \\bar{Y}] = \\mathbb{E}[\\bar{X}] - \\mathbb{E}[\\bar{Y}] = \\mu_1 - \\mu_2\n\\]\n\nEquals 0 under \\(H_0\\), not 0 under \\(H_1\\)\nVariance of difference:\n\n\\[\n\\text{Var}(\\bar{X} - \\bar{Y}) = \\text{Var}(\\bar{X}) + \\text{Var}(\\bar{Y}) - 2\\,\\text{Cov}(\\bar{X}, \\bar{Y})\n\\]\n\nCovariance term is 0 due to independence\n\n\\[\n= \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2} = \\sigma^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\n\\]\n\nTherefore,\n\n\\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\right)\n\\]\n\nIf \\(\\sigma^2\\) were known, then could test using\n\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\n\\]\n\nIf \\(H_0\\) true, compare stat to \\(N(0,1)\\) to get p-value\nAssuming \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\), estimate \\(\\sigma^2\\) with the pooled sample variance:\n\n\\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nEquivalent to\n\n\\[\ns^2 = \\frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \\frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2\n\\]\n\nHigher weight goes to sample with larger \\(n\\)\nTest statistic becomes:\n\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t_{n_1 + n_2 - 2}\n\\]\n\nIf \\(H_0\\) true\nLet:\n\n\\[\nX_1, X_2, \\ldots, X_{n_1} \\sim N(\\mu_1, \\sigma^2)\n\\]\n\\[\nY_1, Y_2, \\ldots, Y_{n_2} \\sim N(\\mu_2, \\sigma^2)\n\\]\n\nPooled variance:\n\n\\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nTest statistic:\n\n\\[\nt = \\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t_{n_1 + n_2 - 2}\n\\]\nXYZ IMAGE HERE\n\nCalculate area in only one tail if alternative is one-sided.\n\\((1 - \\alpha) \\cdot 100\\%\\) confidence interval:\n\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nwhere\n\\(t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2}\\) is the \\(t\\)-quantile with df = \\(n_1 + n_2 - 2\\)\n\nt-test in R\n\n\n§8.6: Test for Equal Variances\n\n\\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\)\n\\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\)\nHypotheses:\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\n\\(H_1\\): \\(\\sigma_1^2 \\ne \\sigma_2^2\\)\n\nRun a test based on \\(s_1^2\\), \\(s_2^2\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nNobody does this in real life because:\n\n\n\nVery sensitive to non-normality — \\(t\\)-test is not sensitive because of the CLT\nEqual variance \\(t\\)-test is robust to violations in equal variance assumption\nNobody assumes equal variances anyway because they all use Welch’s 2-sample \\(t\\)-test (§8.7)\n\n\nIf your boss asks you to test for equal variances, use var.test()\n\n\nF-stat in R"
  },
  {
    "objectID": "05_tests/08_notes.html#two-sample-t-test-with-unequal-variances",
    "href": "05_tests/08_notes.html#two-sample-t-test-with-unequal-variances",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Two-sample t-test with unequal variances",
    "text": "Two-sample t-test with unequal variances\n\nAlways use this unless you know for sure that the variances are equal.\nLet:\n\n\\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) with sample size \\(n_1\\)\n\n\\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\) with sample size \\(n_2\\)\n\nThen:\n\n\\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2,\\ \\frac{1}{n_1} \\sigma_1^2 + \\frac{1}{n_2} \\sigma_2^2\\right)\n\\]\n\nTest statistic:\n\n\\[\nt = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}} = \\frac{\\text{effect}}{\\text{SE}}\n\\]\n\nApproximately \\(t_\\nu\\) if \\(H_0\\) is true\nDegrees of freedom \\(\\nu\\) is a weird thing called the Satterthwaite approximation:\n\n\\[\n\\nu = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1 - 1} + \\frac{(s_2^2/n_2)^2}{n_2 - 1}}\n\\]\n\nYou don’t need to remember this.\n\nXYZ IMAGE HERE\n\nUse this \\(\\nu\\) to make the \\(t\\) as close to the actual distribution of \\(t\\) as possible.\nConfidence interval:\n\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{\\nu,\\ 1 - \\alpha/2} \\cdot \\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}\n\\]\n\nt-tests in R"
  },
  {
    "objectID": "05_tests/08_notes.html#sample-size-and-power-for-2-sample-t-tests",
    "href": "05_tests/08_notes.html#sample-size-and-power-for-2-sample-t-tests",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Sample Size and Power for 2-Sample t-tests",
    "text": "Sample Size and Power for 2-Sample t-tests\n\nIdea: Given\n\n\\(\\mu_1 - \\mu_2\\) (effect)\n\n\\(\\sigma_1^2\\) (variance of sample 1)\n\n\\(\\sigma_2^2\\) (variance of sample 2)\n\n\\(\\alpha\\) (significance level)\n\n\\(n_1\\) (sample size 1)\n\n\\(n_2\\) (sample size 2)\n\nCan calculate power \\(1 - \\beta\\) using similar methods as before.\nTo get \\(n_1\\), \\(n_2\\), assume \\(n_2 = k n_1\\) for known \\(k\\)\n(i.e., known equal sample sizes or group 1 will have twice as many folks)\nThen solve for \\(n_1\\) given a fixed power.\nTotal sample size:\n\n\\[\n\\text{Sample Size} = n_1 + k n_1 = n_1 \\left(1 + k\\right)\n\\]\n\nPower in R\n\nSkip Section 8.10"
  },
  {
    "objectID": "05_tests/08_notes.html#assumptions-of-t-methods",
    "href": "05_tests/08_notes.html#assumptions-of-t-methods",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Assumptions of t-methods",
    "text": "Assumptions of t-methods\n\nIndependence\n\nCheck by thinking about sampling design\n\nIf violated, use more complicated methods\n\nEqual variance\n\nNot required for Welch’s t-test\n\nJust use Welch’s t-test\n\nNormality\n\nCheck using:\n\nHistograms\nBoxplots\nShapiro test\n\nOnly a big deal if \\(n\\) is small (e.g., \\(&lt; 50\\)) and there are lots of skew/outliers\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote: Need normality within each group, not for the pooled or marginal distribution.\n\n\nXYZ IMAGE HERE (Boxplots and density plots for group 1 and group 2)\nMarginal distribution is not normal → that’s okay\n\nCheck by histograms and Q–Q plots (more later) in each group.\nIf violated:\n\n\nIf all \\(&gt; 0\\), try logging \\(X_i\\)’s\nRemove outliers and report both results\nUse a nonparametric method (see Chapter 9)"
  },
  {
    "objectID": "06_nonparametric/09_notes.html",
    "href": "06_nonparametric/09_notes.html",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "",
    "text": "\\(t\\)-tests assume Normality\n\nMaking distributional assumpitons is called “parametric”, because it uses parameters (like \\(\\mu\\) and \\(\\sigma^2\\))\nBut it only makes this assumption for small \\(n\\) (we can rely on the CLT for large \\(n\\))\n\nWhat if you have small \\(n\\) and non-normal data?\n\nUse non-parametric methods (work for all data, not just normal)\n\nAlso, some variables are not on a numeric scale where differences are meaningful, and so the \\(t\\) methods cannot be meaningfully applied.\n\n\n\n\n\n\n\nTipCardinal variable\n\n\n\nA numeric variable where distances between points make sense\n\n\n\nExamples:\n\nBody weight\nSerum creatine levels\nSystolic blood pressure\n\n\n\n\n\n\n\n\nTipOrdinal Variable\n\n\n\nA variable where order matters, but not the specific numbers\n\n\n\nExamples:\n\nVisual acuity: 20–20 vs. 20–30 vs. 20–40\nLikert scales (1 = very strongly disagree, 2 = disagree, …)\n\nMeans and variances not meaningful for ordinal data.\n\n\n\n\n\n\n\nTipNominal Variable\n\n\n\nA categorical variable with no ordering\n\n\n\nExamples:\n\nDeath classification (cancer, cardiovascular disease, …)\nEthnicity\nHair color\n\nMethods described in Chapter 9 are for\n\nOrdinal data, or\nCardinal data with normality violations.\n\nWe’ll talk about analyzing nominal data in Chapter 10."
  },
  {
    "objectID": "06_nonparametric/09_notes.html#sign-test",
    "href": "06_nonparametric/09_notes.html#sign-test",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "Sign Test",
    "text": "Sign Test\n\nThe Sign test is a non-parametric version of the paired \\(t\\)-test\nWherever you would use a paired \\(t\\)-test, you can instead use a sign test.\n\nIt is very robust to assumptions\nIt has very low power, so use a one-sample t-test unless your data don’t allow for a normality assumption.\n\nSuppose we just know (or use) that for two paired observations \\((A, B)\\), that either\n\\(A &gt; B\\), \\(A &lt; B\\), or \\(A = B\\)\nExample: Two ointments (A and B). Randomly apply one to left arm and the other to right. See which ointment produces more redness for each person.\nFor \\(n = 45\\), suppose we saw:\n\n22 with \\(A &lt; B\\)\n\n18 with \\(A &gt; B\\)\n\n5 with \\(A = B\\)\n\n\\(X_i\\) = redness on arm A\n\\(Y_i\\) = redness on arm B\n\\(d_i = X_i - Y_i\\)\n\\(\\Delta = \\text{median}(d_i)\\)\n\npopulation median, not sample median\n\n\\(H_0\\): \\(\\Delta = 0\\)\n\n\\(H_A\\): \\(\\Delta \\ne 0\\)\n\\(d_i\\) not observed, only observe whether \\(d_i &gt; 0\\) (A &gt; B), \\(d_i &lt; 0\\) (A &lt; B), or \\(d_i = 0\\) (A = B)\nLet \\(n = \\#(d_i &gt; 0) + \\#(d_i &lt; 0)\\)\n\nexclude \\(d_i = 0\\)\n\nLet \\(X = \\#(d_i &gt; 0)\\)\nIf \\(H_0\\) were true, \\(X \\sim \\text{Binom}\\left(n, \\frac{1}{2}\\right)\\)\nWhy: “success” = \\(d_i &gt; 0\\)?\n\\[\\begin{align*}\n\\mathbb{P}(\\text{success}) &= \\mathbb{P}(d_i &gt; 0) \\\\\n&= \\mathbb{P}(d_i &gt; \\text{median}(d_i)) \\text{ (if $H_0: \\Delta = 0$ true)}\\\\\n&= \\frac{1}{2} \\text{ (definition of median)}\n\\end{align*}\\]\nSo just use binomial methods on the sign of the differences (normal or exact)\n\n\n\n\n\n\n\nImportantSign Test\n\n\n\nLet \\(X = \\#(A &gt; B)\\) and \\(n = \\#(A&gt;B) + \\#(A&lt;B)\\)\n\n\\(H_0\\): \\(\\Pr(A &gt; B) = \\Pr(A &lt; B)\\)\nIf \\(H_0\\) is true, then \\[\nX | n \\sim \\mathrm{Binom(n, 1/2)}\n\\]\nThe sign test is equivalent to assuming \\(X \\sim \\mathrm{Binom}(n, p)\\) and testing \\(H_0: p = 1/2\\).\nUse Binomial methods (exact or normal).\n\n\n\n\nExample: If \\(X \\sim \\text{Binom}\\left(n, \\frac{1}{2}\\right)\\) (the null is true), then \\[\n\\frac{X}{n} \\sim N(p, p(1-p)/n) = N\\left(\\frac{1}{2}, \\frac{1/2(1-1/2)}{n}\\right) = N\\left(\\frac{1}{2}, \\frac{1}{4n}\\right)\n\\]\nPlugging in \\(X = 22\\) and \\(n = 40\\), we calculate our \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{p} - 1/2 - 1/(2n)}{\\sqrt{1/2(1-1/2)/n}} = \\frac{22/40 - 1/2 - 1/80}{\\sqrt{1/(40 \\times 4)}}\n\\]\n\n(22/40 - 1/2 - 1/80) / sqrt(1 / (40 * 4))\n\n[1] 0.4743\n\n\nWe compare this \\(z\\)-statistic to a standard normal distribution\n\n2 * (1 - pnorm(0.4743))\n\n[1] 0.6353\n\n\n\n\n\n\n\n\n\n\n\nThis agrees with the value from prop.test()\n\nprop.test(x = 22, n = 40, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.635\n\n\n\nOr, we can do exact method with binom.test(). Sum all probabilities less probabe than or equally probable to our observed value of \\(X = 22\\):\n\n\n\n\n\n\n\n\n\n\n\nbinom.test(x = 22, n = 40, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.636"
  },
  {
    "objectID": "06_nonparametric/09_notes.html#wilcoxon-signed-rank-test",
    "href": "06_nonparametric/09_notes.html#wilcoxon-signed-rank-test",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "Wilcoxon Signed-Rank Test",
    "text": "Wilcoxon Signed-Rank Test\n\nThe Wilcoxin Signed-Rank test is also an alternative to the paired $t$-test\nIt’s more powerful than the sign test, but makes some additional assumptions, making it slightly less robust.\nIdea: still use \\(d_i\\), but take into account the rank of the magnitudes\nIntution: suppose we observed \\(d_i\\): -10, -7, -6, -5, 1, 2, 3, 4\n\nSince \\(\\#(d_i &gt; 0) = \\#(d_i &lt; 0)\\), so the sign test would fail to reject.\nBut, the negative differences are way more negative. This makes it seem that the median is less than 0.\n\nIdea:\n\nRank observations from \\(1\\) to \\(n\\) in terms of \\(|d_i|\\) (smallest to largest of the absolute values)\nSum ranks such that \\(d_i &gt; 0\\)\n\nExample:\n\n\n\n\\(d_i\\)\n\\(|d_i|\\)\nRank \\(|d_i|\\)\n\n\n\n\n-10\n10\n8\n\n\n-7\n7\n7\n\n\n-6\n6\n6\n\n\n-5\n5\n5\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n3\n3\n\n\n4\n4\n4\n\n\n\n\nYou now sum the ranks of the positive numbers: 1 + 2 + 3 + 4 = 10\n\nLet \\(R\\) be the rank sum (sum of the ranks of the positive numbers).\nIf \\(H_0: \\Delta = 0\\) is true, then, theoretical results are that:\n\\[\\begin{align*}\nE[R] &= \\frac{n(n+1)}{4}\\\\\n\\text{Var}(R) &= \\frac{n(n+1)(2n+1)}{24}\n\\end{align*}\\]\n\\(R \\sim N\\left(E[R], \\text{Var}(R)\\right)\\) for large \\(n\\)\nCompare \\(R\\) to null distribution to get \\(p\\)-value\nExact methods exist when \\(n\\) is small\nVariations exist when there are ties in \\(d_i\\)\nNote: The null is really that \\(\\Delta = 0\\) and \\(d_i\\) are symmetric (though possibly non-normal).\n\nMight reject \\(H_0\\) if \\(\\Delta = 0\\) but \\(d_i\\) are skewed\nSo really only testing \\(H_0: \\Delta = 0\\) if \\(d_i\\) are symmetric (checkable via histograms)\nIf symmatry is not a good assumption, use the sign test.\n\n\n\nWilcoxon signed-rank test in R"
  },
  {
    "objectID": "06_nonparametric/09_notes.html#wilcoxon-rank-sum-test",
    "href": "06_nonparametric/09_notes.html#wilcoxon-rank-sum-test",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "Wilcoxon Rank-Sum Test",
    "text": "Wilcoxon Rank-Sum Test\n\nThis is also called the Mann–Whitney \\(U\\) test.\nIt’s a nonparametric alternative to two-sample \\(t\\)-test\nWe want to test if the distribution is shifted in one group or the other\nLet \\(F_1\\) be the CDF of group 1\n\nLet \\(F_2\\) be the CDF of group 2\nHypotheses:\n\n\\(H_0\\): \\(F_1 = F_2\\)\n\\(H_A\\): \\(F_1(x) = F_2(x + \\Delta)\\) for some \\(\\Delta \\ne 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssume distribution is same in each group except one is shifted over (for \\(H_1\\))\nProcedure: Rank all values (not magnitudes like before)\n\nAdd up ranks in one group\nLet \\(R_1\\) = sum of ranks in group 1\nUnder \\(H_0\\):\n\\[\\begin{align*}\nE[R_1] &= \\frac{n_1(n_1 + n_2 + 1)}{2}\\\\\n\\text{Var}(R_1) &= \\frac{n_1 n_2 (n_1 + n_2 + 1)}{12}\n\\end{align*}\\]\nBy CLT, for large \\(n\\), \\(R_1 \\sim N(E[R_1], \\text{Var}(R_1))\\)\nSo compare to this distribution to get a \\(p\\)-value\n\nExample: \\(X\\) = -3, -1, 0, 1, 3 \\(Y\\) = -2, 2, 4\n\n\n\nSample\nValue\nRank\n\n\n\n\nX\n-3\n1\n\n\nY\n-2\n2\n\n\nX\n-1\n3\n\n\nX\n0\n4\n\n\nX\n1\n5\n\n\nY\n2\n6\n\n\nX\n3\n7\n\n\nY\n4\n8\n\n\n\nSum ranks for group X: \\[\nR_1 = 1 + 3 + 4 + 5 + 7 = 20\n\\]\n\n\n\n\n\n\n\n\n\n\n\nIntuition for when \\(n_1 = n_2\\):\n\nWe expect \\(R_1 \\approx R_2\\) = So if \\(R_1 \\gg R_2\\) or \\(R_1 \\ll R_2\\), then reject \\(H_0\\)\n\nSince \\(R_1 + R_2 = \\sum_{i=1}^{n_1 + n_2} i = \\frac{(n_1 + n_2)(n_1 + n_2 + 1)}{2}\\) just need to look at distribution of \\(R_1\\).\n\nThe sum of \\(R_1\\) and \\(R_2\\) is equal to some number which does not change from sample to sample\nSo if we know \\(R_1\\) then we automatically know \\(R_2\\)\nSo we only need to see one of these.\n\nFor small \\(n\\), an exact distribution of \\(R_1\\) (when \\(H_0\\) is true) is available\nModifications exist when there are ties\nCan use this to compare ordinal data\nExample: Visual acuity for individuals with dominant form of retinitis pigmentosa (RP) vs. visual acuity for sex-linked RP\n\nVisual acuity: 20–20, 20–25, 20–30, …\n\n\n\nWilcoxon Rank-Sum in R"
  },
  {
    "objectID": "06_nonparametric/09_notes.html#introduction",
    "href": "06_nonparametric/09_notes.html#introduction",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "",
    "text": "\\(t\\)-tests assume Normality\n\nMaking distributional assumpitons is called “parametric”, because it uses parameters (like \\(\\mu\\) and \\(\\sigma^2\\))\nBut it only makes this assumption for small \\(n\\) (we can rely on the CLT for large \\(n\\))\n\nWhat if you have small \\(n\\) and non-normal data?\n\nUse non-parametric methods (work for all data, not just normal)\n\nAlso, some variables are not on a numeric scale where differences are meaningful, and so the \\(t\\) methods cannot be meaningfully applied.\n\n\n\n\n\n\n\nTipCardinal variable\n\n\n\nA numeric variable where distances between points make sense\n\n\n\nExamples:\n\nBody weight\nSerum creatine levels\nSystolic blood pressure\n\n\n\n\n\n\n\n\nTipOrdinal Variable\n\n\n\nA variable where order matters, but not the specific numbers\n\n\n\nExamples:\n\nVisual acuity: 20–20 vs. 20–30 vs. 20–40\nLikert scales (1 = very strongly disagree, 2 = disagree, …)\n\nMeans and variances not meaningful for ordinal data.\n\n\n\n\n\n\n\nTipNominal Variable\n\n\n\nA categorical variable with no ordering\n\n\n\nExamples:\n\nDeath classification (cancer, cardiovascular disease, …)\nEthnicity\nHair color\n\nMethods described in Chapter 9 are for\n\nOrdinal data, or\nCardinal data with normality violations.\n\nWe’ll talk about analyzing nominal data in Chapter 10."
  },
  {
    "objectID": "07_cat/10_notes.html",
    "href": "07_cat/10_notes.html",
    "title": "Chapter 10: Categorical Data",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\nTipCategorical Variable\n\n\n\nA variable that groups units into different categories.\n\n\n\nExamples:\n\nOral contraceptive user vs. Non-OC-user\nHas cancer vs. Does not have cancer\n\nThere are lots of inference problems that are interesting using categorical variables:\nExample 1: Test if cancer incidence is the same between OC users and non-OC users.\n\nTest the association between two categorical variables (cancer status and OC status).\n\nExample 2: Test if heavy OC users, light OC users, and non-OC users have the same cancer rates.\n\nSame as example 1 above, but now OC status has three categories instead of 2.\n\nExample 3: Are the observed frequencies below consistent with the theoretical probabilities?\n\n\n\nDiastolic Blood Pressure\nFrequency\nExpected Probability\n\n\n\n\n&lt; 50\n57\n1%\n\n\n≥ 50, &lt; 60\n330\n7%\n\n\n≥ 60, &lt; 70\n2132\n30%\n\n\n≥ 70, &lt; 80\n4584\n62%\n\n\n\n\n\n\nTwo-sample Test for Binomial Proportions\n\nExample: Age at first birth vs. breast cancer incidence.\n\n\n\n\nControl (No Cancer)\nCase (Cancer)\n\n\n\n\nAge at 1st birth ≤ 29 years\n8747\n2537\n\n\nAge at 1st birth ≥ 30 years\n1498\n683\n\n\nTotal\n10245\n3220\n\n\n\nData collection scheme: Chose women with cancer and women without cancer, then measured age of their first births\nGoal: Test if cancer is associated with age.\nTwo populations:\n\nThose with cancer\nThose without cancer\n\nLet:\n\n\\(p_1 = P(\\text{age} \\geq 30 \\mid \\text{cancer})\\)\n\\(p_2 = P(\\text{age} \\geq 30 \\mid \\text{no cancer})\\)\n\nHypotheses:\n\n\\(H_0\\): \\(p_1 = p_2\\) vs. \\(H_1\\): \\(p_1 \\neq p_2\\)\n\nSample proportions:\n\n\\(\\hat{p}_1 = \\frac{683}{3220} = 0.212\\)\n\\(\\hat{p}_2 = \\frac{1498}{10245} = 0.146\\)\n\nIdea: Base test on \\(\\hat{p}_1 - \\hat{p}_2\\).\nLet:\n\n\\(X_1\\) = number of women age \\(\\geq 30\\) with cancer\n\\(X_2\\) = number of women age \\(\\geq 30\\) without cancer\n\n\\(X_1 \\sim \\text{Binom}(n_1, p_1)\\), where \\(n_1 = 3220\\)\n\n\\(\\hat{p}_1 = X_1 / n_1\\)\n\n\\(X_2 \\sim \\text{Binom}(n_2, p_2)\\), where \\(n_2 = 10245\\)\n\n\\(\\hat{p}_2 = X_2 / n_2\\)\n\nWe have very large sample sizes, so we can use normal theory.\n\n\\(\\hat{p}_1 \\sim N\\left(p_1, \\frac{1}{n_1} p_1 (1 - p_1) \\right)\\)\n\\(\\hat{p}_2 \\sim N\\left(p_2, \\frac{1}{n_2} p_2 (1 - p_2) \\right)\\)\n\nUnder, \\(H_0\\), \\(p_1 = p_2 = p\\), so\n\n\\(\\hat{p}_1 \\sim N\\left(p, \\frac{1}{n_1} p (1 - p) \\right)\\)\n\\(\\hat{p}_2 \\sim N\\left(p, \\frac{1}{n_2} p (1 - p) \\right)\\)\n\nThus, \\[\n\\hat{p}_1 - \\hat{p}_2 \\sim N\\left(0, \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)p(1 - p)\\right)\n\\]\nSo we divide by the standard error to get a \\(z\\) statistic that follows a standard normal distribution (but only if \\(H_0\\) is true): \\[\n\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)p(1 - p)}} \\sim N(0,1)\n\\]\nWe estimate \\(p\\) under \\(H_0\\) by noting that, if \\(H_0\\) were true, then \\[\nX_1 + X_2 \\sim \\text{Binom}(n_1 + n_2, p)\n\\] Thus, \\[\n\\hat{p} = \\frac{X_1 + X_2}{n_1 + n_2}\n\\]\nPlugging in \\(\\hat{p}\\) for \\(p\\) we get something that is asymptotically standard normal \\[\n\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\hat{p}(1 - \\hat{p})}} \\sim N(0,1)\n\\]\nNote: don’t use the \\(t\\)-distribution here. That depends on the data actually being normal.\nIn practice, we need to do a continuity correction for better performance.\n\n\n\n\n\n\n\nImportantTwo-sample Binomial Test\n\n\n\nObserve \\(X_1 \\sim \\mathrm{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(n_2, p_2)\\)\n\nNull Hypothesis: \\(H_0: p_1 = p_2\\)\n\nCalculate - \\(\\hat{p}_1 = X_1 / n_1\\) - \\(\\hat{p}_2 = X_2 / n_2\\) - \\(\\hat{p} = (X_1 + X_2) / (n_1 + n_2)\\)\n\n\\(z\\)-statistic \\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\hat{p}(1 - \\hat{p})}}\n\\]\n\\(H_1: p_1 \\neq p_2\\) \\[\n\\text{p-value} = 2 \\times \\texttt{pnorm}(-|z|)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: p_1 &gt; p_2\\) \\[\n\\text{p-value} = 1 - \\texttt{pnorm}(z)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: p_1 &lt; p_2\\) \\[\n\\text{p-value} = \\texttt{pnorm}(z)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha)100\\%\\) confidence interval for the difference is \\[\n\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\alpha/2}\\sqrt{ \\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nRule of thumb: The normal approximation works if \\(n_1\\hat{p}(1-\\hat{p}) \\geq 5\\) and \\(n_2\\hat{p}(1-\\hat{p}) \\geq 5\\)\n\n\n\n\nAge at first birth vs cancer example continued:\n\n\\(X_1 = 683\\), \\(n_1 = 3220\\)\n\\(X_2 = 1498\\), \\(n_2 = 10245\\)\n\\(\\hat{p}_1 = 683/3220 = 0.2121\\)\n\\(\\hat{p}_2 = 1498 / 10245 = 0.1462\\)\n\\(\\hat{p} = (683 + 1498) / (3220 + 10245) = 0.162\\)\nTest statistic: \\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\hat{p}(1 - \\hat{p})}}\n= \\frac{0.2121 - 0.1462}{\\sqrt{\\left( \\frac{1}{3220} + \\frac{1}{10245} \\right)(0.162)(1 - 0.162)}}\n= 8.866\n\\]\np-value:\n\n2 * pnorm(-8.866)\n\n[1] 7.582e-19\n\n\nWe have strong evidence that women with cancer are more likely to have had their first child after age 30.\n\n\n\n2-sample binomial tests in R\n\n\n\nTest for Homogeneity in 2 \\(\\times\\) 2 Tables\n\nWhat I showed before was a \\(2 \\times 2\\) contingency table:\n\n\n\nStatus\nAge ≥ 30\nAge &lt; 30\nRow Total\n\n\n\n\nCase\n683\n2537\n3220\n\n\nControl\n1498\n8747\n10245\n\n\nCol Total\n2181\n11284\n13465\n\n\n\nRow margins and column margins are shown above. The total is the grand total.\nAn equivalent (exact same \\(p\\)-value) method is to test for homogeneity or independence between the two variables.\nSuppose our table looks like this:\n\n\n\n\nZ = A\nZ = B\nRow Total\n\n\n\n\nW = C\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(m_1\\)\n\n\nW = D\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(m_2\\)\n\n\nCol Total\n\\(n_1\\)\n\\(n_2\\)\n\\(N\\)\n\n\n\nDefinitions:\n\n\\(Z\\) is a variable with possible values of \\(A\\) and \\(B\\)\n\\(W\\) is a variable with possible values \\(C\\) and \\(D\\).\n\\(n_1 = X_{11} + X_{21}\\)\n\\(n_2 = X_{12} + X_{22}\\)\n\\(m_1 = X_{11} + X_{12}\\)\n\\(m_2 = X_{21} + X_{22}\\)\n\\(N = n_1 + n_2 = m_1 + m_2\\)\n\nIf \\(Z \\perp W\\), then:\n\n\\(P(Z = A \\cap W = C) = P(Z = A) \\cdot P(W = C)\\)\n\\(P(Z = A \\cap W = D) = P(Z = A) \\cdot P(W = D)\\)\n\\(P(Z = b \\cap W = C) = P(Z = b) \\cdot P(W = C)\\)\n\\(P(Z = b \\cap W = D) = P(Z = b) \\cdot P(W = D)\\)\n\nIdea: Calculate expected counts from the assumption of independence based on the margin totals.\n\n\\(P(Z = A) = \\frac{n_1}{N}\\)\n\\(P(Z = B) = \\frac{n_2}{N}\\)\n\\(P(W = C) = \\frac{m_1}{N}\\)\n\\(P(W = D) = \\frac{m_2}{N}\\)\n\nThe expected count under independence:\n\n\\(E[X_{11} \\mid Z \\perp W] = N \\Pr(Z = A \\cap W = C) = N\\Pr(Z = A)\\Pr(W=C) =  N \\cdot \\frac{n_1}{N} \\cdot \\frac{m_1}{N} = \\frac{n_1 m_1}{N}\\)\n\\(E[X_{12} \\mid Z \\perp W] = N \\Pr(Z = B \\cap W = C) = N\\Pr(Z = B)\\Pr(W=C) =  N \\cdot \\frac{n_2}{N} \\cdot \\frac{m_1}{N} = \\frac{n_2 m_1}{N}\\)\n\\(E[X_{21} \\mid Z \\perp W] = N \\Pr(Z = A \\cap W = D) = N\\Pr(Z = A)\\Pr(W=D) =  N \\cdot \\frac{n_1}{N} \\cdot \\frac{m_2}{N} = \\frac{n_1 m_2}{N}\\)\n\\(E[X_{22} \\mid Z \\perp W] = N \\Pr(Z = B \\cap W = D) = N\\Pr(Z = B)\\Pr(W=D) =  N \\cdot \\frac{n_2}{N} \\cdot \\frac{m_2}{N} = \\frac{n_2 m_2}{N}\\)\n\nCompare \\(X_{ij}\\) to the expected value of \\(X_{ij}\\) if variables were independent.\nExpected counts from age at first birth example:\n\n\n\n\n\n\n\n\n\n≥ 30\n&lt; 30\n\n\n\n\nCase\n\\(\\frac{2181 \\cdot 3220}{13465}\\)\n\\(\\frac{11284 \\cdot 3220}{13465}\\)\n\n\nControl\n\\(\\frac{2181 \\cdot 10245}{13465}\\)\n\\(\\frac{11284 \\cdot 10245}{13465}\\)\n\n\n\nThis results in:\n\n\n\n\n≥ 30\n&lt; 30\n\n\n\n\nCase\n521.6\n2698.4\n\n\nControl\n1659.4\n8585.6\n\n\n\nHow do these compare to observed counts?\nWhenever you compare observed counts to expected counts, do a Pearson \\(\\chi^2\\) test.\n\\[\nX^2 = \\sum_{\\text{categories}} \\frac{(O - E)^2}{E}\n\\]\n\n\\(O\\) = observed count\n\\(E\\) = expected count\n\nIf \\(H_0\\) is true, then \\(X^2 \\sim \\chi^2_\\nu\\)\n\n\\(\\nu\\) = degrees of freedom\n\nFor a test of homogeneity in \\(2 \\times 2\\) tables, \\(\\nu = 1\\)\nBirth example:\n\\[\nX^2 = \\frac{(683 - 521.6)^2}{521.6} + \\frac{(2537 - 2698.4)^2}{2698.4}\n+ \\frac{(1498 - 1659.4)^2}{1659.4} + \\frac{(8747 - 8585.6)^2}{8585.6}\n= 77.89\n\\]\n\n\n\n\n\n\n\n\n\nIn R:\n\n1 - pchisq(77.89, df = 1)\n\n[1] 0\n\n\nIt’s more numerically accurate to use lower.tail = FALSE because R has better precision for numbers close to 0 than for numbers close to 1.\n\npchisq(77.89, df = 1, lower.tail = FALSE)\n\n[1] 1.089e-18\n\n\n\n\nExerciseHintSolution\n\n\nWhat are the expected counts of the following OC use vs. MI table?\n\n\n\nOC Use\nMI Yes\nMI No\nRow Total\n\n\n\n\nYes\n13\n4987\n5000\n\n\nNo\n7\n9993\n10000\n\n\nCol Total\n20\n14980\n15000\n\n\n\n\n\nUse:\n\\[\nE = \\frac{\\text{row total} \\cdot \\text{column total}}{\\text{grand total}}\n\\]\nApply for each cell.\n\n\n\n\n\n\n\n\n\n\nOC Use\nMI Yes\nMI No\n\n\n\n\nYes\n\\(\\frac{20 \\cdot 5000}{15000}\\)\n\\(\\frac{14980 \\cdot 5000}{15000}\\)\n\n\nNo\n\\(\\frac{20 \\cdot 10000}{15000}\\)\n\\(\\frac{14980 \\cdot 10000}{15000}\\)\n\n\n\n\n\n\nOC Use\nMI Yes\nMI No\n\n\n\n\nYes\n6.7\n4993.3\n\n\nNo\n13.3\n9986.7\n\n\n\n\n\n\n\nExerciseSolution:\n\n\nCalculate the \\(\\chi^2\\) statistic from the previous exercise.\n\n\nChi-squared test statistic:\n\\[\nX^2 =\n\\frac{(13 - 6.7)^2}{6.7} +\n\\frac{(4987 - 4993.3)^2}{4993.3} +\n\\frac{(7 - 13.3)^2}{13.3} +\n\\frac{(9993 - 9986.7)^2}{9986.7} = 8.92\n\\]\n\n\n\n\nContingency test perspective in R\n\n\n\nFisher’s Exact Test\n\nWhat if \\(n\\) is small?\nRule of thumb for “small”: Any expected count \\(&lt; 5\\)\n\nIt’s OK if the observed counts are \\(&lt; 5\\).\n\nExample: Salt diet vs. cardiovascular disease (CVD) death\n\n\n\nCause of Death\nHigh Salt\nLow Salt\nRow Total\n\n\n\n\nNon-CVD\n2\n23\n25\n\n\nCVD\n5\n30\n35\n\n\nCol Total\n7\n53\n60\n\n\n\nExpected count for top-left cell: \\[\nE_{11} = \\frac{7 \\cdot 25}{60} = 2.92 &lt; 5\n\\]\n\nUse \\(2.92\\) (not \\(2\\)) to determine normality approximation validity.\n\n\n\n\n\n\n\n\nTipExact Test\n\n\n\nAn exact test is one that controls Type I error for any \\(n\\), not just large \\(n\\).\n\n\n\n\n\n\n\n\nImportantFisher’s Exact Test\n\n\n\n\nFix the margin totals.\nEnumerate all possible tables with those same margin totals.\nEach table has a known probability (under \\(H_0\\)).\nFind how likely our observed table is (if \\(H_0\\) true) by summing over all tables less than or as probable as our observed table.\n\n\n\n\nTo find all tables that maintain margin totals, you can start from your observed table and perform one of these two operations until the operations become impossible (because doing so would create negative counts).\n\\[\n\\pmatrix{x_{11} - 1 & x_{12} + 1 \\\\ x_{21} + 1 & x_{22} - 1}\n\\] or \\[\n\\pmatrix{x_{11} + 1 & x_{12} - 1 \\\\ x_{21} - 1 & x_{22} + 1}\n\\]\nOur observed table is\n\\[\n\\pmatrix{2 & 23 \\\\ 5 & 30}\n\\]\nAll possible \\(2 \\times 2\\) tables with the exact same margins\n\n\n\nTable\nProbability Under Null\n\n\n\n\n\\(\\pmatrix{0 & 25 \\\\ 7 & 28}\\)\n0.017\n\n\n\\(\\pmatrix{1 & 24 \\\\ 6 & 29}\\)\n0.105\n\n\n\\(\\pmatrix{2 & 23 \\\\ 5 & 30}\\)\n0.252 (observed table)\n\n\n\\(\\pmatrix{3 & 22 \\\\ 4 & 31}\\)\n0.312\n\n\n\\(\\pmatrix{4 & 21 \\\\ 3 & 32}\\)\n0.214\n\n\n\\(\\pmatrix{5 & 20 \\\\ 2 & 33}\\)\n0.082\n\n\n\\(\\pmatrix{6 & 19 \\\\ 1 & 34}\\)\n0.016\n\n\n\\(\\pmatrix{7 & 18 \\\\ 0 & 35}\\)\n0.001\n\n\n\nSum all probabilities \\(\\leq\\) the observed one (in bold) to get the p-value.\nThese probabilities come from the hypergeometric distribution (we won’t cover the exact formula).\n\n\nExerciseSolution\n\n\nWhat are the possible tables with fixed margins from the following?\n\\[\n\\pmatrix{2 & 1 \\\\ 2 & 2}\n\\]\n\n\n\nValid tables with these margins:\n\n\\(\\pmatrix{3 & 0 \\\\ 1 & 3}\\)\n\\(\\pmatrix{2 & 1 \\\\ 2 & 2}\\)\n\\(\\pmatrix{1 & 2 \\\\ 3 & 1}\\)\n\\(\\pmatrix{0 & 3 \\\\ 4 & 0}\\)\n\n\n\n\n\n\nFisher Exact Test in R\n\n\n\nMcNemar’s Test\n\nStudy design: Have matched samples, where each unit has two binary variables.\nExample:\n\nVariable 1: Treatment A vs. Treatment B\n\nVariable 2: Survive vs. Not\nChoose a woman to go into A.\nMatch another woman with similar characteristics to go into B.\nObserve survival for both.\n\n“Similar characteristics”: age, weight, clinical condition, etc.\nNaive Way: Treat each individual as an independent unit and put these in a 2 \\(\\times\\) 2 table:\n\n\n\nTreatment\nSurvive\nDie\nRow Total\n\n\n\n\nA\n526\n95\n621\n\n\nB\n515\n106\n621\n\n\nCol Total\n1041\n201\n1242\n\n\n\n\nYou could run a \\(\\chi^2\\) test for homogeneity, but this is wrong — the data are matched, not independent.\n\nInstead, use a matched-pair contingency table.\n\nAbove, each count is a person.\nInstead, each count should be a pair.\n\nCorrect way: Each matched pair is a unit\n\n\n\n\nB Survive\nB Die\nRow Total\n\n\n\n\nA Survive\n510\n16\n526\n\n\nA Die\n5\n90\n95\n\n\nCol Total\n515\n106\n621\n\n\n\n\n\n\n\n\n\n\nImportantNote\n\n\n\nDon’t use \\(\\chi^2\\) on this table because we know observations are associated (matched).\nWe only want to know which treatment does better.\n\n\n\n\n\n\n\n\nTipConcordant pair\n\n\n\nSame outcome (e.g., both survive or both die)\n\n\n\n\n\n\n\n\nTipDiscordant pair\n\n\n\nDifferent outcomes (e.g. one dies, one survives)\n\n\n\nConcordant pairs tell you nothing about which treatment is better.\nIdea: Treatments are the same if the discordant cells have approximately the same counts.\nLet:\n\n\\(X\\) = number of pairs where A survives and B dies\n\\(n\\) = number of discordant pairs\n\n\\[\nX \\sim \\text{Binom}(n, p)\n\\]\nHypotheses:\n\n\\(H_0\\): \\(p = \\frac{1}{2}\\) (treatments A and B equally effective)\n\\(H_1\\): \\(p \\ne \\frac{1}{2}\\)\n\nJust use binomial methods on discordant pairs.\n\nUse normal approximation for large counts\nUse exact test for small counts\n\n\n\nMcNemar’s Test in R\n\n\nExerciseHintSolution\n\n\nHypertension diagnosis: Each person is assessed (i) by trained observer and (ii) by a machine. Each person is a unit and the assessments are matched. Test if they give the same result (on average).\n\n\n\n\nTrained +\nTrained −\n\n\n\n\nMachine +\n3\n7\n\n\nMachine −\n1\n9\n\n\n\n\n\nOnly discordant pairs (7 and 1) matter.\n\n\nLet: - \\(X \\sim \\text{Binom}(8, p)\\) (discordant total = 8) - \\(H_0: p = \\frac{1}{2}\\) vs. \\(H_1: p \\ne \\frac{1}{2}\\)\nRun an exact binomial test in R:\n\nbinom.test(x = 7, n = 8, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  7 and 8\nnumber of successes = 7, number of trials = 8, p-value = 0.07\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4735 0.9968\nsample estimates:\nprobability of success \n                 0.875 \n\n\nThe p-value is 0.07, so we only have weak evidence that the machines and technicians differ.\nCannot use mcnemar.test() here because it uses a normal approximation.\n\n\n\n\nSummary: If you can group individuals into concordant vs. discordant pairs, then use McNemar’s test.\n\nSkip Power calculations §10.5\n\n\nLarger Contingency Tables\n\nInstead of 2 binary variables, we now have 2 categorical variables.\n\nVariable 1 has \\(R\\) levels\n\nVariable 2 has \\(C\\) levels\n\nExample: Cancer vs. Age at First Birth\n\n\n\nAge\n&lt; 20\n20–24\n25–29\n30–34\n≥ 35\nCol Total\n\n\n\n\nCase\n320\n1206\n1011\n463\n220\n3220\n\n\nControl\n1422\n4432\n2893\n1092\n406\n10245\n\n\nRow Total\n1742\n5638\n3904\n1555\n626\n13465\n\n\n\n\\(H_0\\): Cancer status ⟂ Age at first birth\n\n\\(H_1\\): The two variables are related\nUse the same contingency table approach as for \\(2 \\times 2\\) tables:\n\n\nCalculate expected counts assuming independence\n\nCompare to observed counts using \\(\\chi^2\\) statistic: \\[\nX^2 = \\sum_{\\text{cells}} \\frac{(O - E)^2}{E}\n\\]\n\\(X^2 \\sim \\chi^2_\\nu\\) if \\(H_0\\) is true where \\[\n\\nu = (R - 1)(C - 1)\n\\]\n\\(p\\)-value is the upper tail area\n\n\n\n\n\n\n\n\n\n\n\nFor example, from the table above we have\n\n\\(E_{11} = \\frac{1742 \\cdot 3220}{13465} = 416.6\\)\n\\(E_{12} = \\frac{1742 \\cdot 10245}{13465} = 1348.3\\)\nEtc…\n\n\n\nExerciseSolution\n\n\nCompute \\(E_{25}\\):\n\n\n\\[\nE_{25} = \\frac{626 \\cdot 10245}{13465} = 476.3\n\\]\n\n\n\n\nExerciseSolution\n\n\nIt turns out that the \\(\\chi^2\\) statistic is:\n\\[\nX^2 = \\sum_{\\text{cell}} \\frac{(O - E)^2}{E}\n= \\frac{(416.6 - 320)^2}{416.6} + \\cdots + \\frac{(476.3 - 406)^2}{476.3} = 130.3\n\\]\nWhat is the \\(p\\)-value for the test for independence?\n\n\nThe degrees of freedom of the \\(\\chi^2\\) distribution is: \\[\n\\nu = (2 - 1)(5 - 1) = 4\n\\]\nSo the \\(p\\)-value is\n\npchisq(130.3, df = 4, lower.tail = FALSE)\n\n[1] 3.359e-27\n\n\n\n\n\n\nLarger contingency tables in R\n\nSkip \\(\\chi^2\\) test for trend\n\n\n\\(\\chi^2\\) Goodness-of-Fit Test\n\nThe \\(\\chi^2\\) tests of homogeneity are special cases of \\(\\chi^2\\) goodness-of-fit tests.\nLet \\(e\\) be the expected counts under the null hypothesis. Let \\(o\\) be the observed count. Let \\[\nX^2 = \\sum_{\\text{categories}} \\frac{(e - o)^2}{e}.\n\\]\nUnder \\(H_0\\), \\(\\chi^2 \\sim \\chi^2_\\nu\\)\nDegrees of freedom: \\[\n\\nu = \\text{\\# parameters under } H_1 - \\text{\\# parameters under } H_0\n\\]\n\\(H_1\\) has more parameters (is more complex).\nExample: \\(2 \\times 2\\) Table\n\n\n\n\nC\nD\n\n\n\n\nA\n\\(x_{11}\\)\n\\(x_{12}\\)\n\n\nB\n\\(x_{21}\\)\n\\(x_{22}\\)\n\n\n\n\n\\(H_0\\): Independence\n\n2 parameters: \\(P(A)\\) and \\(P(C)\\)\nSince \\(P(B) = 1 - P(A)\\), it is redundant (know \\(P(A)\\) then know \\(P(B)\\)).\nSince \\(P(D) = 1 - P(C)\\), it is redundant (know \\(P(C)\\) then know \\(P(D)\\)).\n\n\\(H_1\\): Association\n\n3 parameters: \\(P(A \\cap C)\\), \\(P(A \\cap D)\\), and \\(P(B \\cap C)\\)\nSince \\(P(B \\cap D) = 1 - \\left[ P(A \\cap C) + P(A \\cap D) + P(B \\cap C) \\right]\\), it is redundant.\n\nSo: \\[\n\\nu = 3 - 2 = 1\n\\]\n\n\n\n\nCohen’s Kappa\n\nWant a measure of how reliable a test is\nOr measure how similar two judges rate something\nExample: 2 surveys measuring beef consumption\n\n\n\n\n\n\n\n\n\n\nSurvey 2: ≤ 1 serving/week\nSurvey 2: &gt; 1 serving/week\nRow Total\n\n\n\n\nSurvey 1: ≤ 1 serving/week\n136\n92\n228\n\n\nSurvey 1: &gt; 1 serving/week\n69\n240\n304\n\n\nCol Total\n205\n332\n537\n\n\n\nIdea: The “amount” of concordance reflects the reliability of the survey\nObserved proportion concordant: \\[\np_o = \\frac{136 + 240}{537} = 0.7\n\\]\nExpected concordance under independence: \\[\np_e = \\frac{205}{537} \\cdot \\frac{228}{537} + \\frac{332}{537} \\cdot \\frac{304}{537} = 0.518\n\\]\n\n\n\n\n\n\n\nTipCohen’s kappa\n\n\n\n\\[\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n\\]\n\n\n\nProperties:\n\nBounds: \\[\n\\frac{-p_e}{1 - p_e} \\leq \\kappa \\leq 1\n\\]\n\nSet \\(p_o = 0\\) or \\(1\\) to prove this.\n\n\\(\\kappa = 1\\) implies perfect concordance.\nRules of thumb:\n\n\\(\\kappa &gt; 0.75\\) → excellent reproducibility\n\\(0.4 &lt; \\kappa \\leq 0.75\\) → good reproducibility\n\\(0 \\leq \\kappa \\leq 0.4\\) → marginal reproducibility\n\n\nConfidence intervals and tests for \\(\\kappa\\) are possible.\nUse \\(\\kappa\\) for repeated measures of the same variable.\nFor two different variables, use sensitivity and specificity, not \\(\\kappa\\)\n\n\nCohen’s Kappa in R\n\n\n\nExercises 10.8–10.12\nTwo drugs (A, B) are compared for the medical treatment of duodenal ulcer. For this purpose, patients are carefully matched with regard to age, gender, and clinical condition. The treatment results based on 200 matched pairs show that for 89 matched pairs both treatments are effective; for 90 matched pairs both treatments are ineffective; for 5 matched pairs drug A is effective, whereas drug B is ineffective; and for 16 matched pairs drug B is effective, whereas drug A is ineffective.\n\nExercise 10.8Solution\n\n\nWhat test procedure can be used to assess the results?\n\n\nMcNemar’s test. Since we have 5 + 16 = 21 discordant pairs, and 21/4 = 5.25 \\(\\geq\\) 5, we can use either the normal approximation approach or the exact approach.\n\n\n\n\nExercise 10.9Solution\n\n\nPerform the test in Problem 10.8, and report a p-value.\n\n\n\nprop.test(x = 5, n = 21, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0291\n\n\nWe have evidence that there is a difference between drugs (p = 0.0291).\n\n\n\nIn the same study, if the focus is on the 100 matched pairs consisting of male patients, then the following results are obtained: for 52 matched pairs both drugs are effective; for 35 matched pairs both drugs are ineffective; for 4 matched pairs drug A is effective, whereas drug B is ineffective; and for 9 matched pairs drug B is effective, whereas drug A is ineffective.\n\nExercise 10.10Solution\n\n\nHow many concordant pairs are there among the male matched pairs?\n\n\n52 + 35 = 87 concordant pairs\n\n\n\n\nExercise 10.11Solution\n\n\nHow many discordant pairs are there among the male matched pairs?\n\n\n9 + 4 = 13 discordant pairs.\n\n\n\n\nExercise 10.12Solution\n\n\nPerform a significance test to assess any differences in effectiveness between the drugs among males. Report a p-value.\n\n\nSince 13/4 = 3.25 &lt; 5, we should use the exact approach.\n\nbinom.test(x = 4, n = 13, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.267\n\n\nWe do not have any evidence that the two drugs differ in males (p = 0.2668).\n\n\n\n\n\nExercise 10.13\nSuppose researchers do an epidemiologic investigation of people entering a sexually transmitted disease clinic. They find that 160 of 200 patients who are diagnosed as having gonorrhea and 50 of 105 patients who are diagnosed as having nongonococcal urethritis have had previous episodes of urethritis.\nTry doing it “by hand” and in R.\n\nExercise 10.13Solution\n\n\nAre the present diagnosis and prior episodes of urethritis associated? Use the two-sample binomial approach.\n\n\nLet \\(X_1\\) be the number of gonorrhea patients (out of 200) who have had previous episodes of urethritis.\nLet \\(X_2\\) be the number of nongonococcal urethritis patients (out of 105) who have had previous episodes of urethritis.\nWe have \\(X_1 \\sim \\mathrm{Binom}(200, p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(105, p_2)\\). We are testing \\(H_0: p_1 = p_2\\) versus \\(H_1: p_1 \\neq p_2\\).\n\\(\\hat{p}_1 = 160 / 200 = 0.8\\), \\(\\hat{p}_2 = 50/105 = 0.4762\\), \\(\\hat{p} = (160 + 50) / (200 + 105) = 0.6885\\)\nWe can run this test using the two-sample binomial approach since the normal conditions are met.\n\n200 * 0.6885 * (1 - 0.6885)\n\n[1] 42.89\n\n105 * 0.6885 * (1 - 0.6885)\n\n[1] 22.52\n\n\nOur \\(z\\) statistic is\n\n(0.8 - 0.4762) / sqrt(0.6885 * (1 - 0.6885) * (1/200 + 1/105))\n\n[1] 5.802\n\n\nWe compare this to a standard normal distribution to get a \\(p\\)-value\n\n2 * pnorm(-5.802)\n\n[1] 6.553e-09\n\n\nWe can also just use prop.test()\n\nprop.test(x = c(160, 50), n = c(200, 105)) |&gt;\n  tidy()  |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n       p.value\n         &lt;dbl&gt;\n1 0.0000000141\n\n\nThe \\(p\\)-values differ because we didn’t use the continuity correction. If we did, then we would get equivalent results:\n\np1 &lt;- 160/200\np2 &lt;- 50/105\np &lt;- (160 + 50) / (200 + 105)\nzstat &lt;- (p1 - p2 - 1/(2 * 200) - 1/(2 * 105)) / sqrt(p * (1 - p) * (1/200 + 1/105))\n2 * pnorm(-zstat)\n\n[1] 1.412e-08\n\n\nEither way, we have very strong evidence of an association between present diagnosis and prior episodes of urethritis (p &lt; 0.001)\n\n\n\n\nExercise 10.13bSolution\n\n\nRepeat Exercise 10.13 using the chi-squared approach.\n\n\nSetting up our contingency table, we have:\n\ntab &lt;- matrix(\n  c(160, 50, \n    40 , 55), \n  nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(Prior = c(\"Yes\", \"No\"), Gonorrhea = c(\"Yes\", \"No\"))\ntab\n\n     Gonorrhea\nPrior Yes No\n  Yes 160 50\n  No   40 55\n\n\nLet’s get the expected counts:\n\n## Column margins\ncolSums(tab)\n\nYes  No \n200 105 \n\n## Row margins\nrowSums(tab)\n\nYes  No \n210  95 \n\n## Grand total\nsum(tab)\n\n[1] 305\n\n\nExpected counts\n\netab &lt;- matrix(\n  c(200 * 210 / 305, 105 * 210 / 305,\n    200 * 95 / 305 , 105 * 95 / 305),\n  nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(etab) &lt;- list(Prior = c(\"Yes\", \"No\"), Gonorrhea = c(\"Yes\", \"No\"))\netab\n\n     Gonorrhea\nPrior   Yes   No\n  Yes 137.7 72.3\n  No   62.3 32.7\n\n\nLet’s get the \\(\\chi^2\\) statistic:\n\n(160 - 137.7)^2 / 137.7 +\n  (50 - 72.3)^2 / 72.3 +\n  (40 - 62.3)^2 / 62.3 +\n  (55 - 32.7)^2 / 32.7\n\n[1] 33.68\n\n\nWe compare this to a \\(\\chi^2_1\\) distribution to get the \\(p\\)-value\n\npchisq(q = 33.68, df = 1, lower.tail = FALSE)\n\n[1] 6.497e-09\n\n\nIn R, this is:\n\nchisq.test(x = tab) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n       p.value\n         &lt;dbl&gt;\n1 0.0000000141\n\n\nThe p-value is different because we didn’t do the continuity correction. The continuity correction is a little weirder for \\(\\chi^2\\) tests. You subtract off a half value from the absolute value of the differences inside the square.\n\netab &lt;- outer(rowSums(tab), colSums(tab)) / 305\netab\n\n      Yes   No\nYes 137.7 72.3\nNo   62.3 32.7\n\nchstat &lt;- sum((abs(etab - tab) - 0.5)^2 / etab)\npchisq(q = chstat, df = 1, lower.tail = FALSE)\n\n[1] 1.412e-08"
  },
  {
    "objectID": "07_cat/10_notes.html#properties-of-cohens-kappa",
    "href": "07_cat/10_notes.html#properties-of-cohens-kappa",
    "title": "Chapter 10: Categorical Data",
    "section": "Properties of Cohen’s Kappa",
    "text": "Properties of Cohen’s Kappa\n\nBounds: \\[\n\\frac{-p_e}{1 - p_e} \\leq \\kappa \\leq 1\n\\]\n\nSet \\(p_o = 0\\) or \\(1\\) for bounds.\n\n\\(\\kappa = 1\\) implies perfect concordance.\nRules of thumb:\n\n\\(\\kappa &gt; 0.75\\) → excellent reproducibility\n\\(0.4 &lt; \\kappa \\leq 0.75\\) → good reproducibility\n\\(0 \\leq \\kappa \\leq 0.4\\) → marginal reproducibility\n\n\n\nConfidence intervals and tests for \\(\\kappa\\) are possible.\nUse \\(\\kappa\\) for repeated measures of the same variable.\nFor two different variables, use sensitivity and specificity.\n\n\nCohen’s Kappa in R"
  },
  {
    "objectID": "07_cat/10_notes.html#exercise-1",
    "href": "07_cat/10_notes.html#exercise-1",
    "title": "Chapter 10: Categorical Data",
    "section": "Exercise",
    "text": "Exercise\nCalculate the \\(\\chi^2\\) statistic from the previous exercise"
  },
  {
    "objectID": "07_cat/10_notes.html#solution-1",
    "href": "07_cat/10_notes.html#solution-1",
    "title": "Chapter 10: Categorical Data",
    "section": "Solution:",
    "text": "Solution:\nChi-squared test statistic:\n\\[\n\\chi^2 =\n\\frac{(13 - 6.7)^2}{6.7} +\n\\frac{(4987 - 4993.3)^2}{4993.3} +\n\\frac{(7 - 13.3)^2}{13.3} +\n\\frac{(9993 - 9986.7)^2}{9986.7}\n\\]"
  },
  {
    "objectID": "review/practice_problems.html",
    "href": "review/practice_problems.html",
    "title": "Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions.\nI have only kept the problems most relevant for this course.\n\nChapter 2\nChapter 3\nChapter 4\nChapter 5\nChapter 6\nChapter 7\nChapter 8\nChapter 9\nChapter 10"
  },
  {
    "objectID": "review/pp_02.html",
    "href": "review/pp_02.html",
    "title": "Chapter 2 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_02.html#effect-on-statistics",
    "href": "review/pp_02.html#effect-on-statistics",
    "title": "Chapter 2 Practice Problems",
    "section": "Effect on statistics",
    "text": "Effect on statistics\nSuppose the origin for a data set is changed by adding a constant to each observation.\n\n2.1Solution\n\n\nWhat is the effect on the median?\n\n\nIt is shifted by the same constant.\n\n\n\n\n2.2Solution\n\n\nWhat is the effect on the mode?\n\n\nIt is shifted by the same constant.\n\n\n\n\n2.3Solution\n\n\nWhat is the effect on the arithmetic mean?\n\n\nIt is shifted by the same constant.\n\n\n\n\n2.4Solution\n\n\nWhat is the effect on the range?\n\n\nIt is unchanged."
  },
  {
    "objectID": "review/pp_02.html#renal-disease",
    "href": "review/pp_02.html#renal-disease",
    "title": "Chapter 2 Practice Problems",
    "section": "Renal Disease",
    "text": "Renal Disease\nFor a study of kidney disease, the following measurements were made on a sample of women working in several factories in Switzerland. They represent concentrations of bacteria in a standard-size urine specimen. High concentrations of these bacteria may indicate possible kidney pathology. The data are presented in the following data frame\n\nlibrary(tidyverse)\nrenal &lt;- tibble(\n  Concentration = 10^(0:10),\n  Frequency = c(521, 230, 115, 74, 69, 62, 43, 30, 21, 10, 2)\n)\n\n\n\n\n\n\n\n\n\nConcentration\nFrequency\n\n\n\n\n1e+00\n521\n\n\n1e+01\n230\n\n\n1e+02\n115\n\n\n1e+03\n74\n\n\n1e+04\n69\n\n\n1e+05\n62\n\n\n1e+06\n43\n\n\n1e+07\n30\n\n\n1e+08\n21\n\n\n1e+09\n10\n\n\n1e+10\n2\n\n\n\n\n\n\n\nHere is a barplot\n\nggplot(renal, aes(x = as.factor(Concentration), y = Frequency)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n2.5Solution\n\n\nCompute the arithmetic mean for this sample.\n\n\nBy hand:\n\nn &lt;- 521 + 230 + 115 + 74 + 69 + 62 + 43 + 30 + 21 + 10 + 2\nsum_x &lt;- (1e0 * 521 + 1e1 * 230 + 1e2 * 115 + 1e3 * 74 + 1e4 * 69 +\n            1e5 * 62 + 1e6 * 43 + 1e7 * 30 + 1e8 * 21 + 1e9 * 10 + 1e10 * 2)\nsum_x / n\n\n[1] 27570075\n\n\nVerify using R:\n\nsum(renal$Concentration * renal$Frequency) / sum(renal$Frequency)\n\n[1] 27570075\n\n\n\n\n\n\n2.6Solution\n\n\nCompute the median for this sample.\n\n\nThere are \\(n = 1177\\) individuals\n\nn\n\n[1] 1177\n\n\nThis is odd, so we need the \\((n + 1) / 2\\) = 589th value. This is just 10.\n\n\n\n\n2.7Solution\n\n\nWhich do you think is a more appropriate measure of location?\n\n\nObviously the median. This is a super skewed distribution."
  },
  {
    "objectID": "review/pp_02.html#cardiovascular-disease",
    "href": "review/pp_02.html#cardiovascular-disease",
    "title": "Chapter 2 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nThe mortality rates from heart disease (per 100,000 population) for each of the 50 states and the District of Columbia in 1973 are given in descending order in the data frame below.\n\nstate_data &lt;- tibble(\n  State = c(\"West Virginia\", \"Pennsylvania\", \"Maine\", \"Missouri\", \"Illinois\",\n            \"Florida\", \"Rhode Island\", \"Kentucky\", \"New York\", \"Iowa\",\n            \"Arkansas\", \"New Jersey\", \"Massachusetts\", \"Kansas\", \"Oklahoma\",\n            \"Ohio\", \"South Dakota\", \"Wisconsin\", \"Vermont\", \"Nebraska\",\n            \"Tennessee\", \"New Hampshire\", \"Indiana\", \"North Dakota\", \"Delaware\",\n            \"Mississippi\", \"Louisiana\", \"Connecticut\", \"Oregon\", \"Washington\",\n            \"Minnesota\", \"Michigan\", \"Alabama\", \"North Carolina\", \"DC\",\n            \"South Carolina\", \"Montana\", \"Maryland\", \"Georgia\", \"Virginia\",\n            \"California\", \"Wyoming\", \"Texas\", \"Idaho\", \"Colorado\", \"Arizona\",\n            \"Nevada\", \"Utah\", \"New Mexico\", \"Hawaii\", \"Alaska\"),\n  Rate = c(445.4, 442.7, 427.3, 422.9, 420.8, 417.4, 414.4, 407.6, 406.7, 396.9,\n            396.8, 395.2, 394.0, 391.7, 391.0, 377.7, 376.2, 369.8, 369.2, 368.9,\n            361.4, 358.2, 356.4, 353.3, 351.6, 351.6, 349.4, 340.3, 338.7, 334.2,\n            332.7, 330.2, 329.1, 328.4, 327.1, 322.4, 319.1, 315.9, 311.8, 311.2,\n            310.6, 306.8, 300.6, 297.4, 274.6, 265.4, 236.9, 214.2, 194.0, 169.0, \n            83.9)\n)\n\nConsider this data set as a sample of size 51. \\[\n(X_1, X_2, \\ldots, x_{51})\n\\] If \\(\\sum_{i=1}^{51}x_i = 17409\\) and \\(\\sum_{i=1}^{51}(x_i - \\bar{x})^2 = 249063.65\\), then do the following:\n\n2.8Solution\n\n\nCompute the arithmetic mean of this sample.\n\n\n\\[\n\\sum_{i=1}^{51}x_i / n = 17409 / 51\n\\]\n\n17409 / 51\n\n[1] 341.4\n\n\n\n\n\n\n2.9Solution\n\n\nCompute the median of this sample.\n\n\nWe need the (51 + 1) / 2 = 26th largest value. You can count, but you’ll end up with Delaware (or Mississippi), whose rate is\n\nmedian(state_data$Rate)\n\n[1] 351.6\n\n\n\n\n\n\n2.10Solution\n\n\nCompute the standard deviation of this sample.\n\n\n\\[\n\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{51}(x_i - \\bar{x})^2} = \\sqrt{249063.65 / 50}\n\\]\n\nsqrt(249063.65 / 50)\n\n[1] 70.58\n\n\nVerifying in R, we have\n\nsd(state_data$Rate)\n\n[1] 70.58\n\n\n\n\n\n\n2.11Solution\n\n\nThe national mortality rate for heart disease in 1973 was 360.8 per 100,000. Why does this figure not correspond to your answer for Problem 2.8?\n\n\nIn 2.8, we calculated the mean rates of the states. The national mortality rate is by person. To get the same number from the state data, we would need to weight by the state population.\n\n\n\n\n2.12Solution\n\n\nDoes the differential in raw rates between Florida (417.4) and Georgia (311.8) actually imply that the risk of dying from heart disease is greater in Florida than in Georgia? Why or why not?\n\n\nNo. You need to control for demographic variables. E.g., age of the population."
  },
  {
    "objectID": "review/pp_02.html#nutrition",
    "href": "review/pp_02.html#nutrition",
    "title": "Chapter 2 Practice Problems",
    "section": "Nutrition",
    "text": "Nutrition\nThe data frame below shows the distribution of dietary vitamin-A intake as reported by 14 students who filled out a dietary questionnaire in class. The total intake is a combination of intake from individual food items and from vitamin pills. The units are in IU/100 (International Units/100).\n\nvita &lt;- tibble(\n  Student_number = 1:14,\n  Intake_IU_100 = c(31.1, 21.5, 74.7, 95.5, 19.4, 64.8, 108.7,\n                    48.1, 24.4, 13.4, 37.1, 21.3, 78.5, 17.7)\n)\n\n\n\n\n\n\n\n\n\nStudent_number\nIntake_IU_100\n\n\n\n\n1\n31.1\n\n\n2\n21.5\n\n\n3\n74.7\n\n\n4\n95.5\n\n\n5\n19.4\n\n\n6\n64.8\n\n\n7\n108.7\n\n\n8\n48.1\n\n\n9\n24.4\n\n\n10\n13.4\n\n\n11\n37.1\n\n\n12\n21.3\n\n\n13\n78.5\n\n\n14\n17.7\n\n\n\n\n\n\n\n\n2.13Solution\n\n\nCompute the mean and median from these data.\n\n\nThe mean\n\nsumx &lt;- 31.1 + 21.5 + 74.7 + 95.5 + 19.4 + 64.8 + 108.7 +\n  48.1 + 24.4 + 13.4 + 37.1 + 21.3 + 78.5 + 17.7\nn &lt;- 14\nxbar &lt;- sumx / n\nxbar\n\n[1] 46.87\n\n\nThe median is the mean of the 7th and 8th largest observations. Ordering the observations we get \\[\n13.4, 17.7, 19.4, 21.3, 21.5, 24.4, 31.1, 37.1, 48.1, 64.8, 74.7, 78.5, 95.5, 108\n\\] So the median is\n\n(31.1 + 37.1) / 2\n\n[1] 34.1\n\n\nWe can verify in R\n\nmedian(vita$Intake_IU_100)\n\n[1] 34.1\n\n\n\n\n\n\n2.14Solution\n\n\nCompute the standard deviation and coefficient of variation from these data.\n\n\nSum of the squared values\n\nsumx2 &lt;- 31.1^2 + 21.5^2 + 74.7^2 + 95.5^2 + 19.4^2 + 64.8^2 + 108.7^2 +\n  48.1^2 + 24.4^2 + 13.4^2 + 37.1^2 + 21.3^2 + 78.5^2 + 17.7^2\n\nSince \\(\\frac{1}{n}\\sum (x_i - \\bar{x})^2 = \\frac{1}{n}\\sum x_i^2 - \\bar{x}^2\\), we get variance is\n\n(sumx2 / n - xbar^2) * n / (n-1)\n\n[1] 1012\n\n\nAnd so the standard deviation is\n\nsqrt(1012)\n\n[1] 31.81\n\n\nWe can verify in R\n\nsd(vita$Intake_IU_100)\n\n[1] 31.81\n\n\nThe coefficient of variation is the standard deviation divided by the mean. So\n\n31.8 / 46.9\n\n[1] 0.678\n\n\n\n\n\n\n2.15Solution\n\n\nSuppose the data are expressed in IU rather than IU/100. What are the mean, standard deviation, and coefficient of variation in the new units?\n\n\nJust multiply the mean and sd by 100.\n\n46.9 * 100 ## mean\n\n[1] 4690\n\n31.8 * 100 ## sd\n\n[1] 3180\n\n\nThe CV is unchanged.\n\n(31.8 * 100) / (46.9 * 100)\n\n[1] 0.678\n\n\n\n\n\n\n2.17Solution\n\n\nDo you think the mean or median is a more appropriate measure of location for this data set? Here is a histogram\n\nggplot(vita, aes(x = Intake_IU_100)) + \n  geom_histogram(bins = 5)\n\n\n\n\n\n\n\n\n\n\nHard to tell. Maybe the median because of the slight skew? But there is too few data to say for sure."
  },
  {
    "objectID": "review/pp_03.html",
    "href": "review/pp_03.html",
    "title": "Chapter 3 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_03.html#basic-probability",
    "href": "review/pp_03.html#basic-probability",
    "title": "Chapter 3 Practice Problems",
    "section": "Basic Probability",
    "text": "Basic Probability\nLet\n\n\\(A\\) = {serum cholesterol \\(=\\) 250–299},\n\\(B\\) = {serum cholesterol \\(\\geq\\) 300},\n\\(C\\) = {serum cholesterol \\(\\leq\\) 280}.\n\n\n3.1Solution\n\n\nAre the events \\(A\\) and \\(B\\) mutually exclusive?\n\n\nYes. You cannot both have a value between 250 and 299 AND have a value greater than 300.\n\n\n\n\n3.2Solution\n\n\nAre the events \\(A\\) and \\(C\\) mutually exclusive?\n\n\nNo. Both \\(A\\) and \\(C\\) occur if the serum colesterol level is anywhere between 250 and 280.\n\n\n\n\n3.3Solution\n\n\nSuppose \\(P(A) = 0.2\\), \\(P(B) = 0.1\\). What is \\(P(\\text{serum cholesteroal} \\geq 250)\\)?\n\n\nThis is \\(P(A \\cup B) = P(A) + P(B) = 0.2 + 0.1 = 0.3\\)\n\n\n\n\n3.4Solution\n\n\nWhat does \\(A \\cup C\\) mean?\n\n\nThe serum cholesterol level is less than or equal to 299.\n\n\n\n\n3.5Solution\n\n\nWhat does \\(A \\cap C\\) mean?\n\n\nThe serum cholesterol level is between 250 and 280 (inclusive).\n\n\n\n\n3.6Solution\n\n\nWhat does \\(B \\cup C\\) mean?\n\n\nThe serum cholesterol level is either less than or equal to 280, or greater than or equal to 300.\n\n\n\n\n3.7Solution\n\n\nWhat does \\(B \\cap C\\) mean?\n\n\nThe null set. Both events cannot happen.\n\n\n\n\n3.8Solution\n\n\nAre the events \\(B\\) and \\(C\\) mutually exclusive?\n\n\nYes. They cannot both happen.\n\n\n\n\n3.9Solution\n\n\nWhat does the event \\(\\bar{B}\\) mean? What is its probability?\n\n\nThe serum cholesterol level is less than or equal to 299. \\[\nP(\\bar{B}) = 1 - P(B) = 1 - 0.1 = 0.9\n\\]\n\n\n\nSuppose that the gender of successive offspring in the same family are independent events and that the probability of a male or female offspring is 0.5.\n\n3.10Solution\n\n\nWhat is the probability of two successive female offspring?\n\n\nLet \\(A\\) = first female and \\(B\\) = second female. Then \\[\nP(A \\cap B) = P(A)P(B) = 0.5 \\times 0.5  = 0.25\n\\]\n\n\n\n\n3.11Solution\n\n\nWhat is the probability that exactly one of two successive children will be female?\n\n\n\\[\\begin{align*}\nP[(\\bar{A} \\cap B) \\cup (A \\cap \\bar{B})] &= P(\\bar{A} \\cap B) + P(A \\cap \\bar{B}) \\text{ (mutually exclusive events)}\\\\\n&= P(\\bar{A})P(B) + P(A)P(\\bar{B}) \\text{ (independent events)}\\\\\n&= 0.5 \\times 0.5 + 0.5 \\times 0.5 \\text{ (substituting in probabilities)}\\\\\n&= 0.5.\n\\end{align*}\\]\n\n\n\n\n3.12Solution\n\n\nSuppose that three successive offspring are male. What is the probability that a fourth child will be male?\n\n\nStill 0.5. These events are all independent."
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease",
    "href": "review/pp_03.html#cardiovascular-disease",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nA survey was performed among people 65 years of age and older who underwent open-heart surgery. It was found that 30% of patients died within 90 days of the operation, whereas an additional 25% of those who survived 90 days died within 5 years after the operation.\n\n3.13Solution\n\n\nWhat is the probability that a patient undergoing open-heart surgery will die within 5 years?\n\n\n\n\n\n\n\n3.14Solution\n\n\nWhat is the mortality incidence (per patient month) in patients receiving this operation in the first 90 days after the operation? (Assume that 90 days = 3 months.)\n\n\n\n\n\n\n\n3.15Solution\n\n\nAnswer the same question as in Problem 3.14 for the period from 90 days to 5 years after the operation.\n\n\n\n\n\n\n\n3.16Solution\n\n\nCan you tell if the operation prolongs life from the data presented? If not, then what additional data do you need?\n\n\n\n\n\n\nA study relating smoking history to several measures of cardiopulmonary disability was recently reported. The data in the table below were presented relating the number of people with different disabilities according to cigarette-smoking status.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1 Number of people with selected cardiopulmonary disabilities versus cigarette-smoking status\n\n\nDisability\nNone(n=656)\nEx(n=826)\nCurrent&lt; 15 g/day (n=955)\nCurrent&gt;= 15 g/day (n=654)\n\n\n\n\nShortness of breath\n7\n15\n18\n13\n\n\nAngina\n15\n19\n19\n16\n\n\nPossible infarction\n3\n7\n8\n6\n\n\n\n\n\n\n\n\n3.17Solution\n\n\nWhat is the prevalence of angina among light current smokers (&lt; 15g/day)?\n\n\n\n\n\n\n\n3.18Solution\n\n\nWhat is the relative risk of ex-smokers, light current smokers, and heavy current smokers, respectively, for shortness of breath as compared with nonsmokers?\n\n\n\n\n\n\n\n3.19Solution\n\n\nAnswer Problem 3.18 for angina.\n\n\n\n\n\n\n\n3.20Solution\n\n\nAnswer Problem 3.18 for possible infarction"
  },
  {
    "objectID": "review/pp_03.html#pulmonary-disease",
    "href": "review/pp_03.html#pulmonary-disease",
    "title": "Chapter 3 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\nPulmonary embolism is a relatively common condition that necessitates hospitalization and also often occurs in patients hospitalized for other reasons. An oxygen tension (arterial Po\\(_{2}\\)) &lt; 90 mm Hg is one of the important criteria used in diagnosing this condition. Suppose that the sensitivity of this test is 95%, the specificity is 75%, and the estimated prevalence is 20% (i.e., a doctor estimates that a patient has a 20% chance of pulmonary embolism before performing the test).\n\n3.21HintSolution\n\n\nWhat is the predictive value positive of this test? What does it mean in words?\n\n\n\nSensitivity: \\(P(T^+|D^+) = 0.95\\)\nSpecificity: \\(P(T^-|D^-) = 0.75\\)\nPrevalence: \\(P(D^+) = 0.2\\)\n\nWe want\n\n\\(PV^+ = P(D^+|T^+)\\)\n\n\n\n\nSensitivity: \\(P(T^+|D^+) = 0.95\\)\nSpecificity: \\(P(T^-|D^-) = 0.75\\)\nPrevalence: \\(P(D^+) = 0.2\\)\n\nWe want\n\n\\(PV^+ = P(D^+|T^+)\\)\n\nUsing Bayes rule, we have \\[\nP(D^+|T^+) = \\frac{P(T^+|D^+) P(D+^)}{P(T^+)}\n\\] So we can use the law of total probability to get \\[\\begin{align*}\nP(T^+) &= P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-)\\\\\n&= P(T^+|D^+)P(D^+) + [1 - P(T^-|D^-)][1 - P(D^+)]\\\\\n&= 0.95 \\times 0.2 + (1 - 0.75)\\times(1 - 0.2)\n\\end{align*}\\]\nSo we have \\[\nP(D^+|T^+) = \\frac{0.95 \\times 0.2}{0.95 \\times 0.2 + (1 - 0.75)\\times(1 - 0.2)} = 0.4872\n\\]\nIt means the chance of having Pulmonary embolism given that oxygen tension &lt; 90 mm Hg is about 0.487.\n\n\n\n\n3.22Solution\n\n\nWhat is the predictive value negative of this test? What does it mean in words?\n\n\nWe need \\(PV^- = P(D^-|T^-)\\). Using Bayes rule we have \\[\\begin{align*}\nP(D^-|T^-) &= \\frac{P(T^-|D^-)P(D^-)}{P(T^-)}\\\\\n&= \\frac{P(T^-|D^-)P(D^-)}{1 - P(T^+)}\\\\\n&= \\frac{0.75 \\times (1 - 0.2)}{1 - [0.95 \\times 0.2 + (1 - 0.75)\\times(1 - 0.2)]}\\\\\n&= 0.9836\n\\end{align*}\\]\nIt means the chance of not having Pulmonary embolism given that oxygen tension &lt;&gt;90 mm Hg is about 0.9836"
  },
  {
    "objectID": "review/pp_03.html#environmental-health-pediatrics",
    "href": "review/pp_03.html#environmental-health-pediatrics",
    "title": "Chapter 3 Practice Problems",
    "section": "Environmental Health, Pediatrics",
    "text": "Environmental Health, Pediatrics\n\n3.25HintSolution\n\n\nSuppose that a company plans to build a lead smelter in a community and that the city council wishes to assess the health effects of the smelter. In particular, there is concern from previous literature that children living very close to the smelter will experience unusually high rates of lead poisoning in the first 3 years of life. The projected rates of lead poisoning over this time period are 50 per 100,000 for those children living within 2 km of the smelter, 20 per 100,000 for children living &gt; 2 km but ≤ 5 km from the smelter, and 5 per 100,000 for children living &gt; 5 km from the smelter. If 80% of the children live more than 5 km from the smelter, 15% live &gt; 2 km but ≤ 5 km from the smelter, and the remainder live ≤ 2 km from the smelter, then what is the overall probability that a child from this community will get lead poisoning?\n\n\nLet\n\n\\(A_1\\): living within 2 km\n\\(A_2\\): living &gt; 2 km but ≤ 5 km\n\\(A_3\\): living &gt; 5 km\n\\(B\\): Lead poisened.\n\nThen we are given\n\n\\(P(B|A_1) = 50/100000\\)\n\\(P(B|A_2) = 20/100000\\)\n\\(P(B|A_3) = 5/100000\\)\n\\(P(A_1) = 1 - (0.15 + 0.8) = 0.05\\)\n\\(P(A_2) = 0.15\\)\n\\(P(A_3) = 0.8\\)\n\n\n\nLet\n\n\\(A_1\\): living within 2 km\n\\(A_2\\): living &gt; 2 km but ≤ 5 km\n\\(A_3\\): living &gt; 5 km\n\\(B\\): Lead poisened.\n\nThen we are given\n\n\\(P(B|A_1) = 50/100000\\)\n\\(P(B|A_2) = 20/100000\\)\n\\(P(B|A_3) = 5/100000\\)\n\\(P(A_1) = 1 - (0.15 + 0.8) = 0.05\\)\n\\(P(A_2) = 0.15\\)\n\\(P(A_3) = 0.8\\)\n\nWe want \\(P(B)\\), which we can get by the law of total probability \\[\\begin{align*}\nP(B) &= P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + P(B|A_3)P(A_3)\\\\\n&= 50/100000 \\times 0.05 + 20/100000 \\times 0.15 + 5/100000 \\times 0.8\\\\\n&= 0.000095\n\\end{align*}\\]"
  },
  {
    "objectID": "review/pp_03.html#diabetes",
    "href": "review/pp_03.html#diabetes",
    "title": "Chapter 3 Practice Problems",
    "section": "Diabetes",
    "text": "Diabetes\nThe prevalence of diabetes in adults at least 20 years old has been studied in Tecumseh, Michigan. The age-sex-specific prevalence (per 1000) is given in Table 3.2.\n\n\n\n\n\n\n\n\nTable 3.2 Age-sex-specific prevalence of diabetes in Tecumseh, MI (per 1000)\n\n\nAge group (years)\nMale\nFemale\n\n\n\n\n20–39\n5\n7\n\n\n40–54\n23\n31\n\n\n55+\n57\n89\n\n\n\n\n\n\n\n\n3.26HintSolution\n\n\nSuppose we plan a new study in a town that consists of 48% males and 52% females. Of the males, 40% are ages 20–39, 32% are 40–54, and 28% are 55+. Of the females, 44% are ages 20–39, 37% are 40–54, and 19% are 55+. Assuming that the Tecumseh prevalence rates hold, what is the expected prevalence of diabetes in the new study?\n\n\nLet\n\n\\(A\\) be male.\n\\(B_1\\) be 20–39\n\\(B_2\\) be 40–54\n\\(B_3\\) be 55+\n\\(C\\) be Diabetes.\n\nWe are given - \\(P(A) = 0.48\\) and \\(P(\\bar{A}) = 0.52\\) - \\(P(B_1|A) = 0.4\\), \\(P(B_2|A) = 0.32\\), and \\(P(B_3|A) = 0.28\\) - \\(P(B_1|\\bar{A}) = 0.44\\), \\(P(B_2|\\bar{A}) = 0.37\\), and \\(P(B_3|\\bar{A}) = 0.19\\) - \\(P(C|A \\cap B_1) = 5/1000\\) - \\(P(C|A \\cap B_2) = 23/1000\\) - \\(P(C|A \\cap B_3) = 57/1000\\) - \\(P(C|\\bar{A} \\cap B_1) = 7/1000\\) - \\(P(C|\\bar{A} \\cap B_2) = 31/1000\\) - \\(P(C|\\bar{A} \\cap B_3) = 89/1000\\)\nWe are asked to calculate \\(P(C)\\).\n\n\nLet\n\n\\(A\\) be male.\n\\(B_1\\) be 20–39\n\\(B_2\\) be 40–54\n\\(B_3\\) be 55+\n\\(C\\) be Diabetes.\n\nWe are given - \\(P(A) = 0.48\\) and \\(P(\\bar{A}) = 0.52\\) - \\(P(B_1|A) = 0.4\\), \\(P(B_2|A) = 0.32\\), and \\(P(B_3|A) = 0.28\\) - \\(P(B_1|\\bar{A}) = 0.44\\), \\(P(B_2|\\bar{A}) = 0.37\\), and \\(P(B_3|\\bar{A}) = 0.19\\) - \\(P(C|A \\cap B_1) = 5/1000\\) - \\(P(C|A \\cap B_2) = 23/1000\\) - \\(P(C|A \\cap B_3) = 57/1000\\) - \\(P(C|\\bar{A} \\cap B_1) = 7/1000\\) - \\(P(C|\\bar{A} \\cap B_2) = 31/1000\\) - \\(P(C|\\bar{A} \\cap B_3) = 89/1000\\)\nWe are asked to calculate \\(P(C)\\).\n\\[\\begin{align*}\nP(C) &= P(C|A\\cap B_1)P(B_1|A)P(A) + P(C|A\\cap B_2)P(B_2|A)P(A) + P(C|A\\cap B_3)P(B_3|A)P(A) + \\\\\n&P(C|\\bar{A}\\cap B_1)P(B_1|\\bar{A})P(\\bar{A}) + P(C|\\bar{A}\\cap B_2)P(B_2|\\bar{A})P(\\bar{A}) + P(C|\\bar{A}\\cap B_3)P(B_3|\\bar{A})P(\\bar{A})\n\\end{align*}\\]\nPlugging in values, we get: \\[\\begin{align*}\nP(C) &= \\frac{5}{1000} \\times 0.4 \\times 0.48 + \\frac{23}{1000} \\times 0.32 \\times 0.48 + \\frac{57}{1000} \\times 0.28 \\times 0.48 +\\\\\n&\\frac{7}{1000} \\times 0.44 \\times 0.52 + \\frac{31}{1000} \\times 0.37 \\times 0.52 + \\frac{89}{1000} \\times 0.19 \\times 0.52\n\\end{align*}\\]\n\n\n\n\n3.27Solution\n\n\nWhat proportion of diabetics in the new study would be expected in each of the six age-sex groups?\n\n\nE.g., we want \\(P(A \\cap B_1 | C)\\) for the proportion of diabetics that are men between 20 and 29. This is obtained by Bayes theorem \\[\nP(A \\cap B_1 | C) = \\frac{P(C|A\\cap B_1)P(A\\cap B_1)}{P(C)} = \\frac{P(C|A\\cap B_1)P(B_1| A)P(A)}{P(C)}.\n\\] The numerator values are given, the denominator was calculated in 3.26."
  },
  {
    "objectID": "review/pp_03.html#cancer",
    "href": "review/pp_03.html#cancer",
    "title": "Chapter 3 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\nTable 3.3 shows the annual incidence rates for colon cancer, lung cancer, and stomach cancer in males ages 50 years and older from the Connecticut Tumor Registry, 1963–1965.\n\n\n\n\n\n\n\n\nTable 3.3\n\n\nAverage annual incidence per 100,000 males for colon, lung, and stomach cancer from the Connecticut Tumor Registry 1963–1965\n\n\nType of cancer\nAges 50-54\nAges 55-59\nAges 60-64\n\n\n\n\nColon\n35.7\n60.3\n98.9\n\n\nLung\n76.1\n137.5\n231.7\n\n\nStomach\n20.8\n39.1\n46.0\n\n\n\n\n\n\n\n\n3.28Solution\n\n\nWhat is the probability that a 57-year-old, disease-free male will develop lung cancer over the next year?\n\n\nThis is just 137.5 / 100000\n\n\n\n\n3.29Solution\n\n\nWhat is the probability that a 55-year-old, disease-free male will develop colon cancer over the next 5 years?\n\n\nThis is one minus the probability that he does not develop cancer over the next five years, which is \\[\n(1 - 60.3 / 100000)^5\n\\]\n\n\n\n\n3.30Solution\n\n\nSuppose there is a cohort of 1000 50-year-old men who have never had cancer. How many colon cancers would be expected to develop in this cohort over a 15-year period?\n\n\nThis is the probability of having colon cancer during that period times 1000.\nIt’s easier to calculate the proportion that do not get colon cancer.\n\nProbability don’t have it from 50–54: (1 - 35.7/100000)^5\nProbability don’t have it from 55–59: (1 - 60.3/100000)^5\nProbability don’t have it from 60–64: (1 - 98.9/100000)^5\n\nMultiply these together:\n\n(1 - 35.7/100000)^5 * (1 - 60.3/100000)^5 * (1 - 98.9/100000)^5\n\n[1] 0.9903\n\n\nThis is the probability of getting colon cancer. So the probability of not getting it is one minus this, or\n\n1 - (1 - 35.7/100000)^5 * (1 - 60.3/100000)^5 * (1 - 98.9/100000)^5\n\n[1] 0.009701\n\n\nMultiply this by 1000 to get the expected number. Or, about 9.7 cases."
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease-1",
    "href": "review/pp_03.html#cardiovascular-disease-1",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease 1",
    "text": "Cardiovascular Disease 1\nA survey was performed among people 65 years of age and older who underwent open-heart surgery. It was found that 30% of patients died within 90 days of the operation, whereas an additional 25% of those who survived 90 days died within 5 years after the operation.\n\n3.13HintSolution\n\n\nWhat is the probability that a patient undergoing open-heart surgery will die within 5 years?\n\n\nLet \\(A\\) be died in first 90 days. Let \\(B\\) be died between 90 days and 5 years. We are given \\[\nP(A) = 0.3\\\\\nP(B|\\bar{A}) = 0.25\n\\] We are asked to calculate \\(P(A\\cup B)\\)\n\n\nLet \\(A\\) be died in first 90 days. Let \\(B\\) be died between 90 days and 5 years. We are given \\[\nP(A) = 0.3\\\\\nP(B|\\bar{A}) = 0.25\n\\] We are asked to calculate \\(P(A\\cup B)\\)\nNote that \\(A\\) and \\(B\\) are mutually exclusive events. This is because you can’t die twice (some eastern religions excepted). This means that \\[\nP(A \\cup B) = P(A) + P(B)\n\\] \\(P(A)\\) was given to us. Let’s get \\(P(B)\\) from the law of total probability. \\[\\begin{align*}\nP(B) = P(B|A)P(A) + P(B|\\bar{A})P(\\bar{A})\\\\\n&= 0 * 0.3 + 0.25 * (1 \\times 0.3)\\\\\n&= 0.25 \\times (1 - 0.3)\n\\end{align*}\\] (\\(P(B|A) = 0\\) because, again, you cannot die if you already dead).\nSo, putting these together, we get \\[\nP(A \\cup B) = 0.3 + 0.25 \\times (1 - 0.3) = 0.475\n\\]\n\n\n\n\n3.16Solution\n\n\nCan you tell if the operation prolongs life from the data presented? If not, then what additional data do you need?\n\n\nNo. We need a control group to see if folks doing the operation do better than those without doing the operation.\n\n\n\nA study relating smoking history to several measures of cardiopulmonary disability was recently reported. The data in the table below were presented relating the number of people with different disabilities according to cigarette-smoking status.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1 Number of people with selected cardiopulmonary disabilities versus cigarette-smoking status\n\n\nDisability\nNone(n=656)\nEx(n=826)\nCurrent&lt; 15 g/day (n=955)\nCurrent&gt;= 15 g/day (n=654)\n\n\n\n\nShortness of breath\n7\n15\n18\n13\n\n\nAngina\n15\n19\n19\n16\n\n\nPossible infarction\n3\n7\n8\n6\n\n\n\n\n\n\n\n\n3.17Solution\n\n\nWhat is the prevalence of angina among light current smokers (&lt; 15g/day)?\n\n\n\\[\n19 / 955 = 0.0199\n\\]\n\n\n\n\n3.18Solution\n\n\nWhat is the relative risk of ex-smokers, light current smokers, and heavy current smokers, respectively, for shortness of breath as compared with nonsmokers?\n\n\nLet \\(B\\) be shortness of breath. Let \\(A_1\\) be ex-smoker, \\(A_2\\) be light current smoker, \\(A_3\\) be heavy current smoker, and \\(A_0\\) be nonsmoker. Then\n\nRR of ex-smoker vs non-smoker = \\(\\frac{P(B|A_1)}{P(B|A_0)} = \\frac{15/826}{7/656}\\)\nRR of light current smoker vs non-smoker = \\(\\frac{P(B|A_2)}{P(B|A_0)} = \\frac{18/955}{7/656}\\)\nRR of heavy current smoker vs non-smoker = \\(\\frac{P(B|A_3)}{P(B|A_0)} = \\frac{13/654}{7/656}\\)\n\n\n\n\n\n3.19Solution\n\n\nAnswer Problem 3.18 for angina.\n\n\nLet \\(B\\) be angina. Let \\(A_1\\) be ex-smoker, \\(A_2\\) be light current smoker, \\(A_3\\) be heavy current smoker, and \\(A_0\\) be nonsmoker. Then\n\nRR of ex-smoker vs non-smoker = \\(\\frac{P(B|A_1)}{P(B|A_0)} = \\frac{19/826}{15/656}\\)\nRR of light current smoker vs non-smoker = \\(\\frac{P(B|A_2)}{P(B|A_0)} = \\frac{19/955}{15/656}\\)\nRR of heavy current smoker vs non-smoker = \\(\\frac{P(B|A_3)}{P(B|A_0)} = \\frac{16/654}{15/656}\\)\n\n\n\n\n\n3.20Solution\n\n\nAnswer Problem 3.18 for possible infarction\n\n\nLet \\(B\\) be possible infarction Let \\(A_1\\) be ex-smoker, \\(A_2\\) be light current smoker, \\(A_3\\) be heavy current smoker, and \\(A_0\\) be nonsmoker. Then\n\nRR of ex-smoker vs non-smoker = \\(\\frac{P(B|A_1)}{P(B|A_0)} = \\frac{7/826}{3/656}\\)\nRR of light current smoker vs non-smoker = \\(\\frac{P(B|A_2)}{P(B|A_0)} = \\frac{8/955}{3/656}\\)\nRR of heavy current smoker vs non-smoker = \\(\\frac{P(B|A_3)}{P(B|A_0)} = \\frac{6/654}{3/656}\\)"
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease-2",
    "href": "review/pp_03.html#cardiovascular-disease-2",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease 2",
    "text": "Cardiovascular Disease 2\nThe relationship between physical fitness and cardiovascular-disease mortality was recently studied in a group of railroad working men, ages 22–79. Data were presented relating baseline exercise-test heart rate and coronary heart-disease mortality (Table 3.4).\n\n\n\n\n\n\n\n\nTable 3.4\n\n\nRelationship between baseline exercise-test heart rate and coronary heart-disease mortality\n\n\nExercise-test heart rate (beats/min)\nCoronary heart-disease mortality (20 years) (per 100)\n\n\n\n\n≤ 105\n9.1\n\n\n106–115\n8.7\n\n\n116–127\n11.6\n\n\n&gt; 127\n13.2\n\n\n\n\n\n\n\nSuppose that 20, 30, 30, and 20% of the population, respectively, have exercise-test heart rates of ≤ 105, 106–115, 116–127, &gt; 127 beats/minute. Suppose a test is positive if the exercise-test heart rate is &gt; 127 beats/min and negative otherwise.\n\n3.33HintSolution\n\n\nWhat is the probability of a positive test among men who have died over the 20-year period? Is there a name for this quantity?\n\n\nLet\n\n\\(A_1\\) be ≤ 105\n\\(A_2\\) be 106–115\n\\(A_3\\) be 116–127\n\\(A_4\\) be &gt; 127\n\\(D^+\\) be death\n\nWe are given\n\n\\(P(A_1) = 0.2\\)\n\\(P(A_2) = 0.3\\)\n\\(P(A_3) = 0.3\\)\n\\(P(A_4) = 0.2\\)\n\\(P(D^+|A_1) = 9.1/100\\)\n\\(P(D^+|A_2) = 8.7/100\\)\n\\(P(D^+|A_3) = 11.6/100\\)\n\\(P(D^+|A_4) = 13.2/100\\)\n\n\n\nLet\n\n\\(A_1\\) be ≤ 105\n\\(A_2\\) be 106–115\n\\(A_3\\) be 116–127\n\\(A_4\\) be &gt; 127\n\\(D^+\\) be death\n\nWe are given\n\n\\(P(A_1) = 0.2\\)\n\\(P(A_2) = 0.3\\)\n\\(P(A_3) = 0.3\\)\n\\(P(A_4) = 0.2\\)\n\\(P(D^+|A_1) = 9.1/100\\)\n\\(P(D^+|A_2) = 8.7/100\\)\n\\(P(D^+|A_3) = 11.6/100\\)\n\\(P(D^+|A_4) = 13.2/100\\)\n\nWe are told that \\(A_4 = T^+\\) (a positive test result). We want \\(P(T^+|D^+) = P(A_4|D^+)\\). We can use Bayes rule \\[\nP(A_4|D^+) = \\frac{P(D^+|A_4)P(A_4)}{P(D^+)}\n\\]\nThe numerator terms are given, the denominator can be found by the law of total probability. \\[\\begin{align*}\nP(D^+) &= P(D^+|A_1)P(A_1) + P(D^+|A_2)P(A_2) + P(D^+|A_3)P(A_3) + P(D^+|A_4)P(A_4)\\\\\n&= 0.091 \\times 0.2 + 0.087 \\times 0.3 + 0.116 \\times 0.3 + 0.132 \\times 0.2\\\\\n&= 0.1055\n\\end{align*}\\] So, the solution is \\[\nP(A_4|D^+) = \\frac{0.132 \\times 0.2}{0.1055} = 0.2502\n\\] This is called the sensitivity.\n\n\n\n\n3.34Solution\n\n\nWhat is the probability of a positive test among men who survived the 20-year period? Is there a name for this quantity?\n\n\nUsing the same notation from 3.33, we want \\(P(A_4|D^-)\\). We can use Bayes rule \\[\nP(A_4|D^-) = \\frac{P(D^-|A_4)P(A_4)}{P(D^-)} = \\frac{(1 - 0.132) \\times 0.2}{1 - 0.1055} = 0.1941\n\\] \\(P(T^+|D^-)\\) is the false positive rate, but you don’t need to know this term.\n\n\n\n\n3.35HintSolution\n\n\nWhat is the probability of death among men with a negative test? Is there a name for this quantity?\n\n\n\\[\nD^+ \\cap (A_1 \\cup A_2 \\cup A_3) = (D^+\\cap A_1) \\cup (D^+\\cap A_2) \\cup (D^+\\cap A_3)\n\\]\n\n\nUsing the same notation from 3.33, we want \\(P(D^+|\\bar{A_4})\\). This is \\[\\begin{align*}\nP(D^+|\\bar{A_4}) &= P(D^+|A_1 \\cup A_2 \\cup A_3)\\\\\n&= \\frac{P[D^+ \\cap (A_1 \\cup A_2 \\cup A_3)]}{P(A_1 \\cup A_2 \\cup A_3)}\\\\\n&= \\frac{P[(D^+\\cap A_1) \\cup (D^+\\cap A_2) \\cup (D^+\\cap A_3)}{P(A_1) + P(A_2) + P(A_3)}\\\\\n&= \\frac{P(D^+\\cap A_1) + P(D^+\\cap A_2) + P(D^+\\cap A_3)}{P(A_1) + P(A_2) + P(A_3)}\\\\\n&= \\frac{P(D^+|A_1)P(A_1) + P(D^+| A_2)P(A_2) + P(D^+| A_3)P(A_3)}{P(A_1) + P(A_2) + P(A_3)}\\\\\n&= \\frac{0.091 \\times 0.2 + 0.087 \\times 0.3 + 0.116 \\times 0.3}{0.2 + 0.3 + 0.3}\\\\\n&= 0.09888\n\\end{align*}\\]\n\\(P(D^+|T^-)\\) is the false omission rate. But you don’t need to know this term."
  },
  {
    "objectID": "review/pp_03.html#nutrition",
    "href": "review/pp_03.html#nutrition",
    "title": "Chapter 3 Practice Problems",
    "section": "Nutrition",
    "text": "Nutrition\nThe food-frequency questionnaire (FFQ) is a commonly used method for assessing dietary intake, where individuals are asked to record the number of times per week they usually eat for each of about 100 food items over the previous year. It has the advantage of being easy to administer to large groups of people, but has the disadvantage of being subject to recall error. The gold standard instrument for assessing dietary intake is the diet record (DR), where people are asked to record each food item eaten on a daily basis over a 1-week period. To investigate the accuracy of the FFQ, both the FFQ and the DR were administered to 173 participants in the United States. The reporting of alcohol consumption with each instrument is given in Table 3.5, where alcohol is coded as alcohol = some drinking, versus no alcohol = no drinking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.5\n\n\nActual drinking habits as determined from the diet record cross-classified by self-reported drinking status from the food-frequency questionnaire\n\n\n\n\nFood-frequency questionnaire\n\n\n\nAlcohol\nNo alcohol\nTotal\n\n\n\n\nAlcohol\n138\n18\n156\n\n\nNo alcohol\n1\n16\n17\n\n\nTotal\n139\n34\n173\n\n\n\n\n\n\n\n\n3.40Solution\n\n\nWhat is the sensitivity of the FFQ?\n\n\nHere, if the FFQ says alcohol, this is a “positive test”. If the DR says alcohol, then this is a “positive disease”. So, in this context, the sensitivity of FFQ is the proportion of DR+ that are also FFQ+. Or, \\[\n138 / 156 = 0.8846\n\\]\n\n\n\n\n3.41Solution\n\n\nWhat is the specificity of the FFQ?\n\n\nThis is the proportion of DR- that are also FFQ-, or \\[\n16 / 17 = 0.9412\n\\]\n\n\n\nLet us treat the 173 participants in this study as representative of people who would use the FFQ.\n\n3.42Solution\n\n\nWhat is the predictive value positive of the FFQ?\n\n\nThis is the proportion of FFQ+ that are also DR+. So \\[\n138 / 139 = 0.9928\n\\]\n\n\n\n\n3.43Solution\n\n\nWhat is the predictive value negative of the FFQ?\n\n\nThis is the proportion of FFQ- that are also DR-. So \\[\n16 / 34 = 0.4706\n\\]\n\n\n\n\n3.44Solution\n\n\nSuppose the questionnaire is to be administered in a different country where the true proportion of drinkers is 80%. What would be the predictive value positive if administered in this setting?\n\n\nWe are told that \\(P(D^+) = 0.8\\). We will also assume, from the questionnaire, that the sensitivity and specificty are known: \\(P(T^+|D^+) = 138 / 156 = 0.8846\\) and \\(P(T^-|D^-) = 16 / 17 = 0.9412\\). We want \\(P(D^+|T^+)\\), which we can get using Bayes rule \\[\\begin{align*}\nP(D^+|T^+) &= \\frac{P(T^+|D^+)P(D^+)}{P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-)} \\\\\n&= \\frac{0.8846 \\times 0.8}{0.8846 \\times 0.8 + (1 - 0.9412) \\times (1 - 0.8)}\\\\\n&= 0.9837\n\\end{align*}\\]"
  },
  {
    "objectID": "review/pp_03.html#genetics",
    "href": "review/pp_03.html#genetics",
    "title": "Chapter 3 Practice Problems",
    "section": "Genetics",
    "text": "Genetics\nTwo healthy parents have a child with a severe autosomal recessive condition that cannot be identified by prenatal diagnosis. They realize that the risk of this condition for subsequent offspring is 1/4, but wish to embark on a second pregnancy. During the early stages of the pregnancy, an ultrasound test determines that there are twins.\n\n3.45Solution\n\n\nSuppose that there are monozygotic, or MZ (identical) twins. What is the probability that both twins are affected? one twin affected? neither twin affected? Are the outcomes for the two MZ twins independent or dependent events?\n\n\nLet \\(A\\) be the first is affected. Let \\(B\\) be the second is affected. Note that for monozygotic twins, we have \\(P(A|B) = 1\\) and \\(P(\\bar{A}|\\bar{B}) = 1\\), since if one is affected then the other must also be affected. Using this, we have\n\\[\nP(\\text{both}) = P(A \\cap B) = P(A|B)P(B) = 1 \\times 0.25 = 0.25\n\\] \\[\nP(\\text{one}) = P(\\bar{A}|B)P(B) + P(A|\\bar{B})P(\\bar{B}) = 0 \\times 0.25 + 0 \\times (1 - 0.25) = 0\n\\] \\[\nP(\\text{neither}) = P(\\bar{A} \\cap \\bar{B}) = P(\\bar{A}|\\bar{B})P(\\bar{B}) = 1 \\times (1 - 0.25) = 0.75\n\\]\nThese are not independent events because \\(P(A|B) = 1\\) but \\(P(A|\\bar{B}) = 0\\).\n\n\n\n\n3.46Solution\n\n\nSuppose that there are dizygotic, or DZ (fraternal) twins. What is the probability that both twins are affected? one twin affected? neither twin affected? Are the outcomes for the two DZ twins independent or dependent events?\n\n\nBy the assumption of Mendelian segregation, these are independent events. Thus, we have \\[\nP(\\text{both}) = P(A\\cap B) = P(A)P(B) = 0.25^2 = 0.0625\n\\] \\[\nP(\\text{one}) = P(A\\cap \\bar{B}) + P(\\bar{A} \\cap B) = P(A)P(\\bar{B}) + P(\\bar{A})P(B) = 2 \\times 0.25 \\times (1 - 0.25) = 0.375\n\\] \\[\nP(\\text{neither}) = P(\\bar{A} \\cap \\bar{B}) = P(\\bar{A})P(\\bar{B}) = (1 - 0.25)^2 = 0.5625\n\\]\n\n\n\n\n3.47Solution\n\n\nSuppose there is a 1/3 probability of MZ twins and a 2/3 probability of DZ twins. What is the overall probability that both twins are affected? One twin affected? Neither affected?\n\n\n\\[\nP(\\text{both}) = P(\\text{both}|MZ)P(MZ) + P(\\text{both}|DZ)P(DZ) = 0.25 \\times 1/3 + 0.0625 \\times 2/3 = 0.125\n\\] \\[\nP(\\text{one}) = P(\\text{one}|MZ)P(MZ) + P(\\text{one}|DZ)P(DZ) = 0 \\times 1/3 + 0.375 \\times 2/3 = 0.25\n\\] \\[\nP(\\text{neither}) = P(\\text{neither}|MZ)P(MZ) + P(\\text{neither}|DZ)P(DZ) = 0.75 \\times 1/3 + 0.5625 \\times 2/3 = 0.625\n\\]\n\n\n\n\n3.48Solution\n\n\nSuppose we learn that both twins are affected but don’t know whether they are MZ or DZ twins. What is the probability that they are MZ twins given this additional information?\n\n\nWe use Bayes rule.\n\\[\nP(MZ|\\text{both}) = \\frac{P(\\text{both}|MZ)P(MZ)}{P(\\text{both})} = \\frac{0.25 \\times 1/3}{0.125} = 2/3\n\\]"
  },
  {
    "objectID": "review/pp_03.html#cerebrovascular-disease",
    "href": "review/pp_03.html#cerebrovascular-disease",
    "title": "Chapter 3 Practice Problems",
    "section": "Cerebrovascular Disease",
    "text": "Cerebrovascular Disease\nAtrial fibrillation (AF) is a common cardiac condition in the elderly (e.g., former President George H.W. Bush has this condition) characterized by an abnormal heart rhythm that greatly increases the risk of stroke. The following estimates of the prevalence rate of AF and the incidence rate of stroke for people with and without AF by age from the Framingham Heart Study are given in Table 3.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.6: Relationship between atrial fibrillation and stroke\n\n\nAge group\nPrevalence of AF (%)\n\nIncidence rate of stroke per 1000 person-years\n\n\n\nNo AF\nAF\n\n\n\n\n60–69\n1.8\n4.5\n21.2\n\n\n70–79\n4.7\n9.0\n48.9\n\n\n80–89\n10.2\n14.3\n71.4\n\n\n\n\n\n\n\n\n3.49Solution\n\n\nWhat does an incidence rate of 48.9 strokes per 1000 person-years among 70–79-year-olds with AF mean in Table 3.6?\n\n\n\n\n\n\n\n3.50Solution\n\n\nWhat is the relative risk of stroke for people with AF compared with people without AF in each age group? Does the relative risk seem to be the same for different age groups\n\n\n\n\n\n\nSuppose we screen 500 subjects from the general population of age 60–89, of whom 200 are 60–69, 200 are 70–79, and 100 are 80–89.\n\n3.51Solution\n\n\nWhat is the incidence rate of stroke in the screened population over a 1-year period?\n\n\n\n\n\n\n\n3.52Solution\n\n\nSuppose the study of 500 subjects is a “pilot study” for a larger study. How many 60-89 year old subjects need to be screened if we wish to observe an average of 50 strokes in the larger study over 1 year?"
  },
  {
    "objectID": "review/pp_03.html#radiology",
    "href": "review/pp_03.html#radiology",
    "title": "Chapter 3 Practice Problems",
    "section": "Radiology",
    "text": "Radiology\nIt is well-known that there is variation among readers in evaluating radiologic images. For this purpose, we present evaluations from a 2nd reader of the same 100 images for the study described in Section 3.6.1 of the Study Guide. The results are given in Table 3.7.\n\n\n\n\n\n\n\n\nTable 3.7\n\n\nRatings from Reader 2 using PACS film for the study described in Section 3.6.1\n\n\nTrue Status\n1\n2\n3\n4\n5\nTotal\n\n\n\n\nNegative\n0\n3\n7\n14\n9\n33\n\n\nPositive\n0\n49\n11\n4\n3\n67\n\n\nTotal\n0\n52\n18\n18\n12\n100\n\n\n\n\n\n\n\nSuppose different cutpoints are considered for positivity, viz, ≤ 0 , ≤ 1, ≤ 2, ≤ 3, ≤ 4 and ≤ 5.\n\n3.53Solution\n\n\nCompute the sensitivity of the test for each cutpoint for Reader 2.\n\n\n\n\n\n\n\n3.54Solution\n\n\nCompute the specificity of the test for each cutpoint for Reader 2.\n\n\n\n\n\n\n\n3.55Solution\n\n\nDraw the ROC curve for Reader 2.\n\n\n\n\n\n\n\n3.56Solution\n\n\nCompare the accuracy of the test for Reader 2 vs Reader 1."
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease-3",
    "href": "review/pp_03.html#cardiovascular-disease-3",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease 3",
    "text": "Cardiovascular Disease 3\nExercise testing has sometimes been used to diagnose patients with coronary-artery disease. One test criterion that has been used to identify those with disease is the abnormal ejection-fraction criterion; that is, an absolute rise of less than 0.05 with exercise. The validity of this noninvasive test was assessed in 196 patients versus coronary angiography, the gold standard, a procedure that can unequivocally diagnose the disease but the administration of which carries some risk for the patient. A sensitivity of 79% and a specificity of 68% were found for the exercise test in this group.\n\n3.36Solution\n\n\nWhat does the sensitivity mean in words in this setting?\n\n\nThe probability of abnormal ejection-fraction given coronary-artery disease (as determined by coronary angiography).\n\n\n\n\n3.37Solution\n\n\nWhat does the specificity mean in words in this setting?\n\n\nThe probability of a normal ejection-fraction given no coronoary-artery disease (as determined by coronary angiography).\n\n\n\nSuppose a new patient undergoes exercise testing and a physician feels before seeing the exercise-test results that the patient has a 20% chance of having coronary-artery disease.\n\n3.38Solution\n\n\nIf the exercise test is positive, then what is the probability that such a patient has disease?\n\n\nFrom the sensitivity and specificity, we have\n\n\\(P(T^+|D^+) = 0.79\\)\n\\(P(T^-|D^-) = 0.68\\)\n\nThe doctor told us that \\(P(D^+) = 0.2\\).\nWe want \\(P(D^+|T^+)\\). We can do this by Bayes rule: \\[\nP(D^+|T^+) = \\frac{P(T^+|D^+)P(D^+)}{P(T^+)}\n\\] The numerator terms are given. We can calculate the denominator by the law of total probability \\[\nP(T^+) = P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-) = 0.79 \\times 0.2 + (1 - 0.68) \\times (1 - 0.2) = 0.414\n\\]\nSo, \\[\nP(D^+|T^+) = \\frac{0.79 \\times 0.2}{0.414} = 0.3816\n\\]\n\n\n\n\n3.39Solution\n\n\nAnswer Problem 3.38 if the exercise test is negative.\n\n\nWe want \\(P(D^+|T^-)\\), which we can do by Bayes rule \\[\nP(D^+|T^-) = \\frac{P(T^-|D^+)P(D^+)}{P(T^-)}\n=\\frac{(1 - 0.79)\\times 0.2}{1 - 0.414}\n= 0.07167\n\\]"
  },
  {
    "objectID": "review/pp_04.html",
    "href": "review/pp_04.html",
    "title": "Chapter 4 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_04.html#health-services-administration",
    "href": "review/pp_04.html#health-services-administration",
    "title": "Chapter 4 Practice Problems",
    "section": "Health-Services Administration",
    "text": "Health-Services Administration\nThe in-hospital mortality rate for 16 clinical conditions at 981 hospitals was recently reported. It was reported that in-hospital mortality was 10.5% for coronary-bypass surgery and 5.0% for total hip replacement. Suppose an institution changes from an academic institution to a private for-profit institution. They find that after the change, of the first 20 patients receiving coronary-bypass surgery, 5 die, while of 20 patients receiving total hip replacement, 4 die.\n\n4.8Solution\n\n\nWhat is the probability that of 20 patients receiving coronary-bypass surgery, exactly 5 will die in-hospital, if this hospital is representative of the total pool of 981 hospitals?\n\n\n\n\n\n\n\n4.9Solution\n\n\nWhat is the probability of at least 5 deaths among the coronary-bypass patients?\n\n\n\n\n\n\n\n4.10Solution\n\n\nWhat is the probability of no more than 5 deaths among the coronary-bypass patients?\n\n\n\n\n\n\n\n4.11Solution\n\n\nWhat is the probability that exactly 4 will die among the hip-replacement patients?\n\n\n\n\n\n\n\n4.12Solution\n\n\nWhat is the probability that at least 4 will die among the hip-replacement patients?\n\n\n\n\n\n\n\n4.13Solution\n\n\nWhat is the probability of 4 or fewer deaths among the hip-replacement patients?\n\n\n\n\n\n\n\n4.14Solution\n\n\nCan you draw any conclusions based on the results in Problems 4.8–4.13 regarding any effects of the change in hospital administration on in-hospital mortality rates?"
  },
  {
    "objectID": "review/pp_04.html#cardiovascular-disease",
    "href": "review/pp_04.html#cardiovascular-disease",
    "title": "Chapter 4 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nThe rate of myocardial infarction (MI) in 50–59-year-old, disease-free women is approximately 2 per 1000 per year or 10 per 1000 over 5 years. Suppose that 3 MI’s are reported over 5 years among 1000 postmenopausal women initially disease free who have been taking postmenopausal hormones.\n4.15 Use the binomial distribution to see if this experience represents an unusually small number of events based on the overall rate.\n4.16 Answer Problem 4.15 using the Poisson approximation to the binomial distribution.\n4.17 Compare your answers in Problems 4.15 and 4.16."
  },
  {
    "objectID": "review/pp_04.html#cardiovascular-disease-1",
    "href": "review/pp_04.html#cardiovascular-disease-1",
    "title": "Chapter 4 Practice Problems",
    "section": "Cardiovascular Disease 1",
    "text": "Cardiovascular Disease 1\nThe rate of myocardial infarction (MI) in 50–59-year-old, disease-free women is approximately 2 per 1000 per year or 10 per 1000 over 5 years. Suppose that 3 MI’s are reported over 5 years among 1000 postmenopausal women initially disease free who have been taking postmenopausal hormones.\n\n4.15Solution\n\n\nUse the binomial distribution to see if this experience represents an unusually small number of events based on the overall rate.\n\n\n\n\n\n\n\n4.16Solution\n\n\nAnswer Problem 4.15 using the Poisson approximation to the binomial distribution.\n\n\n\n\n\n\n\n4.17Solution\n\n\nCompare your answers in Problems 4.15 and 4.16."
  },
  {
    "objectID": "review/pp_04.html#pediatrics",
    "href": "review/pp_04.html#pediatrics",
    "title": "Chapter 4 Practice Problems",
    "section": "Pediatrics",
    "text": "Pediatrics\nA hospital administrator wants to construct a special-care nursery for low-birthweight infants (≤ 2500 g) and wants to have some idea as to the number of beds she should allocate to the nursery. She is willing to assume that the recovery period of each baby is exactly 4 days and thus is interested in the expected number of premature births over the period.\n\n4.19Solution\n\n\nIf the number of premature births in any 4-day period is binomially distributed with parameters n = 25 and p= .1, then find the probability of 0, 1, 2, …, 7 premature births over this period.\n\n\n\n\n\n\n\n4.20Solution\n\n\nThe administrator wishes to allocate x beds where the probability of having more than x premature births over a 4-day period is less than 5%. What is the smallest value of x that satisfies this criterion?\n\n\n\n\n\n\n\n4.21Solution\n\n\nAnswer Problem 4.20 for 1%."
  },
  {
    "objectID": "review/pp_04.html#cancer",
    "href": "review/pp_04.html#cancer",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\nThe incidence rate of malignant melanoma is suspected to be increasing over time. To document this increase, a questionnaire was mailed to 100,000 U.S. nurses in 1976 and 1978, asking about any current or previous tumors. Thirty new cases of malignant melanoma were found to have developed over the 2-year period among women with no previous cancers in 1976.\n4.22 If the annual incidence rate from previous cancer-registry data is 10 per 100,000, then what is the expected number of new cases over 2 years?\n4.23 Are the preceding results consistent or inconsistent with the cancer-registry data? Specifically, what is the prob- ability of observing at least 30 new cases over a 2-year period if the cancer-registry incidence rate is correct?"
  },
  {
    "objectID": "review/pp_04.html#accident-epidemiology",
    "href": "review/pp_04.html#accident-epidemiology",
    "title": "Chapter 4 Practice Problems",
    "section": "Accident Epidemiology",
    "text": "Accident Epidemiology\nSuppose the annual number of traffic fatalities at a given intersection follows a Poisson distribution with parameter µ = 10 .\n\n4.24Solution\n\n\nWhat is the probability of observing exactly 10 traffic fatalities in 1992?\n\n\n\n\n\n\n\n4.25Solution\n\n\nWhat is the probability of observing exactly 25 traffic fatalities over the 2-year period from January 1, 1990, to December 31, 1991?\n\n\n\n\n\n\n\n4.26Solution\n\n\nSuppose that the traffic intersection is redesigned with better lighting, and 12 traffic fatalities are observed over the next 2 years. Is this rate a meaningful improvement over the previous rate of traffic fatalities?"
  },
  {
    "objectID": "review/pp_04.html#pulmonary-disease-environmental-health",
    "href": "review/pp_04.html#pulmonary-disease-environmental-health",
    "title": "Chapter 4 Practice Problems",
    "section": "Pulmonary Disease, Environmental Health",
    "text": "Pulmonary Disease, Environmental Health\nSuppose the number of people seen for violent asthma attacks in the emergency ward of a hospital over a 1-day period is usually Poisson distributed with parameter λ = 1.5\n\n4.27Solution\n\n\nWhat is the probability of observing 5 or more cases over a 2-day period?\n\n\n\n\n\n\nOn a particular 2-day period, the air-pollution levels increase dramatically and the distribution of attacks over a 1-day period is now estimated to be Poisson distributed with parameter λ = 3.\n\n4.28Solution\n\n\nAnswer Problem 4.27 under these assumptions.\n\n\n\n\n\n\n\n4.29Solution\n\n\nIf 10 days out of every year are high-pollution days, then what is the expected number of asthma cases seen in the emergency ward over a 1-year period? (Assume there are 365 days in a year.)"
  },
  {
    "objectID": "review/pp_04.html#pulmonary-disease",
    "href": "review/pp_04.html#pulmonary-disease",
    "title": "Chapter 4 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\nEach year approximately 4% of current smokers attempt to quit smoking, and 50% of those who try to quit are successful in the sense that they abstain from smoking for at least 1 year from the date they quit.\n\n4.35Solution\n\n\nWhat is the probability that a current smoker will quit for at least 1 year?\n\n\n\n\n\n\n\n4.36Solution\n\n\nWhat is the probability that among 100 current smokers, at least 5 will quit smoking for at least 1 year?\n\n\n\n\n\n\nAn educational program was conducted among smokers who attempt to quit to maximize the likelihood that such individuals would continue to abstain for the long term.\n\n4.37Solution\n\n\nSuppose that of 20 people who enter the program when they first stop smoking, 15 still abstain from smoking 1 year later. Can the program be considered successful?"
  },
  {
    "objectID": "review/pp_04.html#infectious-disease",
    "href": "review/pp_04.html#infectious-disease",
    "title": "Chapter 4 Practice Problems",
    "section": "Infectious Disease",
    "text": "Infectious Disease\nAn outbreak of acute gastroenteritis occurred at a nursing home in Baltimore, Maryland, in December 1980. A total of 46 out of 98 residents of the nursing home became ill. People living in the nursing home shared rooms: 13 rooms contained 2 occupants, 4 rooms contained 3 occupants, and 15 rooms contained 4 occupants. One question that arises is whether or not a geographical clustering of disease occurred for persons living in the same room.\nA summary of the number of affected people and the total number of people in a room is given in Table 4.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.3\n\n\nNumber of affected people and total number of people in a room for an outbreak of acute gastroenteritis in a nursing home in Baltimore, Maryland\n\n\nPeople in room\nTotal number of rooms\n\nNumber of rooms with\n\n\n\n0 affected people\n1 affected person\n2 affected people\n3 affected people\n4 affected people\n\n\n\n\n2\n13\n5\n4\n4\n0\n0\n\n\n3\n4\n1\n2\n0\n1\n0\n\n\n4\n15\n2\n4\n3\n5\n1\n\n\n\n\n\n\n\n\n4.47Solution\n\n\nIf the binomial distribution holds, what is the probabil- ity distribution of the number of affected people in rooms with 2 occupants? That is, what is the probability of finding zero affected people? One affected person? Two affected people?\n\n\n\n\n\n\n\n4.48Solution\n\n\nAnswer Problem 4.47 for the probability distribution of the number of affected people in rooms with 3 occupants.\n\n\n\n\n\n\n\n4.49Solution\n\n\nAnswer Problem 4.47 for the probability distribution of the number of affected people in rooms with 4 occupants.\n\n\n\n\n\n\n\n4.50Solution\n\n\nOne useful summary measure of geographical clustering is the number of rooms with 2 or more affected occupants. If the binomial distribution holds, what is the expected number of rooms with 2 or more affected occupants over the entire nursing home?\n\n\n\n\n\n\n\n4.51Solution\n\n\nCompare the observed number of rooms with 2 or more affected occupants with the expected number of rooms. Does this comparison give any evidence that clustering of disease occurs within rooms?"
  },
  {
    "objectID": "review/pp_04.html#cancer-1",
    "href": "review/pp_04.html#cancer-1",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer 1",
    "text": "Cancer 1\nThe incidence rate of malignant melanoma is suspected to be increasing over time. To document this increase, a questionnaire was mailed to 100,000 U.S. nurses in 1976 and 1978, asking about any current or previous tumors. Thirty new cases of malignant melanoma were found to have developed over the 2-year period among women with no previous cancers in 1976.\n\n4.22Solution\n\n\nIf the annual incidence rate from previous cancer-registry data is 10 per 100,000, then what is the expected number of new cases over 2 years?\n\n\n\n\n\n\n\n4.23Solution\n\n\nAre the preceding results consistent or inconsistent with the cancer-registry data? Specifically, what is the prob- ability of observing at least 30 new cases over a 2-year period if the cancer-registry incidence rate is correct?"
  },
  {
    "objectID": "review/pp_04.html#cancer-2",
    "href": "review/pp_04.html#cancer-2",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer 2",
    "text": "Cancer 2\nThe incidence rate of malignant melanoma in women ages 35–59 is approximately 11 new cases per 100,000 women per year. A study is planned to follow 10,000 women with excessive exposure to sunlight.\n\n4.52Solution\n\n\nWhat is the expected number of cases among 10,000 women over 4 years? (Assume no excess risk due to sunlight exposure.)\n\n\n\n\n\n\n\n4.53Solution\n\n\nSuppose that 9 new cases are observed in this period. How unusual a finding is this?"
  },
  {
    "objectID": "review/pp_04.html#cardiovascular-disease-2",
    "href": "review/pp_04.html#cardiovascular-disease-2",
    "title": "Chapter 4 Practice Problems",
    "section": "Cardiovascular Disease 2",
    "text": "Cardiovascular Disease 2\n\n4.18Solution\n\n\nAn accepted hypothesis in the etiology of heart disease is that aspirin intake of 325 mg per day reduces subsequent cardiovascular mortality in men with a prior heart attack. Suppose that in a pilot study of 50 women who received 1 tablet per day (325 mg), only 2 die over a 3-year period from cardiovascular disease. How likely is it that not more than 2 women will die if the underlying 3-year mortality rate is 10% in such women?"
  },
  {
    "objectID": "review/pp_04.html#cancer-3",
    "href": "review/pp_04.html#cancer-3",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer 3",
    "text": "Cancer 3\nA study was performed in Woburn, MA, looking at the rate of leukemia among children (≤ age 19) in the community in comparison to statewide leukemia rates. Suppose there arc 12,000 children in the community that have lived there for a 10-year period and 12 leukemias have occurred in 10 years.\n\n4.54Solution\n\n\nIf the statewide incidence rate of leukemia in children is 5 events per 100,000 children per year (i.e., per 100,000 person-years) then how many leukemias would be expected in Woburn over the 10-year period if the statewide rates were applicable?\n\n\n\n\n\n\n\n4.55Solution\n\n\nWhat is the probability of obtaining exactly 12 events over a 10-year period if statewide incidence rates were applicable?\n\n\n\n\n\n\n\n4.56Solution\n\n\nWhat is the probability of obtaining at least 12 events over the 10-year period if the statewide incidence rates were applicable?\n\n\n\n\n\n\n\n4.57Solution\n\n\nHow do you interpret the results in Problem 4.56?"
  },
  {
    "objectID": "review/pp_04.html#basic-discrete-probability",
    "href": "review/pp_04.html#basic-discrete-probability",
    "title": "Chapter 4 Practice Problems",
    "section": "Basic Discrete Probability",
    "text": "Basic Discrete Probability\n\n4.1Solution\n\n\nEvaluate the number of ways of selecting 4 objects out of 10 if the order of selection matters. What term is used to denote this quantity?\n\n\n\n\n\n\n\n4.2Solution\n\n\nEvaluate the number of ways of selecting 4 objects out of 10 if the order of selection does not matter. What term is used to denote this quantity?\n\n\n\n\n\n\nSuppose that the probability that a person will develop hypertension over a lifetime is 20%.\n\n4.3Solution\n\n\nWhat is the probability distribution of the number of hypertensives over a lifetime among 20 students graduating from the same high school class?\n\n\n\n\n\n\n\n4.4Solution\n\n\nWhat is the probability that exactly 4 people out of 50 aged 60–64 will die after receiving the flu vaccine if the probability that 1 person will die is .028?\n\n\n\n\n\n\n\n4.5Solution\n\n\nWhat is the probability that at least 4 people will die after receiving the vaccine?\n\n\n\n\n\n\n\n4.6Solution\n\n\nWhat is the expected number of deaths following the flu vaccine?\n\n\n\n\n\n\n\n4.7Solution\n\n\nWhat is the standard deviation of the number of deaths following the flu vaccine?"
  }
]