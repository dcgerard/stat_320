[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Unless otherwise noted, all data are from Rosner (2015), downloaded from the book’s companion site.\nI did some cleaning (changing 9’s to NA’s, recoding numerics to informative characters, etc).\n\nbetacar\nHigh doses of beta-carotene in food have been linked to a reduced cancer risk in some observational studies. A study considered four beta-carotene capsule preparations: Solatene (30 mg), Roche (60 mg), and two from BASF (30 mg and 60 mg). To test their effectiveness in raising plasma-carotene levels, 23 volunteers were randomized to one of the four preparations, taking one pill every other day for 12 weeks. The primary endpoint was the plasma carotene level after prolonged ingestion.\n\nbetacar.csv\n\nPrepar: Preparation.\n\nPossible values: SOL, ROCHE, BASF-30, BASF-60.\n\nId: Subject #\nBase1lvl: 1st Baseline Level\nBase2lvl: 2nd Baseline Level\nWk6lvl: Week 6 Level\nWk8lvl: Week 8 Level\nWk10lvl: Week 10 Level\nWk12lvl: Week 12 Level\n\n\n\n\nbirthweight\nThe birthweights of 1000 consecutive infants born at Boston City Hospital, which serves a low-income population.\n\nbirthweight.csv\n\nid: ID\nweight: Birthweight (oz)\n\n\n\n\nblood\nData from a case-control study investigated various plasma risk factors for breast cancer. The women were matched approximately by age at the time of blood draw, fasting status, and, when possible, current postmenopausal hormone (PMH) use at the time of blood draw. Each matched set included one case and either one or two controls, although some sets are incomplete due to missing data. The matching variable is matchid.\n\nblood.csv\n\nId: ID\nmatchid: Matched ID\ncase: Case/control.\n\nPossible values: case, control.\n\ncurpmh: Current PMH use.\n\nPossible values: yes, no.\n\nageblood: Age at blood draw\nestradol: Estradiol\nestrone: Estrone\ntestost: Testosterone\nprolactn: Prolactin\n\n\n\n\nboneden\nA study in Australia examined the relationship between bone density and cigarette smoking in middle-aged female twins with different smoking histories. Forty-one pairs of twins visited a hospital in Victoria, Australia, where their bone density was measured. Participants also completed questionnaires providing information on their tobacco use, alcohol, coffee, and tea consumption, calcium intake from dairy products, menopausal and reproductive history, fracture history, use of oral contraceptives or estrogen replacement therapy, and physical activity levels. Tobacco consumption was measured in pack-years, with one pack-year defined as smoking one pack of cigarettes per day for one year.\n\nboneden.csv\n\nID: ID\nAge: Age (yrs)\nzyg: Twin type.\n\nPossible values: mz, dz\n\nTwin 1 Lighter Smoking Twin\n\nht1: Height (cm)\nwt1: Weight (kg)\ntea1: Tea (cups/week)\ncof1: Coffee (cups/week)\nalc1: Alcohol (drinks/week)\ncur1: Current Smoking (cigarettes/day)\nmen1: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr1: Pack-years smoking\nls1: Lumbar spine (g/cm\\(^2\\))\nfn1: Femoral neck (g/cm\\(^2\\))\nfs1: Femoral shaft (g/cm\\(^2\\))\n\nTwin 2 Heavier Smoking Twin\n\nht2: Height (cm)\nwt2: Weight (kg)\ntea2: Tea (cups/week)\ncof2: Coffee (cups/week)\nalc2: Alcohol (drinks/week)\ncur2: Current Smoking (cigarettes/day)\nmen2: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr2: Pack-years smoking\nls2: Lumbar spine (g/cm\\(^2\\))\nfn2: Femoral neck (g/cm\\(^2\\))\nfs2: Femoral shaft (g/cm\\(^2\\))\n\n\n\n\n\nbotox\nA study on patients with piriformis syndrome compared the effects of three types of injections: triamcinolone with lidocaine (TL), a placebo (Placebo), and Botox (Botox). The patients were randomly assigned to these groups in a 3:1:2 ratio and received injections directly into the piriformis muscle. They were evaluated at 2 weeks, 1 month, and monthly up to 17 months, though many missed visits. Improvement in pain was measured on a scale from 0% to 100% (higher means more improved). The study involved 69 patients, with one having the condition in both legs. The goal was to compare the efficacy between the groups, considering age, gender, and affected side as potential influencing factors.\n\nbotox.csv\n\nID: ID\ngroup:\n\nPossible values: TL, Placebo, Botox\n\nside: left (L), middle (M), or right (R).\ngender: male or female\nage: in years\npain0: pain score month 0\npain05: pain score month 0.5\npain1: pain score month 1\npain2: pain score month 2\npain3: pain score month 3\npain4: pain score month 4\npain5: pain score month 5\npain6: pain score month 6\npain7: pain score month 7\npain8: pain score month 8\npain9: pain score month 9\npain10: pain score month 10\npain11: pain score month 11\npain12: pain score month 12\npain13: pain score month 13\npain14: pain score month 14\npain15: pain score month 15\npain16: pain score month 16\npain17: pain score month 17\n\n\n\n\nbreast\nThe data set includes 1200 postmenopausal women from the NHS, free of cancer in 1990. Of these, 200 were using postmenopausal hormones (PMH) in 1990, and 1000 had never used them. The study aimed to link PMH use in 1990 to breast cancer incidence from 1990 to 2000. Fifty-three women developed breast cancer during this period. PMH use was categorized by current use and duration of use in 1990, with separate durations for estrogen and estrogen plus progesterone. Each woman has a return date for the 1990 questionnaire and a follow-up date, which is either the date of breast cancer diagnosis or the date of the last questionnaire by 2000. The file also includes data on other breast cancer risk factors as of 1990.\n\nbreast.csv\n\nId: ID\ncase: Treatment\n\nPossible values: case,control\n\nage: age\nagemenar: age at menarche\nagemenop: age at menopause\nafb: age at first birth 98=nullip\nparity: parity\nbbd: Benign Breast disease.\n\nPossible values: yes, no\n\nfamhx: family history breast cancer.\n\nPossible values: yes, no\n\nbmi: BMI (kg/m**2)\nhgt: Height (inches)\nalcohol: Alcohol use (grams/day)\npmh: PMH status.\n\nPossible values: never user, current user\n\ndur3: Duration of Estrogen use (months)\ndur4: Duration of Estrogen + progesterone use (months)\ncsmk: Current Smoker.\n\nPossible values: yes, no\n\npsmk: Past smoker.\n\nPossible values: yes, no\n\nfoluptm: Months of follow up. Note: Some subjects provided no follow up after the 1990 questionnaire and foluptm = 0 for these people\n\n\n\n\ncholesterol\nCholesterol levels from 24 hospital employees who switched from a standard American diet to a vegetarian diet for one month. Their serum cholesterol was measured before and after the diet change.\n\ncholesterol.csv\n\nSubject: ID\nBefore: Serum-cholesterol levels before diet change (mg/dL)\nAfter: Serum-cholesterol levels after diet change (mg/dL)\nDifference: Difference in serum-cholesterol levels, Before - After, where positive numbers indicate a reduction in serum-cholesterol levels.\n\n\n\n\ncorneal\nFluoroquinolones, antibiotics for bacterial infections, are FDA-approved for systemic use. However, post-approval studies indicate a risk of peripheral neuropathy, leading to updated safety labeling.\nA small clinical trial tested the safety and effectiveness of two fluoroquinolone eye drops (drugs M and G) and a placebo (drug P) for bacterial eye infections. Ninety-three subjects were randomly assigned to three groups, each receiving one active drug and a placebo in opposite eyes. Participants used the drops four times daily for 10 days. The primary outcome was corneal sensitivity, measured in millimeters, with higher values indicating more normal sensitivity.\nCorneal sensitivity was assessed at baseline, 7 days, and 14 days, with measurements taken from the central cornea and four quadrants (superior, inferior, temporal, nasal).\n\ncorneal.csv\n\nid: ID\ntr: Treatment.\n\nPossible values: M, G, P\n\nc1: Central visit 1\ns1: Superior visit 1\ni1: Inferior Visit 1\nt1: Temporal visit 1\nn1: Nasal Visit 1\nc2: Central Visit 2(day 7)\ns2: Superior Visit 2\ni2: Inferior Visit 2\nt2: Temporal Visit 2\nn2: Nasal Visit 2\nc3: Central Visit 3(day 14)\ns3: Superior Visit 3\ni3: Inferior Visit 3\nt3: Temporal Visit 3\nn3: Nasal Visit 3\n\n\n\n\ndiabetes\nType I diabetes is common in children and requires regular insulin shots to prevent long-term complications like neurologic, vision, kidney issues, heart disease, and premature death.\nThe impact of diabetes control on childhood growth is less clear. To study this, adolescent boys aged 9−15 were examined about every 3 months. Each exam measured diabetes control using glycosylated hemoglobin (HgbA1c), where higher HgbA1c indicates poorer control (normal &lt;7.0). Age, height, and weight were also recorded. Data includes 94 boys over 910 visits.\nThe main question is the overall relationship between glycemic control and growth, primarily weight, for the entire group, not individual cases.\n\ndiabetes.csv\n\nID: ID\nmon_a1c: Month A1c\nday_a1c: Day A1c\nyr_a1c: Yr A1c\nage_yrs: Age in years\ngly_a1c: Hemoglobin A1c\nht_cm: Height in cm\nwt_kg: Weight in kg\n\n\n\n\near\nThis dataset includes 203 children with acute otitis media (OME) from a randomized clinical trial. Each child had OME in one or both ears and received a 14-day course of either cefaclor (CEF) or amoxicillin (AMO). Middle-ear status was assessed after the 14-day treatment.\n\near.csv\n\nId: ID\nClear: Clearance by 14 days,\n\nPossible values: yes, no\n\nAntibo: Antibiotic.\n\nPossible values: CEF, AMO\n\nAge: Age. Possible values: &lt;2 yrs, 2-5 yrs, 6+ yrs.\nEar: Ear,\n1 = 1st ear\n2 = 2nd ear\n\n\n\n\neff, nephro, and oto\nAminoglycoside antibiotics are crucial for treating severe gram-negative bacterial infections in hospitalized patients. Despite their toxicity and the development of new antibiotics, aminoglycosides remain widely used. Choosing the right aminoglycoside depends on the clinical situation, antimicrobial spectrum, cost, and side effects like nephrotoxicity and auditory toxicity. Numerous trials comparing these antibiotics have varied in design and conclusions, often lacking sufficient sample sizes to detect small differences.\nTo better understand their true effects, a meta-analysis of all randomized trials was conducted. This analysis included 45 trials from 1975 to 1985, comparing amikacin, gentamicin, netilmicin, sisomicin, and tobramycin. Data from 37 trials were suitable for comparison, focusing on efficacy, nephrotoxicity, and auditory toxicity. Efficacy was defined by bacterial or clinical response as reported by each trial, nephrotoxicity by the percentage of kidney-related toxic events reported, and auditory toxicity by the diffence in pre- and post-treatment audiograms. The data is organized into three sets: eff.csv, nephro.csv, and oto.csv, with separate records for each antibiotic and endpoint.\n\neff.csv\n\nName: Study name\nId: Study Number\nEndpnt: Endpoint, 1 = efficacy\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nCured: Number Cured\n\nnephro.csv\n\nname: Study name\nid: Study number\nEndpnt: Endpoint 2 = nephrotoxicity\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin\n\nSamp_sz: Sample size\nSide_eff: Number with side effects\n\noto.csv\n\nName: Study Name\nId: Study Number\nEndpnt: Endpoint.\n\nPossible values: efficacy, nephrotoxicity, ototoxicity.\n\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nSide_eff: Number with side effect\n\n\n\n\nendocrin\nThe data set contains split-sample plasma measurements of four hormones for each of five subjects, all from one laboratory.\n\nendocrin.csv\n\nSubject: Subject number\nReplicat: Replicate number\nEstrone: Estrone\nEstradol: Estradiol\nAndroste: Androstenedione\nTestost: Testosterone\n\n\n\n\nestradl\nObesity is common in American society and a risk factor for breast cancer in postmenopausal women, possibly due to increased estrogen levels, particularly serum estradiol. Researchers studied 151 African American and 60 Caucasian premenopausal women, measuring adiposity using body mass index (BMI) (a measure of overall adioposity) and waist-hip ratio (WHR) (a measure of abdominal adioposity). They also obtained a complete hormonal profile, including serum estradiol, and assessed other breast cancer risk factors: ethnicity, age, number of children, age at first birth, presence of children, and age at menarche.\n\nestradl.csv\n\nId: Identification number\nEstradl: Estradiol\nEthnic: Ethnicity.\n\nPossible values: African-American, Caucasian\n\nEntage: Age\nNumchild: Parity, number of children\nAgefbo: Age at 1st birth (=0 if numchild=0)\nAnykids: any children.\n\nPossible values: yes, no.\n\nAgemenar: age at menarche\nBMI: Body Mass Index\nWHR: waist-hip ratio\n\n\n\n\nestrogen\nThree distinct two-period crossover studies were conducted with different subject groups, measuring systolic and diastolic blood pressure. In Study 1, 0.625 mg of estrogen was compared with a placebo. Study 2 compared 1.25 mg of estrogen with a placebo. Study 3 compared 1.25 mg of estrogen with 0.625 mg of estrogen. Each active treatment period lasted for four weeks, with a two-week washout period between them.\n\nestrogen.csv\n\nId: ID\nstd_typ: Study type.\n\nPossible values: 0.625MG VS PLACEBO, 1.25MG VS PLACEBO, 1.25MG VS 0.625MG\n\nperiod: Period\ntrtgrp: Treatment.\n\nPossible values: PLACEBO, 0.625MG, 1.25MG\n\nsysd1r1: Systolic blood pressure day 1 reading 1\ndiasd1r1: Diastolic blood pressure day 1 reading 1\nsysd1r2: Systolic blood pressure day 1 reading 2\ndiasd1r2: Diastolic blood pressure day 1 reading 2\nsysd1r3: SYSTOLIC BP DAY 1 reading 3\ndiasd1r3: Diastolic blood pressure day 1 reading 3\nsysd2r1: Systolic blood pressure day 2 reading 1\ndiasd2r1: Diastolic blood pressure day 2 reading 1\nsysd2r2: Systolic blood pressure day 2 reading 2\ndiasd2r2: Diastolic blood pressure day 2 reading 2\nsysd2r3: Systolic blood pressure day 2 reading 3\ndiasd2r3: Diastolic blood pressure day 2 reading 3\nsysd3r1: Systolic blood pressure day 3 reading 1\ndiasd3r1: Diastolic blood pressure day 3 reading 1\nsysd3r2: Systolic blood pressure day 3 reading 2\ndiasd3r2: Diastolic blood pressure day 3 reading 2\nsysd3r3: Systolic blood pressure day 3 reading 3\ndiasd3r3: Diastolic blood pressure day 3 reading 3\n\n\n\n\nfev\nForced expiratory volume (FEV) is a measure of pulmonary function that quantifies the volume of air expelled in one second of sustained effort. This dataset includes FEV measurements from 1980 for 654 children aged 3 to 19 who participated in the Childhood Respiratory Disease (CRD) Study in East Boston, Massachusetts. The data are part of a longitudinal study aimed at tracking changes in pulmonary function over time in children.\n\nfev.csv\n\nId: ID number\nAge: Age (yrs)\nFEV: FEV (liters)\nHgt: Height (inches)\nSex: Sex.\n\nPossible values: female, male\n\nSmoke: Smoking Status.\n\nPossible values: non-current, current smoker.\n\n\n\n\n\nfield\nRetinitis pigmentosa (RP) is a hereditary eye disease that can lead to substantial vision loss or blindness. It has dominant, recessive, and sex-linked forms, with mutations in the rhodopsin (RHO) gene linked to dominant cases and RPGR gene mutations linked to sex-linked cases.\nThe data file field.csv contains visual field data for approximately 100 patients each from the RHO and RPGR groups. Visual field, measured in degrees\\(^2\\), indicates the area of vision. The dataset includes longitudinal data with varying follow-up times from a minimum of 3 years to a maximum of about 25-30 years. Measurements are provided separately for the right eye (OD) and the left eye (OS).\n\nfield.csv\n\nid: ID\ngroup: group.\n\nPossible values: RHO, RPGR\n\nage: age at visit (years)\ngender: gender. Note: all RPGR individuals have to be male.\n\nPossible values: male, female.\n\ndtvisit: date of visit (YYYY-MM-DD)\nfolowup: time from 1st visit in years\ntotfldod: total field area right eye (OD) in degrees\ntotfldos: total field area left eye (OS) in degrees\n\n\n\n\nheart\n\nheart.csv\n\nDiagnosis: Possible values:\n\nY1 = normal\nY2 = atrial septal defect without pulmonary stenosis or pulmonary hypertension\nY3 = ventricular septal defect with valvular pulmonary stenosis\nY4 = isolated pulmonary hypertension\nY5 = transposed great vessels\nY6 = ventricular septal defect without pulmonary hypertension\nY7 = ventricular septal defect with pulmonary hypertension\n\nPrevalence: Prevalence\nX1: age 1-20 years old\nX2: age&gt;20 years old\nX3: mild cyanosis\nX4: easy fatigue\nX5: chest pain\nX6: repeated respiratory infections\nX7: EKG axis more than 110\n\n\n\n\nhormone\nAn experiment was conducted to study the effects of avian pancreatic polypeptide (aPP), cholecystokinin (CCK), vasoactive intestinal peptide (VIP), and secretin on pancreatic and biliary secretions in laying hens. Researchers aimed to determine how these hormones affect the flow rates and pH values of these secretions.\nWhite Leghorn hens, aged 14-29 weeks, were surgically fitted with cannulas for collecting secretions and a jugular cannula for hormone infusion. Each hen underwent one trial per day, as long as her cannulas remained functional, leading to varying trial numbers per hen.\nEach trial started with a 20-minute saline infusion, followed by the collection of pancreatic and biliary secretions. The flow rates and pH values were measured. This was followed by a 40-minute hormone infusion, with measurements repeated afterward.\nThe data set “hormone.csv” includes data for the four hormones and saline, with each trial recorded as one entry and 11 associated variables.\n\nhormone.csv\n\nID: ID of hen\nBilsecpr: Biliary secretion-pre\nBilphpr: Biliary pH-pre\nPansecpr: Pancreatic secretion-pre\nPanphpr: Pancreatic pH-pre\nDose: Dose of hormone\nBilsecpt: Biliary secretion-post\nBilphpt: Biliary pH-post\nPansecpt: Pancreatic secretion-post\nPanphpt: Pancreatic pH-post\nHormone: Hormone.\n\nPossible values: SAL, APP, CCK, SEC, VIP.\n\n\n\n\n\nhospital\nThese data are part of a larger data set gathered from individuals discharged from a specific Pennsylvania hospital. It was collected as part of a retrospective chart review focusing on antibiotic usage in hospitals.\n\nhospital.csv\n\nId: id no.\nDur_stay: Duration of hospital stay\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nTemp: First temperature following admission\nWBC: First WBC(x1000) following admission\nAntibio: Received antibiotic.\n\nPossible values: yes, no\n\nBact_cul: Received bacterial culture.\n\nPossible values: yes, no\n\nService: Service.\n\nPossible values: med, surg\n\n\n\n\n\ninfantbp\nResearchers investigated the link between high blood pressure and sodium intake by measuring infants’ responses to salt and sugar solutions. They measured the vigor of infants’ sucking (mean sucks per burst, MSB) when exposed to different solutions: water, 0.1 molar salt, 0.3 molar salt, and sugar. The responses were recorded over a series of periods using different stimuli: (i) nonnutritive sucking (ii) water, (iii) 5% sucrose + water, (iv) 15% sucrose + water, and (v) nonnutritive sucking.\n\ninfantbp.csv\n\nID:ID -Salt Taste Variables\n\nMn_sbp: Mean SBP\nMn_dbp: Mean DBP\nMSB1slt: MSB-trial 1 water\nMSB2slt: MSB-trial 2 water\nMSB3slt: MSB-trial 3 0.1 molar salt + water\nMSB4slt: MSB-trial 4 0.1 molar salt + water\nMSB5slt: MSB-trial 5 water\nMSB6slt: MSB-trial 6 water\nMSB7slt: MSB-trial 7 0.3 molar salt + water\nMSB8slt: MSB-trial 8 0.3 molar salt + water\nMSB9slt: MSB-trial 9 water\nMSB10slt: MSB-trial 10 water\n\nSugar Taste Variables\n\nMSB1sug: MSB-trial 1 non-nutritive sucking\nMSB2sug: MSB-trial 2 water\nMSB3sug: MSB-trial 3 5% sucrose + water\nMSB4sug: MSB-trial 4 15% sucrose + water\nMSB5sug: MSB-trial 5 non-nutritive sucking\n\nFor MSB, 0 indicates the baby did not suck.\n\n\n\n\nlead\nA study examined the psychological and neurological effects of lead exposure on children near a lead smelter in El Paso, Texas. Blood lead levels were measured, categorizing 46 children with levels ≥ 40 μg/mL as the exposed group. Another 78 children with levels &lt; 40 μg/mL functioned as the control group. Key outcomes included finger–wrist taps (neurological function) and Wechsler full-scale IQ scores.\nOther behavioral effects of lead include hyperactivity. In this study, parents also rated their children’s hyperactivity on a scale from 0 (normal) to 3 (very hyperactive). Hyperactivity measures are available for 49 control children and 35 exposed children.\n\nlead.csv\n\nPatient information\n\nid: Identification number\nageyrs: Age in years\nsex: Sex.\n\nPossible values: male, female\n\n\nLead data\n\narea: Distance of esidence from smelter on august 1972. Possible values:\n\n0-1 Miles from smelter\n1-2.5 Miles\n2.5-4.1 Miles\n\nlead_grp: Blood lead level group. possible values:\n\ncontrol = Blood lead levels below 40 micrograms/100ml in both 1972 & 1973 (control group),\ncurrent exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in both 72 & 73 or a level greater than or equal to 40 in 73 alone (3 cases only) (currently exposed group),\nprevious exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in 72 and less than 40 in 73 (previously exposed group)\n\nGroup: group.\n\nPossible values: control, exposed\n\nld72: Blood lead values (micrograms/100ml) in 72\nld73: Blood lead values (micrograms/100ml) in 73\nfst2yrs: Did child live for 1st 2 years within 1 mile of smelter.\n\nPossible values: yes, no.\n\ntotyrs: Total number of years spent within 4.1 miles of smelter\n\nIQ Test Results\n\niqv_inf: INF - information subtest in WISC and WPPSI\niqv_comp: COMP - comprehension subtest in WISC and WPPSI\niqv_ar: AR - arithmetic subtest in WISC and WPPSI\niqv_ds: DS - digit span subtest(WISC) and sentence completion(WPPSI)\niqv_raw: V/RAW - raw score/verbal IQ\niqp_pc: PC - picture completion subtest in WISC and WPPSI\niqp_bd: BD - block design subtest in WISC and WPPSI\niqp_oa: OA - object assembly subtest(WISC), animal house subtest(WPPSI)\niqp_cod: COD - coding subtest(WISC), geometric design subtest(WPPSI)\niqp_raw: P/RAW - raw score/performance IQ (total of scores PC, BD, OA, & COD)\nhh_index: HH/INDEX - Hollingshead index of social status\niqv: IQV - verbal IQ\niqp: IQP - performance IQ\niqf: IQF - full scale IQ (not sum or average of IQV D IQP)\niq_type: Type of IQ test (WISC usually given to children \\(\\geq\\) 5 years and 1 month of age WPPSI usually given to children \\(\\leq\\) 5 years of age).\n\nPossible values: WISC, WPPSI\n\n\nSymptom data (as reported by parents)\n\npica: Pica.\n\nPossible values: yes, no.\n\ncolic: Colic.\n\nPossible values: yes, no.\n\nclumsi: Clumsiness.\n\nPossible values: yes, no.\n\nirrit: Irritability.\n\nPossible values: yes, no.\n\nconvul: Convulsions.\n\nPossible values: yes, no.\n\n\nNeurological test data\n\n_2plat_r: Number of taps for right hand in the 2-plate tapping test (number of taps in one 10 second trial)\n_2plat_l: Number OF taps for left hand in the 2-plate tapping test (number taps in one 10 second trial)\nvisrea_r: Visual reaction time right hand (milliseconds)\nvisrea_l: Visual reation time left hand (milliseconds)\naudrea_r: Auditory reaction time right hand (milliseconds)\naudrea_l: Auditory reaction time left hand (milliseconds)\nfwt_r: Finger-wrist tapping test right hand (number of taps in one 10 second trial)\nfwt_l: Finger-wrist tapping test left hand (#taps in one 10 second trial)\nhyperact: WWPS - Werry-Weiss-Peters scale for hyperactivity\n\n0 = no activity, \\(\\ldots\\), 4 = severly hyperactive (as reported by parents)\n\nmaxfwt: Finger-wrist tapping test in dominant hand (max of fwt_r,fwt_l)\n\n\n\n\n\nlvm\nThe Left Ventricular Mass Index (LVMI) measures the enlargement of the heart’s left side, expressed in gm/ht(m)^2.7. High LVMI values can predict future cardiovascular disease in children. A study investigated the relationship between LVMI levels and blood pressure categories in children and adolescents aged 10-18. Blood pressure was categorized as Normal (bpcat = normal, bp percentile &lt; 80%), Pre-Hypertensive (bpcat = pre-hypertensive, bp percentile ≥ 80% and &lt; 90%), or Hypertensive (bpcat = hypertensive, bp percentile ≥ 90%)..\n\nlvm.csv\n\nID: ID\nlvmht27: Left ventricular mass – height corrected = Left Ventricular Mass/Height(m)\\(^{2.7}\\), \\(g/m^{2.7}\\)\nbpcat: Blood pressure category.\n\nPossible values: normal, pre-hypertensive, hypertensive.\n\ngender: Bender.\n\nPossible values: male, female\n\nage: in years\nBMI: kg/m\\(^2\\)\n\n\n\n\nmice\nRetinitis pigmentosa (RP) is a hereditary eye condition causing night blindness and visual field loss, typically between ages 10 and 40. Some patients become legally blind by 30, while others retain central vision past 60. A specific gene linked RP has been identified whose transmision is autosomal dominant. The disease progression is tracked using electroretinogram (ERG), measuring retinal electrical activity, which decreases as RP advances, affecting routine activities like driving and walking at night.\nTo test if sunlight exposure harms RP patients, researchers introduced the RP gene into mice, creating “RP mice.” These mice were divided into light, dim, and dark lighting conditions from birth. A control group of normal mice was also subjected to similar conditions. ERG amplitudes (BAMP and AAMP) were measured at 15, 20, and 35 days of life for RP mice, and only BAMP was measured for normal mice.\n\nmice.csv\n\nId: ID\nGroup: GROUP.\n\nPossible values: RP, NORMAL\n\nTrtgrp: TREATMENT GROUP.\n\nPossible values: LIGHT, DIM, DARK.\n\nAge: AGE (days)\nB_amp: B AMP\nA_amp: A AMP\n\n\n\n\nnifed\nA clinical trial tested the effectiveness of nifedipine in reducing chest pain in hospitalized angina patients. The study lasted 14 days unless patients were withdrawn, discharged, or died. Patients were randomly assigned to receive either nifedipine or propranolol, starting at a standard dosage. If pain persisted or recurred, the dosage was increased in pre-specified steps. Patients in both groups could also receive nitrates as needed to control pain. The primary goal was to compare pain relief between nifedipine and propranolol, with a secondary goal of examining their effects on heart rate and blood pressure.\n\nnifed.csv\n\nId: ID\ntrtgrp: Treatment group,\n\nN = nifedipine\nP = placebo\n\nbashrtrt: Baseline heart rate immediately prior to randomization (beats/min)\nlv1hrtrt: Level 1 heart rate, Highest heart rate and systolic blood pressure at baseline and each level of therapy respectively (beats/min)\nlv2hrtrt: Level 2 heart rate (beats/min)\nlv3hrtrt: Level 3 heart rate (beats/min)\nbassys: Baseline systolic bp immediately prior to randomization (mm Hg)\nlv1sys: Level 1 systolic bp (mm Hg)\nlv2sys: Level 2 systolic bp (mm Hg)\nlv3sys: Level 3 systolic bp (mm Hg)\n\n\nMissing values indicate that either (a) the patient withdrew from the study prior to entering this level of therapy (b) the patient achieved pain relief prior to reaching this level or therapy, (c) the patient encountered this level of therapy, but this particular piece of data was missing.\n\n\npiriform\nA study evaluated the FAIR test (hip flexion, adduction, and internal rotation) for diagnosing piriformis syndrome (PS), which affects the piriformis muscle in the buttock, causing lumbar and sciatic pain. The test measures nerve-conduction velocity differences between an aggravating and a neutral posture, with higher scores indicating a greater likelihood of PS. Data from 142 participants without PS and 489 with PS (diagnosed clinically) are available. A FAIR test score of ≥ 1.86 ms is proposed to define a positive result. The FAIR test value, MAXCHG, is recorded in milliseconds.\n\npiriform.csv\n\nID: ID\npiriform: Piriformis Syndrome.\n\nPossible values: Negative, Positive\n\nsex: Sex.\n\nPossible values: male, female\n\nage: Age\nmaxchg: Max change between tibia and peroneal\n\n\n\n\nsexrat\nIt is often assumed that the gender distribution of consecutive children is independent. To test this hypothesis, birth records from the first five children in 51,868 families were analyzed. These data contain the frequency of how many times a pattern of child sexes showed up. E.g., MM (only two males) showed up 4400 times.\n\nsexrat.csv\n\nnm_chld: Number of children. For families with 5+ children, the sex of the first 5 children are listed. The number of children is given as 5 for such families.\nsx_1: Sex of 1st born\nsx_2: Sex of 2nd born\nsx_3: Sex of 3rd born\nsx_4: sex of 4th born\nsx_5: sex of 5th born\nsexchldn: Sex of all children. The sex of successive births is given. Thus, MMMF means that the first three children were males and the fourth child was a female. There were 484 such families.\nnum_fam: Number of families. Number of families with specific gender contribution of children\n\n\n\n\nsmoke\nA study was conducted among 234 individuals who wanted to quit smoking but had not yet done so. On the day they quit, their carbon monoxide (CO) levels were measured, and the time since their last cigarette was recorded. This CO level serves as an indicator of the number of cigarettes smoked daily before quitting but is influenced by the time since the last cigarette. Therefore, a “corrected CO level” was provided, adjusted for this time. Participant age, sex, and self-reported daily cigarette consumption were also recorded. The participants were followed for a year to determine the number of days they remained abstinent, ranging from 0 to 365 days.\n\nsmoke.csv\n\nID: ID number\nAge: age\nGender: Gender.\n\nPossible values: male, female\n\nCig_day: Cigarettes/day\nCO: Carbon monoxide (CO) (X 10)\nMin_last: Minutes elapsed since last cigarette\nLogCOadj: Log CO Adj * (X 1000). This variable represents adjusted carbon monoxide (CO) values. CO values were adjusted for minutes elapsed since last cigarette smoked using the formula Log 10 CO (Adjusted) = Log 10 CO - (-0.000638) X (Min - 80), where Min is the number of minutes elapsed since the last cigarette smoked.\nDay_abs: Days abstinent Those abstinent less than 1 day were given a value of zero.\n\n\n\n\nswiss\nThe Swiss Analgesic Study aimed to evaluate the impact of phenacetin-containing analgesics on kidney function and health. It involved 624 women from Basel, Switzerland, who had high phenacetin intake (study group) and 626 women with low or no phenacetin intake (control group). The study used urine N-acetyl-P-aminophenyl (NAPAP) levels to measure recent phenacetin use, dividing the study group into high-NAPAP and low-NAPAP subgroups. Both subgroups had higher NAPAP levels than the control group. The women were examined in 1967-1968 and again in 1969, 1970, 1971, 1972, 1975, and 1978, with kidney function assessed through various laboratory tests, including serum-creatinine levels.\n\nswiss.csv\n\nID: ID\nage: age (yrs)\ngroup: Group.\n\nPossible values: High NAPAP, Low NAPAP, control\n\ncreat_68: Serum Creatinine 1968 (mg/dL)\ncreat_69: Serum Creatinine 1969 (mg/dL)\ncreat_70: Serum Creatinine 1970 (mg/dL)\ncreat_71: Serum Creatinine 1971 (mg/dL)\ncreat_72: Serum Creatinine 1972 (mg/dL)\ncreat_75: Serum Creatinine 1975 (mg/dL)\ncreat_78: Serum Creatinine 1978 (mg/dL)\n\n\n\n\ntear\nA pilot study was conducted to evaluate an eye drop’s effectiveness in preventing dry eye, measured by tear breakup time (TBUT). Fourteen participants tested three protocols:\n\nProtocol A: No blinking for 3 seconds before placebo instillation.\nProtocol B: No blinking for 6 seconds before placebo instillation (standard protocol).\nProtocol C: No blinking for 10 seconds before placebo instillation.\n\nTBUT was recorded at baseline, immediately after, and at 5, 10, and 15 minutes post-instillation in a low-humidity environment. Each protocol was tested on the same participants, measuring both eyes with two replicates.\n\ntear.csv\n\nID: ID\nod3bas1: OD 3sec baseline 1\nod3bas2: OD 3 sec baseline 2\nod3im1: OD 3 sec immediately post 1\nod3im2: OD 3 sec immediately post 2\nod3pst51: OD 3 sec 5min post 1\nod3pst52: OD 3 sec 5min post 2\nod3pt101: OD 3 sec 10min post 1\nod3pt102: OD 3 sec 10min post 2\nod3pt151: OD 3 sec 15min post 1\nod3pt152: OD 3 sec 15min post 2\nos3bas1: OS 3sec baseline 1\nos3bas2: OS 3 sec baseline 2\nos3im1: OS 3 sec immediately post 1\nos3im2: OS 3 sec immediately post 2\nos3pst51: OS 3 sec 5min post 1\nos3pst52: OS 3 sec 5min post 2\nos3pt101: OS 3 sec 10min post 1\nos3pt102: OS 3 sec 10min post 2\nos3pt151: OS 3 sec 15min post 1\nos3pt152: OS 3 sec 15min post 2\nod6bas1: OD 6 sec baseline 1\nod6bas2: OD 6 sec baseline 2\nod6im1: OD 6 sec immediately post 1\nod6im2: OD 6 sec immediately post 2\nod6pst51: OD 6 sec 5min post 1\nod6pst52: OD 6 sec 5min post 2\nod6pt101: OD 6 sec 10min post 1\nod6pt102: OD 6 sec 10min post 2\nod6pt151: OD 6 sec 15min post 1\nod6pt152: OD 6 sec 15min post 2\nos6bas1: OS 6 sec baseline 1\nos6bas2: OS 6 sec baseline 2\nos6im1: OS 6 sec immediately post 1\nos6im2: OS 6 sec immediately post 2\nos6pst51: OS 6 sec 5min post 1\nos6pst52: OS 6 sec 5min post 2\nos6pt101: OS 6 sec 10min post 1\nos6pt102: OS 6 sec 10min post 2\nos6pt151: OS 6 sec 15min post 1\nos6pt152: OS 6 sec 15min post 2\nod10bas1: OD 10 sec baseline 1\nod10bas2: OD 10 sec baseline 2\nod10im1: OD 10 sec immediately post 1\nod10im2: OD 10 sec immediately post 2\nod10ps51: OD 10 sec 5min post 1\nod10ps52: OD 10 sec 5min post 2\nod10p101: OD 10 sec 10min post 1\nod10p102: OD 10 sec 10min post 2\nod10p151: OD 10 sec 15min post 1\nod10p152: OD 10 sec 15min post 2\nos10bas1: OS 10 sec baseline 1\nos10bas2: OS 10 sec baseline 2\nos10im1: OS 10 sec immediately post 1\nos10im2: OS 10 sec immediately post 2\nos10ps51: OS 10 sec 5min post 1\nos10ps52: OS 10 sec 5min post 2\nos10p101: OS 10 sec 10min post 1\nos10p102: OS 10 sec 10min post 2\nos10p151: OS 10 sec 15min post 1\nos10p152: OS 10 sec 15min post 2\n\n\n\n\ntemperat\nA student records temperatures at 20 sites within her house for 30 days each. She records the outside temperature and the weather condition.\n\ntemperat.csv\n\nDate: Date (MDY)\nOut_temp: Outside temerature (Degrees Fahrenheit)\nRoom: Room location\nIn_temp: Inside temperature (Degrees Fahrenheit)\nCor_fac: Correction factor added.\n\nPossible values: yes, no\n\nTyp_wea: Type of weather.\n\nPossible values: SUNNY, PARTLY CLOUDY, CLOUDY, RAINY, FOGGY\n\n\n\n\n\ntennis1\nA survey of tennis club members in the Boston area examined the occurrence of tennis elbow. Subjects reported anywhere from 0 to 8 episodes of tennis elbow. They were also asked about demographic factors and racquet characteristics.\n\ntennis1.csv\n\nId: ID\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nNum_epis: Number of episodes of tennis elbow\nTyp_last: Type of racquet used during last episode.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_last: Weight of racquet used during last episode.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_last: Material of racquet used during last episode.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_last: String type of racquet used during last episode.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\nTyp_curr: Type of racquet used currently.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_curr: Weight of racquet used currently.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_curr: Material of racquet used currently.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_curr: String type of racquet used currently.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\n\n\n\n\ntennis2\nTennis elbow is a painful condition common among tennis players. Treatments include rest, heat, and anti-inflammatory medications like Motrin (ibuprofen). A clinical trial with 87 participants compared the effectiveness of Motrin vs. placebo. Participants were randomly divided into two groups: Group A received Motrin for 3 weeks, followed by a 2-week washout period, and then placebo for 3 weeks; Group B received the treatments in reverse order. Pain levels were measured on a 1-6 scale (1 = worse, 6 = completely improved) at the end of each treatment and washout period. The comparison was made (i) during maximum activity, (ii) 12 hours following maximum activity, (iii) during the average day, and (iv) by overall impression of drug efficacy.\n\ntennis2.csv\n\nid: ID\nage: Age\nsex: Sex.\n\nPossible values: male, female\n\ndrg_ord: Drug order.\n\nPossible values: MOTRIN-PLACEBO, PLACEBO-MOTRIN\n\nPeriod 2 = Pain scores after the first active drug period compared with baseline\n\npainmx_2: During study period, pain during maximum activity vs baesline\n\n1 = Worse\n2 = Unchanged\n3 = Slightly improved (25%)\n4 = Moderately improved (50%)\n5 = Mostly improved (75%)\n6 = Completely improved\n\npain12_2: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_2: During the average day of study period pain vs. baseline (same code as painmx_2)\npainov_2: Overall impression of drug efficacy vs. baseline (same code as painmx_2)\n\nPeriod 3 = Pain scores after the washout period compared with baseline\n\npainmx_3: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_3: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_3: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_3: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\nPeriod 4 = Pain scores after the second active drug period compared with baseline\n\npainmx_4: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_4: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_4: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_4: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\n\n\n\n\nvalid\nThe food-frequency questionnaire (FFQ) is a common tool in dietary epidemiology to assess food consumption. It asks individuals to report their typical daily servings of over 100 food items from the past year, and a food-composition table calculates nutrient intakes. While FFQs are inexpensive, they are less accurate than diet records (DR), where participants document their weekly food intake, and a nutritionist calculates nutrient intakes. In a validation study, 173 nurses from the Nurses’ Health Study completed 4 weeks of diet records and an FFQ. Data for saturated fat, total fat, alcohol consumption, and caloric intake from both methods recorded here.\n\nvalid.csv\n\nId: ID number\nsfat_dr: Saturated fat-DR (g)\nsfat_ffq: Saturated fat-FFQ (g)\ntfat_dr: Total fat-DR (g)\ntfat_ffq: Total fat-FFQ (f)\nalco_dr: Alcohol consumption-DR (oz)\nalco_ffq: Alcohol consumption-FFQ (oz)\ncal_dr: Total calories-DR (K-cal)\ncal_ffq: Total calories-FFQ (K-cal)\n\n\n\n\nwales\nA study in South Wales investigated the hereditary factors of blood pressure in 623 individuals (propositii) over age 5 from two populations. The participants and their first-degree relatives had their blood pressure measured at home by one observer, with a baseline and three follow-up exams from the mid-1950s to the early 1960s. The dataset WALES.DAT includes familial blood pressure data from the Rhondda Fach and Vale of Glamorgan communities.\n\nwales.csv\n\nID: ID\nrecord:\nsex:\narea:\npropositus:\nsexofprop:\nrelationship:\ndoubler:\nsurveystat:\nmaritalstt:\nage:\nparity:\nheight:\nweight:\narmgrth:\ntriceps:\nsysbp:\ndiasbp:\npulse:\nalbumin:\nglucose:\nbacteria:\noccupation:\nhypertension:\ndiabetes:\npregnancy:\npet_fibroids:\nangina:\npmi:\ninter_claud:\n\n\n\n\n\n\n\n\n\n\nReferences\n\nRosner, B. 2015. Fundamentals of Biostatistics. 8th ed. Cengage Learning."
  },
  {
    "objectID": "01_r/01_r_intro.html",
    "href": "01_r/01_r_intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a statistical programming language designed to analyze data.\nThis is not an R course. But you need to know some tools to summarize/plot/model data.\nR is free, widely used, more generally applicable (beyond linear regression), and a useful tool for reproducibility. So this is what we will use.\nPython would have been a good choice too, but it is worse at basic stats (this is controversial)."
  },
  {
    "objectID": "01_r/01_r_intro.html#variables",
    "href": "01_r/01_r_intro.html#variables",
    "title": "Introduction to R",
    "section": "Variables",
    "text": "Variables\n\nA variable stores a value. You use the assignment operator “&lt;-” to assign values to variables. For example, we can assign the value of 10 to the variable x.\n\nx &lt;- 10\n\n\nIt is possible to use =, and I think there is nothing wrong with that. But for some reason the field has decided to only use &lt;-, so you should too.\n\nWhenever we use x later, it will use the value of 10\n\nx\n\n[1] 10\n\n\nThis is useful because you can reuse this value over and over again:\n\ny &lt;- 0\nx + y\nx * y\nx / y\nx - y\n\nTo assign a “string” (a fancy way to say a word) to x, put the string in quotes. For example, we can assign the value of \"Hello World\" to x.\n\nx &lt;- \"Hello World\"\nx\n\n[1] \"Hello World\""
  },
  {
    "objectID": "01_r/01_r_intro.html#functions",
    "href": "01_r/01_r_intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\n\nFunctions take objects (such as numbers or variables) as input and output new objects. Let’s look at a simple function that takes the log of a number:\n\nlog(x = 4, base = 2)\n\nThe inputs are called “arguments”. Generally, every function will be for the form:\n\nfunction_name(arg1 = val1, arg2 = val2, ...)\n\nIf you do not specify the name of the argument, R will assume you are assigning in their order.\n\nlog(4, 2)\n\nYou can change the order of the arguments if you specify them.\n\nlog(base = 2, x = 4)\n\nTo see the list of all possible arguments of a function, use the help() function:\n\nhelp(log)\n\nIn the help file, there are often default values for an argument. For example, the following indicates the the default value of base is exp(1).\n\nlog(x, base = exp(1))\n\nThis indicates that you can omit the base argument and R will assume that it should be exp(1).\n\nlog(x = 4, base = exp(1))\n\n[1] 1.386\n\nlog(x = 4)\n\n[1] 1.386\n\n\nIf an argument does not have a default, then it must be specified when calling a function.\nType this:\n\nlog(x = 4,\n\nThe “+” indicates that R is expecting more input (you forgot either a parentheses or a quotation mark). You can get back to the prompt by hitting the ESCAPE key."
  },
  {
    "objectID": "01_r/01_r_intro.html#useful-functions",
    "href": "01_r/01_r_intro.html#useful-functions",
    "title": "Introduction to R",
    "section": "Useful Functions",
    "text": "Useful Functions\n\nc() creates a vector (sequence of values)\n\ny &lt;- c(8, 1, 3, 4, 2)\ny\n\n[1] 8 1 3 4 2\n\n\nYou can perform vectorized operations on these vectors\n\ny + 2\n\n[1] 10  3  5  6  4\n\ny / 2\n\n[1] 4.0 0.5 1.5 2.0 1.0\n\ny - 2\n\n[1]  6 -1  1  2  0\n\n\nexp(): Exponentiation. This is the inverse of log().\n\nexp(10)\n\n[1] 22026\n\nlog(exp(10))\n\n[1] 10\n\n\nmean(): The mean of a vector\n\nmean(y)\n\n[1] 3.6\n\n\nsd() The standard deviation of a vector\n\nsd(y)\n\n[1] 2.702\n\n\nsum(): Sum the values of a vector.\n\nsum(y)\n\n[1] 18\n\n\nseq(): Create a sequence of numbers\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nExercise: What does the by argument do in seq()? Read the help file and modify it to 2 in the example code above.\nhead(): Show the first six values of an object."
  },
  {
    "objectID": "01_r/01_r_intro.html#r-packages",
    "href": "01_r/01_r_intro.html#r-packages",
    "title": "Introduction to R",
    "section": "R Packages",
    "text": "R Packages\n\nA package is a collection of functions that don’t come with R by default.\nThere are many many packages available. If you need to do any data analysis, there is probably an R package for it.\nUsing install.packages(), you can install packages that contain functions and datasets that are not available by default. Do this now with the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nYou will only need to install a package once per computer. Once it is installed you can gain access to all of the functions and datasets in a package by using the library() function.\n\nlibrary(tidyverse)\n\nYou will need to run library() at the start of every R session if you want to use the functions in a package.\nWhen I want to write the name of a function, I will write it like this()."
  },
  {
    "objectID": "01_r/01_r_intro.html#data-frames",
    "href": "01_r/01_r_intro.html#data-frames",
    "title": "Introduction to R",
    "section": "Data Frames",
    "text": "Data Frames\n\nThe fundamental unit object of data analysis is the data frame.\nA data frame has variables in the columns, and observations in the rows.\n \nR comes with a bunch of famous datasets in the form of a data frame. Such as the airquality dataset, which contains daily air quality measurements in New York from 1973.\n\ndata(\"airquality\")\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nYou can extract individual variables from a data frame using $\n\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6\n [19]  30  11   1  11   4  32  NA  NA  NA  23  45 115  37  NA  NA  NA  NA  NA\n [37]  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA\n [55]  NA  NA  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA\n [73]  10  27  NA   7  48  35  61  79  63  16  NA  NA  80 108  20  52  82  50\n [91]  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22\n[109]  59  23  31  44  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73\n[127]  91  47  32  20  23  21  24  44  21  28   9  13  46  18  13  24  16  13\n[145]  23  36   7  14  30  NA  14  18  20\n\n\nYou can explore these in a spreadsheet format using View() (note the capital “V”). Don’t ever have this in a file though, directly write it in the console.\n\nView(airquality)"
  },
  {
    "objectID": "01_r/01_r_intro.html#reading-in-data-frames",
    "href": "01_r/01_r_intro.html#reading-in-data-frames",
    "title": "Introduction to R",
    "section": "Reading in Data Frames",
    "text": "Reading in Data Frames\n\nMost datasets will nead to be loaded into R. To do so, we will use the {readr} package.\n\nlibrary(readr)\n\nThe only function I will require you to know from this package is read_csv(), which loads in data from a CSV file (“Comma-separated values”), a very popular format for storing data.\nIf you have the CSV file somewhere on your computer, then specify the path from the current working directory, and assign the data frame to a variable.\nFor other file formats, you need to use other functions, such as read_tsv(), read_table(), read_fwf(), etc. I will try to make sure read_csv() works for all datasets in this course.\nI will typicaly post course datasets at https://dcgerard.github.io/stat_320/data.html. You can load those data into R by pasting their URL’s into read_csv().\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nhead(lead)\n\n# A tibble: 6 × 40\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 29 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;\n\n\nExercise: Load in the birthweight data into R and print out the first six rows."
  },
  {
    "objectID": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "href": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "title": "Introduction to R",
    "section": "Basic Data Frame Manipulations",
    "text": "Basic Data Frame Manipulations\n\nYou will need to know just a few data frame manipulations, which we will perform using the {dplyr} package.\n\nlibrary(dplyr)\n\nThe first argument for {dplyr} functions is always the data frame you are modifying. The following arguments typically involve the columns of that data frame.\nUse the mutate() function from the {dplyr} package to make variable transformations.\n\nlead &lt;- mutate(lead, log_iqv_inf = log(iqv_inf))\nhead(lead)\n\n# A tibble: 6 × 41\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse glimpse() to get a brief look at the data frame.\n\nglimpse(lead)\n\nRows: 124\nColumns: 41\n$ id          &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112…\n$ area        &lt;chr&gt; \"2.5-4.1\", \"2.5-4.1\", \"2.5-4.1\", \"1-2.5\", \"0-1\", \"1-2.5\", …\n$ ageyrs      &lt;dbl&gt; 11.08, 9.42, 11.08, 6.92, 11.25, 6.50, 6.92, 15.00, 7.17, …\n$ sex         &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"f…\n$ iqv_inf     &lt;dbl&gt; 3, 7, 4, 4, 5, 5, 7, 3, 13, 7, 6, 11, 11, 6, 9, 4, 13, 4, …\n$ iqv_comp    &lt;dbl&gt; 4, 9, 9, 6, 4, 12, 9, 1, 10, 9, 10, 14, 12, 4, 11, 6, 17, …\n$ iqv_ar      &lt;dbl&gt; 3, 7, 5, 6, 8, 11, 10, 3, 14, 12, 6, 14, 8, 5, 11, 4, 13, …\n$ iqv_ds      &lt;dbl&gt; 5, 6, 3, 6, 5, 9, 7, 6, 13, 9, 7, 11, 8, 8, 9, 8, 14, 12, …\n$ iqv_raw     &lt;dbl&gt; 15, 29, 21, 22, 22, 37, 33, 13, 50, 37, 29, 50, 39, 23, 40…\n$ iqp_pc      &lt;dbl&gt; 10, 8, 10, 5, 5, 14, 10, 6, 8, 6, 6, 13, 8, 9, 14, 9, 16, …\n$ iqp_bd      &lt;dbl&gt; 8, 7, 7, 8, 10, 7, 8, 2, 15, 9, 8, 13, 9, 7, 17, 8, 16, 9,…\n$ iqp_oa      &lt;dbl&gt; 8, 10, 7, 5, 13, 7, 7, 3, 14, 12, 3, 15, 11, 6, 13, 13, 16…\n$ iqp_cod     &lt;dbl&gt; 5, 9, 20, 13, 12, 10, 16, 8, 9, 13, 9, 20, 12, 12, 16, 12,…\n$ iqp_raw     &lt;dbl&gt; 31, 34, 44, 31, 40, 38, 41, 19, 46, 40, 26, 61, 40, 34, 60…\n$ hh_index    &lt;dbl&gt; 77, 77, 30, 77, 62, 72, 54, 73, 22, 77, 63, 48, 48, 48, 48…\n$ iqv         &lt;dbl&gt; 61, 82, 70, 72, 72, 95, 89, 57, 116, 95, 82, 116, NA, 74, …\n$ iqp         &lt;dbl&gt; 85, 90, 107, 85, 100, 97, 101, 64, 111, 100, 76, 136, 100,…\n$ iqf         &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, …\n$ iq_type     &lt;chr&gt; \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"W…\n$ lead_grp    &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ Group       &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ ld72        &lt;dbl&gt; 25, 31, 30, 29, 2, 29, 25, 24, 24, 31, 21, 29, 32, 36, 30,…\n$ ld73        &lt;dbl&gt; 18, 28, 29, 30, 34, 25, 24, 15, 16, 24, 19, 27, 29, 32, 25…\n$ fst2yrs     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ totyrs      &lt;dbl&gt; 11, 6, 5, 5, 11, 6, 6, 15, 7, 7, 12, 10, 12, 12, 10, 10, 1…\n$ pica        &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ colic       &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ clumsi      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no…\n$ irrit       &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ convul      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ `_2plat_r`  &lt;dbl&gt; 16, 17, 16, 11, 17, 16, 10, 19, 15, 16, 17, 17, 15, 23, 19…\n$ `_2plar_l`  &lt;dbl&gt; 16, 16, 17, 9, 16, 14, 13, 14, 13, 11, 16, 17, 14, 21, 20,…\n$ visrea_r    &lt;dbl&gt; 36, 23, 20, 34, 26, 29, 29, 30, 31, 26, 19, 22, 19, 26, 17…\n$ visrea_l    &lt;dbl&gt; 38, 19, 24, 42, 34, 26, 29, 32, 28, 25, 19, 24, 17, 23, 16…\n$ audrea_r    &lt;dbl&gt; 27, 18, 16, 35, 31, 28, 30, 33, 31, 27, 16, 22, 18, 25, 17…\n$ audrea_l    &lt;dbl&gt; 25, 28, 17, 30, 33, 27, 27, 24, 29, 21, 19, 23, 20, 28, 16…\n$ fwt_r       &lt;dbl&gt; 72, 61, 46, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 44, 74…\n$ fwt_l       &lt;dbl&gt; 52, 48, 49, 41, 42, 35, 39, 58, 40, 37, 44, 48, 47, 53, 63…\n$ hyperact    &lt;dbl&gt; NA, 0, NA, 2, NA, 0, 0, NA, 0, 0, NA, 1, NA, NA, NA, 2, NA…\n$ maxfwt      &lt;dbl&gt; 72, 61, 49, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 53, 74…\n$ log_iqv_inf &lt;dbl&gt; 1.0986, 1.9459, 1.3863, 1.3863, 1.6094, 1.6094, 1.9459, 1.…\n\n\nUse View() to see a spreadsheet of the data frame (never put this in an R Markdown file). Note the capital “V”.\n\nView(lead)\n\nUse rename() to rename variables.\n\nlead &lt;- rename(lead, ID = id)\nhead(lead)\n\n# A tibble: 6 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse filter() to remove rows.\n\nUse == to select rows based on equality\nUse &lt; and &gt; to select rows based on inequality\nUse &lt;= and &gt;= to select rows based on inequality/equality.\n\n\nfilter(lead, Group == \"control\")\n\n# A tibble: 78 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   101 2.5-…  11.1  male        3        4      3      5      15     10      8\n 2   102 2.5-…   9.42 male        7        9      7      6      29      8      7\n 3   103 2.5-…  11.1  male        4        9      5      3      21     10      7\n 4   104 1-2.5   6.92 male        4        6      6      6      22      5      8\n 5   105 0-1    11.2  male        5        4      8      5      22      5     10\n 6   106 1-2.5   6.5  male        5       12     11      9      37     14      7\n 7   107 2.5-…   6.92 male        7        9     10      7      33     10      8\n 8   108 0-1    15    fema…       3        1      3      6      13      6      2\n 9   109 1-2.5   7.17 fema…      13       10     14     13      50      8     15\n10   110 1-2.5   7.25 male        7        9     12      9      37      6      9\n# ℹ 68 more rows\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;, …\n\nfilter(lead, ageyrs &lt; 4)  \n\n# A tibble: 7 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   403 0-1      3.92 male        5        6     10      7      28     12      8\n2   406 1-2.5    3.75 male        9        4     10      7      30      8     11\n3   504 1-2.5    3.75 male        8        7      8      7      30      8      7\n4   505 1-2.5    3.75 fema…       6        5      6      3      20      8     12\n5   602 1-2.5    3.75 fema…       6       11      8     11      36     11     10\n6   606 2.5-4…   3.83 male       12        9     18      8      47     12     10\n7   607 0-1      3.92 male        8        4     11      1      24     13      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\nfilter(lead, ageyrs &gt; 4, ageyrs &lt; 5)\n\n# A tibble: 14 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   401 2.5-…   4.33 male        6       13     13      9      41     11     11\n 2   402 1-2.5   4.83 fema…       9        7     13      8      37     11     12\n 3   404 0-1     4.58 male        6        3      4      2      15     12      7\n 4   405 0-1     4.5  male        6        9      9     12      36      8      9\n 5   407 2.5-…   4.25 male        7        4      8      6      25     10     12\n 6   408 2.5-…   4.33 male        7        6      4      5      22      9      6\n 7   409 1-2.5   4.33 fema…       8        8     11     11      38     11      9\n 8   411 1-2.5   4.33 fema…       8        9     14      7      38     14      9\n 9   412 2.5-…   4.75 fema…       7        7     10      8      32     13     13\n10   414 0-1     4.5  male        6        3      7      2      18      6      8\n11   501 0-1     4.17 male       11        7      8      5      31     11     10\n12   502 2.5-…   4.58 male        7        7      9      3      26     10      3\n13   601 1-2.5   4.33 male        9        6      9      8      32      8      9\n14   604 1-2.5   4.58 male        7       10      6     12      35      9     11\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nExercise: The birthweight data is in ounces. There are abour 28.3495 grams in an ounce. Create a new variable called weight_g that is the weight of the baby in grams.\nExercise: From the birthweight data, select just babies that are greater than or equal to 150 ounces.\nExercise: From the lead data, select individuals who are both in the control group and are at least 15.\nExercise: From the lead data, rename ageyrs to just age"
  },
  {
    "objectID": "00_course_outline/00_math_prereqs.html",
    "href": "00_course_outline/00_math_prereqs.html",
    "title": "00 Math Prerequisites",
    "section": "",
    "text": "We do not emphasize the mathematics underlying statistics in this course. However, you should have some familiarity with pre-calculus topics. Here are some facts you should know off the top of your head:\n\nPowers, Exponentials, Logarithms\n\n\\(e^{ab} = {e^{a}}^{b} = {e^{b}}^{a}\\)\n\\(e^{a+b+c} = e^a(e^{b+c}) = e^ae^be^c\\)\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(a/b) = \\log(a) - \\log(b)\\)\n\\(x^ny^n = (xy)^n\\)\n\n\n\nSummations\n\nCapital-sigma notation is useful for writing sums of many numbers/variables: \\[\\sum_{i = 1}^n x_i = x_1 + x_2 + \\cdots x_n\\]\nIf you sum a constant \\(n\\) times, you get \\(n\\) times that constant: \\[\\sum_{i = 1}^n a = an\\]\nYou can factor out multiplicative constants that don’t depend on the summing index: \\[\\sum_{i = 1}^n cx_i = c\\sum_{i = 1}^n x_i\\]\nThe order that you sum elements does not matter: \\[\\sum_{i = 1}^n (x_i + y_i) = \\sum_{i = 1}^n x_i + \\sum_{i = 1}^n y_i\\]"
  },
  {
    "objectID": "00_course_outline/00_overview.html",
    "href": "00_course_outline/00_overview.html",
    "title": "Course Overview",
    "section": "",
    "text": "Learning Objectives\n\nOverview of the course plus some reminders from STAT 202/203/204\nP-values/confidence intervals.\n\\(t\\)-tests for means in R.\nProportion tests in R.\n\n\n\nProbability and Distributions in R.\n\nDistribution: The possible values of a variable and how often it takes those values.\nA density describes the distribution of a quantitative variable. You can think of it as approximating a histogram. It is a curve where\n\nThe area under the curve between any two points is approximately the probability of being between those two points.\nThe total area under the curve is 1 (something must happen).\nThe curve is never negative (can’t have negative probabilities).\n\nThe density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nExercise: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?\nThe \\(t\\)-distribution shows up a lot in Statistics.\n\nIt is also bell-curved but has “thicker tails” (more extreme observations are more likely).\nIt is always centered at 0.\nIt only has one parameter, called the “degrees of freedom”, which determines how thick the tails are.\nSmaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.\nIf the degrees of freedom is large enough, the \\(t\\)-distribution is approximately the same as a normal distribution with mean 0 and variance 1.\n\n\\(t\\)-distributions with different degrees of freedom:\n\n\n\n\n\n\n\n\n\nDensity Function\n\ndt(x = -6, df = 2)\n\n[1] 0.004269\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation\n\nsamp &lt;- rt(n = 1000, df = 2)\nhead(samp)\n\n[1]  0.89857 -1.07176  0.09639  0.79371 -0.42428 -0.64561\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\npt(q = 2, df = 2)\n\n[1] 0.9082\n\n\n\n\n\n\n\n\n\n\n\nQuantile Function\n\nqt(p = 0.9082, df = 2)\n\n[1] 1.999\n\n\n\n\n\n\n\n\n\n\n\nThere are many other distributions implemented in R. To see the most common, run:\n\nhelp(\"Distributions\")\n\n\n\n\nAll of Statistics\n\nObservational/experimental Units: The people/places/things/animals/groups that we collect information about. Also known as “individuals” or “cases”. Sometimes I just say “units”.\nVariable: A property of the observational/experimental units.\n\nE.g.: height of a person, area of a country, marital status.\n\nValue: The specific level of a variable for an observational/experimental unit.\n\nE.g.: Bob is 5’11’’, China has an area of 3,705,407 square miles, Jane is divorced.\n\nQuantitative Variable: The variable takes on numerical values where arithmetic operations (plus/minus/divide/times) make sense.\n\nE.g.: height, weight, area, income.\nCounterexample: Phone numbers, social security numbers.\n\nCategorical Variable: The variable puts observational/experimental units into different groups/categories based on the values of that variable.\n\nE.g.: race/ethnicity, marital status, religion.\n\nBinary Variable: A categorical variable that takes on only two values.\n\nE.g.: dead/alive, treatment/control.\n\nPopulation: The collection of all observational units we are interested in.\nParameter: A numerical summary of the population.\n\nE.g.: Average height, proportion of people who are divorced, standard deviation of weight.\n\nSample: A subset of the population (some observational units, but not all of them).\nStatistic: A numeric summary of the sample.\n\nE.g.: Average height of the sample, proportion of people who are divorced in the sample, standard deviation of weight of a sample.\n\nGraphic:\n \nSampling Distribution: The distribution of a statistic over many hypothetical random samples from the population.\n \nAll of Statistics: We see a pattern in the sample.\n\nEstimation: Guess the pattern in the population based on the sample. Guess a parameter with a statistic. A statistic which is a guess for a parameter is called an estimate.\nHypothesis Testing: Ask if the pattern we see in the sample also exists in the population. Test if a parameter is some value.\nConfidence Intervals: Quantify our (un)certainty of the pattern in the population based on the sample. Provide a range of likely parameter values.\n\nWe will go through a lot of examples of this below\n\nlibrary(tidyverse)\nlibrary(broom)\n\nExercise: Read about the boneden data here What are the observational units? What are the variables? Which are quantitative and which are categorical?\nExercise: Read about the lead data here. What are the observational units? What are the variables? Which are quantitative and which are categorical?\n\n\n\nPattern: Mean is shifted (one quantitative variable)\n\nExample: The boneden data explores the difference in bone density between a heavier and a lighter smoking twin.\nObservational Units: The twins.\nPopulation: All twins where one smokes more than the other.\nSample: The 41 twins in our study.\nVariable: The difference in lumbar spine density (in g/cm2) between the twins. We derived this quantitative variable by subtracting one density from another.\n\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\n\nPattern: Use a histogram/boxplot to visualize the shift from 0.\n\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(bins = 20, fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = 0, lty = 2) +\n  xlab(\"Difference in Bone Density\")\n\n\n\n\n\n\n\n\nGraphic:\n \nParameter of interest: Mean difference in bone density for all twins.\nEstimate: Use sample mean\n\nboneden %&gt;%\n  summarize(meandiff = mean(ls_diff))\n\n# A tibble: 1 × 1\n  meandiff\n     &lt;dbl&gt;\n1   0.0359\n\n\n0.03585 is our “best guess” for the parameter, but it is almost certainly not the value of the parameter (since we didn’t measure everyone).\nHypothesis Testing:\n\nWe are interested in if the mean difference is different from 0.\nTwo possibilities:\n\nNull Hypothesis: Mean is not different from 0, we just happened by chance to get twins that had some difference in density.\nAlternative Hypothesis: Mean is different from 0.\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a \\(p\\)-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\np-value: the probability that you would see data as or more supportive of the alternative hypothesis than what you saw assuming that the null hypothesis is true.\n\nGraphic:\n \nThe distribution of possible null sample means is given by statistical theory. Specifically, the \\(t\\)-statistic (mean divided by the standard deviation of the sampling distribution of the mean) has a \\(t\\) distribution with \\(n - 1\\) degrees of freedom (\\(n\\) is the sample size). It works as long as your data aren’t too skewed or if you have a large enough sample size.\nFunction: t.test()\n\ntout &lt;- t.test(ls_diff ~ 1, data = boneden)\ntout\n\n\n  One Sample t-test\n\ndata:  ls_diff\nt = 2.6, df = 40, p-value = 0.01\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.007986 0.063721\nsample estimates:\nmean of x \n  0.03585 \n\n\nThe tidy() function from the broom package will format the output of common procedures to a convenient data frame.\n\ntdf &lt;- tidy(tout)\ntdf$estimate\n\nmean of x \n  0.03585 \n\ntdf$p.value\n\n[1] 0.01299\n\n\nWe often want a range of “likely” values. These are called confidence intervals. t.test() will return these confidence intervals, giving lowest and highest likely values for the mean difference in bone density:\n\ntdf$conf.low\n\n[1] 0.007986\n\ntdf$conf.high\n\n[1] 0.06372\n\n\nInterpreting confidence intervals:\n\nCORRECT: We used a procedure that would capture the true parameter in 95% of repeated samples.\nCORRECT: Prior to sampling, the probability of capturing the true parameter is 0.95.\nWRONG: After sampling, the probability of capturing the true parameter is 0.95.\n\nBecause after sampling the parameter is either in the interval or it’s not. We just don’t know which.\n\nWRONG: 95% of twins have bone density differences within the bounds of the 95% confidence interval.\n\nBecause confidence intervals are statements about parameters, not observational units or statistics.\n\n\nGraphic:\n \nIntuition: Statistical theory tells us that the sample mean will be within (approximately) 2 standard deviations of the population mean in 95% of repeated samples. This is two standard deviations of the sampling distribution of the sample mean, not two standard deviations of the sample. So we just add and subtract (approximately) two standard deviations of the sampling distribution from the sample mean.\nExercise: The birthweight data available here contains the birthweights (in ounces) of 1000 newborns born in a Boston area hospital. Wikipedia says the average birthweight for individuals of European and African descent is 123 ounces. Does this Boston hospital have the same mean as what Wikipedia says? Explain.\n\n\n\nPattern: Means of two groups are different (one quantitative, one binary)\n\nExample: IQ differences between children with high levels of lead (exposed) and those with low levels of lead (control).\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead |&gt;\n  select(Group, iqf) |&gt;\n  glimpse()\n\nRows: 124\nColumns: 2\n$ Group &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"control\"…\n$ iqf   &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, 118, 8…\n\n\nObservational Units: Children\nPopulation: All children\nSample: The 120 children for whom we have both lead and IQ measurements.\nVariables: The lead level group (binary/categorical) and the full scale IQ (quantitative).\nPattern: Use a boxplot to see if the groups differ.\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nParameter of interest: Difference in mean IQ levels between the control and exposed groups.\nEstimate: The difference in mean IQ between the two groups in our sample.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(meaniq = mean(iqf, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  Group   meaniq\n  &lt;chr&gt;    &lt;dbl&gt;\n1 control   92.6\n2 exposed   88.0\n\n92.55 - 88.02\n\n[1] 4.53\n\n\nThe control group is about 4.5 points higher on average\nHypothesis Test:\n\nWe want to know if the difference in the mean IQ in the two groups is actually different.\nTwo possibilities:\n\nNull Hypothesis: The mean IQs are the same in the two groups. We just happened by chance to get a lower IQ lead group and a higher IQ control group.\nAlternative Hypothesis: The mean IQ are different in the two groups.\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\n\n\nGraphic:\n \nThe distribution of possible null sample means comes from statistical theory. The t-statistic has a \\(t\\) distribution with a complicated degrees of freedom.\nFunction: t.test(). The quantitative variable goes to the left of the tilde and the binary variable goes to the right of the tilde.\n\ntout &lt;- t.test(iqf ~ Group, data = lead)\ntdf &lt;- tidy(tout)\ntdf$estimate\n\n[1] 4.532\n\ntdf$p.value\n\n[1] 0.07966\n\n\nt.test() also returns a 95% confidence interval for the difference in means. This has the exact same interpretation as in the previous section.\n\nc(tdf$conf.low, tdf$conf.high)\n\n[1] -0.5448  9.6094\n\n\nAssumptions (in decreasing order of importance):\n\nIndependence: conditional on group, IQ of one child doesn’t give us any information on the IQs of any other children (reasonable).\nApproximate normality: The distribution of IQ’s is bell-curved in group. Doesn’t matter for moderate-large sample sizes because of the central limit theorem.\n\nExercise: Is there a difference between control and exposed groups when it comes to the finger-wrist tapping test in the dominant hand (maxfwt).\n\n\n\nPattern: Proportion is shifted (one binary variable).\n\nThe exposed individuals were mostly males. There were 30 males and 16 females. In the US, about 51.22% of all births are boys. Are boys more likely to be recruited to the study than girls?\n\nlead |&gt;\n  filter(Group == \"exposed\") |&gt;\n  group_by(sex) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female    16\n2 male      30\n\n\nObservational Units: U.S. children exposed to lead\nPopulation: All U.S. children exposed to lead\nSample: The 46 children in our sample who were exposed to lead.\nVariable: Sex (male/female)\nPattern: Calculate sample proportion.\n\n30 / 46\n\n[1] 0.6522\n\n\nParameter of interest: Proportion of children exposed to lead who are boys\nEstimate with sample proportion, 0.6522\nHypothesis Testing:\n\nWe are interested in if our sample had some bias in selecting more boys.\nTwo possibilities:\n\nNull Hypothesis: Probability of a boy being included in the sample is 0.5122. We just happened by chance to get a lot more boys.\nAlternative Hypothesis: There is bias and the probability of a boy being in the sample is greater than for a girl(because boys are more likely to be exposed to lead, or because they were more likely to be recruited to the study).\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If this probability is high, we don’t conclude anything.\n\nGraphic:\n \nThe distribution of possible null sample proportions comes from statistical theory. The number of successes has a binomial distribution with success probability 0.5122 and size parameter equal to the sample size. The sample proportion is the number successes divided by the sample size.\nFunction: prop.test() (when you have a large number of both successes and failures) or binom.test() (for any number of successes and failures).\n\nbout &lt;- tidy(binom.test(x = 30, n = 46, p = 0.5122))\nbout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0757    0.498     0.786\n\n\n\npout &lt;- tidy(prop.test(x = 30, n = 46, p = 0.5122))\npout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0798    0.497     0.782\n\n\nExercise: Is there a sex bias for the control group?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 320: Biostatistics",
    "section": "",
    "text": "Syllabus\n\n00 R\n\nIntroduction to R\nPlotting in R\nDocuments in R\n\n01 Overview\n\nMath Prereqs\nCourse Overview\n\n02 Descriptive Statistics\n\nDescriptive Statistics in R\n\n03 Probability\n\nConfusion Matrix\nDiscrete Probability in R\nNormal Distribution in R\n\n04 Estimation\n\nRandom Selection/Assignment\nCentral Limit Theorem Illustration\nCI Interpretation\nt-distribution\nBone Density Case Study\nchi-squared distribution\nEstimating Binomial Proportion\n\n05 Testing\n\nOne Sample \\(t\\)-Tests in R\nPower Calculations in R\nOne Sample Binomial Tests in R\nOne Sample Binomial Power Calculations\nTwo Sample \\(t\\)-Tests in R"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STAT 320 - Biostatistics",
    "section": "",
    "text": "Instructor: Dr. David Gerard\nEmail: dgerard@american.edu\nOffice: DMTI 106E\n\n\nRequired Text\n\nRosner, B. (2016) Fundamentals of Biostatistics, Eighth Edition. Brooks/Cole, Boston, MA, USA.\n\n\nThere will be occasional readings from other sources, such as journal articles, for class discussion or for homework assignments. These will be posted in Blackboard or links will be given to find these online.\n\n\n\nCourse Content and Learning Objectives\nSTAT-320 is an introduction to the statistical methodology commonly used in public health, medical, and biological studies. This course emphasizes working with data and communicating statistical ideas. A breadth of topics will be covered including: study design, tests of significance, confidence intervals, t-procedures, chi-square and Fisher’s exact test, linear regression, logistic regression, analysis of variance, nonparametric methods, and more advanced topics as time permits. The R computer program will be used to conduct analyses.\nThe major focus for this course is the ideas behind, and the methods for, drawing conclusions about a population from a sample. At the end of this course you will be expected to identify the major concepts related to statistical reasoning and to statistical inferences for drawing such conclusions, recognize how these concepts are used in disciplines related to health and medicine, and implement the methods yourself in statistical analyses using the methods covered. In particular, you are expected to be able to (1) identify the appropriate statistical model or models for a given analysis, (2) write the model in the correct notation, (3) implement the model in the R software package on a given set of data, (4) interpret the output in the context of the study, (5) diagnose model deficiences, (6) suggest improvements to the model if necessary, and (7) summarize the results of the analysis. Work will be a balance between understanding the concepts underlying a method, implementation of the method, and interpretation of the results.\n\n\nGrading\n\n\n\n\n\nAssignment\nPercent\n\n\n\n\nQuarto Notes\n10%\n\n\nHomeworks\n30%\n\n\nExam 1\n20%\n\n\nExam 2\n20%\n\n\nExam 3\n20%\n\n\n\n\n\n\nQuarto Notes:\n\nMost of my notes are handwritten. Each week, I will assign one student to convert my hand-written notes into a quarto document that can be parsed to HTML. I will (lightly) modify these notes and post them on the course website. You will only have to do this once this semester.\nI think we have more students than classes, so for some of you I will ask that you convert your exam reference sheets into a Quarto document.\n\nExams:\n\nNot officially cumulative. But all material in Statistics builds off of previous material, so effectively cumulative.\nYou can bring a 1 page (8.5’’ by 11’’ piece of paper) reference sheet that is handwritten (no typing). You can use both sides.\nNo other resources are allowed. If you touch your phone/computer/smart watch/smart glasses/etc during the exam then that is an automatic fail for the course.\nI will drop your lowest exam score. Because of this:\n\nThere will be no make up exams. You will just use your drop if you have to miss one.\nYou cannot leave the room during the exam unless you turn it in. If it is an emergency and you need to leave the room, then you can use your drop. You will need to be in contact with ASAC for an official accommodation if you are unable to stay in the room for 1 hour and 15 minutes.\n\nIf you miss two exams, you should consider withdrawing from the class. The last day to withdraw from the course is 11/01/2024.\n\nHomeworks:\n\nTo me (and most of you), homeworks are meant to solidify your understanding. To (some of) you, homeworks are meant to help you prepare for the exams. You can use generative AI for the homeworks if you want. But you won’t get any other study aids, so I wouldn’t use it except to check my work after I’ve done all of the problems.\nLots of education research points to practice problems as the best form of learning, and just reading solutions (e.g. created by AI) as the worst form of learning. If you just use AI to get solutions on the homeworks, you should expect to have bad exam grades.\nI will drop your lowest homework score.\n\n\nUsual grade cutoffs will be used:\n\n\n\n\n\nGrade\nLower\nUpper\n\n\n\n\nA\n93\n100\n\n\nA-\n90\n92\n\n\nB+\n88\n89\n\n\nB\n83\n87\n\n\nB-\n80\n82\n\n\nC+\n78\n79\n\n\nC\n73\n77\n\n\nC-\n70\n72\n\n\nD\n60\n69\n\n\nF\n0\n59\n\n\n\n\n\nIndividual assignments will not be curved. However, at the discretion of the instructor, the overall course grade at the end of the semester may be curved.\n\n\nLate Work Policy\n\nAll assignments must be submitted on the day they are due.\nHomeworks will typically be due on Mondays by end-of-day.\nEach student will have two three-day extensions, where you can turn in the assignment on Thursday by end-of-day.\nPlease just let me know ahead of time that you will be using one of your two extensions.\nPlease do not tell me why you need the extension. Any reason is a fine reason.\nAny homeworks not submitted by the due date will receive a grade of 0.\n\n\n\nImportant Dates\n\n09/27 (tentative): First exam (Chapters 1 through 5)\n10/11: Fall break (no class)\n11/01 (tentative): Second exam (Chapters 6 through 8)\n11/01: Last day to withdraw\n11/05: Election day (no class)\n11/26: Class meets via Zoom\n11/29: Thanksgiving break (no class)\n12/13 at 8:10AM-10:40AM: Third exam (Chapters 10 through 12)\n\n\n\nComputing and Software\nWe will use the R computing language to complete some assignment questions. R is free and may be downloaded from the R website (http://cran.r-project.org/). In addition, I highly recommend you interface with R through the free RStudio IDE (https://www.rstudio.com/). R and RStudio are also available on computers in the Anderson Computing Complex, the Center for Teaching, Research, and Learning Lab (CTRL) in Hurst Hall, in addition to various labs across campus. R Studio may also be run from your web browser using American University’s Virtual Applications System. Please see me during office hours if you have questions regarding R.\n\n\nData\nData sets for homeworks assignments and examples from the textbook are available on the Data page. Almost all of these are cleaned versions of the data from the book’s companion website.\n\n\nAcademic Integrity\n\nStandards of academic conduct are set forth in the university’s Academic Integrity Code. By registering for this course, students have acknowledged their awareness of the Academic Integrity Code and they are obliged to become familiar with their rights and responsibilities as defined by the Code. Violations of the Academic Integrity Code will not be treated lightly and disciplinary action will be taken should violations occur. This includes cheating, fabrication, and plagiarism.\nI expect you to work with others and me, and I expect you to use online resources as you work on your assignments. However, your submissions must be composed of your own thoughts, coding, and words. You should be able to explain your work on assignments/projects and your rationale. Based on your explanation (or lack thereof), I may modify your grade.\nYou can use generative AI (e.g. ChatGPT, CoPilot, etc) on the homeworks if you want. But\n\nThese are your only study exercises for the exams. So I wouldn’t use AI to do them for me except to check my work after I am done.\nYou are still expected to “own” all of your responses. I reserve the right to ask you to explain any of your solutions. If you write something weird or too advanced in the homework, I’ll call you in and ask you questions about it. Based on your explanation (or lack thereof), I may modify your grade.\n\nNo resources are allowed for the exam except the 1 page (8.5’’ by 11’’) handwritten cheat sheet (and a pen or pencil, of course). If you touch your phone/computer/smart watch/smart glasses/etc during the exam then that is an automatic fail for the course.\nAll solutions that I provide are under my copyright. These solutions are for personal use only and may not be distributed to anyone else. Giving these solutions to others, including other students or posting them on the internet, is a violation of my copyright and a violation of the student code of conduct.\n\n\n\nSharing Course Content:\nStudents are not permitted to make visual or audio recordings (including livestreams) of lectures or any class-related content or use any type of recording device unless prior permission from the instructor is obtained and there are no objections from any student in the class. If permission is granted, only students registered in the course may use or share recordings and any electronic copies of course materials (e.g., PowerPoints, formulas, lecture notes, and any discussions – online or otherwise). Use is limited to educational purposes even after the end of the course. Exceptions will be made for students who present a signed Letter of Accommodation from the Academic Support and Access Center. Further details are available from the ASAC website.\n\n\nSyllabus Change Policy\nThis syllabus is a guide for the course and is subject to change with advanced notice. These changes may come via Canvas. Make sure to check Canvas announcements regularly. You are accountable for all such communications."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#learning-objectives",
    "href": "00_course_outline/00_course_outline.html#learning-objectives",
    "title": "Course Outline for Stat 320",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nThree aspects of Statistics\nPopulation/Sample"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics",
    "href": "00_course_outline/00_course_outline.html#statistics",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nData — Numerical or qualitative descriptions of people/places/things that we want to study."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-1",
    "href": "00_course_outline/00_course_outline.html#statistics-1",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nSome examples\n\nLead Exposure\n\nData: Retrospective study measuring lead exposure in children along with variou outcome variables like IQ score, different measures of neurological function, and hyperactivity assessmenets.\nQuestion: What are the neurological and behavioral effects of lead exposure in young children?\n\nSmoking and bone density\n\nData: Pairs of twins, one of whom is a lighter smoker and one of whome is a heavier smoker. Different bone density measures were taken.\nQuestion: Do individuals who smoke have lower densities?"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-2",
    "href": "00_course_outline/00_course_outline.html#statistics-2",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nThree aspects:\n\nData Design\nData Description\nData Inference — informed by Probability"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-design",
    "href": "00_course_outline/00_course_outline.html#data-design",
    "title": "Course Outline for Stat 320",
    "section": "Data Design",
    "text": "Data Design\nWhere do we get data?\n\nWhat is the proper way to collect data?\nWhen can we claim a causal connection between variables? (e.g. Does smoking lead to lower bone density? Does lead lead to increased neurological and behavioral problems?)\nWhat are some sources of bias (unwanted systematic tendencies in the data collection)?\nOnly touched on in this course."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-description",
    "href": "00_course_outline/00_course_outline.html#data-description",
    "title": "Course Outline for Stat 320",
    "section": "Data Description",
    "text": "Data Description\nHow do we describe the data we have?\n\nNumerical summaries — use numbers to describe the data.\nGraphical summaries — use pictures to describe the data.\nExploratory data analysis — play with the data to get a “feel” for it.\nLots of R.\nFirst week of the semester."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-inference-probability",
    "href": "00_course_outline/00_course_outline.html#data-inference-probability",
    "title": "Course Outline for Stat 320",
    "section": "Data Inference (Probability)",
    "text": "Data Inference (Probability)\nHow can we tell if our conclusions from the exploratory data analysis are real?\n\nLast thirteen weeks of the semester.\nProbability — subdiscipline of Mathematics that provides a foundation for modeling chance events.\nInference — describing a population (probabilistically) by using information from sample."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#population",
    "href": "00_course_outline/00_course_outline.html#population",
    "title": "Course Outline for Stat 320",
    "section": "Population",
    "text": "Population\nStatisticians (among others) are interested in characteristics of a large group of people/countries/objects\n\nCharacterize/describe neurological and behavioral health of young children\nCharacterize/describe bone health of smokers.\nCharacterize/describe the effectiveness of a drug on a all adults.\n\nA population is a group of individuals/objects/locations for which you want information."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#sample",
    "href": "00_course_outline/00_course_outline.html#sample",
    "title": "Course Outline for Stat 320",
    "section": "Sample",
    "text": "Sample\nIt is usually expensive/impossible to measure characteristics of every case in a population.\nA sample is a subgroup of individuals/objects/locations of the population.\n\nMeasure lead intake and different measures of neurological and behavioral health in a sample of 124 children.\nFind a group of 41 twins who have different smoking behaviors and compare their bone densities."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference",
    "href": "00_course_outline/00_course_outline.html#inference",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\nFrom the sample, describe the population using probability.\n\nWe have strong evidence that lighter smoking twins have heavier lumbar spine bone density than heavier smoking twins (pair \\(t\\)-test \\(p = 0.006494\\)). The corresponding 95% confidence interval for the difference in bone density is 0.00799 g/cm2 0.06372 g/cm2. Since the twins are not a random sample, the generalizability of this result depends on how representative the twins are of the general population of interest. Since this is an observational study and not an experiment, the statistics alone cannot make a claim for causality — such a claim would have to depend on other arguments."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference-1",
    "href": "00_course_outline/00_course_outline.html#inference-1",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\n\nIn this class, we will learn how to formulate such statements and interpret them."
  },
  {
    "objectID": "01_r/01_quarto.html",
    "href": "01_r/01_quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Quarto is a file format that is a combination of plain text and R code.\nLots of great educational material is available at https://quarto.org/\nYou write code and commentary of code in one file. You may then compile (RStudio calls this “rendering”) the Quarto file to many different kinds of output: pdf (including beamer presentations), html (including various presentation formats), Word, PowerPoint, etc.\nQuarto is useful for:\n\nCommunication of statistical results.\nCollaborating with other data scientists.\nUsing it as a modern lab notebook to do data science.\n\nQuarto can also make “literate programming” documents for python, Julia, JavaScript, etc…\n\n\n\n\nInstall Quarto via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/\n\n\n\n\n\n\nOpen up a new Quarto file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto:\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: \"`r Sys.Date()`\"\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\nThe `r Sys.Date()` will insert the current date.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_quarto.html#getting-statrted",
    "href": "01_r/01_quarto.html#getting-statrted",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Install Quarto via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/"
  },
  {
    "objectID": "01_r/01_quarto.html#playing-with-quarto",
    "href": "01_r/01_quarto.html#playing-with-quarto",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Open up a new Quarto file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto:\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: \"`r Sys.Date()`\"\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\nThe `r Sys.Date()` will insert the current date.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_ggplot.html",
    "href": "01_r/01_ggplot.html",
    "title": "R Graphics with {ggplot2}",
    "section": "",
    "text": "Basic plotting in R using the {ggplot2} package."
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous",
    "href": "01_r/01_ggplot.html#continuous",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous",
    "text": "Continuous\n\nHistogram:\n\nVariable should be on the \\(x\\)-axis.\nUse the geom_histogram() function.\n\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nMake the bin lines black and the fill white, and change the number of bins.\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\")\n\n\n\n\n\n\n\n\nExercise: Load in the boneden data (see here for a description) and make a histogram of lumbar spine density for the lighter smoking twin with 20 bins. Make the bins red."
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete",
    "href": "01_r/01_ggplot.html#discrete",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete",
    "text": "Discrete\n\nBarplot:\n\nPut the variable on the \\(x\\)-axis.\nUse geom_bar().\n\n\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nExercise: What variables from the lead data are appropriately plotted using a bar plot? Plot a couple of them."
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "href": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous X, Continuous Y",
    "text": "Continuous X, Continuous Y\n\nScatterplot:\n\nSay what variables should be on the \\(x\\)- and \\(y\\)-axes.\nUse geom_point().\n\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point()\n\n\n\n\n\n\n\n\nJitter points to account for overlaying points.\n\nUse geom_jitter() instead of geom_point().\n\n\nggplot(data = lead, mapping = aes(x = totyrs, y = hyperact)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nAdd a Loess Smoother by adding geom_smooth().\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nExercise: Using the boneden data, make a scatterplot exploring the association between the lumbar spine densities for the two twin types (smoking status)."
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "href": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete X, Continuous Y",
    "text": "Discrete X, Continuous Y\n\nBoxplot\n\nPlace one variable on \\(x\\)-axis and other on \\(y\\)-axis.\nTypically, but not always, continuous goes on \\(y\\)-axis.\nUse geom_boxplot().\n\n\nggplot(data = lead, mapping = aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nExercise: Using the boneden data, first calculate the difference in lumbar spine densities between the two twins (you’ll need to use mutate() here). Then plot this difference versus zygosity (monozygotic versus dizygotic)."
  },
  {
    "objectID": "01_r/01_figures/formatting.html",
    "href": "01_r/01_figures/formatting.html",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#text-formatting",
    "href": "01_r/01_figures/formatting.html#text-formatting",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#headings",
    "href": "01_r/01_figures/formatting.html#headings",
    "title": "1st Level Header",
    "section": "Headings",
    "text": "Headings"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#nd-level-header",
    "href": "01_r/01_figures/formatting.html#nd-level-header",
    "title": "1st Level Header",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#lists",
    "href": "01_r/01_figures/formatting.html#lists",
    "title": "1st Level Header",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output."
  },
  {
    "objectID": "01_r/01_figures/formatting.html#links-and-images",
    "href": "01_r/01_figures/formatting.html#links-and-images",
    "title": "1st Level Header",
    "section": "Links and images",
    "text": "Links and images\nhttp://example.com\nlinked phrase\n\n\n\noptional caption text"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#tables",
    "href": "01_r/01_figures/formatting.html#tables",
    "title": "1st Level Header",
    "section": "Tables",
    "text": "Tables\n\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell\nContent Cell\n\n\nContent Cell\nContent Cell"
  },
  {
    "objectID": "02_descriptive/02_descriptive.html",
    "href": "02_descriptive/02_descriptive.html",
    "title": "Summary Statistics in R",
    "section": "",
    "text": "We’ll use the lead data as an example. Read about it here.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nYou calculate the summary statistics (mean/median/quantiles/variance/standard deviation) all within a summarize() call.\n\nsummarize(\n  lead, \n  Mean = mean(iqf, na.rm = TRUE), \n  Min = min(iqf, na.rm = TRUE),\n  Q25 = quantile(iqf, probs = 0.25, na.rm = TRUE),\n  Med = median(iqf, na.rm = TRUE), \n  Q75 = quantile(iqf, probs = 0.75, na.rm = TRUE),\n  Max = max(iqf, na.rm = TRUE),\n  Var = var(iqf, na.rm = TRUE),\n  SD = sd(iqf, na.rm = TRUE)\n)\n\n# A tibble: 1 × 8\n   Mean   Min   Q25   Med   Q75   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  90.8    46    80  89.5  98.5   141  212.  14.6\n\n\nThe values on the left of = are the names of the summaries and are up to you.\nThe values on the right of = are the function calls for the summaries.\n\nmean(): the arithmetic mean.\nmin(): the minimum. Same as quantile(x, probs = 0)\nquantile(): the quantiles. You specify which quantile with the probs argument.\nmedian(): the median. Same as quantile(x, probs = 0.5)\nmax(): the maximum. Same as quantile(x, probs = 1)\nvar(): the sample variance.\nsd(): the sample standard deviation.\n\nI have the na.rm = TRUE argument because there are some children who did not have a iqf score. These are “missing” and encoded with NA. If you do not provide that argument, R doesn’t know what those values are and so returns an NA or errors.\n\nsummarize(\n  lead, \n  Mean = mean(iqf), \n  Min = min(iqf),\n  # Q25 = quantile(iqf, probs = 0.25), # errors\n  Med = median(iqf), \n  # Q75 = quantile(iqf, probs = 0.75), # errors\n  Max = max(iqf),\n  Var = var(iqf),\n  SD = sd(iqf)\n)\n\n# A tibble: 1 × 6\n   Mean   Min   Med   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    NA    NA    NA    NA    NA    NA\n\n\nYou can also apply these functions on vectors that you extract from the data frame.\n\nvar(lead$iqf, na.rm = TRUE)\n\n[1] 212.3\n\n\nLet’s demonstrate some properties. Variance is invariant to shift\n\nvar(lead$iqf + 10000, na.rm = TRUE)\n\n[1] 212.3\n\n\nbut scales with the square of the multiplicative factor\n\nvar(10 * lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n10^2 * var(lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n\nThe standard deviation scales with the multiplicative factor because it is the square root of the variance.\n\nsd(10 * lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n10 * sd(lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n\nThe mean and quantiles shift and scale with the additive and multiplicative factors.\n\nmean(lead$iqf * 10 + 20, na.rm = TRUE)\n\n[1] 928.2\n\n10 * mean(lead$iqf, na.rm = TRUE) + 20\n\n[1] 928.2\n\nquantile(lead$iqf * 10 + 20, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)\n\n 25%  50%  75% \n 820  915 1005 \n\n10 * quantile(lead$iqf, probs = c(0.25, 0.5, 0.75), na.rm = TRUE) + 20\n\n 25%  50%  75% \n 820  915 1005 \n\n\nExercise: Calculate the mean and median of the birthweight data. What is the more appropriate measure of center?\nYou can calculate grouped summaries (a summary for each group) by grouping the data first.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(\n    Mean = mean(iqf, na.rm = TRUE),\n    SD = sd(iqf, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 3\n  Group    Mean    SD\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 control  92.6  15.7\n2 exposed  88.0  12.2\n\n\nGroup summaries are where the power of descriptive statistics really comes into play. Here, we see that the exposed group has a lower IQ on average than the control group. Whether this is real signal will have to be answered via a formal hypothesis test. But the descriptive statistics gives us some initial information on the data.\nExercise: What about different lead groups? Calculate descriptive statistics for the different lead groups."
  },
  {
    "objectID": "hw/hw_ch2/hw_ch2.html",
    "href": "hw/hw_ch2/hw_ch2.html",
    "title": "Homework 01L Chapters 1 and 2",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives: - Chapter 2 of Rosner - Descriptive Statistics - Graphics - R\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: &lt; https://www.adobe.com/acrobat/online/merge-pdf.html&gt;\nUse the tidyverse way to answer these questions. If you use the base R way, that’s fine, but I’ll call you in and have you show me that you really know how the base R way works by doing some practice problems in front of me.\n\n\nQuestion 1: Cardiovascular Disease\nRead in the data using the read_csv() function, which you can read about here: https://dcgerard.github.io/stat_320/data.html#lvm\n\nlibrary(tidyverse)\nlvm &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lvm.csv\")\n\n\nUse R to print what variables are in lvm?\n\n\nnames(lvm)\n\n[1] \"ID\"      \"lvmht27\" \"bpcat\"   \"gender\"  \"age\"     \"BMI\"    \n\n\n\nWhat is the arithmetic mean of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(Mean = mean(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat             Mean\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      34.1\n2 normal            29.3\n3 pre-hypertensive  33.8\n\n\n\nIs it appropriate to use the arithmetic mean (by blood pressure group) for LVMI, or should we have used the median? Explain your reasoning and justify with an appropriate box plot.\n\n\nggplot(lvm, aes(x = bpcat, y = lvmht27)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n## Since the distributions are mostly symmetric, and there \n## do not appear to be any major outliers, it looks like\n## the sample mean is an appropriate measure of center.\n\n\nWhat does the boxplot from the previous questiontell you?\n\n\n## The center of group 3 is larger than group 2 than group 1. \n## But groups 1 and 3 are more spread out than group 2. All \n## groups appear to have roughly symmetric distributions.\n\n\nWhat is the standard deviation of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(SD = sd(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat               SD\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      8.56\n2 normal            6.66\n3 pre-hypertensive  5.75\n\n\n\nDoes there appear to be any association between age and lvmht27? Use an appropriate plot to make your case.\n\n\nggplot(lvm, aes(x = age, y = lvmht27)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x)\n\n\n\n\n\n\n\n## Maybe a weak positive association.\n\n\n\nValidity Data\nRead about the validity study on the food frequency questionnaire here: https://dcgerard.github.io/stat_320/data.html#valid\n\nThe data are available at &lt;&gt;. Load these data into R as the valid data frame.\n\n\nvalid &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/valid.csv\")\n\nRows: 173 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Id, sfat_dr, sfat_ffq, tfat_dr, tfat_ffq, alco_dr, alco_ffq, cal_dr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nUse descriptive statistics to relate nutrient intake for the DR and FFQ. Do you think the FFQ is a reasonably ac- curate approximation to the DR? Why or why not?\nA frequently used method for quantifying dietary intake is in the form of quintiles. Compute quintiles for each nutri- ent and each method of recording, and relate the nutrient composition for DR and FFQ using the quintile scale. (That is, how does the quintile category based on DR relate to the quintile category based on FFQ for the same individual?) Do you get the same impression about the concordance between DR and FFQ using quintiles as in the previous problem, in which raw (ungrouped) nutrient intake is considered?\nIn nutritional epidemiology, it is customary to assess nutrient intake in relation to total caloric intake. One measure used to accomplish this is nutrient density, which is defined as 100% × (caloric intake of a nutrient/total caloric intake). For fat consumption, 1 g of fat is equivalent to 9 calories. Compute the nutrient density for total fat for the DR and FFQ.\n\n\nvalid |&gt;\n  mutate(nd_dr = tfat_dr * 9 / cal_dr * 100,\n         nd_ffq = tfat_ffq * 9 / cal_ffq * 100) -&gt;\n  valid\n\n\nObtain appropriate descriptive statistics for nutrient density for both DR and FFQ. How do they compare?\n\n\nRelate the nutrient density for total fat for the DR versus the FFQ using the quintile approach in Problem 2.28. Is the concordance between total fat for DR and FFQ stronger, weaker, or the same when total fat is expressed in terms of nutrient density as opposed to raw nutrient?"
  },
  {
    "objectID": "03_prob/03_prob.html",
    "href": "03_prob/03_prob.html",
    "title": "Probability Definitions and Properties",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution"
  },
  {
    "objectID": "03_prob/03_prob_discrete.html",
    "href": "03_prob/03_prob_discrete.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts."
  },
  {
    "objectID": "03_prob/03_confusion.html",
    "href": "03_prob/03_confusion.html",
    "title": "Confusion Matrix",
    "section": "",
    "text": "The below is a simplified version of the Confusion matrix table from Wikipedia that emphasizes the terminology more common to biostatistics (e.g. sensitivity/specificity/\\(PV^+\\)/\\(PV^-\\))\n\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\nTest Positive \\(T^+\\)\n\n\nTest Negative \\(T^-\\)\n\n\n\n\n\n\n\n\nTruth\n\n\n\nPositive \\(D^+\\)\n\n\nTrue Positive (TP)\n\n\nFalse Negative (FN)  (Type II Error)\n\n\nSensitivity  (True Positive Rate, Recall)  \\(\\frac{TP}{D^+}\\)\n\n\nFalse Negative Rate  (Type II Error Rate)  \\(\\frac{FN}{D^+}\\)\n\n\n\n\nNegative \\(D^-\\)\n\n\nFalse Positive (FP)  (Type I Error)\n\n\nTrue Negative (TN)\n\n\nFalse Positive Rate  (Type I Error Rate)  \\(\\frac{FP}{D^-}\\)\n\n\nSpecificity (True Negative Rate)  \\(\\frac{TN}{D^-}\\)\n\n\n\n\n\n\nPrevalence  \\(\\frac{D^+}{D^+ + D^-}\\)\n\n\nPositive Predictive Value  (Precision)  \\(PV^+ = \\frac{TP}{T^+}\\)\n\n\nFalse Omission Rate  \\(\\frac{FN}{T^-}\\)\n\n\n\n\n\n\n\n\n\n\nFalse Discovery Rate  \\(\\frac{FP}{T^+}\\)\n\n\nNegative Predictive Value  \\(PV^- = \\frac{TN}{T^-}\\)"
  },
  {
    "objectID": "03_prob/03_prob_cont.html",
    "href": "03_prob/03_prob_cont.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "The density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.9990 0.4384 0.9445 2.3738 1.5226 1.6511\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2"
  },
  {
    "objectID": "hw/hw_ch5/hw_ch5.html",
    "href": "hw/hw_ch5/hw_ch5.html",
    "title": "Homework, Chapter 4",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives:\n\nChapter 5 of Rosner\nNormal Distribution\n\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: https://www.adobe.com/acrobat/online/merge-pdf.html\nI will only grade a randomly chosen subset of these questions. Please complete all of them, since you don’t know which ones I will grade.\n\n\nBlood Chemistry\nIn pharmacologic research a variety of clinical chemistry measurements are routinely monitored closely for evidence of side effects of the medication under study. Suppose typical blood-glucose levels are normally distributed, with mean = 90 mg/dL and standard deviation = 38 mg/dL.\n\nIf the normal range is 65−120 mg/dL, then what percentage of values will fall in the normal range?\n\n\n# X ~ N(90, 38^2)\npnorm(q = 120, mean = 90, sd = 38) - pnorm(q = 65, mean = 90, sd = 38)\n\n[1] 0.5298\n\n\n\nIn some studies only values at least 1.5 times as high as the upper limit of normal are identified as abnormal. What percentage of values would fall in this range?\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 0.008932\n\n\n\nAnswer Problem 2 for values 2.0 times the upper limit of normal.\n\n\n# Calculate upper limit\nu &lt;- 120 * 2\nu\n\n[1] 240\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 3.951e-05\n\n\n\nFrequently, tests that yield abnormal results are re- peated for confirmation. What is the probability that for a normal person a test will be at least 1.5 times as high as the upper limit of normal on two separate occasions? Assume the two tests are independent.\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\np1 &lt;- pnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\np1^2\n\n[1] 7.978e-05\n\n\n\nSuppose that in a pharmacologic study involving 6000 patients, 75 patients have blood-glucose levels at least 1.5 times the upper limit of normal on one occasion. What is the probability that this result could be due to chance? Assume the patient tests are independent.\n\n\n## Y = number of patients above 1.5 times upper limit\n## Y ~ Binom(6000, p1)\n## Want Pr(Y &gt;= 75)\n1 - pbinom(q = 74, size = 6000, prob = p1)\n\n[1] 0.003166\n\n## Only a 0.3% chance, so very unlikely\n\n\n\nOrthopedics\nA study was conducted of a diagnostic test (the FAIR test, i.e., hip flexion, adduction, and internal rotation) used to identify people with piriformis syndrome (PS), a pelvic condition that involves malfunction of the piriformis muscle (a deep buttock muscle), which often causes lumbar and buttock pain with sciatica (pain radiating down the leg) [7]. The FAIR test is based on nerve-conduction velocity and is expressed as a difference score (nerve-conduction velocity in an aggravating posture minus nerve-conduction velocity in a neutral posture). It is felt that the larger the FAIR test score, the more likely a participant will be to have PS. Data are given in the Data Set PIRIFORM.DAT for 142 participants without PS (piriform = 1) and 489 participants with PS (piriform = 2) for whom the diagnosis of PS was based on clinical criteria. The FAIR test value is called MAXCHG and is in milliseconds (ms). A cutoff point of ≥ 1.86 ms on the FAIR test is proposed to define a positive test.\n\nWhat is the sensitivity of the test for this cutoff point?\nWhat is the specificity of the test for this cutoff point?\nSuppose that 70% of the participants who are referred to an orthopedist who specializes in PS will actually have the condition. If a test score of ≥ 1.86 ms is obtained for a par- ticipant, then what is the probability that the person has PS?\nThe criterion of ≥ 1.86 ms to define a positive test is arbitrary. Using different cutoff points to define positivity, obtain the ROC curve for the FAIR test. What is the area under the ROC curve? What does it mean in this context?\nDo you think the distribution of FAIR test scores within a group is normally distributed? Why or why not?"
  },
  {
    "objectID": "04_est/04_sample.html",
    "href": "04_est/04_sample.html",
    "title": "Random Sampling",
    "section": "",
    "text": "Before doing sampling, make sure you set the seed so that you have reproducible results. E.g., this makes it so that your “random selection” is the same every time you first run the seed.\n\nset.seed(3574927)\n\nIf you are having trouble coming up with a random seed, you could NIST’s Interoperable Randomness Beacons API, which generates a new truly random number every 60 seconds. You would only run this once and copy the resulting number in your script. You would not include this script anywhere in any code you have.\n\n## HTTP request for a random number\nlibrary(httr2)\nreqout &lt;- request(base_url = \"https://beacon.nist.gov/beacon/2.0/pulse/last\") |&gt;\n  req_perform()\n\n## A random value represented as a 64-byte (512 bits) hex string\nhex &lt;- resp_body_json(reqout)$pulse$localRandomValue \n\n## select only first few digits to make number small. You can increase this.\nhexsmall &lt;- substr(hex, start = 1, stop = 6) \n\n## convert to an integer. This is your seed.\nstrtoi(x = hexsmall, base = 16) \n\nYou generate ID’s for with seq() or :\n\n# 1:100\nidvec &lt;- seq(from = 1, to = 20)\nidvec\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nYou can sample (without replacement) from this vector with sample()\n\nsample(x = idvec, size = 5)\n\n[1]  6 18 11  1  3\n\n\nIf you don’t give an argument for size, then sample will randomly permute the values.\n\nsample(x = idvec)\n\n [1]  1  5  6 16 13 18 19 17 20 12  7  8  3  2 14 11  4 10  9 15\n\n\nThis is useful for random assignment.\nYou should generally also randomize order, but if you need to see the group ID’s in an easier to read format, use sort().\n\nsample(x = idvec, size = 5) |&gt;\n  sort()\n\n[1]  4 10 12 13 15\n\n\nIf you are doing random assignment, you have a data frame of individuals. E.g., from the birthweight data.\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nThen you create a column with the number of groups via rep(), and then randomly permute it with sample(). E.g., suppose we want three groups:\n\nbirthweight |&gt;\n  mutate(group = rep(1:3, length.out = n())) |&gt; ## choose groups of equal size\n  mutate(group = sample(group)) ## randomly assign\n\n# A tibble: 1,000 × 3\n      id weight group\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1     0    116     2\n 2     1    124     1\n 3     2    119     2\n 4     3    100     1\n 5     4    127     3\n 6     5    103     2\n 7     6    140     1\n 8     7     82     3\n 9     8    107     3\n10     9    132     1\n# ℹ 990 more rows\n\n\nExercise: Randomly assign 400 individuals (with IDs 1 through 400) into two groups, \"treatment\" and \"control\".\nExercise: The treatment is way more expensive than the control, so randomly assign only 100 to \"treatment\" and 300 to \"control\"."
  },
  {
    "objectID": "04_est/04_clt.html",
    "href": "04_est/04_clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Consider the birthweight data:\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\nggplot(birthweight, aes(x = weight)) +\n  geom_histogram(bins = 30, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nLet’s take samples of size \\(n\\) = 1, 5, 10, 20, 50, 100 from this distribution with replacement. For each sample size, we college 10000 repeat samples, each time calculating the sample mean \\(\\bar{X}\\), to obtain \\(\\bar{X}_1,\\bar{X}_2,\\ldots,\\bar{X}_{10000}\\). Below are histograms of these \\(\\bar{X}\\)’s\nThe distribution of the \\(\\bar{X}\\)’s has smaller and smaller variance as the sample size increases since \\(\\mathrm{var}(\\bar{X}) = \\sigma^2/n\\).\n \nThe distribution of the \\(\\bar{X}\\)’s gets closer to a normal as the sample size increases. Though it’s already sufficiently normal for most purposes once \\(n = 10\\).\n \nThe true mean \\(\\mu\\) is the vertical dashed red line. You see that the distribution of the sample mean has a mean of \\(\\mu\\), \\(\\mathrm{E}[\\bar{X}] = \\mu\\)."
  },
  {
    "objectID": "04_est/04_t.html",
    "href": "04_est/04_t.html",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_t.html#learning-objectives",
    "href": "04_est/04_t.html#learning-objectives",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_boneden.html",
    "href": "04_est/04_boneden.html",
    "title": "Bone Density Case Study",
    "section": "",
    "text": "Twin study with \\(n=41\\) where one smoked more than the other. Bone density was measured in both twins. More detail here.\n\nlibrary(tidyverse)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nLet’s explore if lumbar spine bone density differed between the twins. First, we’ll calculate the difference in density between the twins:\n\nboneden |&gt;\n  mutate(diff = ls1 - ls2) -&gt; #ls1 = lighter smoking, ls2 = heavy smoking\n  boneden \n\nLet’s plot the data\n\nggplot(boneden, aes(x = diff)) +\n  geom_histogram(bins = 7, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the difference\n\nboneden |&gt;\n  summarize(xbar = mean(diff), s = sd(diff), n = n())\n\n# A tibble: 1 × 3\n    xbar      s     n\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 0.0359 0.0883    41\n\n\nWe can use this to get a 95% confidence interval for the mean difference in lumbar spine bone density between heavy and light smoking twins.\nThe standard error is\n\n0.08829 / sqrt(41)\n\n[1] 0.01379\n\n\nThe appropriate quantile of the t-distribution is\n\nqt(p = 0.975, df = 41 - 1)\n\n[1] 2.021\n\n\nSo, the 95% confidence interval is\n\n0.03585 - 2.021 * 0.01379 # lower\n\n[1] 0.00798\n\n0.03585 + 2.021 * 0.01379 # upper\n\n[1] 0.06372\n\n\nSince the lower bound of the 95% CI is above 0, we can be fairly confident that the true mean difference is greater than 0. That is, we are pretty sure that the lighter smoking twin has heavier bone density. We will formalize what “pretty sure” means in Chapter 7.\n\n\nReal Way\n\nIt would be crazy to do the above calculations, by hand, every time. For this class, I’ll occasionally ask you do that to solidify your understanding. But in real data analysis we use code to automate inference.\nWe will use the {broom} package to summarize inference output.\n\nlibrary(broom)\n\nYou calculate an interval for a mean using t.test() using one of two ways:\n\n## tout &lt;- t.test(boneden$diff)\ntout &lt;- t.test(diff ~ 1, data = boneden)\n\nYou get a summary of the output with broom::tidy()\n\ntidy(tout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359  0.00799    0.0637\n\n\nYou can change the level with the conf.level argument in t.test()\n\nt.test(diff ~ 1, data = boneden, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359   0.0126    0.0591\n\n\nExercise: Calculate an 80% confidence interval for the mean birth weight of newborns using the birth weight data that you can download from here: https://dcgerard.github.io/stat_320/data/birthweight.csv. Do this both “by hand” (after calculating the summary statistics and t-quantile) and using R’s automated functions."
  },
  {
    "objectID": "04_est/04_chi2.html",
    "href": "04_est/04_chi2.html",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/04_chi2.html#learning-objectives",
    "href": "04_est/04_chi2.html#learning-objectives",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/binom.html",
    "href": "04_est/binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\nExercise: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?"
  },
  {
    "objectID": "04_est/04_binom.html",
    "href": "04_est/04_binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\nExercise: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?"
  },
  {
    "objectID": "05_tests/05_ttest.html",
    "href": "05_tests/05_ttest.html",
    "title": "One Sample t-Tests in R",
    "section": "",
    "text": "Suppose we know the average birthweight in America is 110 oz. We are curious if the babies in a Boston area hospital have a different birthweight. Let \\(X_i\\) be the birthweight of the \\(i\\) Boston baby, then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\) and are independent. We want to test\n\\[\\begin{align}\nH_0: \\mu &= 110\\\\\nH_1: \\mu &\\neq 110\n\\end{align}\\]\nLet’s first read in the data on the \\(n = 1000\\) babies:\n\nlibrary(tidyverse)\nlibrary(broom)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nWe then use t.test() to run the \\(t\\)-test. The arguments are:\n\nformula: a formula object (generated with a tilde ~).\n\nWe put the name of the variable we are exploring to the left of the tilde.\nWe put the number 1 to the right of the tilde.\n\ndata: the name of the data frame containing the variable.\nmu: The null value. The default is 0 since this is the most common test.\nalternative: We use the default \"two.sided\", since our alternative hypothesis is of the form parameter \\(\\neq\\) value.\n\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110)\n\nWe then use broom::tidy() to get a summary of the \\(t\\)-test output.\n\nbout &lt;- tidy(tout)\nbout\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     107.     -3.32 0.000920       999     105.      109. One Samp… two.sided  \n\n\nWe can manually verify these results (though, you wouldn’t do this step in real life):\n\nxbar &lt;- mean(birthweight$weight)\ns &lt;- sd(birthweight$weight)\nn &lt;- length(birthweight$weight)\nmu0 &lt;- 110\ntstat &lt;- (xbar - mu0) / (s / sqrt(n))\npval &lt;- 2 * pt(-abs(tstat), df = n - 1)\ntstat\n\n[1] -3.324\n\npval\n\n[1] 0.0009204\n\n\nIf instead we had the alternative of \\(H_1: \\mu &lt; \\mu_0\\), then we would use the alternative = \"less\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"less\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.0004602\n\n\nIf instead we had the alternative of \\(H_1: \\mu &gt; \\mu_0\\), then we would use the alternative = \"greater\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"greater\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.9995\n\n\n\nExercise: Consider the lead data that you can read about here and download from https://dcgerard.github.io/stat_320/data/lead.csv. IQ tests are designed to have a mean of 100. Use iqf to test if the control group has an average IQ value of 100. Separately test if the exposed group has an average IQ less than 100. State the hypotheses, test results, and conclusions."
  },
  {
    "objectID": "05_tests/05_power.html",
    "href": "05_tests/05_power.html",
    "title": "Power Calculations",
    "section": "",
    "text": "For \\(t\\)-methods, you use power.t.test() to calculate do power and smaple size calculations. It takes as input four of the following:\n\nn: The sample size \\(n\\)\ndelta: The effect size (difference in means) \\(|\\mu_1 - \\mu_0|\\)\nsd: The standard deviation of the data \\(\\sigma\\)\nsig.level: The signficicance level \\(\\alpha\\)\npower: The power \\(1 - \\beta\\).\n\nYou must put values for exactly four of the above. The fifth should be NULL and the function will return the fifth value.\nOther inputs are for the type of test:\n\ntype: Use \"one.sample\" for one-sample \\(t\\)-tests and \"two.sample\" for two-sample \\(t\\)-tests.\nalternative: Either \"two.sided\" or \"one.sided\".\n\nSuppose we plan on running a study with 100 participants of low socioeconomic status (SES). The mean birthweight in america is 120 oz. A pilot study suggested that the average birthweight of low SES babies is 115 oz with a standard deviation of 24 oz. What is the power of a test with a significance level of 0.05?\n\npower.t.test(\n  n = 100, \n  delta = 5, \n  sd = 24, \n  sig.level = 0.05, \n  type = \"one.sample\", \n  alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 5\n             sd = 24\n      sig.level = 0.05\n          power = 0.6643\n    alternative = one.sided\n\n\nLet’s compare that power to the \\(z\\)-test calculation from Rosner\n\npnorm(qnorm(0.05) + sqrt(100) * 5 / 24)\n\n[1] 0.6695\n\n\nIt’s a little different because Rosner uses \\(z\\)-methods instead of \\(t\\)-methods for power and sample size calculations. But it’s not too different to be practically important, especially since power and sample size calculations are mostly just wild educated guesses.\nExercise: What sample size would be needed for a power of at least 0.8?\n\n\nExercise: A new drug is proposed to prevent glaucoma among people with high intraocular pressure (IOP). A pilot study is conducted with 10 individuals. After 1 month of using the drug, their IOP decreases by 5 mm HG with a standard deviation of 10 mm HG. What is the sample size needed to achieve 90% power for a two-sided test with significance level of 0.05."
  },
  {
    "objectID": "05_tests/05_binom.html#example",
    "href": "05_tests/05_binom.html#example",
    "title": "One Sample Binomial Tests in R",
    "section": "Example",
    "text": "Example\nSuppose \\(x\\) = 5 and \\(n\\) = 8.\n\nx &lt;- 5\nn &lt;- 8\nbinom.test(x = x, n = n)\n\n\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 5, number of trials = 8, p-value = 0.7\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2449 0.9148\nsample estimates:\nprobability of success \n                 0.625"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound",
    "href": "05_tests/05_binom.html#finding-lower-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-1",
    "href": "05_tests/05_binom.html#finding-lower-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-2",
    "href": "05_tests/05_binom.html#finding-lower-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-3",
    "href": "05_tests/05_binom.html#finding-lower-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-4",
    "href": "05_tests/05_binom.html#finding-lower-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-5",
    "href": "05_tests/05_binom.html#finding-lower-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-6",
    "href": "05_tests/05_binom.html#finding-lower-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-7",
    "href": "05_tests/05_binom.html#finding-lower-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound",
    "href": "05_tests/05_binom.html#finding-upper-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-1",
    "href": "05_tests/05_binom.html#finding-upper-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-2",
    "href": "05_tests/05_binom.html#finding-upper-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-3",
    "href": "05_tests/05_binom.html#finding-upper-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-4",
    "href": "05_tests/05_binom.html#finding-upper-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-5",
    "href": "05_tests/05_binom.html#finding-upper-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-6",
    "href": "05_tests/05_binom.html#finding-upper-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-7",
    "href": "05_tests/05_binom.html#finding-upper-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval",
    "href": "05_tests/05_binom.html#final-confidence-interval",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(np_1\\)\nRight: \\(np_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval-1",
    "href": "05_tests/05_binom.html#final-confidence-interval-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(p_1\\)\nRight: \\(p_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html",
    "href": "05_tests/05_binom.html",
    "title": "One Sample Binomial Tests in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nWe have \\[\n  X \\sim \\mathrm{Binom}(n,p)\n  \\] and we are testing\n\n\\(H_0\\): \\(p = p_0\\)\n\\(H_1\\): \\(p \\neq p_0\\) or \\(p &gt; p_0\\) or \\(p &lt; p_0\\)\n\n\n\nApproximate Approach\n\nWe use the central limit theorem. If (rule-of-thumb) \\(np_0(1-p_0) \\geq 5\\) then \\[\n  X \\sim N(p_0, p_0(1-p_0)/n)\n  \\] and we calculate the tail probabilities of (for \\(\\hat{p} = X/n\\)) \\[\n  Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\n  \\]\nThis is done via prop.test(), which you can feed into broom::tidy()\n\nx: the observed number of successes\nn: The total number of trials\np: The null value of the success probability\nalternative: Either \"two.sided\", \"less\", or \"greater\"\n\nSuppose that about 20% of women who are trying to concieve take 12 months or more to get pregnant, which we will call infertility. Researchers are interested in if a genetic marker is associated with infertility. Of 40 women with this marker, 10 were infertile. Is this marker associated with infertility?\n\n\\(X \\sim \\mathrm{Binom}(40, p)\\)\n\\(H_0\\): \\(p = 0.2\\)\n\\(H_1\\): \\(p &gt; 0.2\\)\n\n\npout &lt;- prop.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(pout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.277\n\n\nSince the \\(p\\)-value is 0.2766, we don’t have evidence that the marker is associated with infertility.\nWe can do this calculation by hand (but you would never do this):\n\nphat &lt;- 10 / 40 \np0 &lt;- 0.2\nz &lt;- (phat - p0 - 1/80) / sqrt(p0 * (1 - p0) / 40) ## continuity correction\npnorm(q = z, lower.tail = FALSE)\n\n[1] 0.2766\n\n\n\n\n\nExact Approach\n\nThe exact approach calculates the probability under the null of being as more supportive of the alternative as the data we observed.\nE.g., for our infertility example, we would calculate \\(\\mathrm{Pr}(X \\geq 10 | p = 0.2)\\)\n\n1 - pbinom(q = 9, size = 40, prob = 0.2)\n\n[1] 0.2682\n\n\nThis procedure is implemented in the binom.test() function, which has the same inputs as prop.test().\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.268\n\n\nWhen the alternative is 2-sided, \\(H_1 \\neq p_0\\), R is a little different than the procedure Rosner proposes. It calculates the sums the probabilities under the null of all \\(X\\) that are less probable than our observed \\(x\\)\n\\[\n  \\sum_{k \\text{ s.t. } Pr(k) \\leq Pr(x)}\\binom{n}{k} p_0^k(1-p_0)^{n-k}\n  \\]\nIn the infertility example, this would be\n\nprob &lt;- dbinom(x = 0:40, size = 40, prob = 0.2)\nsum(prob[prob &lt;= dbinom(x = 10, size = 40, prob = 0.2)])\n\n[1] 0.4296\n\n\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2)\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.430\n\n\nExercise: Out of 13 deaths at a nuclear facility among men aged 55-64, 5 of them were due to cancer. The proportion of deaths caused by cancer in that group in the greater population is 0.2. Is there more cancer deaths in this nuclear facility? Use both the normal and exact approaches."
  },
  {
    "objectID": "05_tests/05_binom_power.html",
    "href": "05_tests/05_binom_power.html",
    "title": "Power Calculations for Binomial Tests",
    "section": "",
    "text": "There are no base R functions that do power and sample size calculations. But I created some for you:\n\n#' Power/sample size calculation of 1-sample proportion test\n#' \n#' Uses central limit theorem, so make sure `p0 * (1 - p0) * n &gt;= 5`\n#' \n#' Exactly one of `n`, `power`, `p0`, `p1`, or `alpha` needs to be `NULL`.\n#' \n#' @param n The sample size\n#' @param power The power\n#' @param p0 The null proportion\n#' @param p1 The alternative proportion\n#' @param alpha The significance level\n#' @param TOL Tolerance level\n#' \n#' @author David Gerard\n#' \n#' @examples\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' b1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' \n#' ## two p1's\n#' b1power(n = 500, power = 0.9, p0 = 0.02, p1 = NULL, alpha = 0.05)\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.00406, alpha = 0.05)$power\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.044, alpha = 0.05)$power\nb1power &lt;- function(\n    n = NULL, \n    power = NULL,\n    p0 = NULL,\n    p1 = NULL, \n    alpha = 0.05,\n    TOL = 1e-6) {\n  \n  if (is.null(n) + is.null(power) + is.null(p0) + is.null(p1) + is.null(alpha) != 1) {\n    stop(\"exactly one of n, power, p0, p1, and alpha need to be NULL\")\n  }\n  \n  oout &lt;- list(n = n, power = power, p0 = p0, p1 = p1, alpha = alpha)\n  \n  pfun &lt;- function(n, p0, p1, alpha) {\n    za2 &lt;- stats::qnorm(alpha / 2)\n    stats::pnorm(sqrt(p0 * (1 - p0) / (p1 * (1 - p1))) * (za2 +\n                   abs(p0 - p1) * sqrt(n) / sqrt(p0 * (1 - p0))))\n  }\n  \n  if (is.null(power)) {\n    oout$power &lt;- pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)\n  } else if (is.null(n)) {\n    z1a2 &lt;- stats::qnorm(1 - alpha / 2)\n    zp &lt;- stats::qnorm(power)\n    oout$n &lt;- p0 * (1 - p0) * (z1a2 + zp * sqrt(p1 * (1 - p1) / (p0 * (1 - p0))))^2 / (p1 - p0)^2\n    oout$n &lt;- ceiling(oout$n)\n  } else if (is.null(p0)) {\n    rp0 &lt;- function(p0) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp0(p0 = TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp0, interval = c(TOL, p1))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp0(p0 = 1 - TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp0, interval = c(p1, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p0 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(p1)) {\n    rp1 &lt;- function(p1) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp1(p1 = TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp1, interval = c(TOL, p0))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp1(p1 = 1 - TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp1, interval = c(p0, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p1 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(alpha)) {\n    ralpha &lt;- function(alpha) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    rout &lt;- stats::uniroot(f = ralpha, interval = c(TOL, 1-TOL))\n    oout$alpha &lt;- rout$root\n  }\n  \n  if (any(oout$p0[!is.na(oout$p0)] * (1 - oout$p0[!is.na(oout$p0)]) * oout$n &lt; 5)) {\n    warning(\"too small sample size\")\n  }\n  return(oout)\n}\n\n\nAssumes the sample size is large enough to use the central limit theorem (\\(np_0(1-p_0) \\geq 5\\)).\nSuppose we wish to test the hypothesis that women with a sister history of breast cancer are at higher risk of developing breast cancer themselves. Suppose the prevalence rate of breast cancer is 2% among 50 to 54 year-old US women, whereas it is 5% among women with a sister history. We wish to interval 500 women 50 to 54 years old with a sistory history of the disease. What is the power of such a study assuming that we conduct a two-sided test with \\(\\alpha = 0.05\\)?\n\n# 0.9655\nb1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 500\n\n$power\n[1] 0.9655\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05\n\n\nHow many women should we interview in the study proposed to achieve 90% power?\n\n# 341\nb1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 341\n\n$power\n[1] 0.9\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05"
  },
  {
    "objectID": "05_tests/two_sample_t.html",
    "href": "05_tests/two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women containt eh systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\nExercise: A study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)"
  },
  {
    "objectID": "05_tests/05_two_sample_t.html",
    "href": "05_tests/05_two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women containt eh systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\nExercise: A study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)\n\n\n\n\nUnpaired (Equal Variance)\n\nConsider the lead data that you can read about here.\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\n\nWe are interested in if the exposed and control groups have the same mean full scale IQ. Let’s explore the data\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet \\(X_i\\) be the \\(i\\)th IQ score in the control group, let \\(Y_i\\) be the \\(i\\)th IQ score in the exposed group.\nThen we assume that \\(X_i \\sim \\mathrm{N}(\\mu_1, \\sigma^2)\\) and \\(Y_i \\sim \\mathrm{N}(\\mu_2, \\sigma^2)\\), and that all observations are independent.\nWe use t.test() to run a two-sample \\(t\\)-test.\n\nThe quantitative variable is to the left of the tilde ~\nThe variable encoding the two groups is to the right of the tilde\nIf we assume equal variances in each group, we set var.equal = TRUE\n\n\nt.test(iqf ~ Group, data = lead, var.equal = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.67  0.0977       118   -0.845      9.91\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWe can verify this result manually (you would never do this in real life, but you might on an exam).\n\n## Get summary statistics of the two groups\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(mean = mean(iqf), var = var(iqf), n = n()) -&gt;\n  sumdf\nxbar &lt;- sumdf$mean[[1]]\nybar &lt;- sumdf$mean[[2]]\ns2x &lt;- sumdf$var[[1]]\ns2y &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## Calculate pooled sample standard deviation\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\n\n## Calculate t-statistic\ntstat &lt;- (xbar - ybar) / (s * sqrt(1 / n1 + 1 / n2))\n\n## compare to t distribution with n1 + n2 - 2 df\npval &lt;- 2 * pt(-abs(tstat), df = n1 + n2 - 2)\n\n## Get confidence intervals\nlower &lt;- (xbar - ybar) - qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nupper &lt;- (xbar - ybar) + qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\n\nc(pval = pval, lower = lower, upper = upper)\n\n    pval    lower    upper \n 0.09772 -0.84454  9.90917 \n\n\nExercise: Is there any difference between exposed and control groups when it comes to the finger-wrist tapping test in dominant hand (maxfwt)? Assume equal variances.\n\n\nExercise: A sample of eight 35- to 39-year-old non-pregnant, premenoposaul OC users have a mean systoolic blood pressure (SBP) of 132.82 mm Hg and a sample standard deviation of 15.34 mm Hg. A different sample of 21 non-pregnant, premenopausal, non-OC users have a mean SBP of 127.44 mm Hg and a sample standard deviation of 18.23 mm Hg. What can be said about the underlying mean difference in blood pressure between the two groups? Provide a measure of how sure we are that there is a difference, and provide some interval estimate for this difference. Assume equal variances.\n\n\n\nTest for Equal Variance\n\nSuppose we have \\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y_i \\sim N(\\mu_2,\\sigma_2^2)\\).\nIt is possible to test \\(H_0: \\sigma_1 = \\sigma_2\\) versus \\(H_1: \\sigma_1 \\neq \\sigma_2\\).\nFolks don’t typically do this test because:\n\nIt is very sensitive to the normality assumption. Conversely, the \\(t\\)-test is not.\nThe \\(t\\)-test with equal variances is relatively robust to violations in the equal variance assumption.\nFolks typically just use the unequal variances \\(t\\)-test below.\n\nBut if your boss asks you run such a test, use var.test().\n\nvar.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\nMultiple parameters; naming those columns num.df, den.df\n\n\n# A tibble: 1 × 9\n  estimate num.df den.df statistic p.value conf.low conf.high method alternative\n     &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      \n1     1.65     73     45      1.65  0.0719    0.955      2.76 F tes… two.sided  \n\n\nThis test is based on the ratio of the variances \\(s_1^2 / s_2^2\\). Under the null, this follows a \\(F\\)-distribution with \\(n_1 - 1\\) numerator degrees of freedom and \\(n_2 - 1\\) denominator degrees of freedom.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(var = var(iqf), n = n()) -&gt;\n  sumdf\nvar1 &lt;- sumdf$var[[1]]\nvar2 &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## for fstat &gt; 1\nfstat1 &lt;- var1 / var2\n2 * pf(q = fstat1, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)\n\n[1] 0.07194\n\n## For fstat &lt; 1\nfstat2 &lt;- var2 / var1\n2 * pf(q = fstat2, df1 = n2 - 1, df2 = n1 - 1, lower.tail = TRUE)\n\n[1] 0.07194\n\n\n\n\n\nUnpaired (Unequal Variance)\n\nWhen you don’t want to assume equal variances (typically the case), just use the default settings of t.test() that has var.equal = FALSE.\n\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\nt.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.77  0.0797      112.   -0.545      9.61\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\nDon’t bother memorizing Satterthwaite’s approximation for the degrees of freedom. Just do this in the computer.\nExercise: Is there a difference in finger tapping between groups? Don’t assume equal variances.\n\n\n\nPower and Sample Size Calculations in Two-sample \\(t\\)-tests\n\nUse power.t.test().\nIn the two-sample case, n means the sample size per group. It assumes the sample sizes are equal, so the total sample size is 2 * n.\nIt also assumes the standard deviations are equal, so you need to use a pooled estimate of the standard deviation.\nIf you need more precise power or sample size calculations, then those exist.\n\nBut I think these calculations are so much guess work that the error of assuming equal sample sizes is less than the error of the wild guesses you are giving it.\n\nE.g., suppose we have the OC user exercise above as a pilot experiment.\n\nxbar &lt;- 132.82\ns2x &lt;- 15.34^2\nn1 &lt;- 8\n\nybar &lt;- 127.44\ns2y &lt;- 18.23^2\nn2 &lt;- 21\n\nLet’s calculate a pooled estimate of the variance, and we will assume that is the true variance for the power calculations.\n\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\ns\n\n[1] 17.53\n\n\nLet’s suppose we want a power of 0.8. Then the sample size we would need is 168 per group:\n\npower.t.test(\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  power = 0.8, \n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 167.6\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf a researcher can only afford \\(n = 100\\) per gropu, then the power calculation would be 0.58:\n\npower.t.test(\n  n = 100,\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.5793\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExercise: Suppose a new drug is proposed to lower intraocular pressure (IOP) among people with glaucoma. It is anticipated that mean IOP will drop by 8 mm Hg after 1 month with the new drug. The comparison group will get the standard drug, which is anticipated to have a mean drop in IOP of 5 mm Hg after 1 month. It is expected that the sd of change within each group will be 10 mm Hg. How many subjects need to be enrolled to achieve 90% power if an equal sample size is planned within each group and a two-sided test with \\(\\alpha\\) = 0.05 will be used?"
  }
]