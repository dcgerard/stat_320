[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Unless otherwise noted, all data are from Rosner (2015), downloaded from the book’s companion site.\nI did some cleaning (changing 9’s to NA’s, recoding numerics to informative characters, etc).\n\nbetacar\nHigh doses of beta-carotene in food have been linked to a reduced cancer risk in some observational studies. A study considered four beta-carotene capsule preparations: Solatene (30 mg), Roche (60 mg), and two from BASF (30 mg and 60 mg). To test their effectiveness in raising plasma-carotene levels, 23 volunteers were randomized to one of the four preparations, taking one pill every other day for 12 weeks. The primary endpoint was the plasma carotene level after prolonged ingestion.\n\nbetacar.csv\n\nPrepar: Preparation.\n\nPossible values: SOL, ROCHE, BASF-30, BASF-60.\n\nId: Subject #\nBase1lvl: 1st Baseline Level\nBase2lvl: 2nd Baseline Level\nWk6lvl: Week 6 Level\nWk8lvl: Week 8 Level\nWk10lvl: Week 10 Level\nWk12lvl: Week 12 Level\n\n\n\n\nbirthweight\nThe birthweights of 1000 consecutive infants born at Boston City Hospital, which serves a low-income population.\n\nbirthweight.csv\n\nid: ID\nweight: Birthweight (oz)\n\n\n\n\nblood\nData from a case-control study investigated various plasma risk factors for breast cancer. The women were matched approximately by age at the time of blood draw, fasting status, and, when possible, current postmenopausal hormone (PMH) use at the time of blood draw. Each matched set included one case and either one or two controls, although some sets are incomplete due to missing data. The matching variable is matchid.\n\nblood.csv\n\nId: ID\nmatchid: Matched ID\ncase: Case/control.\n\nPossible values: case, control.\n\ncurpmh: Current PMH use.\n\nPossible values: yes, no.\n\nageblood: Age at blood draw\nestradol: Estradiol\nestrone: Estrone\ntestost: Testosterone\nprolactn: Prolactin\n\n\n\n\nbody\nData were collected from 20 healthy females, 25–34 years old, to study the relationship between body fat and three predictors (Neter et al. 1996).\n\nbody.csv\n\ntriceps: Triceps skinfold thickness (mm).\nthigh: Thigh circumference (cm).\nmidarm: Midarm circumference (cm&gt;\nfat: Body fat (percent).\n\n\n\n\nboneden\nA study in Australia examined the relationship between bone density and cigarette smoking in middle-aged female twins with different smoking histories. Forty-one pairs of twins visited a hospital in Victoria, Australia, where their bone density was measured. Participants also completed questionnaires providing information on their tobacco use, alcohol, coffee, and tea consumption, calcium intake from dairy products, menopausal and reproductive history, fracture history, use of oral contraceptives or estrogen replacement therapy, and physical activity levels. Tobacco consumption was measured in pack-years, with one pack-year defined as smoking one pack of cigarettes per day for one year.\n\nboneden.csv\n\nID: ID\nAge: Age (yrs)\nzyg: Twin type.\n\nPossible values: mz, dz\n\nTwin 1 Lighter Smoking Twin\n\nht1: Height (cm)\nwt1: Weight (kg)\ntea1: Tea (cups/week)\ncof1: Coffee (cups/week)\nalc1: Alcohol (drinks/week)\ncur1: Current Smoking (cigarettes/day)\nmen1: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr1: Pack-years smoking\nls1: Lumbar spine (g/cm\\(^2\\))\nfn1: Femoral neck (g/cm\\(^2\\))\nfs1: Femoral shaft (g/cm\\(^2\\))\n\nTwin 2 Heavier Smoking Twin\n\nht2: Height (cm)\nwt2: Weight (kg)\ntea2: Tea (cups/week)\ncof2: Coffee (cups/week)\nalc2: Alcohol (drinks/week)\ncur2: Current Smoking (cigarettes/day)\nmen2: Menopause Status.\n\nPossible values: pre, post, unknown\n\npyr2: Pack-years smoking\nls2: Lumbar spine (g/cm\\(^2\\))\nfn2: Femoral neck (g/cm\\(^2\\))\nfs2: Femoral shaft (g/cm\\(^2\\))\n\n\n\n\n\nbotox\nA study on patients with piriformis syndrome compared the effects of three types of injections: triamcinolone with lidocaine (TL), a placebo (Placebo), and Botox (Botox). The patients were randomly assigned to these groups in a 3:1:2 ratio and received injections directly into the piriformis muscle. They were evaluated at 2 weeks, 1 month, and monthly up to 17 months, though many missed visits. Improvement in pain was measured on a scale from 0% to 100% (higher means more improved). The study involved 69 patients, with one having the condition in both legs. The goal was to compare the efficacy between the groups, considering age, gender, and affected side as potential influencing factors.\n\nbotox.csv\n\nID: ID\ngroup:\n\nPossible values: TL, Placebo, Botox\n\nside: left (L), middle (M), or right (R).\ngender: male or female\nage: in years\npain0: pain score month 0\npain05: pain score month 0.5\npain1: pain score month 1\npain2: pain score month 2\npain3: pain score month 3\npain4: pain score month 4\npain5: pain score month 5\npain6: pain score month 6\npain7: pain score month 7\npain8: pain score month 8\npain9: pain score month 9\npain10: pain score month 10\npain11: pain score month 11\npain12: pain score month 12\npain13: pain score month 13\npain14: pain score month 14\npain15: pain score month 15\npain16: pain score month 16\npain17: pain score month 17\n\n\n\n\nbreast\nThe data set includes 1200 postmenopausal women from the NHS, free of cancer in 1990. Of these, 200 were using postmenopausal hormones (PMH) in 1990, and 1000 had never used them. The study aimed to link PMH use in 1990 to breast cancer incidence from 1990 to 2000. Fifty-three women developed breast cancer during this period. PMH use was categorized by current use and duration of use in 1990, with separate durations for estrogen and estrogen plus progesterone. Each woman has a return date for the 1990 questionnaire and a follow-up date, which is either the date of breast cancer diagnosis or the date of the last questionnaire by 2000. The file also includes data on other breast cancer risk factors as of 1990.\n\nbreast.csv\n\nId: ID\ncase: Whether a woman had breast cancer or not.\n\nPossible values: case,control\n\nage: age\nagemenar: age at menarche\nagemenop: age at menopause\nafb: age at first birth\nparity: parity\nbbd: Benign Breast disease.\n\nPossible values: yes, no\n\nfamhx: family history breast cancer.\n\nPossible values: yes, no\n\nbmi: BMI (kg/m**2)\nhgt: Height (inches)\nalcohol: Alcohol use (grams/day)\npmh: PMH status.\n\nPossible values: never user, current user\n\ndur3: Duration of Estrogen use (months)\ndur4: Duration of Estrogen + progesterone use (months)\ncsmk: Current Smoker.\n\nPossible values: yes, no\n\npsmk: Past smoker.\n\nPossible values: yes, no\n\nfoluptm: Months of follow up. Note: Some subjects provided no follow up after the 1990 questionnaire and foluptm = 0 for these people\n\n\n\n\ncholesterol\nCholesterol levels from 24 hospital employees who switched from a standard American diet to a vegetarian diet for one month. Their serum cholesterol was measured before and after the diet change.\n\ncholesterol.csv\n\nSubject: ID\nBefore: Serum-cholesterol levels before diet change (mg/dL)\nAfter: Serum-cholesterol levels after diet change (mg/dL)\nDifference: Difference in serum-cholesterol levels, Before - After, where positive numbers indicate a reduction in serum-cholesterol levels.\n\n\n\n\ncorneal\nFluoroquinolones, antibiotics for bacterial infections, are FDA-approved for systemic use. However, post-approval studies indicate a risk of peripheral neuropathy, leading to updated safety labeling.\nA small clinical trial tested the safety and effectiveness of two fluoroquinolone eye drops (drugs M and G) and a placebo (drug P) for bacterial eye infections. Ninety-three subjects were randomly assigned to three groups, each receiving one active drug and a placebo in opposite eyes. Participants used the drops four times daily for 10 days. The primary outcome was corneal sensitivity, measured in millimeters, with higher values indicating more normal sensitivity.\nCorneal sensitivity was assessed at baseline, 7 days, and 14 days, with measurements taken from the central cornea and four quadrants (superior, inferior, temporal, nasal).\n\ncorneal.csv\n\nid: ID\ntr: Treatment.\n\nPossible values: M, G, P\n\nc1: Central visit 1\ns1: Superior visit 1\ni1: Inferior Visit 1\nt1: Temporal visit 1\nn1: Nasal Visit 1\nc2: Central Visit 2(day 7)\ns2: Superior Visit 2\ni2: Inferior Visit 2\nt2: Temporal Visit 2\nn2: Nasal Visit 2\nc3: Central Visit 3(day 14)\ns3: Superior Visit 3\ni3: Inferior Visit 3\nt3: Temporal Visit 3\nn3: Nasal Visit 3\n\n\n\n\ndiabetes\nType I diabetes is common in children and requires regular insulin shots to prevent long-term complications like neurologic, vision, kidney issues, heart disease, and premature death.\nThe impact of diabetes control on childhood growth is less clear. To study this, adolescent boys aged 9−15 were examined about every 3 months. Each exam measured diabetes control using glycosylated hemoglobin (HgbA1c), where higher HgbA1c indicates poorer control (normal &lt;7.0). Age, height, and weight were also recorded. Data includes 94 boys over 910 visits.\nThe main question is the overall relationship between glycemic control and growth, primarily weight, for the entire group, not individual cases.\n\ndiabetes.csv\n\nID: ID\nmon_a1c: Month A1c\nday_a1c: Day A1c\nyr_a1c: Yr A1c\nage_yrs: Age in years\ngly_a1c: Hemoglobin A1c\nht_cm: Height in cm\nwt_kg: Weight in kg\n\n\n\n\near\nThis dataset includes 203 children with acute otitis media (OME) from a randomized clinical trial. Each child had OME in one or both ears and received a 14-day course of either cefaclor (CEF) or amoxicillin (AMO). Middle-ear status was assessed after the 14-day treatment.\n\near.csv\n\nId: ID\nClear: Clearance by 14 days,\n\nPossible values: yes, no\n\nAntibo: Antibiotic.\n\nPossible values: CEF, AMO\n\nAge: Age. Possible values: &lt;2 yrs, 2-5 yrs, 6+ yrs.\nEar: Ear,\n1 = 1st ear\n2 = 2nd ear\n\n\n\n\neff, nephro, and oto\nAminoglycoside antibiotics are crucial for treating severe gram-negative bacterial infections in hospitalized patients. Despite their toxicity and the development of new antibiotics, aminoglycosides remain widely used. Choosing the right aminoglycoside depends on the clinical situation, antimicrobial spectrum, cost, and side effects like nephrotoxicity and auditory toxicity. Numerous trials comparing these antibiotics have varied in design and conclusions, often lacking sufficient sample sizes to detect small differences.\nTo better understand their true effects, a meta-analysis of all randomized trials was conducted. This analysis included 45 trials from 1975 to 1985, comparing amikacin, gentamicin, netilmicin, sisomicin, and tobramycin. Data from 37 trials were suitable for comparison, focusing on efficacy, nephrotoxicity, and auditory toxicity. Efficacy was defined by bacterial or clinical response as reported by each trial, nephrotoxicity by the percentage of kidney-related toxic events reported, and auditory toxicity by the diffence in pre- and post-treatment audiograms. The data is organized into three sets: eff.csv, nephro.csv, and oto.csv, with separate records for each antibiotic and endpoint.\n\neff.csv\n\nName: Study name\nId: Study Number\nEndpnt: Endpoint, 1 = efficacy\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nCured: Number Cured\n\nnephro.csv\n\nname: Study name\nid: Study number\nEndpnt: Endpoint 2 = nephrotoxicity\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin\n\nSamp_sz: Sample size\nSide_eff: Number with side effects\n\noto.csv\n\nName: Study Name\nId: Study Number\nEndpnt: Endpoint.\n\nPossible values: efficacy, nephrotoxicity, ototoxicity.\n\nAntibio: Antibiotic.\n\nPossible values: Amikacin, Gentamicin, Netilmicin, Sisomycin, Tobramycin.\n\nSamp_sz: Sample Size\nSide_eff: Number with side effect\n\n\n\n\nendocrin\nThe data set contains split-sample plasma measurements of four hormones for each of five subjects, all from one laboratory.\n\nendocrin.csv\n\nSubject: Subject number\nReplicat: Replicate number\nEstrone: Estrone\nEstradol: Estradiol\nAndroste: Androstenedione\nTestost: Testosterone\n\n\n\n\nestradl\nObesity is common in American society and a risk factor for breast cancer in postmenopausal women, possibly due to increased estrogen levels, particularly serum estradiol. Researchers studied 151 African American and 60 Caucasian premenopausal women, measuring adiposity using body mass index (BMI) (a measure of overall adioposity) and waist-hip ratio (WHR) (a measure of abdominal adioposity). They also obtained a complete hormonal profile, including serum estradiol, and assessed other breast cancer risk factors: ethnicity, age, number of children, age at first birth, presence of children, and age at menarche.\n\nestradl.csv\n\nId: Identification number\nEstradl: Estradiol\nEthnic: Ethnicity.\n\nPossible values: African-American, Caucasian\n\nEntage: Age\nNumchild: Parity, number of children\nAgefbo: Age at 1st birth (=0 if numchild=0)\nAnykids: any children.\n\nPossible values: yes, no.\n\nAgemenar: age at menarche\nBMI: Body Mass Index\nWHR: waist-hip ratio\n\n\n\n\nestriol\nThese data come from Greene Jr and Touchstone (1963), where they measured the association between the level of estriol in pregnant women and their offspring’s birthweight. Estriol is a female sex hormone which is elevated during pregnancy.\n\nestriol.csv\n\nid: Patient ID\nestriol: Estriol level in mg / 24 hr\nbirthweight: Birthweight in g / 100\n\n\n\n\nestrogen\nThree distinct two-period crossover studies were conducted with different subject groups, measuring systolic and diastolic blood pressure. In Study 1, 0.625 mg of estrogen was compared with a placebo. Study 2 compared 1.25 mg of estrogen with a placebo. Study 3 compared 1.25 mg of estrogen with 0.625 mg of estrogen. Each active treatment period lasted for four weeks, with a two-week washout period between them.\n\nestrogen.csv\n\nId: ID\nstd_typ: Study type.\n\nPossible values: 0.625MG VS PLACEBO, 1.25MG VS PLACEBO, 1.25MG VS 0.625MG\n\nperiod: Period\ntrtgrp: Treatment.\n\nPossible values: PLACEBO, 0.625MG, 1.25MG\n\nsysd1r1: Systolic blood pressure day 1 reading 1\ndiasd1r1: Diastolic blood pressure day 1 reading 1\nsysd1r2: Systolic blood pressure day 1 reading 2\ndiasd1r2: Diastolic blood pressure day 1 reading 2\nsysd1r3: SYSTOLIC BP DAY 1 reading 3\ndiasd1r3: Diastolic blood pressure day 1 reading 3\nsysd2r1: Systolic blood pressure day 2 reading 1\ndiasd2r1: Diastolic blood pressure day 2 reading 1\nsysd2r2: Systolic blood pressure day 2 reading 2\ndiasd2r2: Diastolic blood pressure day 2 reading 2\nsysd2r3: Systolic blood pressure day 2 reading 3\ndiasd2r3: Diastolic blood pressure day 2 reading 3\nsysd3r1: Systolic blood pressure day 3 reading 1\ndiasd3r1: Diastolic blood pressure day 3 reading 1\nsysd3r2: Systolic blood pressure day 3 reading 2\ndiasd3r2: Diastolic blood pressure day 3 reading 2\nsysd3r3: Systolic blood pressure day 3 reading 3\ndiasd3r3: Diastolic blood pressure day 3 reading 3\n\n\n\n\nfev\nForced expiratory volume (FEV) is a measure of pulmonary function that quantifies the volume of air expelled in one second of sustained effort. This dataset includes FEV measurements from 1980 for 654 children aged 3 to 19 who participated in the Childhood Respiratory Disease (CRD) Study in East Boston, Massachusetts. The data are part of a longitudinal study aimed at tracking changes in pulmonary function over time in children.\n\nfev.csv\n\nId: ID number\nAge: Age (yrs)\nFEV: FEV (liters)\nHgt: Height (inches)\nSex: Sex.\n\nPossible values: female, male\n\nSmoke: Smoking Status.\n\nPossible values: non-current, current smoker.\n\n\n\n\n\nfield\nRetinitis pigmentosa (RP) is a hereditary eye disease that can lead to substantial vision loss or blindness. It has dominant, recessive, and sex-linked forms, with mutations in the rhodopsin (RHO) gene linked to dominant cases and RPGR gene mutations linked to sex-linked cases.\nThe data file field.csv contains visual field data for approximately 100 patients each from the RHO and RPGR groups. Visual field, measured in degrees\\(^2\\), indicates the area of vision. The dataset includes longitudinal data with varying follow-up times from a minimum of 3 years to a maximum of about 25-30 years. Measurements are provided separately for the right eye (OD) and the left eye (OS).\n\nfield.csv\n\nid: ID\ngroup: group.\n\nPossible values: RHO, RPGR\n\nage: age at visit (years)\ngender: gender. Note: all RPGR individuals have to be male.\n\nPossible values: male, female.\n\ndtvisit: date of visit (YYYY-MM-DD)\nfolowup: time from 1st visit in years\ntotfldod: total field area right eye (OD) in degrees\ntotfldos: total field area left eye (OS) in degrees\n\n\n\n\nheart\n\nheart.csv\n\nDiagnosis: Possible values:\n\nY1 = normal\nY2 = atrial septal defect without pulmonary stenosis or pulmonary hypertension\nY3 = ventricular septal defect with valvular pulmonary stenosis\nY4 = isolated pulmonary hypertension\nY5 = transposed great vessels\nY6 = ventricular septal defect without pulmonary hypertension\nY7 = ventricular septal defect with pulmonary hypertension\n\nPrevalence: Prevalence\nX1: age 1-20 years old\nX2: age&gt;20 years old\nX3: mild cyanosis\nX4: easy fatigue\nX5: chest pain\nX6: repeated respiratory infections\nX7: EKG axis more than 110\n\n\n\n\nhormone\nAn experiment was conducted to study the effects of avian pancreatic polypeptide (aPP), cholecystokinin (CCK), vasoactive intestinal peptide (VIP), and secretin on pancreatic and biliary secretions in laying hens. Researchers aimed to determine how these hormones affect the flow rates and pH values of these secretions.\nWhite Leghorn hens, aged 14-29 weeks, were surgically fitted with cannulas for collecting biliary and pancreatic secretions, and a jugular cannula for hormone infusion. Each hen underwent one trial per day, as long as her cannulas remained functional, leading to varying trial numbers per hen.\nEach trial started with a 20-minute saline infusion, followed by the collection of pancreatic and biliary secretions. The flow rates (in microliters per minute) and pH values were measured. This was followed by a 40-minute hormone infusion, with measurements repeated afterward.\nThe data set “hormone.csv” includes data for the four hormones and saline, with each trial recorded as one entry and 11 associated variables.\n\nhormone.csv\n\nID: ID of hen\nBilsecpr: Biliary secretion-pre (microliters per minute)\nBilphpr: Biliary pH-pre\nPansecpr: Pancreatic secretion-pre (microliters per minute)\nPanphpr: Pancreatic pH-pre\nDose: Dose of hormone\nBilsecpt: Biliary secretion-post (microliters per minute)\nBilphpt: Biliary pH-post\nPansecpt: Pancreatic secretion-post (microliters per minute)\nPanphpt: Pancreatic pH-post\nHormone: Hormone.\n\nPossible values: SAL, APP, CCK, SEC, VIP.\n\n\n\n\n\nhospital\nThese data are part of a larger data set gathered from individuals discharged from a specific Pennsylvania hospital. It was collected as part of a retrospective chart review focusing on antibiotic usage in hospitals.\n\nhospital.csv\n\nId: id no.\nDur_stay: Duration of hospital stay\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nTemp: First temperature following admission\nWBC: First WBC(x1000) following admission\nAntibio: Received antibiotic.\n\nPossible values: yes, no\n\nBact_cul: Received bacterial culture.\n\nPossible values: yes, no\n\nService: Service.\n\nPossible values: med, surg\n\n\n\n\n\ninfantbp\nResearchers investigated the link between high blood pressure and sodium intake by measuring infants’ responses to salt and sugar solutions. They measured the vigor of infants’ sucking (mean sucks per burst, MSB) when exposed to different solutions: water, 0.1 molar salt, 0.3 molar salt, and sugar. The responses were recorded over a series of periods using different stimuli: (i) nonnutritive sucking (ii) water, (iii) 5% sucrose + water, (iv) 15% sucrose + water, and (v) nonnutritive sucking.\n\ninfantbp.csv\n\nID: Infant ID\nMn_sbp: Mean Systolic Blood Pressure\nMn_dbp: Mean Diastolic Blood Pressure\nSalt Taste Variables\n\nMSB1slt: MSB-trial 1 water\nMSB2slt: MSB-trial 2 water\nMSB3slt: MSB-trial 3 0.1 molar salt + water\nMSB4slt: MSB-trial 4 0.1 molar salt + water\nMSB5slt: MSB-trial 5 water\nMSB6slt: MSB-trial 6 water\nMSB7slt: MSB-trial 7 0.3 molar salt + water\nMSB8slt: MSB-trial 8 0.3 molar salt + water\nMSB9slt: MSB-trial 9 water\nMSB10slt: MSB-trial 10 water\n\nSugar Taste Variables\n\nMSB1sug: MSB-trial 1 non-nutritive sucking\nMSB2sug: MSB-trial 2 water\nMSB3sug: MSB-trial 3 5% sucrose + water\nMSB4sug: MSB-trial 4 15% sucrose + water\nMSB5sug: MSB-trial 5 non-nutritive sucking\n\nNOTE: For MSB, 0 indicates the baby did not suck.\n\n\n\n\nlead\nA study examined the psychological and neurological effects of lead exposure on children near a lead smelter in El Paso, Texas. Blood lead levels were measured, categorizing 46 children with levels ≥ 40 μg/mL as the exposed group. Another 78 children with levels &lt; 40 μg/mL functioned as the control group. Key outcomes included finger–wrist taps (neurological function) and Wechsler full-scale IQ scores.\nOther behavioral effects of lead include hyperactivity. In this study, parents also rated their children’s hyperactivity on a scale from 0 (normal) to 3 (very hyperactive). Hyperactivity measures are available for 49 control children and 35 exposed children.\n\nlead.csv\n\nPatient information\n\nid: Identification number\nageyrs: Age in years\nsex: Sex.\n\nPossible values: male, female\n\n\nLead data\n\narea: Distance of esidence from smelter on august 1972. Possible values:\n\n0-1 Miles from smelter\n1-2.5 Miles\n2.5-4.1 Miles\n\nlead_grp: Blood lead level group. possible values:\n\ncontrol = Blood lead levels below 40 micrograms/100ml in both 1972 & 1973 (control group),\ncurrent exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in both 72 & 73 or a level greater than or equal to 40 in 73 alone (3 cases only) (currently exposed group),\nprevious exposed = Blood lead levels greater than or equal to 40 micrograms/100ml in 72 and less than 40 in 73 (previously exposed group)\n\nGroup: group.\n\nPossible values: control, exposed\n\nld72: Blood lead values (micrograms/100ml) in 72\nld73: Blood lead values (micrograms/100ml) in 73\nfst2yrs: Did child live for 1st 2 years within 1 mile of smelter.\n\nPossible values: yes, no.\n\ntotyrs: Total number of years spent within 4.1 miles of smelter\n\nIQ Test Results\n\niqv_inf: INF - information subtest in WISC and WPPSI\niqv_comp: COMP - comprehension subtest in WISC and WPPSI\niqv_ar: AR - arithmetic subtest in WISC and WPPSI\niqv_ds: DS - digit span subtest(WISC) and sentence completion(WPPSI)\niqv_raw: V/RAW - raw score/verbal IQ\niqp_pc: PC - picture completion subtest in WISC and WPPSI\niqp_bd: BD - block design subtest in WISC and WPPSI\niqp_oa: OA - object assembly subtest(WISC), animal house subtest(WPPSI)\niqp_cod: COD - coding subtest(WISC), geometric design subtest(WPPSI)\niqp_raw: P/RAW - raw score/performance IQ (total of scores PC, BD, OA, & COD)\nhh_index: HH/INDEX - Hollingshead index of social status\niqv: IQV - verbal IQ\niqp: IQP - performance IQ\niqf: IQF - full scale IQ (not sum or average of IQV D IQP)\niq_type: Type of IQ test (WISC usually given to children \\(\\geq\\) 5 years and 1 month of age WPPSI usually given to children \\(\\leq\\) 5 years of age).\n\nPossible values: WISC, WPPSI\n\n\nSymptom data (as reported by parents)\n\npica: Pica.\n\nPossible values: yes, no.\n\ncolic: Colic.\n\nPossible values: yes, no.\n\nclumsi: Clumsiness.\n\nPossible values: yes, no.\n\nirrit: Irritability.\n\nPossible values: yes, no.\n\nconvul: Convulsions.\n\nPossible values: yes, no.\n\n\nNeurological test data\n\n_2plat_r: Number of taps for right hand in the 2-plate tapping test (number of taps in one 10 second trial)\n_2plat_l: Number OF taps for left hand in the 2-plate tapping test (number taps in one 10 second trial)\nvisrea_r: Visual reaction time right hand (milliseconds)\nvisrea_l: Visual reation time left hand (milliseconds)\naudrea_r: Auditory reaction time right hand (milliseconds)\naudrea_l: Auditory reaction time left hand (milliseconds)\nfwt_r: Finger-wrist tapping test right hand (number of taps in one 10 second trial)\nfwt_l: Finger-wrist tapping test left hand (#taps in one 10 second trial)\nhyperact: WWPS - Werry-Weiss-Peters scale for hyperactivity\n\n0 = no activity, \\(\\ldots\\), 4 = severly hyperactive (as reported by parents)\n\nmaxfwt: Finger-wrist tapping test in dominant hand (max of fwt_r,fwt_l)\n\n\n\n\n\nlvm\nThe Left Ventricular Mass Index (LVMI) measures the enlargement of the heart’s left side, expressed in gm/ht(m)^2.7. High LVMI values can predict future cardiovascular disease in children. A study investigated the relationship between LVMI levels and blood pressure categories in children and adolescents aged 10-18. Blood pressure was categorized as Normal (bpcat = normal, bp percentile &lt; 80%), Pre-Hypertensive (bpcat = pre-hypertensive, bp percentile ≥ 80% and &lt; 90%), or Hypertensive (bpcat = hypertensive, bp percentile ≥ 90%)..\n\nlvm.csv\n\nID: ID\nlvmht27: Left ventricular mass – height corrected = Left Ventricular Mass/Height(m)\\(^{2.7}\\), \\(g/m^{2.7}\\)\nbpcat: Blood pressure category.\n\nPossible values: normal, pre-hypertensive, hypertensive.\n\ngender: Bender.\n\nPossible values: male, female\n\nage: in years\nBMI: kg/m\\(^2\\)\n\n\n\n\nmice\nRetinitis pigmentosa (RP) is a hereditary eye condition causing night blindness and visual field loss, typically between ages 10 and 40. Some patients become legally blind by 30, while others retain central vision past 60. A specific gene linked RP has been identified whose transmision is autosomal dominant. The disease progression is tracked using electroretinogram (ERG), measuring retinal electrical activity, which decreases as RP advances, affecting routine activities like driving and walking at night.\nTo test if sunlight exposure harms RP patients, researchers introduced the RP gene into mice, creating “RP mice.” These mice were divided into light, dim, and dark lighting conditions from birth. A control group of normal mice was also subjected to similar conditions. ERG amplitudes (BAMP and AAMP) were measured at 15, 20, and 35 days of life for RP mice, and only BAMP was measured for normal mice.\n\nmice.csv\n\nId: ID\nGroup: GROUP.\n\nPossible values: RP, NORMAL\n\nTrtgrp: TREATMENT GROUP.\n\nPossible values: LIGHT, DIM, DARK.\n\nAge: AGE (days)\nB_amp: B AMP\nA_amp: A AMP\n\n\n\n\nnifed\nA clinical trial tested the effectiveness of nifedipine in reducing chest pain in hospitalized angina patients. The study lasted 14 days unless patients were withdrawn, discharged, or died. Patients were randomly assigned to receive either nifedipine or propranolol, starting at a standard dosage. If pain persisted or recurred, the dosage was increased in pre-specified steps. Patients in both groups could also receive nitrates as needed to control pain. The primary goal was to compare pain relief between nifedipine and propranolol, with a secondary goal of examining their effects on heart rate and blood pressure.\n\nnifed.csv\n\nId: ID\ntrtgrp: Treatment group,\n\nN = nifedipine\nP = placebo\n\nbashrtrt: Baseline heart rate immediately prior to randomization (beats/min)\nlv1hrtrt: Level 1 heart rate, Highest heart rate and systolic blood pressure at baseline and each level of therapy respectively (beats/min)\nlv2hrtrt: Level 2 heart rate (beats/min)\nlv3hrtrt: Level 3 heart rate (beats/min)\nbassys: Baseline systolic bp immediately prior to randomization (mm Hg)\nlv1sys: Level 1 systolic bp (mm Hg)\nlv2sys: Level 2 systolic bp (mm Hg)\nlv3sys: Level 3 systolic bp (mm Hg)\n\n\nMissing values indicate that either (a) the patient withdrew from the study prior to entering this level of therapy (b) the patient achieved pain relief prior to reaching this level or therapy, (c) the patient encountered this level of therapy, but this particular piece of data was missing.\n\n\npiriform\nA study evaluated the FAIR test (hip flexion, adduction, and internal rotation) for diagnosing piriformis syndrome (PS), which affects the piriformis muscle in the buttock, causing lumbar and sciatic pain. The test measures nerve-conduction velocity differences between an aggravating and a neutral posture, with higher scores indicating a greater likelihood of PS. Data from 142 participants without PS and 489 with PS (diagnosed clinically) are available. A FAIR test score of ≥ 1.86 ms is proposed to define a positive result. The FAIR test value, MAXCHG, is recorded in milliseconds.\n\npiriform.csv\n\nID: ID\npiriform: Piriformis Syndrome.\n\nPossible values: Negative, Positive\n\nsex: Sex.\n\nPossible values: male, female\n\nage: Age\nmaxchg: Max change between tibia and peroneal\n\n\n\n\nsexrat\nIt is often assumed that the gender distribution of consecutive children is independent. To test this hypothesis, birth records from the first five children in 51,868 families were analyzed. These data contain the frequency of how many times a pattern of child sexes showed up. E.g., MM (only two males) showed up 4400 times.\n\nsexrat.csv\n\nnm_chld: Number of children. For families with 5+ children, the sex of the first 5 children are listed. The number of children is given as 5 for such families.\nsx_1: Sex of 1st born\nsx_2: Sex of 2nd born\nsx_3: Sex of 3rd born\nsx_4: sex of 4th born\nsx_5: sex of 5th born\nsexchldn: Sex of all children. The sex of successive births is given. Thus, MMMF means that the first three children were males and the fourth child was a female. There were 484 such families.\nnum_fam: Number of families. Number of families with specific gender contribution of children\n\n\n\n\nsmoke\nA study was conducted among 234 individuals who wanted to quit smoking but had not yet done so. On the day they quit, their carbon monoxide (CO) levels were measured, and the time since their last cigarette was recorded. This CO level serves as an indicator of the number of cigarettes smoked daily before quitting but is influenced by the time since the last cigarette. Therefore, a “corrected CO level” was provided, adjusted for this time. Participant age, sex, and self-reported daily cigarette consumption were also recorded. The participants were followed for a year to determine the number of days they remained abstinent, ranging from 0 to 365 days.\n\nsmoke.csv\n\nID: ID number\nAge: age\nGender: Gender.\n\nPossible values: male, female\n\nCig_day: Cigarettes/day\nCO: Carbon monoxide (CO) (X 10)\nMin_last: Minutes elapsed since last cigarette\nLogCOadj: Log CO Adj * (X 1000). This variable represents adjusted carbon monoxide (CO) values. CO values were adjusted for minutes elapsed since last cigarette smoked using the formula Log 10 CO (Adjusted) = Log 10 CO - (-0.000638) X (Min - 80), where Min is the number of minutes elapsed since the last cigarette smoked.\nDay_abs: Days abstinent Those abstinent less than 1 day were given a value of zero.\n\n\n\n\nswiss\nThe Swiss Analgesic Study aimed to evaluate the impact of phenacetin-containing analgesics on kidney function and health. It involved 624 women from Basel, Switzerland, who had high phenacetin intake (study group) and 626 women with low or no phenacetin intake (control group). The study used urine N-acetyl-P-aminophenyl (NAPAP) levels to measure recent phenacetin use, dividing the study group into high-NAPAP and low-NAPAP subgroups. Both subgroups had higher NAPAP levels than the control group. The women were examined in 1967-1968 and again in 1969, 1970, 1971, 1972, 1975, and 1978, with kidney function assessed through various laboratory tests, including serum-creatinine levels.\n\nswiss.csv\n\nID: ID\nage: age (yrs)\ngroup: Group.\n\nPossible values: High NAPAP, Low NAPAP, control\n\ncreat_68: Serum Creatinine 1968 (mg/dL)\ncreat_69: Serum Creatinine 1969 (mg/dL)\ncreat_70: Serum Creatinine 1970 (mg/dL)\ncreat_71: Serum Creatinine 1971 (mg/dL)\ncreat_72: Serum Creatinine 1972 (mg/dL)\ncreat_75: Serum Creatinine 1975 (mg/dL)\ncreat_78: Serum Creatinine 1978 (mg/dL)\n\n\n\n\ntear\nA pilot study was conducted to evaluate an eye drop’s effectiveness in preventing dry eye, measured by tear breakup time (TBUT). Fourteen participants tested three protocols:\n\nProtocol A: No blinking for 3 seconds before placebo instillation.\nProtocol B: No blinking for 6 seconds before placebo instillation (standard protocol).\nProtocol C: No blinking for 10 seconds before placebo instillation.\n\nTBUT was recorded at baseline, immediately after, and at 5, 10, and 15 minutes post-instillation in a low-humidity environment. Each protocol was tested on the same participants, measuring both eyes with two replicates.\n\ntear.csv\n\nID: ID\nod3bas1: OD 3sec baseline 1\nod3bas2: OD 3 sec baseline 2\nod3im1: OD 3 sec immediately post 1\nod3im2: OD 3 sec immediately post 2\nod3pst51: OD 3 sec 5min post 1\nod3pst52: OD 3 sec 5min post 2\nod3pt101: OD 3 sec 10min post 1\nod3pt102: OD 3 sec 10min post 2\nod3pt151: OD 3 sec 15min post 1\nod3pt152: OD 3 sec 15min post 2\nos3bas1: OS 3sec baseline 1\nos3bas2: OS 3 sec baseline 2\nos3im1: OS 3 sec immediately post 1\nos3im2: OS 3 sec immediately post 2\nos3pst51: OS 3 sec 5min post 1\nos3pst52: OS 3 sec 5min post 2\nos3pt101: OS 3 sec 10min post 1\nos3pt102: OS 3 sec 10min post 2\nos3pt151: OS 3 sec 15min post 1\nos3pt152: OS 3 sec 15min post 2\nod6bas1: OD 6 sec baseline 1\nod6bas2: OD 6 sec baseline 2\nod6im1: OD 6 sec immediately post 1\nod6im2: OD 6 sec immediately post 2\nod6pst51: OD 6 sec 5min post 1\nod6pst52: OD 6 sec 5min post 2\nod6pt101: OD 6 sec 10min post 1\nod6pt102: OD 6 sec 10min post 2\nod6pt151: OD 6 sec 15min post 1\nod6pt152: OD 6 sec 15min post 2\nos6bas1: OS 6 sec baseline 1\nos6bas2: OS 6 sec baseline 2\nos6im1: OS 6 sec immediately post 1\nos6im2: OS 6 sec immediately post 2\nos6pst51: OS 6 sec 5min post 1\nos6pst52: OS 6 sec 5min post 2\nos6pt101: OS 6 sec 10min post 1\nos6pt102: OS 6 sec 10min post 2\nos6pt151: OS 6 sec 15min post 1\nos6pt152: OS 6 sec 15min post 2\nod10bas1: OD 10 sec baseline 1\nod10bas2: OD 10 sec baseline 2\nod10im1: OD 10 sec immediately post 1\nod10im2: OD 10 sec immediately post 2\nod10ps51: OD 10 sec 5min post 1\nod10ps52: OD 10 sec 5min post 2\nod10p101: OD 10 sec 10min post 1\nod10p102: OD 10 sec 10min post 2\nod10p151: OD 10 sec 15min post 1\nod10p152: OD 10 sec 15min post 2\nos10bas1: OS 10 sec baseline 1\nos10bas2: OS 10 sec baseline 2\nos10im1: OS 10 sec immediately post 1\nos10im2: OS 10 sec immediately post 2\nos10ps51: OS 10 sec 5min post 1\nos10ps52: OS 10 sec 5min post 2\nos10p101: OS 10 sec 10min post 1\nos10p102: OS 10 sec 10min post 2\nos10p151: OS 10 sec 15min post 1\nos10p152: OS 10 sec 15min post 2\n\n\n\n\ntemperat\nA student records temperatures at 20 sites within her house for 30 days each. She records the outside temperature and the weather condition.\n\ntemperat.csv\n\nDate: Date (MDY)\nOut_temp: Outside temerature (Degrees Fahrenheit)\nRoom: Room location\nIn_temp: Inside temperature (Degrees Fahrenheit)\nCor_fac: Correction factor added.\n\nPossible values: yes, no\n\nTyp_wea: Type of weather.\n\nPossible values: SUNNY, PARTLY CLOUDY, CLOUDY, RAINY, FOGGY\n\n\n\n\n\ntennis1\nA survey of tennis club members in the Boston area examined the occurrence of tennis elbow. Subjects reported anywhere from 0 to 8 episodes of tennis elbow. They were also asked about demographic factors and racquet characteristics.\n\ntennis1.csv\n\nId: ID\nAge: Age\nSex: Sex.\n\nPossible values: male, female\n\nNum_epis: Number of episodes of tennis elbow\nTyp_last: Type of racquet used during last episode.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_last: Weight of racquet used during last episode.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_last: Material of racquet used during last episode.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_last: String type of racquet used during last episode.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\nTyp_curr: Type of racquet used currently.\n\nPossible values: CONVENTIONAL SIZE, MID-SIZE, OVER-SIZE\n\nWgt_curr: Weight of racquet used currently.\n\nPossible values: HEAVY, MEDIUM, LIGHT, DO NOT KNOW\n\nMat_curr: Material of racquet used currently.\n\nPossible values: WOOD, ALUMINUM, FIBERGLASS AND COMPOSITE, GRAPHITE, STEEL, COMPOSITE, OTHER\n\nStr_curr: String type of racquet used currently.\n\nPossible values: NYLON, GUT, DON'T KNOW\n\n\n\n\n\ntennis2\nTennis elbow is a painful condition common among tennis players. Treatments include rest, heat, and anti-inflammatory medications like Motrin (ibuprofen). A clinical trial with 87 participants compared the effectiveness of Motrin vs. placebo. Participants were randomly divided into two groups: Group A received Motrin for 3 weeks, followed by a 2-week washout period, and then placebo for 3 weeks; Group B received the treatments in reverse order. Pain levels were measured on a 1-6 scale (1 = worse, 6 = completely improved) at the end of each treatment and washout period. The comparison was made (i) during maximum activity, (ii) 12 hours following maximum activity, (iii) during the average day, and (iv) by overall impression of drug efficacy.\n\ntennis2.csv\n\nid: ID\nage: Age\nsex: Sex.\n\nPossible values: male, female\n\ndrg_ord: Drug order.\n\nPossible values: MOTRIN-PLACEBO, PLACEBO-MOTRIN\n\nPeriod 2 = Pain scores after the first active drug period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_2: During study period, pain during maximum activity vs baseline\n\n1 = Worse\n2 = Unchanged\n3 = Slightly improved (25%)\n4 = Moderately improved (50%)\n5 = Mostly improved (75%)\n6 = Completely improved\n\npain12_2: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_2: During the average day of study period pain vs. baseline (same code as painmx_2)\npainov_2: Overall impression of drug efficacy vs. baseline (same code as painmx_2)\n\nPeriod 3 = Pain scores after the washout period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_3: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_3: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_3: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_3: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\nPeriod 4 = Pain scores after the second active drug period compared with baseline (baseline = just before start of first active drug period).\n\npainmx_4: During study period, pain during maximum activity vs baseline (same code as painmx_2)\npain12_4: Within 12 hours following maximal activity, compared to same period at baseline (same code as painmx_2)\npainav_4: During the average day of study period pain vs baseline (same code as painmx_2)\npainov_4: Overall impression of drug efficacy vs baseline (same code as painmx_2)\n\n\n\n\n\nvalid\nThe food-frequency questionnaire (FFQ) is a common tool in dietary epidemiology to assess food consumption. It asks individuals to report their typical daily servings of over 100 food items from the past year, and a food-composition table calculates nutrient intakes. While FFQs are inexpensive, they are less accurate than diet records (DR), where participants document their weekly food intake, and a nutritionist calculates nutrient intakes. In a validation study, 173 nurses from the Nurses’ Health Study completed 4 weeks of diet records and an FFQ. Data for saturated fat, total fat, alcohol consumption, and caloric intake from both methods recorded here.\n\nvalid.csv\n\nId: ID number\nsfat_dr: Saturated fat-DR (g)\nsfat_ffq: Saturated fat-FFQ (g)\ntfat_dr: Total fat-DR (g)\ntfat_ffq: Total fat-FFQ (g)\nalco_dr: Alcohol consumption-DR (oz)\nalco_ffq: Alcohol consumption-FFQ (oz)\ncal_dr: Total calories-DR (K-cal)\ncal_ffq: Total calories-FFQ (K-cal)\n\n\n\n\nwales\nA study in South Wales investigated the hereditary factors of blood pressure in 623 individuals (propositii) over age 5 from two populations. The participants and their first-degree relatives had their blood pressure measured at home by one observer, with a baseline and three follow-up exams from the mid-1950s to the early 1960s. The dataset WALES.DAT includes familial blood pressure data from the Rhondda Fach and Vale of Glamorgan communities.\n\nwales.csv\n\nID: ID\nrecord:\nsex:\narea:\npropositus:\nsexofprop:\nrelationship:\ndoubler:\nsurveystat:\nmaritalstt:\nage:\nparity:\nheight:\nweight:\narmgrth:\ntriceps:\nsysbp:\ndiasbp:\npulse:\nalbumin:\nglucose:\nbacteria:\noccupation:\nhypertension:\ndiabetes:\npregnancy:\npet_fibroids:\nangina:\npmi:\ninter_claud:\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGreene Jr, John W, and Joseph C Touchstone. 1963. “Urinary Estriol as an Index of Placental Function. A Study of 279 Cases.” American Journal of Obstetrics and Gynecology 85: 1–9. https://doi.org/10.1016/S0002-9378(16)35333-9.\n\n\nNeter, John, Michael H Kutner, Christopher J Nachtsheim, William Wasserman, et al. 1996. “Applied Linear Statistical Models.”\n\n\nRosner, B. 2015. Fundamentals of Biostatistics. 8th ed. Cengage Learning."
  },
  {
    "objectID": "01_r/01_r_intro.html",
    "href": "01_r/01_r_intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "R is a statistical programming language designed to analyze data.\nThis is not an R course. But you need to know some tools to summarize/plot/model data.\nR is free, widely used, more generally applicable (beyond linear regression), and a useful tool for reproducibility. So this is what we will use.\nPython would have been a good choice too, but it is worse at basic stats (this is controversial)."
  },
  {
    "objectID": "01_r/01_r_intro.html#variables",
    "href": "01_r/01_r_intro.html#variables",
    "title": "Introduction to R",
    "section": "Variables",
    "text": "Variables\n\nA variable stores a value. You use the assignment operator “&lt;-” to assign values to variables. For example, we can assign the value of 10 to the variable x.\n\nx &lt;- 10\n\n\nIt is possible to use =, and I think there is nothing wrong with that. But for some reason the field has decided to only use &lt;-, so you should too.\n\nWhenever we use x later, it will use the value of 10\n\nx\n\n[1] 10\n\n\nThis is useful because you can reuse this value over and over again:\n\ny &lt;- 0\nx + y\nx * y\nx / y\nx - y\n\nTo assign a “string” (a fancy way to say a word) to x, put the string in quotes. For example, we can assign the value of \"Hello World\" to x.\n\nx &lt;- \"Hello World\"\nx\n\n[1] \"Hello World\""
  },
  {
    "objectID": "01_r/01_r_intro.html#functions",
    "href": "01_r/01_r_intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\n\nFunctions take objects (such as numbers or variables) as input and output new objects. Let’s look at a simple function that takes the log of a number:\n\nlog(x = 4, base = 2)\n\nThe inputs are called “arguments”. Generally, every function will be for the form:\n\nfunction_name(arg1 = val1, arg2 = val2, ...)\n\nIf you do not specify the name of the argument, R will assume you are assigning in their order.\n\nlog(4, 2)\n\nYou can change the order of the arguments if you specify them.\n\nlog(base = 2, x = 4)\n\nTo see the list of all possible arguments of a function, use the help() function:\n\nhelp(log)\n\nIn the help file, there are often default values for an argument. For example, the following indicates the the default value of base is exp(1).\n\nlog(x, base = exp(1))\n\nThis indicates that you can omit the base argument and R will assume that it should be exp(1).\n\nlog(x = 4, base = exp(1))\n\n[1] 1.386\n\nlog(x = 4)\n\n[1] 1.386\n\n\nIf an argument does not have a default, then it must be specified when calling a function.\nType this:\n\nlog(x = 4,\n\nThe “+” indicates that R is expecting more input (you forgot either a parentheses or a quotation mark). You can get back to the prompt by hitting the ESCAPE key."
  },
  {
    "objectID": "01_r/01_r_intro.html#useful-functions",
    "href": "01_r/01_r_intro.html#useful-functions",
    "title": "Introduction to R",
    "section": "Useful Functions",
    "text": "Useful Functions\n\nc() creates a vector (sequence of values)\n\ny &lt;- c(8, 1, 3, 4, 2)\ny\n\n[1] 8 1 3 4 2\n\n\nYou can perform vectorized operations on these vectors\n\ny + 2\n\n[1] 10  3  5  6  4\n\ny / 2\n\n[1] 4.0 0.5 1.5 2.0 1.0\n\ny - 2\n\n[1]  6 -1  1  2  0\n\n\nexp(): Exponentiation. This is the inverse of log().\n\nexp(10)\n\n[1] 22026\n\nlog(exp(10))\n\n[1] 10\n\n\nsqrt(): Square root\n\nsqrt(9)\n\n[1] 3\n\n\nmean(): The mean of a vector\n\nmean(y)\n\n[1] 3.6\n\n\nsd() The standard deviation of a vector\n\nsd(y)\n\n[1] 2.702\n\n\nsum(): Sum the values of a vector.\n\nsum(y)\n\n[1] 18\n\n\nseq(): Create a sequence of numbers\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nhead(): Show the first six values of an object.\n\n\nExerciseSolution\n\n\nCalculate this in R (hint: pi is \\(\\pi\\) in R) \\[\n\\frac{1}{\\sqrt{2\\pi}}e^{-1.3^2}\n\\]\n\n\n\nexp(-1.3^2) / sqrt(2 * pi)\n\n[1] 0.07361\n\n\n\n\n\n\nExerciseSolution\n\n\nCreate the following vector \\(x = (1, -2, 1.3)\\). What is the mean and standard deviation of this variable?\n\n\n\nx &lt;- c(1, -2, 1.3)\nmean(x)\n\n[1] 0.1\n\nsd(x)\n\n[1] 1.825\n\n\n\n\n\n\nExerciseSolution\n\n\nCreate a vector from 1 to 1000, take ths square root of each element, then sum them up.\n\n\n\nsum(sqrt(seq(1, 1000)))\n\n[1] 21097\n\n\n\n\n\n\nExerciseSolution\n\n\nWhat does the by argument do in seq()? Try it out.\n\n\nCreates increments of 2 instead of 1.\n\nseq(1, 10, by = 2)\n\n[1] 1 3 5 7 9"
  },
  {
    "objectID": "01_r/01_r_intro.html#r-packages",
    "href": "01_r/01_r_intro.html#r-packages",
    "title": "Introduction to R",
    "section": "R Packages",
    "text": "R Packages\n\nA package is a collection of functions that don’t come with R by default.\nThere are many many packages available. If you need to do any data analysis, there is probably an R package for it.\nUsing install.packages(), you can install packages that contain functions and datasets that are not available by default. Do this now with the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nYou will only need to install a package once per computer. Once it is installed you can gain access to all of the functions and datasets in a package by using the library() function.\n\nlibrary(tidyverse)\n\nYou will need to run library() at the start of every R session if you want to use the functions in a package.\nWhen I want to write the name of a function, I will write it like this()."
  },
  {
    "objectID": "01_r/01_r_intro.html#data-frames",
    "href": "01_r/01_r_intro.html#data-frames",
    "title": "Introduction to R",
    "section": "Data Frames",
    "text": "Data Frames\n\nThe fundamental unit object of data analysis is the data frame.\nA data frame has variables in the columns, and observations in the rows.\n \nR comes with a bunch of famous datasets in the form of a data frame. Such as the airquality dataset, which contains daily air quality measurements in New York from 1973.\n\ndata(\"airquality\")\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nYou can extract individual variables from a data frame using $\n\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6\n [19]  30  11   1  11   4  32  NA  NA  NA  23  45 115  37  NA  NA  NA  NA  NA\n [37]  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA\n [55]  NA  NA  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA\n [73]  10  27  NA   7  48  35  61  79  63  16  NA  NA  80 108  20  52  82  50\n [91]  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22\n[109]  59  23  31  44  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73\n[127]  91  47  32  20  23  21  24  44  21  28   9  13  46  18  13  24  16  13\n[145]  23  36   7  14  30  NA  14  18  20\n\n\nYou can explore these in a spreadsheet format using View() (note the capital “V”). Don’t ever have this in a file though, directly write it in the console.\n\nView(airquality)"
  },
  {
    "objectID": "01_r/01_r_intro.html#reading-in-data-frames",
    "href": "01_r/01_r_intro.html#reading-in-data-frames",
    "title": "Introduction to R",
    "section": "Reading in Data Frames",
    "text": "Reading in Data Frames\n\nMost datasets will nead to be loaded into R. To do so, we will use the {readr} package.\n\nlibrary(readr)\n\nThe only function I will require you to know from this package is read_csv(), which loads in data from a CSV file (“Comma-separated values”), a very popular format for storing data.\nIf you have the CSV file somewhere on your computer, then specify the path from the current working directory, and assign the data frame to a variable.\nFor other file formats, you need to use other functions, such as read_tsv(), read_table(), read_fwf(), etc. I will try to make sure read_csv() works for all datasets in this course.\nI will typicaly post course datasets at https://dcgerard.github.io/stat_320/data.html. You can load those data into R by pasting their URL’s into read_csv().\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nhead(lead)\n\n# A tibble: 6 × 40\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 29 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;\n\n\n\n\nExerciseSolution\n\n\nLoad in the birthweight data into R and print out the first six rows.\n\n\n\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nRows: 1000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): id, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(birthweight)\n\n# A tibble: 6 × 2\n     id weight\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     0    116\n2     1    124\n3     2    119\n4     3    100\n5     4    127\n6     5    103"
  },
  {
    "objectID": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "href": "01_r/01_r_intro.html#basic-data-frame-manipulations",
    "title": "Introduction to R",
    "section": "Basic Data Frame Manipulations",
    "text": "Basic Data Frame Manipulations\n\nYou will need to know just a few data frame manipulations, which we will perform using the {dplyr} package.\n\nlibrary(dplyr)\n\nThe first argument for {dplyr} functions is always the data frame you are modifying. The following arguments typically involve the columns of that data frame.\nUse the mutate() function from the {dplyr} package to make variable transformations.\n\nlead &lt;- mutate(lead, log_iqv_inf = log(iqv_inf))\nhead(lead)\n\n# A tibble: 6 × 41\n     id area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse glimpse() to get a brief look at the data frame.\n\nglimpse(lead)\n\nRows: 124\nColumns: 41\n$ id          &lt;dbl&gt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112…\n$ area        &lt;chr&gt; \"2.5-4.1\", \"2.5-4.1\", \"2.5-4.1\", \"1-2.5\", \"0-1\", \"1-2.5\", …\n$ ageyrs      &lt;dbl&gt; 11.08, 9.42, 11.08, 6.92, 11.25, 6.50, 6.92, 15.00, 7.17, …\n$ sex         &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"f…\n$ iqv_inf     &lt;dbl&gt; 3, 7, 4, 4, 5, 5, 7, 3, 13, 7, 6, 11, 11, 6, 9, 4, 13, 4, …\n$ iqv_comp    &lt;dbl&gt; 4, 9, 9, 6, 4, 12, 9, 1, 10, 9, 10, 14, 12, 4, 11, 6, 17, …\n$ iqv_ar      &lt;dbl&gt; 3, 7, 5, 6, 8, 11, 10, 3, 14, 12, 6, 14, 8, 5, 11, 4, 13, …\n$ iqv_ds      &lt;dbl&gt; 5, 6, 3, 6, 5, 9, 7, 6, 13, 9, 7, 11, 8, 8, 9, 8, 14, 12, …\n$ iqv_raw     &lt;dbl&gt; 15, 29, 21, 22, 22, 37, 33, 13, 50, 37, 29, 50, 39, 23, 40…\n$ iqp_pc      &lt;dbl&gt; 10, 8, 10, 5, 5, 14, 10, 6, 8, 6, 6, 13, 8, 9, 14, 9, 16, …\n$ iqp_bd      &lt;dbl&gt; 8, 7, 7, 8, 10, 7, 8, 2, 15, 9, 8, 13, 9, 7, 17, 8, 16, 9,…\n$ iqp_oa      &lt;dbl&gt; 8, 10, 7, 5, 13, 7, 7, 3, 14, 12, 3, 15, 11, 6, 13, 13, 16…\n$ iqp_cod     &lt;dbl&gt; 5, 9, 20, 13, 12, 10, 16, 8, 9, 13, 9, 20, 12, 12, 16, 12,…\n$ iqp_raw     &lt;dbl&gt; 31, 34, 44, 31, 40, 38, 41, 19, 46, 40, 26, 61, 40, 34, 60…\n$ hh_index    &lt;dbl&gt; 77, 77, 30, 77, 62, 72, 54, 73, 22, 77, 63, 48, 48, 48, 48…\n$ iqv         &lt;dbl&gt; 61, 82, 70, 72, 72, 95, 89, 57, 116, 95, 82, 116, NA, 74, …\n$ iqp         &lt;dbl&gt; 85, 90, 107, 85, 100, 97, 101, 64, 111, 100, 76, 136, 100,…\n$ iqf         &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, …\n$ iq_type     &lt;chr&gt; \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"WISC\", \"W…\n$ lead_grp    &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ Group       &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"co…\n$ ld72        &lt;dbl&gt; 25, 31, 30, 29, 2, 29, 25, 24, 24, 31, 21, 29, 32, 36, 30,…\n$ ld73        &lt;dbl&gt; 18, 28, 29, 30, 34, 25, 24, 15, 16, 24, 19, 27, 29, 32, 25…\n$ fst2yrs     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ totyrs      &lt;dbl&gt; 11, 6, 5, 5, 11, 6, 6, 15, 7, 7, 12, 10, 12, 12, 10, 10, 1…\n$ pica        &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ colic       &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ clumsi      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no…\n$ irrit       &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"…\n$ convul      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ `_2plat_r`  &lt;dbl&gt; 16, 17, 16, 11, 17, 16, 10, 19, 15, 16, 17, 17, 15, 23, 19…\n$ `_2plar_l`  &lt;dbl&gt; 16, 16, 17, 9, 16, 14, 13, 14, 13, 11, 16, 17, 14, 21, 20,…\n$ visrea_r    &lt;dbl&gt; 36, 23, 20, 34, 26, 29, 29, 30, 31, 26, 19, 22, 19, 26, 17…\n$ visrea_l    &lt;dbl&gt; 38, 19, 24, 42, 34, 26, 29, 32, 28, 25, 19, 24, 17, 23, 16…\n$ audrea_r    &lt;dbl&gt; 27, 18, 16, 35, 31, 28, 30, 33, 31, 27, 16, 22, 18, 25, 17…\n$ audrea_l    &lt;dbl&gt; 25, 28, 17, 30, 33, 27, 27, 24, 29, 21, 19, 23, 20, 28, 16…\n$ fwt_r       &lt;dbl&gt; 72, 61, 46, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 44, 74…\n$ fwt_l       &lt;dbl&gt; 52, 48, 49, 41, 42, 35, 39, 58, 40, 37, 44, 48, 47, 53, 63…\n$ hyperact    &lt;dbl&gt; NA, 0, NA, 2, NA, 0, 0, NA, 0, 0, NA, 1, NA, NA, NA, 2, NA…\n$ maxfwt      &lt;dbl&gt; 72, 61, 49, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 53, 74…\n$ log_iqv_inf &lt;dbl&gt; 1.0986, 1.9459, 1.3863, 1.3863, 1.6094, 1.6094, 1.9459, 1.…\n\n\nUse View() to see a spreadsheet of the data frame (never put this in a Quarto file). Note the capital “V”.\n\nView(lead)\n\nUse rename() to rename variables.\n\nlead &lt;- rename(lead, ID = id)\nhead(lead)\n\n# A tibble: 6 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   101 2.5-4…  11.1  male        3        4      3      5      15     10      8\n2   102 2.5-4…   9.42 male        7        9      7      6      29      8      7\n3   103 2.5-4…  11.1  male        4        9      5      3      21     10      7\n4   104 1-2.5    6.92 male        4        6      6      6      22      5      8\n5   105 0-1     11.2  male        5        4      8      5      22      5     10\n6   106 1-2.5    6.5  male        5       12     11      9      37     14      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\nUse filter() to remove rows.\n\nUse == to select rows based on equality\nUse &lt; and &gt; to select rows based on inequality\nUse &lt;= and &gt;= to select rows based on inequality/equality.\n\n\nfilter(lead, Group == \"control\")\n\n# A tibble: 78 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   101 2.5-…  11.1  male        3        4      3      5      15     10      8\n 2   102 2.5-…   9.42 male        7        9      7      6      29      8      7\n 3   103 2.5-…  11.1  male        4        9      5      3      21     10      7\n 4   104 1-2.5   6.92 male        4        6      6      6      22      5      8\n 5   105 0-1    11.2  male        5        4      8      5      22      5     10\n 6   106 1-2.5   6.5  male        5       12     11      9      37     14      7\n 7   107 2.5-…   6.92 male        7        9     10      7      33     10      8\n 8   108 0-1    15    fema…       3        1      3      6      13      6      2\n 9   109 1-2.5   7.17 fema…      13       10     14     13      50      8     15\n10   110 1-2.5   7.25 male        7        9     12      9      37      6      9\n# ℹ 68 more rows\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;, …\n\nfilter(lead, ageyrs &lt; 4)  \n\n# A tibble: 7 × 41\n     ID area   ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   403 0-1      3.92 male        5        6     10      7      28     12      8\n2   406 1-2.5    3.75 male        9        4     10      7      30      8     11\n3   504 1-2.5    3.75 male        8        7      8      7      30      8      7\n4   505 1-2.5    3.75 fema…       6        5      6      3      20      8     12\n5   602 1-2.5    3.75 fema…       6       11      8     11      36     11     10\n6   606 2.5-4…   3.83 male       12        9     18      8      47     12     10\n7   607 0-1      3.92 male        8        4     11      1      24     13      7\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\nfilter(lead, ageyrs &gt; 4, ageyrs &lt; 5)\n\n# A tibble: 14 × 41\n      ID area  ageyrs sex   iqv_inf iqv_comp iqv_ar iqv_ds iqv_raw iqp_pc iqp_bd\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1   401 2.5-…   4.33 male        6       13     13      9      41     11     11\n 2   402 1-2.5   4.83 fema…       9        7     13      8      37     11     12\n 3   404 0-1     4.58 male        6        3      4      2      15     12      7\n 4   405 0-1     4.5  male        6        9      9     12      36      8      9\n 5   407 2.5-…   4.25 male        7        4      8      6      25     10     12\n 6   408 2.5-…   4.33 male        7        6      4      5      22      9      6\n 7   409 1-2.5   4.33 fema…       8        8     11     11      38     11      9\n 8   411 1-2.5   4.33 fema…       8        9     14      7      38     14      9\n 9   412 2.5-…   4.75 fema…       7        7     10      8      32     13     13\n10   414 0-1     4.5  male        6        3      7      2      18      6      8\n11   501 0-1     4.17 male       11        7      8      5      31     11     10\n12   502 2.5-…   4.58 male        7        7      9      3      26     10      3\n13   601 1-2.5   4.33 male        9        6      9      8      32      8      9\n14   604 1-2.5   4.58 male        7       10      6     12      35      9     11\n# ℹ 30 more variables: iqp_oa &lt;dbl&gt;, iqp_cod &lt;dbl&gt;, iqp_raw &lt;dbl&gt;,\n#   hh_index &lt;dbl&gt;, iqv &lt;dbl&gt;, iqp &lt;dbl&gt;, iqf &lt;dbl&gt;, iq_type &lt;chr&gt;,\n#   lead_grp &lt;chr&gt;, Group &lt;chr&gt;, ld72 &lt;dbl&gt;, ld73 &lt;dbl&gt;, fst2yrs &lt;chr&gt;,\n#   totyrs &lt;dbl&gt;, pica &lt;chr&gt;, colic &lt;chr&gt;, clumsi &lt;chr&gt;, irrit &lt;chr&gt;,\n#   convul &lt;chr&gt;, `_2plat_r` &lt;dbl&gt;, `_2plar_l` &lt;dbl&gt;, visrea_r &lt;dbl&gt;,\n#   visrea_l &lt;dbl&gt;, audrea_r &lt;dbl&gt;, audrea_l &lt;dbl&gt;, fwt_r &lt;dbl&gt;, fwt_l &lt;dbl&gt;,\n#   hyperact &lt;dbl&gt;, maxfwt &lt;dbl&gt;, log_iqv_inf &lt;dbl&gt;\n\n\n\n\nExerciseSolution\n\n\nThe birthweight data is in ounces. There are abour 28.3495 grams in an ounce. Create a new variable called weight_g that is the weight of the baby in grams.\n\n\n\nbirthweight &lt;- mutate(birthweight, weight_g = 28.3495 * weight)\nglimpse(birthweight)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the birthweight data, select just babies that are greater than or equal to 150 ounces.\n\n\n\nfilter(birthweight, weight &gt;= 150)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the lead data, select individuals who are both in the control group and are at least 15.\n\n\n\nfilter(lead, Group == \"control\", ageyrs &gt;= 15)\n\n\n\n\n\nExerciseSolution\n\n\nFrom the lead data, rename ageyrs to just age\n\n\n\nrename(lead, age = ageyrs)"
  },
  {
    "objectID": "00_course_outline/00_math_prereqs.html",
    "href": "00_course_outline/00_math_prereqs.html",
    "title": "00 Math Prerequisites",
    "section": "",
    "text": "We do not emphasize the mathematics underlying statistics in this course. However, you should have some familiarity with pre-calculus topics. Here are some facts you should know off the top of your head:\n\nSummations\n\nCapital-sigma notation is useful for writing sums of many numbers/variables: \\[\\sum_{i = 1}^n x_i = x_1 + x_2 + \\cdots x_n\\]\nIf you sum a constant \\(n\\) times, you get \\(n\\) times that constant: \\[\\sum_{i = 1}^n a = an\\]\nYou can factor out multiplicative constants that don’t depend on the summing index: \\[\\sum_{i = 1}^n cx_i = c\\sum_{i = 1}^n x_i\\]\nThe order that you sum elements does not matter: \\[\\sum_{i = 1}^n (x_i + y_i) = \\sum_{i = 1}^n x_i + \\sum_{i = 1}^n y_i\\]\n\n\nExerciseSolution\n\n\nLet \\(X_1 = 5\\), \\(X_2 = 10\\), \\(X_3 = 1\\), \\(X_4 = 3\\)\n\nWhat is \\(\\sum_{i=1}^4 X_i\\)?\nWhat is \\(\\sum_{i=2}^3 X_i\\)?\nWhat is \\(\\sum_{i=2}^3 2X_i\\)?\nWhat is \\(\\sum_{i=1}^4 (X_i + 1)\\)?\nWhat is \\(\\sum_{i=1}^4 X_i^2\\)?\n\n\n\n\n\\(X_1 + X_2 + X_3 + X_4 = 5 + 10 + 1 + 3 = 19\\)\n\\(X_2 + X_3= 10 + 1 = 11\\)\n\\(2X_2 + 2X_3 = 2\\times 10 + 2\\times 1 = 22\\). You can also just multiply part 2 by 2 since \\(\\sum_{i=2}^3 2X_i = 2 \\sum_{i=2}^3 X_i\\).\n\\(\\sum_{i=1}^4 (X_i + 1) = \\sum_{i=1}^4 X_i + \\sum_{i=1}^4 1 = \\sum_{i=1}^4 X_i + 4 = 19 + 4 = 23\\)\n\\(X_1^2 + X_2^2 + X_3^2 + X_4^2 = 5^2 + 10^2 + 1^2 + 3^2 = 135\\)\n\n\n\n\n\n\nPowers, Exponentials, Logarithms\n\n\\(e^{ab} = {e^{a}}^{b} = {e^{b}}^{a}\\)\n\\(e^{a+b+c} = e^a(e^{b+c}) = e^ae^be^c\\)\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(a/b) = \\log(a) - \\log(b)\\)\n\\(x^ny^n = (xy)^n\\)"
  },
  {
    "objectID": "00_course_outline/00_overview.html",
    "href": "00_course_outline/00_overview.html",
    "title": "Course Overview",
    "section": "",
    "text": "Learning Objectives\n\nOverview of the course plus some reminders from STAT 202/203/204\nP-values/confidence intervals.\n\\(t\\)-tests for means in R.\nProportion tests in R.\n\n\n\nProbability and Distributions in R.\n\nDistribution: The possible values of a variable and how often it takes those values.\nA density describes the distribution of a quantitative variable. You can think of it as approximating a histogram. It is a curve where\n\nThe area under the curve between any two points is approximately the probability of being between those two points.\nThe total area under the curve is 1 (something must happen).\nThe curve is never negative (can’t have negative probabilities).\n\nThe density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nExercise: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?\nThe \\(t\\)-distribution shows up a lot in Statistics.\n\nIt is also bell-curved but has “thicker tails” (more extreme observations are more likely).\nIt is always centered at 0.\nIt only has one parameter, called the “degrees of freedom”, which determines how thick the tails are.\nSmaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.\nIf the degrees of freedom is large enough, the \\(t\\)-distribution is approximately the same as a normal distribution with mean 0 and variance 1.\n\n\\(t\\)-distributions with different degrees of freedom:\n\n\n\n\n\n\n\n\n\nDensity Function\n\ndt(x = -6, df = 2)\n\n[1] 0.004269\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation\n\nsamp &lt;- rt(n = 1000, df = 2)\nhead(samp)\n\n[1]  0.89857 -1.07176  0.09639  0.79371 -0.42428 -0.64561\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\npt(q = 2, df = 2)\n\n[1] 0.9082\n\n\n\n\n\n\n\n\n\n\n\nQuantile Function\n\nqt(p = 0.9082, df = 2)\n\n[1] 1.999\n\n\n\n\n\n\n\n\n\n\n\nThere are many other distributions implemented in R. To see the most common, run:\n\nhelp(\"Distributions\")\n\n\n\n\nAll of Statistics\n\nObservational/experimental Units: The people/places/things/animals/groups that we collect information about. Also known as “individuals” or “cases”. Sometimes I just say “units”.\nVariable: A property of the observational/experimental units.\n\nE.g.: height of a person, area of a country, marital status.\n\nValue: The specific level of a variable for an observational/experimental unit.\n\nE.g.: Bob is 5’11’’, China has an area of 3,705,407 square miles, Jane is divorced.\n\nQuantitative Variable: The variable takes on numerical values where arithmetic operations (plus/minus/divide/times) make sense.\n\nE.g.: height, weight, area, income.\nCounterexample: Phone numbers, social security numbers.\n\nCategorical Variable: The variable puts observational/experimental units into different groups/categories based on the values of that variable.\n\nE.g.: race/ethnicity, marital status, religion.\n\nBinary Variable: A categorical variable that takes on only two values.\n\nE.g.: dead/alive, treatment/control.\n\nPopulation: The collection of all observational units we are interested in.\nParameter: A numerical summary of the population.\n\nE.g.: Average height, proportion of people who are divorced, standard deviation of weight.\n\nSample: A subset of the population (some observational units, but not all of them).\nStatistic: A numeric summary of the sample.\n\nE.g.: Average height of the sample, proportion of people who are divorced in the sample, standard deviation of weight of a sample.\n\nGraphic:\n \nSampling Distribution: The distribution of a statistic over many hypothetical random samples from the population.\n \nAll of Statistics: We see a pattern in the sample.\n\nEstimation: Guess the pattern in the population based on the sample. Guess a parameter with a statistic. A statistic which is a guess for a parameter is called an estimate.\nHypothesis Testing: Ask if the pattern we see in the sample also exists in the population. Test if a parameter is some value.\nConfidence Intervals: Quantify our (un)certainty of the pattern in the population based on the sample. Provide a range of likely parameter values.\n\nWe will go through a lot of examples of this below\n\nlibrary(tidyverse)\nlibrary(broom)\n\nExercise: Read about the boneden data here What are the observational units? What are the variables? Which are quantitative and which are categorical?\nExercise: Read about the lead data here. What are the observational units? What are the variables? Which are quantitative and which are categorical?\n\n\n\nPattern: Mean is shifted (one quantitative variable)\n\nExample: The boneden data explores the difference in bone density between a heavier and a lighter smoking twin.\nObservational Units: The twins.\nPopulation: All twins where one smokes more than the other.\nSample: The 41 twins in our study.\nVariable: The difference in lumbar spine density (in g/cm2) between the twins. We derived this quantitative variable by subtracting one density from another.\n\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\n\nPattern: Use a histogram/boxplot to visualize the shift from 0.\n\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(bins = 20, fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = 0, lty = 2) +\n  xlab(\"Difference in Bone Density\")\n\n\n\n\n\n\n\n\nGraphic:\n \nParameter of interest: Mean difference in bone density for all twins.\nEstimate: Use sample mean\n\nboneden %&gt;%\n  summarize(meandiff = mean(ls_diff))\n\n# A tibble: 1 × 1\n  meandiff\n     &lt;dbl&gt;\n1   0.0359\n\n\n0.03585 is our “best guess” for the parameter, but it is almost certainly not the value of the parameter (since we didn’t measure everyone).\nHypothesis Testing:\n\nWe are interested in if the mean difference is different from 0.\nTwo possibilities:\n\nNull Hypothesis: Mean is not different from 0, we just happened by chance to get twins that had some difference in density.\nAlternative Hypothesis: Mean is different from 0.\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a \\(p\\)-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\np-value: the probability that you would see data as or more supportive of the alternative hypothesis than what you saw assuming that the null hypothesis is true.\n\nGraphic:\n \nThe distribution of possible null sample means is given by statistical theory. Specifically, the \\(t\\)-statistic (mean divided by the standard deviation of the sampling distribution of the mean) has a \\(t\\) distribution with \\(n - 1\\) degrees of freedom (\\(n\\) is the sample size). It works as long as your data aren’t too skewed or if you have a large enough sample size.\nFunction: t.test()\n\ntout &lt;- t.test(ls_diff ~ 1, data = boneden)\ntout\n\n\n  One Sample t-test\n\ndata:  ls_diff\nt = 2.6, df = 40, p-value = 0.01\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.007986 0.063721\nsample estimates:\nmean of x \n  0.03585 \n\n\nThe tidy() function from the broom package will format the output of common procedures to a convenient data frame.\n\ntdf &lt;- tidy(tout)\ntdf$estimate\n\nmean of x \n  0.03585 \n\ntdf$p.value\n\n[1] 0.01299\n\n\nWe often want a range of “likely” values. These are called confidence intervals. t.test() will return these confidence intervals, giving lowest and highest likely values for the mean difference in bone density:\n\ntdf$conf.low\n\n[1] 0.007986\n\ntdf$conf.high\n\n[1] 0.06372\n\n\nInterpreting confidence intervals:\n\nCORRECT: We used a procedure that would capture the true parameter in 95% of repeated samples.\nCORRECT: Prior to sampling, the probability of capturing the true parameter is 0.95.\nWRONG: After sampling, the probability of capturing the true parameter is 0.95.\n\nBecause after sampling the parameter is either in the interval or it’s not. We just don’t know which.\n\nWRONG: 95% of twins have bone density differences within the bounds of the 95% confidence interval.\n\nBecause confidence intervals are statements about parameters, not observational units or statistics.\n\n\nGraphic:\n \nIntuition: Statistical theory tells us that the sample mean will be within (approximately) 2 standard deviations of the population mean in 95% of repeated samples. This is two standard deviations of the sampling distribution of the sample mean, not two standard deviations of the sample. So we just add and subtract (approximately) two standard deviations of the sampling distribution from the sample mean.\nExercise: The birthweight data available here contains the birthweights (in ounces) of 1000 newborns born in a Boston area hospital. Wikipedia says the average birthweight for individuals of European and African descent is 123 ounces. Does this Boston hospital have the same mean as what Wikipedia says? Explain.\n\n\n\nPattern: Means of two groups are different (one quantitative, one binary)\n\nExample: IQ differences between children with high levels of lead (exposed) and those with low levels of lead (control).\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead |&gt;\n  select(Group, iqf) |&gt;\n  glimpse()\n\nRows: 124\nColumns: 2\n$ Group &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"control\"…\n$ iqf   &lt;dbl&gt; 70, 85, 86, 76, 84, 96, 94, 56, 115, 97, 77, 128, NA, 80, 118, 8…\n\n\nObservational Units: Children\nPopulation: All children\nSample: The 120 children for whom we have both lead and IQ measurements.\nVariables: The lead level group (binary/categorical) and the full scale IQ (quantitative).\nPattern: Use a boxplot to see if the groups differ.\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nParameter of interest: Difference in mean IQ levels between the control and exposed groups.\nEstimate: The difference in mean IQ between the two groups in our sample.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(meaniq = mean(iqf, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  Group   meaniq\n  &lt;chr&gt;    &lt;dbl&gt;\n1 control   92.6\n2 exposed   88.0\n\n92.55 - 88.02\n\n[1] 4.53\n\n\nThe control group is about 4.5 points higher on average\nHypothesis Test:\n\nWe want to know if the difference in the mean IQ in the two groups is actually different.\nTwo possibilities:\n\nNull Hypothesis: The mean IQs are the same in the two groups. We just happened by chance to get a lower IQ lead group and a higher IQ control group.\nAlternative Hypothesis: The mean IQ are different in the two groups.\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If the this probability is high, we don’t conclude anything.\n\n\nGraphic:\n \nThe distribution of possible null sample means comes from statistical theory. The t-statistic has a \\(t\\) distribution with a complicated degrees of freedom.\nFunction: t.test(). The quantitative variable goes to the left of the tilde and the binary variable goes to the right of the tilde.\n\ntout &lt;- t.test(iqf ~ Group, data = lead)\ntdf &lt;- tidy(tout)\ntdf$estimate\n\n[1] 4.532\n\ntdf$p.value\n\n[1] 0.07966\n\n\nt.test() also returns a 95% confidence interval for the difference in means. This has the exact same interpretation as in the previous section.\n\nc(tdf$conf.low, tdf$conf.high)\n\n[1] -0.5448  9.6094\n\n\nAssumptions (in decreasing order of importance):\n\nIndependence: conditional on group, IQ of one child doesn’t give us any information on the IQs of any other children (reasonable).\nApproximate normality: The distribution of IQ’s is bell-curved in group. Doesn’t matter for moderate-large sample sizes because of the central limit theorem.\n\nExercise: Is there a difference between control and exposed groups when it comes to the finger-wrist tapping test in the dominant hand (maxfwt).\n\n\n\nPattern: Proportion is shifted (one binary variable).\n\nThe exposed individuals were mostly males. There were 30 males and 16 females. In the US, about 51.22% of all births are boys. Are boys more likely to be recruited to the study than girls?\n\nlead |&gt;\n  filter(Group == \"exposed\") |&gt;\n  group_by(sex) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female    16\n2 male      30\n\n\nObservational Units: U.S. children exposed to lead\nPopulation: All U.S. children exposed to lead\nSample: The 46 children in our sample who were exposed to lead.\nVariable: Sex (male/female)\nPattern: Calculate sample proportion.\n\n30 / 46\n\n[1] 0.6522\n\n\nParameter of interest: Proportion of children exposed to lead who are boys\nEstimate with sample proportion, 0.6522\nHypothesis Testing:\n\nWe are interested in if our sample had some bias in selecting more boys.\nTwo possibilities:\n\nNull Hypothesis: Probability of a boy being included in the sample is 0.5122. We just happened by chance to get a lot more boys.\nAlternative Hypothesis: There is bias and the probability of a boy being in the sample is greater than for a girl(because boys are more likely to be exposed to lead, or because they were more likely to be recruited to the study).\n\nStrategy: We calculate the probability of the data assuming possibility 1 (called a p-value). If this probability is low, we conclude possibility 2. If this probability is high, we don’t conclude anything.\n\nGraphic:\n \nThe distribution of possible null sample proportions comes from statistical theory. The number of successes has a binomial distribution with success probability 0.5122 and size parameter equal to the sample size. The sample proportion is the number successes divided by the sample size.\nFunction: prop.test() (when you have a large number of both successes and failures) or binom.test() (for any number of successes and failures).\n\nbout &lt;- tidy(binom.test(x = 30, n = 46, p = 0.5122))\nbout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0757    0.498     0.786\n\n\n\npout &lt;- tidy(prop.test(x = 30, n = 46, p = 0.5122))\npout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.652  0.0798    0.497     0.782\n\n\nExercise: Is there a sex bias for the control group?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 320: Biostatistics",
    "section": "",
    "text": "Syllabus\nPractice Problems\n\n00 R\n\nIntroduction to R\nPlotting in R\nDocuments in R\n\n01 Overview\n\nMath Prereqs\nCourse Overview\n\n02 Descriptive Statistics\n\nChapter 2 Notes (Descriptive Statistics)\n\n03 Probability\n\nChapter 3 Notes (Probability)\n\nConfusion Matrix\n\nChapter 4 Notes (Discrete Probability Distributions)\nChapter 5 Notes (Continuous Probability Distributions)\n\n04 Estimation\n\nChapter 6 Notes (Estimation)\n\nRandom Selection/Assignment\nCentral Limit Theorem Illustration\nCI Interpretation\nt-distribution\nBone Density Case Study\nchi-squared distribution\nEstimating Binomial Proportion\n\n\n05 Testing\n\nChapter 7 Notes (One Sample Hypothesis Tests)\n\nOne Sample \\(t\\)-Tests in R\nPower Calculations in R\nOne Sample Binomial Tests in R\nOne Sample Binomial Power Calculations\n\nChapter 8 Notes (Two Sample Hypothesis Tests)\n\nTwo Sample \\(t\\)-Tests in R\n\n\n06 Nonparametric Methods\n\nChapter 9 Notes (Nonparametric Methods)\n\nOne Sample Inference in R\nTwo Sample Inference in R\n\n\n07 Categorical Tests\n\nChapter 10 Notes (Categorical Methods)\n\n2x2 Contingency Tables in R\nMcNemar’s Test in R\nLarger Contingency Tables in R\nCohen’s Kappa in R\n\n\n08 Regression\n\nChapter 11 Notes (Regression)\nChapter 12 Notes (ANOVA)\n\nNext stop:\n\nRegression\n\n\nHandwritten Notes:\n\nChapter 2 Notes (Descriptive Statistics)\nChapter 3 Notes (Probability)\nChapter 4 Notes (Discrete Probability Distributions)\nChapter 5 Notes (Continuous Probability Distributions)\nChapter 6 Notes (Estimation)\nChapter 7 Notes (One Sample Hypothesis Tests)\nChapter 8 Notes (Two Sample Hypothesis Tests)\nChapter 9 Notes (Nonparametric Methods)\nChapter 10 Notes (Categorical Methods)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STAT 320 - Biostatistics",
    "section": "",
    "text": "Instructor: Dr. David Gerard\nEmail: dgerard@american.edu\nOffice: DMTI 106E\n\n\nQ1 Learning Outcomes:\n \n\nStudents will solve quantitative problems including approaches that go beyond memorized procedures.\nStudents will demonstrate an understanding of mathematical relationships from multiple perspectives, such as functions from graphical, verbal, numerical, and analytic points of view.\n\n\n\nQ2 Learning Outcomes:\n \n\nTranslate real-world questions or intellectual inquiries into quantitative frameworks.\nSelect and apply appropriate quantitative methods or reasoning.\nDraw appropriate insights from the application of a quantitative framework.\nExplain quantitative reasoning and insights using appropriate forms of representation so that others could replicate the findings.\n\n\n\nCourse Description\nSTAT-320 is an introduction to the statistical methodology commonly used in public health, medical, and biological studies. This course emphasizes working with data and communicating statistical ideas. A breadth of topics will be covered including: study design, tests of significance, confidence intervals, t-procedures, chi-square and Fisher’s exact test, linear regression, logistic regression, analysis of variance, nonparametric methods, and more advanced topics as time permits. The R computer program will be used to conduct analyses.\nThe major focus for this course is the ideas behind, and the methods for, drawing conclusions about a population from a sample. At the end of this course you will be expected to identify the major concepts related to statistical reasoning and to statistical inferences for drawing such conclusions, recognize how these concepts are used in disciplines related to health and medicine, and implement the methods yourself in statistical analyses using the methods covered. In particular, you are expected to be able to (1) identify the appropriate statistical model or models for a given analysis, (2) write the model in the correct notation, (3) implement the model in the R software package on a given set of data, (4) interpret the output in the context of the study, (5) diagnose model deficiences, (6) suggest improvements to the model if necessary, and (7) summarize the results of the analysis. Work will be a balance between understanding the concepts underlying a method, implementation of the method, and interpretation of the results.\n\n\nRequired Text\n\nRosner, B. (2016) Fundamentals of Biostatistics, Eighth Edition. Brooks/Cole, Boston, MA, USA.\n\n\nThere will be occasional readings from other sources, such as journal articles, for class discussion or for homework assignments. These will be posted in Canvas or links will be given to find these online.\n\n\n\nGrading\n\n\n\n\n\nAssignment\nPercent\n\n\n\n\nHomeworks\n20%\n\n\nParticipation\n20%\n\n\nExams 1, 2, and 3\n60%\n\n\n\n\n\n\n\n\n\nParticipation:\n\nShow up to class. Stay off your phones. Engage with the in-class exercises. You don’t need to complete or turn in the exercises—just make a genuine effort to try them.\nParticipation points will only be deducted under the following circumstances:\n\nYou miss many classes without explanation. Occasional absences are fine. I will take attendance most days, and if you start missing a lot of class, I’ll send you warnings about the impact on your grade before deducting any points.\nYou’re not making a good-faith effort on in-class exercises. For example, if you’re clearly working on something else, I’ll make a note and begin sending you warnings before deducting points. Again, you just need to try the exercises—you don’t need to complete them perfectly.\nYou engage in behavior that is clearly disrespectful to me or your classmates. This includes things like repeated interruptions or dismissive comments. Our classroom is a space for learning, where everyone is respected and discourse remains civil and scholarly.\n\n\nExams\n\nExams are not officially cumulative, but since statistical concepts build on one another, they are effectively cumulative.\nYou may bring one handwritten reference sheet (8.5’’ × 11’’, both sides). Typed sheets are not allowed.\nNo other resources are permitted.\n\nIf you touch your calculator, phone, computer, smartwatch, smart glasses, or any similar device during the exam, it will result in an automatic fail for the course.\n\nI will drop your lowest exam score. Because of this:\n\nNo make-up exams will be offered. If you miss an exam, it will count as your drop.\nYou may not leave the room during the exam unless you are turning it in.\n\nIf you leave mid-exam (even for an emergency), it will count as your drop.\nIf you are unable to remain in the room for the full 1 hour and 15 minutes, please contact ASAC to request an official accommodation.\n\nIf you miss two exams, you should consider withdrawing from the course. The last day to withdraw is October 31, 2025.\n\n\nHomeworks\n\nHomework assignments are designed to reinforce your understanding of course concepts and to help you prepare for the exams. You are permitted to use generative AI tools (e.g., ChatGPT) to assist with homework. However, I strongly recommend using such tools only after you have made a sincere effort to solve the problems on your own, such as checking your work or seeking clarification.\nEducational research (e.g. https://doi.org/10.1177/1529100612453266) consistently shows that actively working through practice problems is among the most effective ways to learn quantitative material. In contrast, passively reading solutions—whether written by others or generated by AI—offers minimal learning benefit. If you rely primarily on AI-generated solutions, you may not be adequately prepared for the exams.\nTo allow some flexibility, your lowest homework score will be dropped.\n\n\nUsual grade cutoffs will be used:\n\n\n\n\n\nGrade\nLower\nUpper\n\n\n\n\nA\n93\n100\n\n\nA-\n90\n92\n\n\nB+\n88\n89\n\n\nB\n83\n87\n\n\nB-\n80\n82\n\n\nC+\n78\n79\n\n\nC\n73\n77\n\n\nC-\n70\n72\n\n\nD\n60\n69\n\n\nF\n0\n59\n\n\n\n\n\nIndividual assignments will not be curved. However, at the discretion of the instructor, the overall course grade at the end of the semester may be curved.\n\n\nStudy Recommendations\nIf you are interested, the study techniques in https://doi.org/10.1177/1529100612453266 are listed as\n\nHigh Utility:\n\nPractice testing\n\nSelf-testing or taking practice tests over to-be-learned material\n\nDistributed practice\n\nImplementing a schedule of practice that spreads out study activities over time\n\n\nModerate Utility:\n\nElaborative interrogation:\n\nGenerating an explanation for why an explicitly stated fact or concept is true\n\nSelf-explanation:\n\nExplaining how new information is related to known information, or explaining steps taken during problem solving\n\nInterleaved practice\n\nImplementing a schedule of practice that mixes different kinds of problems, or a schedule of study that mixes different kinds of material, within a single study session\n\n\nLow Utility:\n\nRereading\n\nRestudying text material again after an initial reading\n\nSummarization\n\nWriting summaries (of various lengths) of to-be-learned texts\n\nHighlighting\n\nMarking potentially important portions of to-be-learned materials while reading\n\nThe keyword mnemonic\n\nUsing keywords and mental imagery to associate verbal materials\n\nImagery use for text learning\n\nAttempting to form mental images of text materials while reading or listening\n\n\n\n\n\nLate Work Policy\n\nAll assignments must be submitted on the day they are due.\nEach student will have two three-day extensions, where you can turn in the assignment on Thursday by end-of-day.\nPlease just let me know ahead of time that you will be using one of your two extensions.\nPlease do not tell me why you need the extension. Any reason is a fine reason.\nAny homeworks not submitted by the due date will receive a grade of 0.\n\n\n\nImportant Dates\n\n09/01: Labor Day (no classes or office hours).\n09/29: (tentative): Exam 1 (Chapters 1 through 5)\n10/30: (tentative): Exam 2 (Chapters 6 though 8)\n10/31: Last day to withdraw.\n11/24: Classes meat online via Zoom (or a recorded lecture). No in-person class.\n11/27: Thanksgiving break (no classes or office hours).\n12/11 from 09:25 AM – 10:40 AM: Exam 3 (Chapters 9 through 11).\n\n\n\nComputing and Software\nWe will use the R computing language to complete some assignment questions. R is free and may be downloaded from the R website (http://cran.r-project.org/). In addition, I highly recommend you interface with R through the free RStudio IDE (https://www.rstudio.com/). R and RStudio are also available on computers in the Anderson Computing Complex in addition to various labs across campus. R Studio may also be run from your web browser using American University’s Virtual Applications System. Please see me during office hours if you have questions regarding R.\n\n\nData\nData sets for homeworks assignments and examples from the textbook are available on the Data page. Almost all of these are cleaned versions of the data from the book’s companion website.\n\n\nAcademic Integrity\n\nStandards of academic conduct are set forth in the university’s Academic Integrity Code. By registering for this course, students have acknowledged their awareness of the Academic Integrity Code and they are obliged to become familiar with their rights and responsibilities as defined by the Code. Violations of the Academic Integrity Code will not be treated lightly and disciplinary action will be taken should violations occur. This includes cheating, fabrication, and plagiarism.\nI expect you to work with others and me, and I expect you to use online resources as you work on your assignments. However, your submissions must be composed of your own thoughts, coding, and words. You should be able to explain your work on assignments/projects and your rationale. Based on your explanation (or lack thereof), I may modify your grade.\nYou can use generative AI (e.g. ChatGPT, CoPilot, etc) on the homeworks if you want. But\n\nThese are your only study exercises for the exams. So I wouldn’t use AI to do them for me except to check my work after I am done.\nYou are still expected to “own” all of your responses. I reserve the right to ask you to explain any of your solutions. If you write something weird or too advanced in the homework, I’ll call you in and ask you questions about it. Based on your explanation (or lack thereof), I may modify your grade.\n\nNo resources are allowed for the exam except the 1 page (8.5’’ by 11’’) handwritten cheat sheet (and a pen or pencil, of course). If you touch your phone/computer/smart watch/smart glasses/etc during the exam then that is an automatic fail for the course.\nAll solutions that I provide are under my copyright. These solutions are for personal use only and may not be distributed to anyone else. Giving these solutions to others, including other students or posting them on the internet, is a violation of my copyright and a violation of the student code of conduct.\n\n\n\nSharing Course Content:\nStudents are not permitted to make visual or audio recordings (including livestreams) of lectures or any class-related content or use any type of recording device unless prior permission from the instructor is obtained and there are no objections from any student in the class. If permission is granted, only students registered in the course may use or share recordings and any electronic copies of course materials (e.g., PowerPoints, formulas, lecture notes, and any discussions – online or otherwise). Use is limited to educational purposes even after the end of the course. Exceptions will be made for students who present a signed Letter of Accommodation from the Academic Support and Access Center. Further details are available from the ASAC website.\n\n\nUse of Student Work\nThe professor will use academic work that you complete for educational purposes in this course during this semester. Your registration and continued enrollment constitute your consent.\n\n\nSyllabus Change Policy\nThis syllabus is a guide for the course and is subject to change with advanced notice. These changes may come via email or Canvas. Make sure to check Canvas and your university-supplied email regularly. You are accountable for all such communications."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#learning-objectives",
    "href": "00_course_outline/00_course_outline.html#learning-objectives",
    "title": "Course Outline for Stat 320",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nThree aspects of Statistics\nPopulation/Sample"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics",
    "href": "00_course_outline/00_course_outline.html#statistics",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nData — Numerical or qualitative descriptions of people/places/things that we want to study."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-1",
    "href": "00_course_outline/00_course_outline.html#statistics-1",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nStatistics — the field of answering questions using data.\nSome examples\n\nLead Exposure\n\nData: Retrospective study measuring lead exposure in children along with variou outcome variables like IQ score, different measures of neurological function, and hyperactivity assessmenets.\nQuestion: What are the neurological and behavioral effects of lead exposure in young children?\n\nSmoking and bone density\n\nData: Pairs of twins, one of whom is a lighter smoker and one of whome is a heavier smoker. Different bone density measures were taken.\nQuestion: Do individuals who smoke have lower densities?"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#statistics-2",
    "href": "00_course_outline/00_course_outline.html#statistics-2",
    "title": "Course Outline for Stat 320",
    "section": "Statistics",
    "text": "Statistics\nThree aspects:\n\nData Design\nData Description\nData Inference — informed by Probability"
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-design",
    "href": "00_course_outline/00_course_outline.html#data-design",
    "title": "Course Outline for Stat 320",
    "section": "Data Design",
    "text": "Data Design\nWhere do we get data?\n\nWhat is the proper way to collect data?\nWhen can we claim a causal connection between variables? (e.g. Does smoking lead to lower bone density? Does lead lead to increased neurological and behavioral problems?)\nWhat are some sources of bias (unwanted systematic tendencies in the data collection)?\nOnly touched on in this course."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-description",
    "href": "00_course_outline/00_course_outline.html#data-description",
    "title": "Course Outline for Stat 320",
    "section": "Data Description",
    "text": "Data Description\nHow do we describe the data we have?\n\nNumerical summaries — use numbers to describe the data.\nGraphical summaries — use pictures to describe the data.\nExploratory data analysis — play with the data to get a “feel” for it.\nLots of R.\nFirst week of the semester."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#data-inference-probability",
    "href": "00_course_outline/00_course_outline.html#data-inference-probability",
    "title": "Course Outline for Stat 320",
    "section": "Data Inference (Probability)",
    "text": "Data Inference (Probability)\nHow can we tell if our conclusions from the exploratory data analysis are real?\n\nLast thirteen weeks of the semester.\nProbability — subdiscipline of Mathematics that provides a foundation for modeling chance events.\nInference — describing a population (probabilistically) by using information from sample."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#population",
    "href": "00_course_outline/00_course_outline.html#population",
    "title": "Course Outline for Stat 320",
    "section": "Population",
    "text": "Population\nStatisticians (among others) are interested in characteristics of a large group of people/countries/objects\n\nCharacterize/describe neurological and behavioral health of young children\nCharacterize/describe bone health of smokers.\nCharacterize/describe the effectiveness of a drug on a all adults.\n\nA population is a group of individuals/objects/locations for which you want information."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#sample",
    "href": "00_course_outline/00_course_outline.html#sample",
    "title": "Course Outline for Stat 320",
    "section": "Sample",
    "text": "Sample\nIt is usually expensive/impossible to measure characteristics of every case in a population.\nA sample is a subgroup of individuals/objects/locations of the population.\n\nMeasure lead intake and different measures of neurological and behavioral health in a sample of 124 children.\nFind a group of 41 twins who have different smoking behaviors and compare their bone densities."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference",
    "href": "00_course_outline/00_course_outline.html#inference",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\nFrom the sample, describe the population using probability.\n\nWe have strong evidence that lighter smoking twins have heavier lumbar spine bone density than heavier smoking twins (pair \\(t\\)-test \\(p = 0.006494\\)). The corresponding 95% confidence interval for the difference in bone density is 0.00799 g/cm2 0.06372 g/cm2. Since the twins are not a random sample, the generalizability of this result depends on how representative the twins are of the general population of interest. Since this is an observational study and not an experiment, the statistics alone cannot make a claim for causality — such a claim would have to depend on other arguments."
  },
  {
    "objectID": "00_course_outline/00_course_outline.html#inference-1",
    "href": "00_course_outline/00_course_outline.html#inference-1",
    "title": "Course Outline for Stat 320",
    "section": "Inference",
    "text": "Inference\n\nIn this class, we will learn how to formulate such statements and interpret them."
  },
  {
    "objectID": "01_r/01_quarto.html",
    "href": "01_r/01_quarto.html",
    "title": "Introduction to Quarto ",
    "section": "",
    "text": "Quarto  is a file format that is a combination of plain text and R code.\nLots of great educational material is available at https://quarto.org/\nYou write code and commentary of code in one file. You may then compile (RStudio calls this “rendering”) the Quarto  file to many different kinds of output: pdf (including beamer presentations), html (including various presentation formats), Word, PowerPoint, etc.\nQuarto  is useful for:\n\nCommunication of statistical results.\nCollaborating with other data scientists.\nUsing it as a modern lab notebook to do data science.\n\nQuarto  can also make “literate programming” documents for python, Julia, JavaScript, etc…\n\n\n\n\nInstall Quarto  via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/\n\n\n\n\n\n\nOpen up a new Quarto  file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto  file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto  contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto  document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto  file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto :\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: today\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_quarto.html#getting-statrted",
    "href": "01_r/01_quarto.html#getting-statrted",
    "title": "Introduction to Quarto ",
    "section": "",
    "text": "Install Quarto  via: https://quarto.org/docs/get-started/\nTo make PDF files, you will need to install \\(\\LaTeX\\) if you don’t have it already. To install it, type in R:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nIf you get an error while trying to install tinytex, try manually installing  instead:\n\nFor Windows users, go to http://miktex.org/download\nFor OS users, go to https://tug.org/mactex/\nFor Linux users, go to https://www.tug.org/texlive/"
  },
  {
    "objectID": "01_r/01_quarto.html#playing-with-quarto",
    "href": "01_r/01_quarto.html#playing-with-quarto",
    "title": "Introduction to Quarto ",
    "section": "",
    "text": "Open up a new Quarto  file:\n\n \n\nChoose the options for the type of output you want\n\n \n\nYou should now have a rudimentary Quarto  file.\nSave a copy of this file in your “analysis” folder in the “week1” project.\nQuarto  contains three things\n\nA YAML (Yet Another Markup Language) header that controls options for the Quarto  document. These are surrounded by ---.\nCode chunks — bits of R code that that are surrounded by ```{r} and ```. Only valid R code should go in here.\nPlain text that contains simple formatting options.\n\nAll of these are are displayed in the default Quarto  file. You can compile this file by clicking the “Render” button at the top of the screen or by typing CONTROL + SHIFT + K. Do this now.\n\n\n\n\nHere is Hadley’s brief intro to formatting text in Quarto :\n\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](img.png){fig-alt=\"accessibility text\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n\n\n\n\n\n\nYou can insert new code-chunks using CONTROL + ALT + I (or using the “Insert” button at the top of RStudio).\nYou write all R code in chunks. You can send the current line of R code (the line where the cursor is) using CONTROL + ENTER (or the “Run” button at the top of RStudio).\nYou can run all of the code in a chunk using CONTROL + ALT + C (or using the “Run” button at the top of RStudio).\nYou can run all of the code in the next chunk using CONTROL + ALT + N (or using the “Run” button at the top of RStudio).\n\n\n\n\n\nMy typical YAML header will looks like this\n\n\n---\ntitle: \"Week 1 Worksheet: Installing R, Rmarkdown, Rbasics\"\nauthor: \"David Gerard\"\ndate: today\nformat: pdf\nurlcolor: \"blue\"\n---\n\n\nAll of those settings are fairly self-explanatory.\n\n\n\n\n\nSometimes, you want to write the output of some R code inline (rather than as the output of some chunk). You can do this by placing code within `r `.\nI used this in the previous section for automatically writing the date.\n\nmy_name &lt;- \"David\"\n\nThen “my name is `r my_name`” will result in “my name is David”.\nFor a more realistic example, you might calculate the \\(p\\)-value from a linear regression, then write this \\(p\\)-value in the paragraph of a report."
  },
  {
    "objectID": "01_r/01_ggplot.html",
    "href": "01_r/01_ggplot.html",
    "title": "R Graphics with {ggplot2}",
    "section": "",
    "text": "Basic plotting in R using the {ggplot2} package."
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous",
    "href": "01_r/01_ggplot.html#continuous",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous",
    "text": "Continuous\n\nHistogram:\n\nVariable should be on the \\(x\\)-axis.\nUse the geom_histogram() function.\n\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nMake the bin lines black and the fill white, and change the number of bins.\n\nggplot(data = lead, mapping = aes(x = iqf)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\")\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nLoad in the boneden data (see here for a description) and make a histogram of lumbar spine density for the lighter smoking twin with 20 bins. Make the bins red.\n\n\n\nlibrary(readr)\nlibrary(ggplot2)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nRows: 41 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): zyg, men1, men2\ndbl (22): ID, age, ht1, wt1, tea1, cof1, alc1, cur1, pyr1, ls1, fn1, fs1, ht...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(data = boneden, mapping = aes(x = ls1)) +\n  geom_histogram(bins = 20, fill = \"red\")"
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete",
    "href": "01_r/01_ggplot.html#discrete",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete",
    "text": "Discrete\n\nBarplot:\n\nPut the variable on the \\(x\\)-axis.\nUse geom_bar().\n\n\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nWhat variables from the lead data are appropriately plotted using a bar plot? Plot a couple of them.\n\n\n\nggplot(data = lead, mapping = aes(x = sex)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = area)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = lead_grp)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = Group)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = fst2yrs)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = iq_type)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = pica)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = colic)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = clumsi)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = irrit)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = convul)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = hyperact)) +\n  geom_bar()\nggplot(data = lead, mapping = aes(x = maxfwt)) +\n  geom_bar()"
  },
  {
    "objectID": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "href": "01_r/01_ggplot.html#continuous-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Continuous X, Continuous Y",
    "text": "Continuous X, Continuous Y\n\nScatterplot:\n\nSay what variables should be on the \\(x\\)- and \\(y\\)-axes.\nUse geom_point().\n\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point()\n\n\n\n\n\n\n\n\nJitter points to account for overlaying points.\n\nUse geom_jitter() instead of geom_point().\n\n\nggplot(data = lead, mapping = aes(x = totyrs, y = hyperact)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nAdd a Loess Smoother by adding geom_smooth().\n\nggplot(data = lead, mapping = aes(x = ld73, y = iqf)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nUsing the boneden data, make a scatterplot exploring the association between the lumbar spine densities for the two twin types (smoking status).\n\n\n\nggplot(data = boneden, mapping = aes(x = ls1, y = ls2)) +\n  geom_point()"
  },
  {
    "objectID": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "href": "01_r/01_ggplot.html#discrete-x-continuous-y",
    "title": "R Graphics with {ggplot2}",
    "section": "Discrete X, Continuous Y",
    "text": "Discrete X, Continuous Y\n\nBoxplot\n\nPlace one variable on \\(x\\)-axis and other on \\(y\\)-axis.\nTypically, but not always, continuous goes on \\(y\\)-axis.\nUse geom_boxplot().\n\n\nggplot(data = lead, mapping = aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nUsing the boneden data, first calculate the difference in lumbar spine densities between the two twins (you’ll need to use mutate() here). Then plot this difference versus zygosity (monozygotic versus dizygotic).\n\n\n\nboneden &lt;- mutate(boneden, ls_diff = ls1 - ls2)\nggplot(data = boneden, mapping = aes(x = zyg, y = ls_diff)) +\n  geom_boxplot()"
  },
  {
    "objectID": "01_r/01_figures/formatting.html",
    "href": "01_r/01_figures/formatting.html",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#text-formatting",
    "href": "01_r/01_figures/formatting.html#text-formatting",
    "title": "1st Level Header",
    "section": "",
    "text": "italic bold strikeout code\nsuperscript2 subscript2\nunderline small caps"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#headings",
    "href": "01_r/01_figures/formatting.html#headings",
    "title": "1st Level Header",
    "section": "Headings",
    "text": "Headings"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#nd-level-header",
    "href": "01_r/01_figures/formatting.html#nd-level-header",
    "title": "1st Level Header",
    "section": "2nd Level Header",
    "text": "2nd Level Header\n\n3rd Level Header"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#lists",
    "href": "01_r/01_figures/formatting.html#lists",
    "title": "1st Level Header",
    "section": "Lists",
    "text": "Lists\n\nBulleted list item 1\nItem 2\n\nItem 2a\nItem 2b\n\n\n\nNumbered list item 1\nItem 2. The numbers are incremented automatically in the output."
  },
  {
    "objectID": "01_r/01_figures/formatting.html#links-and-images",
    "href": "01_r/01_figures/formatting.html#links-and-images",
    "title": "1st Level Header",
    "section": "Links and images",
    "text": "Links and images\nhttp://example.com\nlinked phrase\n\n\n\noptional caption text"
  },
  {
    "objectID": "01_r/01_figures/formatting.html#tables",
    "href": "01_r/01_figures/formatting.html#tables",
    "title": "1st Level Header",
    "section": "Tables",
    "text": "Tables\n\n\n\nFirst Header\nSecond Header\n\n\n\n\nContent Cell\nContent Cell\n\n\nContent Cell\nContent Cell"
  },
  {
    "objectID": "02_descriptive/02_descriptive.html",
    "href": "02_descriptive/02_descriptive.html",
    "title": "Summary Statistics in R",
    "section": "",
    "text": "We’ll use the lead data as an example. Read about it here.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nYou calculate the summary statistics (mean/median/quantiles/variance/standard deviation) all within a summarize() call.\n\nsummarize(\n  lead, \n  Mean = mean(iqf, na.rm = TRUE), \n  Min = min(iqf, na.rm = TRUE),\n  Q25 = quantile(iqf, probs = 0.25, na.rm = TRUE),\n  Med = median(iqf, na.rm = TRUE), \n  Q75 = quantile(iqf, probs = 0.75, na.rm = TRUE),\n  Max = max(iqf, na.rm = TRUE),\n  Var = var(iqf, na.rm = TRUE),\n  SD = sd(iqf, na.rm = TRUE)\n)\n\n# A tibble: 1 × 8\n   Mean   Min   Q25   Med   Q75   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  90.8    46    80  89.5  98.5   141  212.  14.6\n\n\nThe values on the left of = are the names of the summaries and are up to you.\nThe values on the right of = are the function calls for the summaries.\n\nmean(): the arithmetic mean.\nmin(): the minimum. Same as quantile(x, probs = 0)\nquantile(): the quantiles. You specify which quantile with the probs argument.\nmedian(): the median. Same as quantile(x, probs = 0.5)\nmax(): the maximum. Same as quantile(x, probs = 1)\nvar(): the sample variance.\nsd(): the sample standard deviation.\n\nI have the na.rm = TRUE argument because there are some children who did not have a iqf score. These are “missing” and encoded with NA. If you do not provide that argument, R doesn’t know what those values are and so returns an NA or errors.\n\nsummarize(\n  lead, \n  Mean = mean(iqf), \n  Min = min(iqf),\n  # Q25 = quantile(iqf, probs = 0.25), # errors\n  Med = median(iqf), \n  # Q75 = quantile(iqf, probs = 0.75), # errors\n  Max = max(iqf),\n  Var = var(iqf),\n  SD = sd(iqf)\n)\n\n# A tibble: 1 × 6\n   Mean   Min   Med   Max   Var    SD\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    NA    NA    NA    NA    NA    NA\n\n\nYou can also apply these functions on vectors that you extract from the data frame.\n\nvar(lead$iqf, na.rm = TRUE)\n\n[1] 212.3\n\n\nLet’s demonstrate some properties. Variance is invariant to shift\n\nvar(lead$iqf + 10000, na.rm = TRUE)\n\n[1] 212.3\n\n\nbut scales with the square of the multiplicative factor\n\nvar(10 * lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n10^2 * var(lead$iqf, na.rm = TRUE)\n\n[1] 21227\n\n\nThe standard deviation scales with the multiplicative factor because it is the square root of the variance.\n\nsd(10 * lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n10 * sd(lead$iqf, na.rm = TRUE)\n\n[1] 145.7\n\n\nThe mean and quantiles shift and scale with the additive and multiplicative factors.\n\nmean(lead$iqf * 10 + 20, na.rm = TRUE)\n\n[1] 928.2\n\n10 * mean(lead$iqf, na.rm = TRUE) + 20\n\n[1] 928.2\n\nquantile(lead$iqf * 10 + 20, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)\n\n 25%  50%  75% \n 820  915 1005 \n\n10 * quantile(lead$iqf, probs = c(0.25, 0.5, 0.75), na.rm = TRUE) + 20\n\n 25%  50%  75% \n 820  915 1005 \n\n\nExercise: Calculate the mean and median of the birthweight data. What is the more appropriate measure of center?\nYou can calculate grouped summaries (a summary for each group) by grouping the data first.\n\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(\n    Mean = mean(iqf, na.rm = TRUE),\n    SD = sd(iqf, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 3\n  Group    Mean    SD\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 control  92.6  15.7\n2 exposed  88.0  12.2\n\n\nGroup summaries are where the power of descriptive statistics really comes into play. Here, we see that the exposed group has a lower IQ on average than the control group. Whether this is real signal will have to be answered via a formal hypothesis test. But the descriptive statistics gives us some initial information on the data.\nExercise: What about different lead groups? Calculate descriptive statistics for the different lead groups."
  },
  {
    "objectID": "hw/hw_ch2/hw_ch2.html",
    "href": "hw/hw_ch2/hw_ch2.html",
    "title": "Homework 01L Chapters 1 and 2",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives: - Chapter 2 of Rosner - Descriptive Statistics - Graphics - R\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: &lt; https://www.adobe.com/acrobat/online/merge-pdf.html&gt;\nUse the tidyverse way to answer these questions. If you use the base R way, that’s fine, but I’ll call you in and have you show me that you really know how the base R way works by doing some practice problems in front of me.\n\n\nQuestion 1: Cardiovascular Disease\nRead in the data using the read_csv() function, which you can read about here: https://dcgerard.github.io/stat_320/data.html#lvm\n\nlibrary(tidyverse)\nlvm &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lvm.csv\")\n\n\nUse R to print what variables are in lvm?\n\n\nnames(lvm)\n\n[1] \"ID\"      \"lvmht27\" \"bpcat\"   \"gender\"  \"age\"     \"BMI\"    \n\n\n\nWhat is the arithmetic mean of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(Mean = mean(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat             Mean\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      34.1\n2 normal            29.3\n3 pre-hypertensive  33.8\n\n\n\nIs it appropriate to use the arithmetic mean (by blood pressure group) for LVMI, or should we have used the median? Explain your reasoning and justify with an appropriate box plot.\n\n\nggplot(lvm, aes(x = bpcat, y = lvmht27)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n## Since the distributions are mostly symmetric, and there \n## do not appear to be any major outliers, it looks like\n## the sample mean is an appropriate measure of center.\n\n\nWhat does the boxplot from the previous questiontell you?\n\n\n## The center of group 3 is larger than group 2 than group 1. \n## But groups 1 and 3 are more spread out than group 2. All \n## groups appear to have roughly symmetric distributions.\n\n\nWhat is the standard deviation of LVMI by blood pressure group?\n\n\nlvm |&gt;\n  group_by(bpcat) |&gt;\n  summarize(SD = sd(lvmht27))\n\n# A tibble: 3 × 2\n  bpcat               SD\n  &lt;chr&gt;            &lt;dbl&gt;\n1 hypertensive      8.56\n2 normal            6.66\n3 pre-hypertensive  5.75\n\n\n\nDoes there appear to be any association between age and lvmht27? Use an appropriate plot to make your case.\n\n\nggplot(lvm, aes(x = age, y = lvmht27)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x)\n\n\n\n\n\n\n\n## Maybe a weak positive association.\n\n\n\nValidity Data\nRead about the validity study on the food frequency questionnaire here: https://dcgerard.github.io/stat_320/data.html#valid\n\nThe data are available at &lt;&gt;. Load these data into R as the valid data frame.\n\n\nvalid &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/valid.csv\")\n\nRows: 173 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Id, sfat_dr, sfat_ffq, tfat_dr, tfat_ffq, alco_dr, alco_ffq, cal_dr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nUse descriptive statistics to relate nutrient intake for the DR and FFQ. Do you think the FFQ is a reasonably ac- curate approximation to the DR? Why or why not?\nA frequently used method for quantifying dietary intake is in the form of quintiles. Compute quintiles for each nutri- ent and each method of recording, and relate the nutrient composition for DR and FFQ using the quintile scale. (That is, how does the quintile category based on DR relate to the quintile category based on FFQ for the same individual?) Do you get the same impression about the concordance between DR and FFQ using quintiles as in the previous problem, in which raw (ungrouped) nutrient intake is considered?\nIn nutritional epidemiology, it is customary to assess nutrient intake in relation to total caloric intake. One measure used to accomplish this is nutrient density, which is defined as 100% × (caloric intake of a nutrient/total caloric intake). For fat consumption, 1 g of fat is equivalent to 9 calories. Compute the nutrient density for total fat for the DR and FFQ.\n\n\nvalid |&gt;\n  mutate(nd_dr = tfat_dr * 9 / cal_dr * 100,\n         nd_ffq = tfat_ffq * 9 / cal_ffq * 100) -&gt;\n  valid\n\n\nObtain appropriate descriptive statistics for nutrient density for both DR and FFQ. How do they compare?\n\n\nRelate the nutrient density for total fat for the DR versus the FFQ using the quintile approach in Problem 2.28. Is the concordance between total fat for DR and FFQ stronger, weaker, or the same when total fat is expressed in terms of nutrient density as opposed to raw nutrient?"
  },
  {
    "objectID": "03_prob/03_prob.html",
    "href": "03_prob/03_prob.html",
    "title": "Probability Definitions and Properties",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution"
  },
  {
    "objectID": "03_prob/03_prob_discrete.html",
    "href": "03_prob/03_prob_discrete.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "library(tidyverse)\n\n\nProvided Distribution\nIf given a probability mass function, can create a data frame of it\n\npmf &lt;- tibble(r = 0:4,\n       pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\nExercise: The underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\nExercise: Suppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\n\nWhat is the expected number of such women (out of 100) that we would expect to die in th next year?\n\n\n\n\nPoisson Distribution\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts."
  },
  {
    "objectID": "03_prob/03_confusion.html",
    "href": "03_prob/03_confusion.html",
    "title": "Confusion Matrix",
    "section": "",
    "text": "The below is a simplified version of the Confusion matrix table from Wikipedia that emphasizes the terminology more common to biostatistics (e.g. sensitivity/specificity/\\(PV^+\\)/\\(PV^-\\))\n\n\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\nTest Positive \\(T^+\\)\n\n\nTest Negative \\(T^-\\)\n\n\n\n\n\n\n\n\nTruth\n\n\n\nPositive \\(D^+\\)\n\n\nTrue Positive (TP)\n\n\nFalse Negative (FN)  (Type II Error)\n\n\nSensitivity  (True Positive Rate, Recall, Power)  \\(\\frac{TP}{D^+}\\)\n\n\nFalse Negative Rate  (Type II Error Rate)  \\(\\frac{FN}{D^+}\\)\n\n\n\n\nNegative \\(D^-\\)\n\n\nFalse Positive (FP)  (Type I Error)\n\n\nTrue Negative (TN)\n\n\nFalse Positive Rate  (Type I Error Rate)  \\(\\frac{FP}{D^-}\\)\n\n\nSpecificity (True Negative Rate)  \\(\\frac{TN}{D^-}\\)\n\n\n\n\n\n\nPrevalence  \\(\\frac{D^+}{D^+ + D^-}\\)\n\n\nPositive Predictive Value  (Precision)  \\(PV^+ = \\frac{TP}{T^+}\\)\n\n\nFalse Omission Rate  \\(\\frac{FN}{T^-}\\)\n\n\n\n\n\n\n\n\n\n\nFalse Discovery Rate  \\(\\frac{FP}{T^+}\\)\n\n\nNegative Predictive Value  \\(PV^- = \\frac{TN}{T^-}\\)"
  },
  {
    "objectID": "03_prob/03_prob_cont.html",
    "href": "03_prob/03_prob_cont.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "The density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] -1.0742  0.3714  1.6070  0.2678  0.4236 -0.8769\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2"
  },
  {
    "objectID": "hw/hw_ch5/hw_ch5.html",
    "href": "hw/hw_ch5/hw_ch5.html",
    "title": "Homework, Chapter 4",
    "section": "",
    "text": "Learning Objectives and Instructions\nLearning objectives:\n\nChapter 5 of Rosner\nNormal Distribution\n\nPlease turn in this homework as one document on Canvas. You can combine a scan of handwritten work and R work if you want via Adobe’s free merge website: https://www.adobe.com/acrobat/online/merge-pdf.html\nI will only grade a randomly chosen subset of these questions. Please complete all of them, since you don’t know which ones I will grade.\n\n\nBlood Chemistry\nIn pharmacologic research a variety of clinical chemistry measurements are routinely monitored closely for evidence of side effects of the medication under study. Suppose typical blood-glucose levels are normally distributed, with mean = 90 mg/dL and standard deviation = 38 mg/dL.\n\nIf the normal range is 65−120 mg/dL, then what percentage of values will fall in the normal range?\n\n\n# X ~ N(90, 38^2)\npnorm(q = 120, mean = 90, sd = 38) - pnorm(q = 65, mean = 90, sd = 38)\n\n[1] 0.5298\n\n\n\nIn some studies only values at least 1.5 times as high as the upper limit of normal are identified as abnormal. What percentage of values would fall in this range?\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 0.008932\n\n\n\nAnswer Problem 2 for values 2.0 times the upper limit of normal.\n\n\n# Calculate upper limit\nu &lt;- 120 * 2\nu\n\n[1] 240\n\n# Probability above this\npnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\n\n[1] 3.951e-05\n\n\n\nFrequently, tests that yield abnormal results are re- peated for confirmation. What is the probability that for a normal person a test will be at least 1.5 times as high as the upper limit of normal on two separate occasions? Assume the two tests are independent.\n\n\n# Calculate upper limit\nu &lt;- 120 * 1.5\nu\n\n[1] 180\n\n# Probability above this\np1 &lt;- pnorm(q = u, mean = 90, sd = 38, lower.tail = FALSE)\np1^2\n\n[1] 7.978e-05\n\n\n\nSuppose that in a pharmacologic study involving 6000 patients, 75 patients have blood-glucose levels at least 1.5 times the upper limit of normal on one occasion. What is the probability that this result could be due to chance? Assume the patient tests are independent.\n\n\n## Y = number of patients above 1.5 times upper limit\n## Y ~ Binom(6000, p1)\n## Want Pr(Y &gt;= 75)\n1 - pbinom(q = 74, size = 6000, prob = p1)\n\n[1] 0.003166\n\n## Only a 0.3% chance, so very unlikely\n\n\n\nOrthopedics\nA study was conducted of a diagnostic test (the FAIR test, i.e., hip flexion, adduction, and internal rotation) used to identify people with piriformis syndrome (PS), a pelvic condition that involves malfunction of the piriformis muscle (a deep buttock muscle), which often causes lumbar and buttock pain with sciatica (pain radiating down the leg) [7]. The FAIR test is based on nerve-conduction velocity and is expressed as a difference score (nerve-conduction velocity in an aggravating posture minus nerve-conduction velocity in a neutral posture). It is felt that the larger the FAIR test score, the more likely a participant will be to have PS. Data are given in the Data Set PIRIFORM.DAT for 142 participants without PS (piriform = 1) and 489 participants with PS (piriform = 2) for whom the diagnosis of PS was based on clinical criteria. The FAIR test value is called MAXCHG and is in milliseconds (ms). A cutoff point of ≥ 1.86 ms on the FAIR test is proposed to define a positive test.\n\nWhat is the sensitivity of the test for this cutoff point?\nWhat is the specificity of the test for this cutoff point?\nSuppose that 70% of the participants who are referred to an orthopedist who specializes in PS will actually have the condition. If a test score of ≥ 1.86 ms is obtained for a par- ticipant, then what is the probability that the person has PS?\nThe criterion of ≥ 1.86 ms to define a positive test is arbitrary. Using different cutoff points to define positivity, obtain the ROC curve for the FAIR test. What is the area under the ROC curve? What does it mean in this context?\nDo you think the distribution of FAIR test scores within a group is normally distributed? Why or why not?"
  },
  {
    "objectID": "04_est/04_sample.html",
    "href": "04_est/04_sample.html",
    "title": "Random Sampling",
    "section": "",
    "text": "Before doing sampling, make sure you set the seed so that you have reproducible results. E.g., this makes it so that your “random selection” is the same every time you first run the seed.\n\nset.seed(3574927)\n\nIf you are having trouble coming up with a random seed, you could NIST’s Interoperable Randomness Beacons API, which generates a new truly random number every 60 seconds. You would only run this once and copy the resulting number in your script. You would not include this script anywhere in any code you have.\n\n## HTTP request for a random number\nlibrary(httr2)\nreqout &lt;- request(base_url = \"https://beacon.nist.gov/beacon/2.0/pulse/last\") |&gt;\n  req_perform()\n\n## A random value represented as a 64-byte (512 bits) hex string\nhex &lt;- resp_body_json(reqout)$pulse$localRandomValue \n\n## select only first few digits to make number small. You can increase this.\nhexsmall &lt;- substr(hex, start = 1, stop = 6) \n\n## convert to an integer. This is your seed.\nstrtoi(x = hexsmall, base = 16) \n\nYou generate ID’s for with seq() or :\n\n# 1:100\nidvec &lt;- seq(from = 1, to = 20)\nidvec\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nYou can sample (without replacement) from this vector with sample()\n\nsample(x = idvec, size = 5)\n\n[1]  6 18 11  1  3\n\n\nIf you don’t give an argument for size, then sample will randomly permute the values.\n\nsample(x = idvec)\n\n [1]  1  5  6 16 13 18 19 17 20 12  7  8  3  2 14 11  4 10  9 15\n\n\nThis is useful for random assignment.\nYou should generally also randomize order, but if you need to see the group ID’s in an easier to read format, use sort().\n\nsample(x = idvec, size = 5) |&gt;\n  sort()\n\n[1]  4 10 12 13 15\n\n\nIf you are doing random assignment, you have a data frame of individuals. E.g., from the birthweight data.\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nThen you create a column with the number of groups via rep(), and then randomly permute it with sample(). E.g., suppose we want three groups:\n\nbirthweight |&gt;\n  mutate(group = rep(1:3, length.out = n())) |&gt; ## choose groups of equal size\n  mutate(group = sample(group)) ## randomly assign\n\n# A tibble: 1,000 × 3\n      id weight group\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1     0    116     2\n 2     1    124     1\n 3     2    119     2\n 4     3    100     1\n 5     4    127     3\n 6     5    103     2\n 7     6    140     1\n 8     7     82     3\n 9     8    107     3\n10     9    132     1\n# ℹ 990 more rows\n\n\n\n\nExerciseSolution\n\n\nRandomly assign 400 individuals (with IDs 1 through 400) into two groups, \"treatment\" and \"control\".\n\n\n\ntibble(id = 1:400) |&gt;\n  mutate(group = rep(c(\"treatmnet\", \"control\"), length.out = n())) |&gt; \n  mutate(group = sample(group))\n\n# A tibble: 400 × 2\n      id group    \n   &lt;int&gt; &lt;chr&gt;    \n 1     1 control  \n 2     2 control  \n 3     3 treatmnet\n 4     4 control  \n 5     5 treatmnet\n 6     6 control  \n 7     7 control  \n 8     8 control  \n 9     9 treatmnet\n10    10 treatmnet\n# ℹ 390 more rows\n\n\n\n\n\n\nExerciseSolution\n\n\nThe treatment is way more expensive than the control, so randomly assign only 100 to \"treatment\" and 300 to \"control\".\n\n\n\ntibble(id = 1:400) |&gt;\n  mutate(group = c(rep(\"treatment\", 100), rep(\"control\", 300))) |&gt; \n  mutate(group = sample(group))\n\n# A tibble: 400 × 2\n      id group    \n   &lt;int&gt; &lt;chr&gt;    \n 1     1 control  \n 2     2 treatment\n 3     3 control  \n 4     4 treatment\n 5     5 control  \n 6     6 treatment\n 7     7 control  \n 8     8 control  \n 9     9 control  \n10    10 treatment\n# ℹ 390 more rows"
  },
  {
    "objectID": "04_est/04_clt.html",
    "href": "04_est/04_clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Consider the birthweight data:\n\nlibrary(tidyverse)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\nggplot(birthweight, aes(x = weight)) +\n  geom_histogram(bins = 30, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nLet’s take samples of size \\(n\\) = 1, 5, 10, 20, 50, 100 from this distribution with replacement. For each sample size, we college 10000 repeat samples, each time calculating the sample mean \\(\\bar{X}\\), to obtain \\(\\bar{X}_1,\\bar{X}_2,\\ldots,\\bar{X}_{10000}\\). Below are histograms of these \\(\\bar{X}\\)’s\nThe distribution of the \\(\\bar{X}\\)’s has smaller and smaller variance as the sample size increases since \\(\\mathrm{var}(\\bar{X}) = \\sigma^2/n\\).\n \nThe distribution of the \\(\\bar{X}\\)’s gets closer to a normal as the sample size increases. Though it’s already sufficiently normal for most purposes once \\(n = 10\\).\n \nThe true mean \\(\\mu\\) is the vertical dashed red line. You see that the distribution of the sample mean has a mean of \\(\\mu\\), \\(\\mathrm{E}[\\bar{X}] = \\mu\\)."
  },
  {
    "objectID": "04_est/04_t.html",
    "href": "04_est/04_t.html",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_t.html#learning-objectives",
    "href": "04_est/04_t.html#learning-objectives",
    "title": "t-distribution",
    "section": "",
    "text": "Work with \\(t\\)-distribution\nUnderstand \\(t\\)-distribution"
  },
  {
    "objectID": "04_est/04_boneden.html",
    "href": "04_est/04_boneden.html",
    "title": "Bone Density Case Study",
    "section": "",
    "text": "Twin study with \\(n=41\\) where one smoked more than the other. Bone density was measured in both twins. More detail here.\n\nlibrary(tidyverse)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nLet’s explore if lumbar spine bone density differed between the twins. First, we’ll calculate the difference in density between the twins:\n\nboneden |&gt;\n  mutate(diff = ls1 - ls2) -&gt; #ls1 = lighter smoking, ls2 = heavy smoking\n  boneden \n\nLet’s plot the data\n\nggplot(boneden, aes(x = diff)) +\n  geom_histogram(bins = 7, fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the difference\n\nboneden |&gt;\n  summarize(xbar = mean(diff), s = sd(diff), n = n())\n\n# A tibble: 1 × 3\n    xbar      s     n\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 0.0359 0.0883    41\n\n\nWe can use this to get a 95% confidence interval for the mean difference in lumbar spine bone density between heavy and light smoking twins.\nThe standard error is\n\n0.08829 / sqrt(41)\n\n[1] 0.01379\n\n\nThe appropriate quantile of the t-distribution is\n\nqt(p = 0.975, df = 41 - 1)\n\n[1] 2.021\n\n\nSo, the 95% confidence interval is\n\n0.03585 - 2.021 * 0.01379 # lower\n\n[1] 0.00798\n\n0.03585 + 2.021 * 0.01379 # upper\n\n[1] 0.06372\n\n\nSince the lower bound of the 95% CI is above 0, we can be fairly confident that the true mean difference is greater than 0. That is, we are pretty sure that the lighter smoking twin has heavier bone density. We will formalize what “pretty sure” means in Chapter 7.\n\n\nReal Way\n\nIt would be crazy to do the above calculations, by hand, every time. For this class, I’ll occasionally ask you do that to solidify your understanding. But in real data analysis we use code to automate inference.\nWe will use the {broom} package to summarize inference output.\n\nlibrary(broom)\n\nYou calculate an interval for a mean using t.test() using one of two ways:\n\n## tout &lt;- t.test(boneden$diff)\ntout &lt;- t.test(diff ~ 1, data = boneden)\n\nYou get a summary of the output with broom::tidy()\n\ntidy(tout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359  0.00799    0.0637\n\n\nYou can change the level with the conf.level argument in t.test()\n\nt.test(diff ~ 1, data = boneden, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0359   0.0126    0.0591\n\n\n\n\nExerciseSolution\n\n\nCalculate an 80% confidence interval for the mean birth weight of newborns using the birth weight data that you can download from here: https://dcgerard.github.io/stat_320/data/birthweight.csv. Do this both “by hand” (after calculating the summary statistics and t-quantile) and using R’s automated functions.\n\n\nFirst, we load in the data\n\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nThe “by hand” way is of the form \\(\\bar{x} \\pm t_{n-1,1-\\alpha/2}s/\\sqrt{n}\\):\n\nn &lt;- nrow(birthweight)\nmult &lt;- qt(p = 0.9, df = n - 1)\nxbar &lt;- mean(birthweight$weight)\nse &lt;- sd(birthweight$weight) / sqrt(n)\nxbar - mult * se\n\n[1] 105.6\n\nxbar + mult * se\n\n[1] 108.1\n\n\nThis is implemented in t.test():\n\nt.test(weight ~ 1, data = birthweight, conf.level = 0.8) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     107.     106.      108."
  },
  {
    "objectID": "04_est/04_chi2.html",
    "href": "04_est/04_chi2.html",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/04_chi2.html#learning-objectives",
    "href": "04_est/04_chi2.html#learning-objectives",
    "title": "chi-squared distribution",
    "section": "",
    "text": "Work with \\(\\chi^2\\)-distribution\nUnderstand \\(\\chi^2\\)-distribution"
  },
  {
    "objectID": "04_est/binom.html",
    "href": "04_est/binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\nExercise: Of 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?"
  },
  {
    "objectID": "04_est/04_binom.html",
    "href": "04_est/04_binom.html",
    "title": "Estimates and Intervals for Binomial Proportions",
    "section": "",
    "text": "Normal approach\n\nWe want to estimate the rate of bladder cancer in rats that have been fed a diet high in saccharin. Of 20 rats fed this diet, 2 develop bladder cancer.\nLet \\(X\\) be the number of rats with bladder cancer. Then \\(X \\sim \\mathrm{Binom}(20, p)\\) (our observed \\(x = 2\\)) and our goal is to estimate \\(p\\).\nWe estimate \\(p\\) with \\(\\hat{p} = 2 / 20\\)\n\nphat = 2 / 20\nphat\n\n[1] 0.1\n\n\nThe standard error of this estimate is \\(\\sqrt{\\hat{p}(1-\\hat{p})/n} = \\sqrt{0.1 * (1 - 0.1)/20}\\)\n\nn &lt;- 20\nse &lt;- sqrt(phat * (1 - phat) / n)\nse\n\n[1] 0.06708\n\n\nSuppose we want a 90% confidence interval for this proportion. Then, assuming the normal approximation is OK (more on this later), we have \\(\\alpha = 1 - 0.9 = 0.1\\), so we need the \\(1 - \\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the standard normal distribution.\n\nz &lt;- qnorm(0.95)\nz\n\n[1] 1.645\n\n\nNow we can obtain an approximate 90% confidence interval by \\(\\hat{p} \\pm z_{0.95} \\sqrt{\\hat{p}(1 - \\hat{p}) / n}\\)\n\nphat - z * se\n\n[1] -0.01034\n\nphat + z * se\n\n[1] 0.2103\n\n\n\n\n\nNormal Real way\n\nIt’s crazy to do this by hand in the real-world. You will only do this by hand for me to show me you understand the concepts. Real folks use code to automate this interval procedure. We will do so with the {broom} package\n\nlibrary(tidyverse)\nlibrary(broom)\n\nYou use the prop.test() function, optionally providing it with a specified confidence level (default of 0.95) and feed the output into tidy().\n\npout &lt;- prop.test(x = 2, n = 20, conf.level = 0.9)\ntidy(pout) |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0216     0.292\n\n\nThe results differ because Rosner teaches you the Wald interval, but the interval in R is the Wilson interval (because it works better). The Wilson interval also uses the normal approximation to the binomial, but does not substitute in the estimated standard error for the standard error. \\[\n\\mathrm{Pr}\\left(-z_{1-\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/2}}\\leq z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n\\] You then solve for \\(p\\) on both sides of the inequality. This involves a solving a quadratic equation, which is not too hard.\nThe Wald and the Wilson approaches are approximately the same for large \\(n\\).\n\n# Wald\nn &lt;- 5000\nx &lt;- 2000\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.95)\nphat - z * se\n\n[1] 0.3886\n\nphat + z * se\n\n[1] 0.4114\n\n# Wilson\nprop.test(x = x, n = n, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.4    0.389     0.412\n\n\n\n\n\nExact approach\n\nThe rat bladder example above does not allow for a normal approximation using our rule-of-thumb, since \\(n\\hat{p}(1-\\hat{p}) = 1.8 &lt; 5\\). Thus, the above intervals would be suspect.\nThe exact approach finds a \\(p_1\\) such that \\(\\mathrm{Pr}(X \\leq x|p_1) = \\alpha/2\\) and a \\(p_2\\) such \\(\\mathrm{Pr}(X \\geq x|p_1) = \\alpha/2\\). The interval is then \\((p_1, p_2)\\).\n\n\nLet’s visualize finding this \\(p_1\\) an \\(p_2\\) for a 95% confidence interval where \\(\\alpha / 2 = 0.025\\).\nFind a \\(p_1\\) such that being greater than or equal to \\(x\\) is 0.025.\n \nFind a \\(p_2\\) such that being less than or equal to \\(x\\) is 0.025.\n \nIn practice, you do this using binom.test().\n\nbinom.test(x = 2, n = 20, conf.level = 0.95) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0123     0.317\n\nbinom.test(x = 2, n = 20, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.1   0.0181     0.283\n\n\n\n\nExerciseSolution\n\n\nOf 10 smokers who gave up smoking, 6 took it up again after a year. Provide an 80% confidence interval for the proportion of ex-smokers who take up smoking again after one year. Use both the normal approximation (by hand and in R) and the exact binomial approach (just in R). Does it matter which approach to use here?\n\n\nWe will start with the Wald solution by hand. This is of the form \\(\\hat{p} \\pm z_{1-\\alpha/2}\\mathrm{SE}(\\hat{p})\\):\n\nn &lt;- 10\nx &lt;- 6\nphat &lt;- x / n\nse &lt;- sqrt(phat * (1 - phat) / n)\nz &lt;- qnorm(0.9)\nphat - z * se\n\n[1] 0.4015\n\nphat + z * se\n\n[1] 0.7985\n\n\nR’s prop.test() give’s Wilson intervals, not Wald intervals:\n\nprop.test(x = x, n = n, conf.level = 0.8) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.6    0.356     0.809\n\n\nThe exact method is done via binom.test():\n\nbinom.test(x = x, n = n, conf.level = 0.8) |&gt;\n  tidy() |&gt;\n  select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      0.6    0.354     0.812\n\n\nYes it matters. We should use the binomial approach since 10 * 0.6 * 0.4 = 2.4 &lt; 5"
  },
  {
    "objectID": "05_tests/05_ttest.html",
    "href": "05_tests/05_ttest.html",
    "title": "One Sample t-Tests in R",
    "section": "",
    "text": "Suppose we know the average birthweight in America is 110 oz. We are curious if the babies in a Boston area hospital have a different birthweight. Let \\(X_i\\) be the birthweight of the \\(i\\) Boston baby, then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\) and are independent. We want to test\n\\[\\begin{align}\nH_0: \\mu &= 110\\\\\nH_1: \\mu &\\neq 110\n\\end{align}\\]\nLet’s first read in the data on the \\(n = 1000\\) babies:\n\nlibrary(tidyverse)\nlibrary(broom)\nbirthweight &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/birthweight.csv\")\n\nWe then use t.test() to run the \\(t\\)-test. The arguments are:\n\nformula: a formula object (generated with a tilde ~).\n\nWe put the name of the variable we are exploring to the left of the tilde.\nWe put the number 1 to the right of the tilde.\n\ndata: the name of the data frame containing the variable.\nmu: The null value. The default is 0 since this is the most common test.\nalternative: We use the default \"two.sided\", since our alternative hypothesis is of the form parameter \\(\\neq\\) value.\n\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110)\n\nWe then use broom::tidy() to get a summary of the \\(t\\)-test output.\n\nbout &lt;- tidy(tout)\nbout\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     107.     -3.32 0.000920       999     105.      109. One Samp… two.sided  \n\n\nWe can manually verify these results (though, you wouldn’t do this step in real life):\n\nxbar &lt;- mean(birthweight$weight)\ns &lt;- sd(birthweight$weight)\nn &lt;- length(birthweight$weight)\nmu0 &lt;- 110\ntstat &lt;- (xbar - mu0) / (s / sqrt(n))\npval &lt;- 2 * pt(-abs(tstat), df = n - 1)\ntstat\n\n[1] -3.324\n\npval\n\n[1] 0.0009204\n\n\nIf instead we had the alternative of \\(H_1: \\mu &lt; \\mu_0\\), then we would use the alternative = \"less\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"less\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.0004602\n\n\nIf instead we had the alternative of \\(H_1: \\mu &gt; \\mu_0\\), then we would use the alternative = \"greater\" argument.\n\ntout &lt;- t.test(weight ~ 1, data = birthweight, mu = 110, alternative = \"greater\")\nbout &lt;- tidy(tout)\nbout$p.value\n\n[1] 0.9995\n\n\n\nExerciseSolution\n\n\nConsider the lead data that you can read about here and download from https://dcgerard.github.io/stat_320/data/lead.csv. IQ tests are designed to have a mean of 100. Use iqf to test if the control group has an average IQ value of 100. Separately test if the exposed group has an average IQ less than 100. State the hypotheses, test results, and conclusions.\n\n\nLet’s load in the lead data\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nWe’ll filter for just the control group\n\nlead |&gt;\n  filter(Group == \"control\", !is.na(iqf)) -&gt;\n  df_control\n\nLet \\(X_i\\) be the IQ of the \\(i\\)th control individual. We assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 100\\) versus \\(H_1: \\mu \\neq 100\\). We can run this test using t.test():\n\nt.test(iqf ~ 1, data = df_control, mu = 100) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     92.6     -4.08 0.000113        73     88.9      96.2 One Samp… two.sided  \n\n\nThe \\(p\\)-value is 0.0001128, so we have very strong evidence that mean IQ in the control group is not 100\nWe’ll now filter for the exposed group\n\nlead |&gt;\n  filter(Group == \"exposed\", !is.na(iqf)) -&gt;\n  df_exposed\n\nLet \\(X_i\\) be the IQ of the \\(i\\)th exposed individual. We assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 100\\) versus \\(H_1: \\mu &lt; 100\\). We can run this test using t.test():\n\nt.test(iqf ~ 1, data = df_exposed, mu = 100, alternative = \"less\") |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic     p.value parameter conf.low conf.high method alternative\n     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      \n1     88.0     -6.66     1.65e-8        45     -Inf      91.0 One S… less       \n\n\nThe \\(p\\)-value is 1.7e-08, so we again have very strong evidence that mu &lt; 100."
  },
  {
    "objectID": "05_tests/05_power.html",
    "href": "05_tests/05_power.html",
    "title": "Power Calculations",
    "section": "",
    "text": "For \\(t\\)-methods, you use power.t.test() to calculate do power and smaple size calculations. It takes as input four of the following:\n\nn: The sample size \\(n\\)\ndelta: The effect size (difference in means) \\(|\\mu_1 - \\mu_0|\\)\nsd: The standard deviation of the data \\(\\sigma\\)\nsig.level: The signficicance level \\(\\alpha\\)\npower: The power \\(1 - \\beta\\).\n\nYou must put values for exactly four of the above. The fifth should be NULL and the function will return the fifth value.\nOther inputs are for the type of test:\n\ntype: Use \"one.sample\" for one-sample \\(t\\)-tests and \"two.sample\" for two-sample \\(t\\)-tests.\nalternative: Either \"two.sided\" or \"one.sided\".\n\nSuppose we plan on running a study with 100 participants of low socioeconomic status (SES). The mean birthweight in america is 120 oz. A pilot study suggested that the average birthweight of low SES babies is 115 oz with a standard deviation of 24 oz. What is the power of a test with a significance level of 0.05?\n\npower.t.test(\n  n = 100, \n  delta = 5, \n  sd = 24, \n  sig.level = 0.05, \n  type = \"one.sample\", \n  alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 5\n             sd = 24\n      sig.level = 0.05\n          power = 0.6643\n    alternative = one.sided\n\n\nLet’s compare that power to the \\(z\\)-test calculation from Rosner\n\npnorm(qnorm(0.05) + sqrt(100) * 5 / 24)\n\n[1] 0.6695\n\n\nIt’s a little different because Rosner uses \\(z\\)-methods instead of \\(t\\)-methods for power and sample size calculations. But it’s not too different to be practically important, especially since power and sample size calculations are mostly just wild educated guesses.\n\n\nExerciseSolution\n\n\nWhat sample size would be needed for a power of at least 0.8?\n\n\n\npower.t.test(\n  power = 0.8, \n  delta = 5, \n  sd = 24, \n  sig.level = 0.05, \n  type = \"one.sample\", \n  alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 143.8\n          delta = 5\n             sd = 24\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\n\nWe need a sample size of at least 144.\n\n\n\n\nExerciseSolution\n\n\nA new drug is proposed to prevent glaucoma among people with high intraocular pressure (IOP). A pilot study is conducted with 10 individuals. After 1 month of using the drug, their IOP decreases by 5 mm HG with a standard deviation of 10 mm HG. What is the sample size needed to achieve 90% power for a two-sided test with significance level of 0.05.\n\n\n\npower.t.test(\n  delta = 5, \n  sd = 10,\n  sig.level = 0.05,\n  power = 0.9,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 44\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\nWe need a smaple size of at least 44."
  },
  {
    "objectID": "05_tests/05_binom.html#example",
    "href": "05_tests/05_binom.html#example",
    "title": "One Sample Binomial Tests in R",
    "section": "Example",
    "text": "Example\nSuppose \\(x\\) = 5 and \\(n\\) = 8.\n\nx &lt;- 5\nn &lt;- 8\nbinom.test(x = x, n = n)\n\n\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 5, number of trials = 8, p-value = 0.7\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2449 0.9148\nsample estimates:\nprobability of success \n                 0.625"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound",
    "href": "05_tests/05_binom.html#finding-lower-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-1",
    "href": "05_tests/05_binom.html#finding-lower-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-2",
    "href": "05_tests/05_binom.html#finding-lower-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-3",
    "href": "05_tests/05_binom.html#finding-lower-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-4",
    "href": "05_tests/05_binom.html#finding-lower-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-5",
    "href": "05_tests/05_binom.html#finding-lower-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-6",
    "href": "05_tests/05_binom.html#finding-lower-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-lower-bound-7",
    "href": "05_tests/05_binom.html#finding-lower-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Lower Bound",
    "text": "Finding Lower Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_1\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound",
    "href": "05_tests/05_binom.html#finding-upper-bound",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-1",
    "href": "05_tests/05_binom.html#finding-upper-bound-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-2",
    "href": "05_tests/05_binom.html#finding-upper-bound-2",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-3",
    "href": "05_tests/05_binom.html#finding-upper-bound-3",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-4",
    "href": "05_tests/05_binom.html#finding-upper-bound-4",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-5",
    "href": "05_tests/05_binom.html#finding-upper-bound-5",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-6",
    "href": "05_tests/05_binom.html#finding-upper-bound-6",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#finding-upper-bound-7",
    "href": "05_tests/05_binom.html#finding-upper-bound-7",
    "title": "One Sample Binomial Tests in R",
    "section": "Finding Upper Bound",
    "text": "Finding Upper Bound\n\nBlack line = observed value\nBlue line = expected Value (\\(np_2\\))"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval",
    "href": "05_tests/05_binom.html#final-confidence-interval",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(np_1\\)\nRight: \\(np_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html#final-confidence-interval-1",
    "href": "05_tests/05_binom.html#final-confidence-interval-1",
    "title": "One Sample Binomial Tests in R",
    "section": "Final Confidence Interval",
    "text": "Final Confidence Interval\n\nLeft: \\(p_1\\)\nRight: \\(p_2\\)"
  },
  {
    "objectID": "05_tests/05_binom.html",
    "href": "05_tests/05_binom.html",
    "title": "One Sample Binomial Tests in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nWe have \\[\nX \\sim \\mathrm{Binom}(n,p)\n\\] and we are testing\n\n\\(H_0\\): \\(p = p_0\\)\n\\(H_1\\): \\(p \\neq p_0\\) or \\(p &gt; p_0\\) or \\(p &lt; p_0\\)\n\n\n\nApproximate Approach\n\nWe use the central limit theorem. If (rule-of-thumb) \\(np_0(1-p_0) \\geq 5\\) then \\[\nX \\sim N(p_0, p_0(1-p_0)/n)\n\\] and we calculate the tail probabilities of (for \\(\\hat{p} = X/n\\)) \\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\\]\nThis is done via prop.test(), which you can feed into broom::tidy()\n\nx: the observed number of successes\nn: The total number of trials\np: The null value of the success probability\nalternative: Either \"two.sided\", \"less\", or \"greater\"\n\nSuppose that about 20% of women who are trying to concieve take 12 months or more to get pregnant, which we will call infertility. Researchers are interested in if a genetic marker is associated with infertility. Of 40 women with this marker, 10 were infertile. Is this marker associated with infertility?\n\n\\(X \\sim \\mathrm{Binom}(40, p)\\)\n\\(H_0\\): \\(p = 0.2\\)\n\\(H_1\\): \\(p &gt; 0.2\\)\n\n\npout &lt;- prop.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(pout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.277\n\n\nSince the \\(p\\)-value is 0.2766, we don’t have evidence that the marker is associated with infertility.\nWe can do this calculation by hand (but you would never do this):\n\nphat &lt;- 10 / 40 \np0 &lt;- 0.2\nz &lt;- (phat - p0 - 1/80) / sqrt(p0 * (1 - p0) / 40) ## continuity correction\npnorm(q = z, lower.tail = FALSE)\n\n[1] 0.2766\n\n\n\n\n\nExact Approach\n\nThe exact approach calculates the probability under the null of being as more supportive of the alternative as the data we observed.\nE.g., for our infertility example, we would calculate \\(\\mathrm{Pr}(X \\geq 10 | p = 0.2)\\)\n\n1 - pbinom(q = 9, size = 40, prob = 0.2)\n\n[1] 0.2682\n\n\nThis procedure is implemented in the binom.test() function, which has the same inputs as prop.test().\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2, alternative = \"greater\")\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.268\n\n\nWhen the alternative is 2-sided, \\(H_1 \\neq p_0\\), R is a little different than the procedure Rosner proposes. It calculates the sums the probabilities under the null of all \\(X\\) that are less probable than our observed \\(x\\)\n\\[\n\\sum_{k \\text{ s.t. } Pr(k) \\leq Pr(x)}\\binom{n}{k} p_0^k(1-p_0)^{n-k}\n\\]\nIn the infertility example, this would be\n\nprob &lt;- dbinom(x = 0:40, size = 40, prob = 0.2)\nsum(prob[prob &lt;= dbinom(x = 10, size = 40, prob = 0.2)])\n\n[1] 0.4296\n\n\n\nbout &lt;- binom.test(x = 10, n = 40, p = 0.2)\ntidy(bout) |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.430\n\n\n\n\nExerciseSolution\n\n\nOut of 13 deaths at a nuclear facility among men aged 55-64, 5 of them were due to cancer. The proportion of deaths caused by cancer in that group in the greater population is 0.2. Is there more cancer deaths in this nuclear facility? Use both the normal and exact approaches.\n\n\nWe are testing \\(H_0: p = 0.2\\) versus \\(H_1: p &gt; 0.2\\) assuming that \\(X \\sim \\mathrm{Binom}(13,p)\\), where \\(X\\) is the number who had cancer.\nThe normal approach is done via prop.test():\n\nprop.test(x = 5, n = 13, p = 0.2, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\nWarning in prop.test(x = 5, n = 13, p = 0.2, alternative = \"greater\"):\nChi-squared approximation may be incorrect\n\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0939\n\n\nThe exact approach is done via binom.test():\n\nbinom.test(x = 5, n = 13, p = 0.2, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0991\n\n\nWe should be using the exact approach by our rule-of-thumb being less than 5:\n\n0.2 * (1 - 0.2) * 13\n\n[1] 2.08\n\n\nThe \\(p\\)-value is about 0.1, so no real evidence of more cancer deaths."
  },
  {
    "objectID": "05_tests/05_binom_power.html",
    "href": "05_tests/05_binom_power.html",
    "title": "Power Calculations for Binomial Tests",
    "section": "",
    "text": "There are no base R functions that do power and sample size calculations. But I created one for you based on Equations 7.32 and 7.33 from Rosner.\n\n#' Power/sample size calculation of 1-sample proportion test\n#' \n#' Uses central limit theorem, so make sure `p0 * (1 - p0) * n &gt;= 5`\n#' \n#' Exactly one of `n`, `power`, `p0`, `p1`, or `alpha` needs to be `NULL`.\n#' \n#' @param n The sample size\n#' @param power The power\n#' @param p0 The null proportion\n#' @param p1 The alternative proportion\n#' @param alpha The significance level\n#' @param TOL Tolerance level\n#' \n#' @author David Gerard\n#' \n#' @examples\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' b1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n#' \n#' ## two p1's\n#' b1power(n = 500, power = 0.9, p0 = 0.02, p1 = NULL, alpha = 0.05)\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.00406, alpha = 0.05)$power\n#' b1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.044, alpha = 0.05)$power\nb1power &lt;- function(\n    n = NULL, \n    power = NULL,\n    p0 = NULL,\n    p1 = NULL, \n    alpha = 0.05,\n    TOL = 1e-6) {\n  \n  if (is.null(n) + is.null(power) + is.null(p0) + is.null(p1) + is.null(alpha) != 1) {\n    stop(\"exactly one of n, power, p0, p1, and alpha need to be NULL\")\n  }\n  \n  oout &lt;- list(n = n, power = power, p0 = p0, p1 = p1, alpha = alpha)\n  \n  pfun &lt;- function(n, p0, p1, alpha) {\n    za2 &lt;- stats::qnorm(alpha / 2)\n    stats::pnorm(sqrt(p0 * (1 - p0) / (p1 * (1 - p1))) * (za2 +\n                   abs(p0 - p1) * sqrt(n) / sqrt(p0 * (1 - p0))))\n  }\n  \n  if (is.null(power)) {\n    oout$power &lt;- pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)\n  } else if (is.null(n)) {\n    z1a2 &lt;- stats::qnorm(1 - alpha / 2)\n    zp &lt;- stats::qnorm(power)\n    oout$n &lt;- p0 * (1 - p0) * (z1a2 + zp * sqrt(p1 * (1 - p1) / (p0 * (1 - p0))))^2 / (p1 - p0)^2\n    oout$n &lt;- ceiling(oout$n)\n  } else if (is.null(p0)) {\n    rp0 &lt;- function(p0) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp0(p0 = TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp0, interval = c(TOL, p1))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp0(p0 = 1 - TOL)) * sign(rp0(p0 = p1)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp0, interval = c(p1, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p0 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(p1)) {\n    rp1 &lt;- function(p1) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    \n    if (sign(rp1(p1 = TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r1 &lt;- stats::uniroot(f = rp1, interval = c(TOL, p0))\n    } else {\n      r1 &lt;- list(root = NA)\n    }\n    if (sign(rp1(p1 = 1 - TOL)) * sign(rp1(p1 = p0)) &lt; 0) {\n      r2 &lt;- stats::uniroot(f = rp1, interval = c(p0, 1 - TOL))\n    } else {\n      r2 &lt;- list(root = NA)\n    }\n    oout$p1 &lt;- c(r1$root, r2$root)  \n  } else if (is.null(alpha)) {\n    ralpha &lt;- function(alpha) {power - pfun(n = n, p0 = p0, p1 = p1, alpha = alpha)}\n    rout &lt;- stats::uniroot(f = ralpha, interval = c(TOL, 1-TOL))\n    oout$alpha &lt;- rout$root\n  }\n  \n  if (any(oout$p0[!is.na(oout$p0)] * (1 - oout$p0[!is.na(oout$p0)]) * oout$n &lt; 5)) {\n    warning(\"too small sample size\")\n  }\n  return(oout)\n}\n\n\nAssumes the sample size is large enough to use the central limit theorem (\\(np_0(1-p_0) \\geq 5\\)).\nSuppose we wish to test the hypothesis that women with a sister history of breast cancer are at higher risk of developing breast cancer themselves. Suppose the prevalence rate of breast cancer is 2% among 50 to 54 year-old US women, whereas it is 5% among women with a sister history. We wish to interval 500 women 50 to 54 years old with a sistory history of the disease. What is the power of such a study assuming that we conduct a two-sided test with \\(\\alpha = 0.05\\)?\n\n# 0.9655\nb1power(n = 500, power = NULL, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 500\n\n$power\n[1] 0.9655\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05\n\n\nHow many women should we interview in the study proposed to achieve 90% power?\n\n# 341\nb1power(n = NULL, power = 0.9, p0 = 0.02, p1 = 0.05, alpha = 0.05)\n\n$n\n[1] 341\n\n$power\n[1] 0.9\n\n$p0\n[1] 0.02\n\n$p1\n[1] 0.05\n\n$alpha\n[1] 0.05\n\n\nThere is also the {pwr} package that you can try out. It uses different equations from Rosner, so you need to do a funky pre-calculation for the effect size. The numbers are slightly different too:\n\nlibrary(pwr)\nh &lt;- ES.h(p1 = 0.05, p2 = 0.02)\npwr.p.test(h = h, n = 500, sig.level = 0.05)\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1672\n              n = 500\n      sig.level = 0.05\n          power = 0.9624\n    alternative = two.sided\n\n\n\nh &lt;- ES.h(p1 = 0.05, p2 = 0.02)\npwr.p.test(h = h, sig.level = 0.05, power = 0.9)\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1672\n              n = 375.7\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided"
  },
  {
    "objectID": "05_tests/two_sample_t.html",
    "href": "05_tests/two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women containt eh systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\nExercise: A study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)"
  },
  {
    "objectID": "05_tests/05_two_sample_t.html",
    "href": "05_tests/05_two_sample_t.html",
    "title": "Two-sample t-Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\n\n\nPaired \\(t\\)-tests\n\nData from 10 women contain the systolic blood pressure (SBP) (in mm Hg) before and while using an oral contraceptive.\n\noc_df &lt;- data.frame(\n  pre_sbp = c(115, 112, 107, 119, 115, 138, 126, 105, 104, 115),\n  post_sbp = c(128, 115, 106, 128, 122, 145, 132, 109, 102, 117)\n)\n\nWe use t.test() to run a paired \\(t\\)-test.\n\nx: The first column.\ny: The second column.\npaired: set to TRUE to make it a paired \\(t\\)-test.\n\n\nt.test(x = oc_df$post_sbp, y = oc_df$pre_sbp, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 Paired t-… two.sided  \n\n\nThis is the exact same as just first calculating the differences then running a one-sample \\(t\\)-test.\n\noc_df &lt;- mutate(oc_df, diff = post_sbp - pre_sbp)\nt.test(diff ~ 1, data = oc_df) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      4.8      3.32 0.00887         9     1.53      8.07 One Sampl… two.sided  \n\n\nNotice that the paired \\(t\\)-test uses x - y, not y - x, as the vector of differences.\nOur conclusion might read like this:\n\nWe have strong evidence that women who use an oral contraceptive (OC) have a different mean systolic blood pressure (SBP) than women who do not use an OC (\\(p\\) = 0.008874, \\(n\\) = 10). We estimate that women who use an OC have on average an SBP 4.8 mm Hg higher than women who do not use an OC (95% CI 1.534 mm Hg to 8.066 mm Hg higher).\n\n\n\nExerciseSolution\n\n\nA study included 15 twins where one has schizophrenia and the other does not. These data contain the volume (in cm\\(^3\\)) of the left hippocampus of each twin. These data are from The Statistical Sleuth, which in turn obtained the data from doi:10.1056/NEJM199003223221201. Evaluate if there are any physical differences between the twins. Also, provide an interval estimate on the mean difference in volume between twin-types. Do this in two ways (i) by using t.test() and (ii) “by hand” after calculating the appropriate summary statistics.\n\nsc_df &lt;- data.frame(\n  Unaffected = c(1.94, 1.44, 1.56, 1.58, 2.06, 1.66, 1.75, 1.77, \n                 1.78, 1.92, 1.25, 1.93, 2.04, 1.62, 2.08), \n  Affected = c(1.27, 1.63, 1.47, 1.39, 1.93, 1.26, 1.71, 1.67, \n               1.28, 1.85, 1.02, 1.34, 2.02, 1.59, 1.97)\n)\n\n\n\nLet \\(X_i\\) be the difference in volume in the \\(i\\)th twin. Then we assume \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 0\\) versus \\(H_1: \\mu \\neq 0\\). In R, we run this test using:\n\nt.test(x = sc_df$Affected, y = sc_df$Unaffected, paired = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1   -0.199     -3.23 0.00606        14   -0.331   -0.0667 Paired t-… two.sided  \n\n\nWe have strong evidence of a difference in mean volume between the twin types (\\(p\\) = 0.00602, n = 15). We estimate that the unaffected twin has on average 0.2 cm\\(^3\\) larger hippocampus than the affected twin (95% CI 0.0667 cm\\(^3\\) to 0.3306 cm\\(^3\\) larger).\nBy hand,w e first calculate the difference between the two twins:\n\nsc_df &lt;- mutate(sc_df, diff = Affected - Unaffected)\n\nWe now need the summary statistics\n\nxbar &lt;- mean(sc_df$diff)\ns &lt;- sd(sc_df$diff)\nn &lt;- nrow(sc_df)\n\nThe \\(t\\)-statistic is \\(\\frac{\\bar{x}}{s/\\sqrt{n}}\\)\n\nt &lt;- xbar / (s / sqrt(n))\nt\n\n[1] -3.229\n\n\nWe calculate the \\(p\\)-value by two times the tail area:\n\np_value &lt;- 2 * pt(-abs(t), df = n - 1)\np_value\n\n[1] 0.006062\n\n\nThe 95% confidence interval is \\(\\bar{x} \\pm t_{1-\\alpha/2,n-1}s/\\sqrt{n}\\)\n\nlower &lt;- xbar - qt(p = 0.975, df = n - 1) * s / sqrt(n)\nupper &lt;- xbar + qt(p = 0.975, df = n - 1) * s / sqrt(n)\nc(lower, upper)\n\n[1] -0.3306 -0.0667\n\n\n\n\n\n\n\nUnpaired (Equal Variance)\n\nConsider the lead data that you can read about here.\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\n\nWe are interested in if the exposed and control groups have the same mean full scale IQ. Let’s explore the data\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet \\(X_i\\) be the \\(i\\)th IQ score in the control group, let \\(Y_i\\) be the \\(i\\)th IQ score in the exposed group.\nThen we assume that \\(X_i \\sim \\mathrm{N}(\\mu_1, \\sigma^2)\\) and \\(Y_i \\sim \\mathrm{N}(\\mu_2, \\sigma^2)\\), and that all observations are independent.\nWe use t.test() to run a two-sample \\(t\\)-test.\n\nThe quantitative variable is to the left of the tilde ~\nThe variable encoding the two groups is to the right of the tilde\nIf we assume equal variances in each group, we set var.equal = TRUE\n\n\nt.test(iqf ~ Group, data = lead, var.equal = TRUE) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.67  0.0977       118   -0.845      9.91\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWe can verify this result manually (you would never do this in real life, but you might on an exam).\n\n## Get summary statistics of the two groups\nlead |&gt;\n  group_by(Group) |&gt;\n  summarize(mean = mean(iqf), var = var(iqf), n = n()) -&gt;\n  sumdf\nxbar &lt;- sumdf$mean[[1]]\nybar &lt;- sumdf$mean[[2]]\ns2x &lt;- sumdf$var[[1]]\ns2y &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## Calculate pooled sample standard deviation\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\n\n## Calculate t-statistic\ntstat &lt;- (xbar - ybar) / (s * sqrt(1 / n1 + 1 / n2))\n\n## compare to t distribution with n1 + n2 - 2 df\npval &lt;- 2 * pt(-abs(tstat), df = n1 + n2 - 2)\n\n## Get confidence intervals\nlower &lt;- (xbar - ybar) - qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nupper &lt;- (xbar - ybar) + qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\n\nc(pval = pval, lower = lower, upper = upper)\n\n    pval    lower    upper \n 0.09772 -0.84454  9.90917 \n\n\n\n\nExerciseSolution\n\n\nIs there any difference between exposed and control groups when it comes to the finger-wrist tapping test in dominant hand (maxfwt)? Assume equal variances.\n\n\nWe read in the data\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nIt appears that the exposed group might have a lower mean tapping value than the control group. But it is unclear.\n\nlead |&gt;\n  filter(!is.na(maxfwt)) |&gt;\n  ggplot(aes(x = Group, y = maxfwt)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe run the test using t.test().\n\nt.test(maxfwt ~ Group, data = lead, var.equal = TRUE) |&gt; \n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     7.01      54.4      47.4      2.68 0.00872        97     1.81      12.2\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nYes, we have strong evidence of a difference between groups (p = 0.005). We estimate that the control group taps about 7 more times on average (95% CI of 1.8 to 12.2 more times).\n\n\n\n\nExerciseSolution\n\n\nA sample of eight 35- to 39-year-old non-pregnant, premenoposaul OC users have a mean systoolic blood pressure (SBP) of 132.82 mm Hg and a sample standard deviation of 15.34 mm Hg. A different sample of 21 non-pregnant, premenopausal, non-OC users have a mean SBP of 127.44 mm Hg and a sample standard deviation of 18.23 mm Hg. What can be said about the underlying mean difference in blood pressure between the two groups? Provide a measure of how sure we are that there is a difference, and provide some interval estimate for this difference. Assume equal variances.\n\n\nWe plug in these summary statistics:\n\nxbar &lt;- 132.82\ns2x &lt;- 15.34^2\nn1 &lt;- 8\n\nybar &lt;- 127.44\ns2y &lt;- 18.23^2\nn2 &lt;- 21\n\nLet’s calculate the pooled sample standard deviation \\(s_p = \\sqrt{\\frac{(n_1-1)s_x^2 + (n_2 - 1) s_y^2}{n_1 + n_2 - 2}}\\)\n\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\n\nNow the \\(t\\)-statistic \\(t = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{1/n_1 + 1/n_2}}\\)\n\ntstat &lt;- (xbar - ybar) / (s * sqrt(1 / n1 + 1 / n2))\n\nThe \\(p\\)-value of the test is area in both tails\n\npval &lt;- 2 * pt(-abs(tstat), df = n1 + n2 - 2)\npval\n\n[1] 0.4664\n\n\nThe confidence interval:\n\nlower &lt;- (xbar - ybar) - qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nupper &lt;- (xbar - ybar) + qt(0.975, df = n1 + n2 - 2) * s * sqrt(1 / n1 + 1 / n2)\nc(lower = lower, upper = upper)\n\n lower  upper \n-9.561 20.321 \n\n\nWe don’t have evidence that the non-OC and OC using groups differ in mean systolic blood pressure (\\(p\\)-value = 0.46664). The difference in mean blood pressure between groups is estimated to be 5.38 (95% CI of -9.561 to 20.321).\n\n\n\n\n\nTest for Equal Variance\n\nSuppose we have \\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y_i \\sim N(\\mu_2,\\sigma_2^2)\\).\nIt is possible to test \\(H_0: \\sigma_1 = \\sigma_2\\) versus \\(H_1: \\sigma_1 \\neq \\sigma_2\\).\nFolks don’t typically do this test because:\n\nIt is very sensitive to the normality assumption. Conversely, the \\(t\\)-test is not.\nThe \\(t\\)-test with equal variances is relatively robust to violations in the equal variance assumption.\nFolks typically just use the unequal variances \\(t\\)-test below.\n\nBut if your boss asks you run such a test, use var.test().\n\nvar.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\nMultiple parameters; naming those columns num.df and den.df.\n\n\n# A tibble: 1 × 9\n  estimate num.df den.df statistic p.value conf.low conf.high method alternative\n     &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      \n1     1.65     73     45      1.65  0.0719    0.955      2.76 F tes… two.sided  \n\n\nThis test is based on the ratio of the variances \\(s_1^2 / s_2^2\\). Under the null, this follows a \\(F\\)-distribution with \\(n_1 - 1\\) numerator degrees of freedom and \\(n_2 - 1\\) denominator degrees of freedom.\n\nlead |&gt;\n  filter(!is.na(iqf)) |&gt;\n  group_by(Group) |&gt;\n  summarize(var = var(iqf), n = n()) -&gt;\n  sumdf\nvar1 &lt;- sumdf$var[[1]]\nvar2 &lt;- sumdf$var[[2]]\nn1 &lt;- sumdf$n[[1]]\nn2 &lt;- sumdf$n[[2]]\n\n## for fstat &gt; 1\nfstat1 &lt;- var1 / var2\n2 * pf(q = fstat1, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)\n\n[1] 0.07194\n\n## For fstat &lt; 1\nfstat2 &lt;- var2 / var1\n2 * pf(q = fstat2, df1 = n2 - 1, df2 = n1 - 1, lower.tail = TRUE)\n\n[1] 0.07194\n\n\n\n\n\nUnpaired (Unequal Variance)\n\nWhen you don’t want to assume equal variances (typically the case), just use the default settings of t.test() that has var.equal = FALSE.\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(iqf))\nt.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4.53      92.6      88.0      1.77  0.0797      112.   -0.545      9.61\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nDon’t bother memorizing Satterthwaite’s approximation for the degrees of freedom. Just do this in the computer.\n\n\nExerciseSolution\n\n\nIs there a difference in finger tapping between groups? Don’t assume equal variances.\n\n\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\nlead &lt;- filter(lead, !is.na(maxfwt))\nt.test(maxfwt ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     7.01      54.4      47.4      2.61  0.0113      65.0     1.64      12.4\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nSome evidence of a difference (p = 0.01125). We estimate that the control group has 7 more taps on average (95% CI of 1.6 to 12.3 more taps on average).\n\n\n\n\n\nPower and Sample Size Calculations in Two-sample \\(t\\)-tests\n\nUse power.t.test().\nIn the two-sample case, n means the sample size per group. It assumes the sample sizes are equal, so the total sample size is 2 * n.\nIt also assumes the standard deviations are equal, so you need to use a pooled estimate of the standard deviation.\nIf you need more precise power or sample size calculations, then those exist.\n\nBut I think these calculations are so much guess work that the error of assuming equal sample sizes is less than the error of the wild guesses you are giving it.\n\nE.g., suppose we have the OC user exercise above as a pilot experiment.\n\nxbar &lt;- 132.82\ns2x &lt;- 15.34^2\nn1 &lt;- 8\n\nybar &lt;- 127.44\ns2y &lt;- 18.23^2\nn2 &lt;- 21\n\nLet’s calculate a pooled estimate of the variance, and we will assume that is the true variance for the power calculations.\n\ns &lt;- sqrt(((n1 - 1) * s2x + (n2 - 1) * s2y) / (n1 + n2 - 2))\ns\n\n[1] 17.53\n\n\nLet’s suppose we want a power of 0.8. Then the sample size we would need is 168 per group:\n\npower.t.test(\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  power = 0.8, \n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 167.6\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIf a researcher can only afford \\(n = 100\\) per gropu, then the power calculation would be 0.58:\n\npower.t.test(\n  n = 100,\n  delta = xbar - ybar, \n  sd = s, \n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 5.38\n             sd = 17.53\n      sig.level = 0.05\n          power = 0.5793\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nExerciseSolution\n\n\nSuppose a new drug is proposed to lower intraocular pressure (IOP) among people with glaucoma. It is anticipated that mean IOP will drop by 8 mm Hg after 1 month with the new drug. The comparison group will get the standard drug, which is anticipated to have a mean drop in IOP of 5 mm Hg after 1 month. It is expected that the sd of change within each group will be 10 mm Hg. How many subjects need to be enrolled to achieve 90% power if an equal sample size is planned within each group and a two-sided test with \\(\\alpha\\) = 0.05 will be used?\n\n\n\npower.t.test(delta = 3, sd = 10, sig.level = 0.05, power = 0.9)\n\n\n     Two-sample t test power calculation \n\n              n = 234.5\n          delta = 3\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n235 subjects per group"
  },
  {
    "objectID": "06_nonparametric/06_wilcoxin_1.html",
    "href": "06_nonparametric/06_wilcoxin_1.html",
    "title": "One-sample Nonparametric Tests",
    "section": "",
    "text": "We will load the boneden data, that you can read about here.\n\nlibrary(tidyverse)\nlibrary(broom)\nboneden &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/boneden.csv\")\n\nRecall that this is a twin study with a heavier smoking twin and a lighter smoking twin. We are interested in the difference in bone density between these pairs of twins.\n\nboneden |&gt;\n  mutate(ls_diff = ls1 - ls2) -&gt;\n  boneden\nggplot(boneden, aes(x = ls_diff)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 10)\n\n\n\n\n\n\n\n\n\n\nSign test\n\nWe might not be willing to assume a normal distribution for these data. An alternative is the sign test, which tests whether the number of positive values is greater than expected by chance.\nThe sign test is the exact same thing as the binomial test using the number of positive values as the data and \\(p = 1/2\\) as the null.\nFirst, we calculate the number of positive values and the total number of values.\n\nboneden |&gt;\n  summarize(x = sum(ls_diff &gt; 0), n = sum(ls_diff != 0))\n\n# A tibble: 1 × 2\n      x     n\n  &lt;int&gt; &lt;int&gt;\n1    28    41\n\n\nWe can now do a normal approximation proportion test, or an exact binomial test on these data\n\nprop.test(x = 28, n = 41, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.683      4.78  0.0288         1    0.518     0.814 1-sample … two.sided  \n\n\n\nbinom.test(x = 28, n = 41, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.683        28  0.0275        41    0.519     0.819 Exact bin… two.sided  \n\n\nSo we have some evidence that there is a difference in median bone density between the two twins (\\(p \\approx 0.028\\)).\n\n\n\nWilcoxin signed-rank test\n\nIf we are willing to at least assume that the data are symmetric, we can gain some power by doing the Wilcoxin signed-rank test\n\nwilcox.test(ls_diff ~ 1, data = boneden, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      618.  0.0156 Wilcoxon signed rank test with continuity corre… two.sided  \n\n\nI set exact = FALSE because the exact test as implemented in R cannot handle ties.\nThe idea of the Wilcoxin signed-rank test is that if you rank the observations by the magnitude (absolute value), then the rank sum of the negative numbers should be about the same as the rank sum of the positive numbers.\n\nset.seed(77)\n## Draw data from a distribution with median 0\nx &lt;- rnorm(1000) \n## Calculate ranks of absolue values\nr &lt;- rank(abs(x))\n## Mean rank of positive numbers\nsum(r[x &gt; 0])\n\n[1] 256612\n\n## Mean rank of negative numbers\nsum(r[x &lt; 0])\n\n[1] 243888\n\n\nIf the median is positive, you would expect the magnitude of the positive numbers to be larger than the maginude of the negative numbers.\n\nset.seed(77)\n## Draw data from a distribution with median 1\nx &lt;- rnorm(1000, mean = 1) \n## Calculate ranks of absolue values\nr &lt;- rank(abs(x))\n## Mean rank of positive numbers\nsum(r[x &gt; 0])\n\n[1] 465908\n\n## Mean rank of negative numbers\nsum(r[x &lt; 0])\n\n[1] 34592\n\n\nA visualization: The true distribution is the curve, symmetric about 0. A sample of 10 individuals is the rug plot. The ranks of the magnitudes are above the rug plot. The red numbers are the rank sums of the positive and negative values.\n\n\n\n\n\n\n\n\n\nSame as before, but the true distribution is symmetric about 1.\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nDo a sign test, Wilcoxin signed-rank test, and a paired \\(t\\)-test for the difference in femoral neck density between the two twins. Which do you think is more appropriate?\n\n\nWe’ll first calculate the difference between bone densities:\n\nboneden |&gt;\n  mutate(fn_diff = fn1 - fn2) -&gt;\n  boneden\n\nWe’ll run an exact sign test using the number of positive and negative differences:\n\nboneden |&gt;\n  summarize(x = sum(fn_diff &gt; 0), n = sum(fn_diff &gt; 0 | fn_diff &lt; 0))\n\n# A tibble: 1 × 2\n      x     n\n  &lt;int&gt; &lt;int&gt;\n1    22    39\n\nbinom.test(x = 22, n = 39) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.564        22   0.522        39    0.396     0.722 Exact bin… two.sided  \n\n\nThe Wilcoxin signed rank test is done via wilcox.test().\n\nwilcox.test(x = boneden$fn_diff, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      404.   0.856 Wilcoxon signed rank test with continuity corre… two.sided  \n\n\nThe t-test is done via t.test().\n\nt.test(fn_diff ~ 1, data = boneden) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1 0.000732    0.0503   0.960        40  -0.0287    0.0301 One Sampl… two.sided  \n\n\nWe’ll do a histogram and a QQ-plot to check symmetry and normality.\n\nggplot(boneden, aes(x = fn_diff)) +\n  geom_histogram(bins = 10, color = \"black\", fill = \"white\")\n\n\n\n\n\n\n\nggplot(boneden, aes(sample = fn_diff)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nIt looks extremely symmetric and normal, so the t-test is probably the best. All the p-values are large, so we see no evidence of a difference between twin types in femerol neck bone density.\n\n\n\n\nExerciseSolution\n\n\nWhat is the signed-rank sum from these data. Do it by hand then check your work using R.\n\nx &lt;- c(11, 39, 75, 60, 66, -28, 55, 61, -69, -7)\n\n\n\nOrdering by magnitude, we have: -7, 11, -28, 39, 55, 60, 61, 66, -69, 75\nSo the ranks of the positive numbers are 2, 4, 5, 6, 7, 8, 10\nAdding these up we get 42\nWe can check in R via\n\nsum(rank(abs(x))[x &gt; 0])\n\n[1] 42"
  },
  {
    "objectID": "06_nonparametric/06_wilcoxin_2.html",
    "href": "06_nonparametric/06_wilcoxin_2.html",
    "title": "Two-sample Nonparametric Methods",
    "section": "",
    "text": "Consider the study on the effects of lead, described here.\n\nlibrary(tidyverse)\nlibrary(broom)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\n\nWilcoxin Rank-sum test (AKA Mann-Whitney \\(U\\) test)\n\nThe Wilcoxin rank-sum test is the nonparametric version of the two-sample \\(t\\)-test.\nThe null is that the distributions of the two samples are the same. The alternative is that they are the same except shifted. That is,\n\n\\(X_i \\sim F_1\\) for \\(i = 1,\\ldots,n_1\\) and \\(Y_j \\sim F_2\\) for \\(j = 1,\\ldots,n_2\\). Here, \\(F_1\\) and \\(F_2\\) are the CDF’s of samples 1 and 2, respectively.\n\\(H_0\\): \\(F_1 = F_1\\)\n\\(H_1\\): \\(F_1(x) = F_2(x + \\Delta)\\) for some \\(\\Delta \\neq 0\\).\n\nThis shift interpretation is only valid if the distributions are about the same but shifted (checkable using histograms). If the distributions vary wildly, the hypothesis test is actually\n\n\\(H_0\\): \\(P(X &gt; Y) = P(Y &gt; X)\\)\n\\(H_1\\): \\(P(X &gt; Y) \\neq P(Y &gt; X)\\)\n\nThe idea of this test is that the rank-sums should be about the same on average (if \\(n_1 = n_2\\))\n\nset.seed(68)\ndf &lt;- tibble(\n  value = c(rnorm(20), rnorm(20)),\n  group = c(rep(1, 20), rep(2, 20))\n)\ndf |&gt;\n  mutate(rank = rank(value)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1   413\n2     2   407\n\n\nBut, if the two distributions differ by some location shift, then the distribution shifted up will have higher ranks on average, and so a larger rank sum.\n\nset.seed(68)\ndf &lt;- tibble(\n  value = c(rnorm(20), rnorm(20, mean = 1)),\n  group = c(rep(1, 20), rep(2, 20))\n)\ndf |&gt;\n  mutate(rank = rank(value)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1   303\n2     2   517\n\n\nVisualization: If the two distributions are the same, the average rankings should be about the same (when \\(n_1 = n_2\\)). Sample of 5 individuals from each group (rug plot) from distribution whose PDF is plotted. The ranks are the numbers above the rug plots. The rank sum of each sample is in red.\n\n\n\n\n\n\n\n\n\nVisualization: If one distribution is shifted, the rankings for the shifted to the right distribution will in general be larger than expected than if the distributions were the same.\n\n\n\n\n\n\n\n\n\nWe can evaluate if the exposed and the control groups have different distributions of IQF\n\nggplot(lead, aes(x = Group, y = iqf)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nUse wilcox.text() to run a Wilcoxin rank-sum test.\n\nResponse variable is to the left of the tilde ~\nGrouping variable is to the right of the tilde\ndata: The data frame that containst hte variables\n\n\nwilcox.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     1996.   0.114 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\nThe \\(p\\)-value above was pretty large, so we don’t have evidence of a difference of IQ between the two groups.\n\n\nExerciseSolution\n\n\nCalculate the rank sum statistic using these data. First by hand and then using R.\n\ndf &lt;- tibble(\n  group = c(1, 1, 1, 2, 2, 2),\n  val = c(80, -36, -83, 63, 79, 93)\n)\n\n\n\nBy hand, we have the ranks are: (5, 2, 1, 3, 4, 6)\nWe add up the first three to get 8\nWe can also do this in R:\n\ndf |&gt;\n  mutate(rank = rank(val)) |&gt;\n  group_by(group) |&gt;\n  summarize(sum = sum(rank))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     8\n2     2    13\n\n\n\n\n\n\nExercise*Solution\n\n\nThe Wilcoxin rank-sum test is most often used for ordinal data. E.g. consider the Werry-Weiss-Peters scale for hyperactivity (as reported by parents), which goes from 0 for no hyperactivity to 4 for severe hyperactivity. Is there a difference in hyperactivity between the exposed and control groups? Do an EDA and then answer with a formal hypothesis test.\n\n\nLooks like exposed group is a little higher in the scale on average.\n\nlead |&gt;\n  filter(!is.na(hyperact)) |&gt;\n  group_by(Group, hyperact) |&gt;\n  summarize(n = n()) |&gt;\n  ungroup() |&gt;\n  group_by(Group) |&gt;\n  mutate(prop = n / sum(n))\n\n`summarise()` has grouped output by 'Group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Group [2]\n  Group   hyperact     n   prop\n  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1 control        0    24 0.490 \n2 control        1    20 0.408 \n3 control        2     3 0.0612\n4 control        3     2 0.0408\n5 exposed        0    15 0.429 \n6 exposed        1    14 0.4   \n7 exposed        2     5 0.143 \n8 exposed        3     1 0.0286\n\n\nLet’s run the Wilcoxin rank sum test:\n\nwilcox.test(hyperact ~ Group, data = lead, exact = FALSE) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      784.   0.465 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\nP-value for group differences is 0.4649, so we don’t have evidence of any group differences.\n\n\n\n\n\nPermutation Tests\n\nThe the distribution of the two groups are indeed the exact same, then hypothetically we could arbitrarily choose which individuals belong to which group and the distribution of the rank sum should be the same.\nThis is the idea of the permutation test.\nYou generate a null distribution via:\n\nRandomly assign group labels to individuals\nCalculate the rank-sum statistic\nRepeat 1 and 2 many many times.\n\nIf the null is true, then our observed rank sum statistic should be about the same as the rank sum statistics from this null distribution. So we calculate a \\(p\\)-value by seeing how extreme our observed rank sum statistic is.\nYou can randomly assign labels by randomly permuting them with sample().\n\nset.seed(1)\nlead |&gt;\n  filter(!is.na(iqf)) |&gt; ## always remove NA's first\n  mutate(rank = rank(iqf)) |&gt;\n  select(iqf, rank, Group) -&gt;\n  lead_sub\nlead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  head()\n\n# A tibble: 6 × 4\n    iqf  rank Group   new_group\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    \n1    70   4   control exposed  \n2    85  40   control control  \n3    86  46   control control  \n4    76  15.5 control control  \n5    84  36   control exposed  \n6    96  81.5 control control  \n\n\nSo one iteration of generating the null distribution would be to first choose each individual’s group, the calculate the rank sum statistic.\n\nlead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  group_by(new_group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nsumdf$rsum[[1]]\n\n[1] 4622\n\n\nYou can replicate this process with replicate().\n\nrout &lt;- replicate(n = 1000, expr = {\n  lead_sub |&gt;\n  mutate(new_group = sample(Group)) |&gt;\n  group_by(new_group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nsumdf$rsum[[1]]\n})\n\nOur observed rank-sum statistic is\n\nlead_sub |&gt; \n  group_by(Group) |&gt;\n  summarize(rsum = sum(rank)) -&gt;\n  sumdf\nrobs &lt;- sumdf$rsum[[1]]\nrobs\n\n[1] 4770\n\n\nOur observed rank-sum statistic is a little rare:\n\ndata.frame(rsum = rout) |&gt;\n  ggplot(aes(x = rsum)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"white\") +\n  geom_vline(xintercept = robs, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nWe can quantify how rare by seeing how many null rank sum statistics are at or above our observed statistic. This is our permutation test \\(p\\)-value\n\n2 * mean(robs &lt;= rout)\n\n[1] 0.12\n\n\nFor large \\(n\\), this will be about the same as the normality approximation from the Wilcoxin rank-sum test:\n\nwilcox.test(iqf ~ Group, data = lead) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     1996.   0.114 Wilcoxon rank sum test with continuity correcti… two.sided"
  },
  {
    "objectID": "07_cat/07_binom.html",
    "href": "07_cat/07_binom.html",
    "title": "Categorical Tests",
    "section": "",
    "text": "library(broom)\n\n\nTwo-sample Binomial Test\n\nWe have the following 2x2 contingency table from a study comparing age of a mother at her first birth against breast cancer status. The hypothesis is that women who have their first births later in life are at higher risks of breast cancer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge at First Birth\n\nStatus\n\nTotal\n\n\nCase\nControl\n\n\n\n\n≥30\n683\n1498\n2181\n\n\n≤29\n2537\n8747\n11284\n\n\nTotal\n3220\n10245\n13465\n\n\n\n\n\n\n\nLet \\(n_1 = 3220\\) and \\(n_2 = 10245\\) be the sample sizes among case and control women, respectively.\nLet \\(x_1 = 683\\) and \\(x_2 = 1498\\) be the number of women older than 30 at first birth for case and control women, respectively.\nOur model is the \\(X_1 \\sim \\text{Binom}(n_1, p_1)\\) and \\(X_2 \\sim \\text{Binom}(n_2, p_2)\\).\nOur hypotheses are\n\n\\(H_0\\): \\(p_1 = p_2\\)\n\\(H_1\\): \\(p_1 \\neq p_2\\)\n\nYou can run this test in R via\n\nprop.test(x = c(683, 1498), n = c(3220, 10245)) |&gt;\n  tidy()\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic  p.value parameter conf.low conf.high method    \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1     0.212     0.146      77.9 1.09e-18         1   0.0500    0.0818 2-sample …\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nIf we did this manually (which you will only do for exams and homeworks, not in real life), we first calculate the estimated proportions\n\np1hat &lt;- 683 / 3220\np2hat &lt;- 1498 / 10245\n\nWe then calculate the pooled estimated proportion, which is our estimate if the null is true\n\nphat &lt;- (683 + 1498) / (3220 + 10245)\n\nOur test statistic (I’m skipping the continuity correction)\n\nz &lt;- (p1hat - p2hat) / sqrt((1 / 3220 + 1 / 10245) * phat * (1 - phat))\nz\n\n[1] 8.853\n\n\nAnd our \\(p\\)-value compares this to the standard normal\n\n2 * pnorm(-abs(z))\n\n[1] 8.545e-19\n\n\n\n\nExerciseSolution\n\n\nA study looked at the effects of oral contraceptive (OC) use on heart disease in women 40 to 44 years of age. The researchers found that among 5000 current OC users at baseline, 13 women developed a myocardial infarction (MI) over a 3-year period, whereas among 10,000 never-OC users, 7 developed an MI over a 3-year period. Assess the statistical significance of the results. Do this both “by hand” and using prop.test(). State your results.\n\n\nLet \\(X_1\\) be the number of OC users that have MI (out of \\(n_1 = 5000\\)). Let \\(X_2\\) be the number of never-OC users that have MI (out of \\(n_1 = 10000\\)). Then \\(X_1 \\sim \\mathrm{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(n_2,p_2)\\). We are testing \\(H_0: p_1 = p_2\\) versus \\(H_1: p_1 \\neq p_2\\).\nUsing prop.test() we get:\n\nprop.test(x = c(13, 7), n = c(5000, 10000)) |&gt;\n  tidy()\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic p.value parameter conf.low conf.high method     \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1    0.0026    0.0007      7.67 0.00563         1 0.000246   0.00355 2-sample t…\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nWe calculate \\(\\hat{p}_1\\), \\(\\hat{p}_2\\), and \\(\\hat{p}\\)\n\np1hat &lt;- 13 / 5000\np2hat &lt;- 7 / 10000\nphat &lt;- (13 + 7) / (5000 + 10000)\n\nThe \\(z\\) statistic is\n\nz &lt;- (p1hat - p2hat) / sqrt((1 / 5000 + 1 / 10000) * phat * (1 - phat))\n\nThe two-sided \\(p\\)-value is:\n\n2 * pnorm(-abs(z))\n\n[1] 0.002646\n\n\nWith continuity correction, this becomes:\n\nz &lt;- (p1hat - p2hat - 1/10000 - 1/20000) / sqrt((1 / 5000 + 1 / 10000) * phat * (1 - phat))\n2 * pnorm(-abs(z))\n\n[1] 0.005626\n\n\nWe have strong evidence that there is a difference in MI rates between OC users and non-OC users (p = 0.005626).\n\n\n\n\n\nContingency Table Perspective\n\nIn this study design, we collected case women and control women, and measured their age.\nIf we would have run the test accidentally assuming that we had collected younger and older women, and measured their cancer status, then it turns out that we would have gotten the exact same \\(p\\)-value.\n\nprop.test(x = c(683, 1498), n = c(3220, 10245))$p.value\n\n[1] 1.092e-18\n\nprop.test(x = c(683, 2537), n = c(2181, 11284))$p.value\n\n[1] 1.092e-18\n\n\nYou can consider the binomial test, then, as a test for association between two variables that each are binary (categorical with only two categories).\nTo run the equivalent test of association using a contingency table, first create it using matrix():\n\ntab &lt;- matrix(c(683, 1498, 2537, 8747), nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(Age = c(\"≥30\", \"≤29\"), Status = c(\"Case\", \"Control\"))\ntab\n\n     Status\nAge   Case Control\n  ≥30  683    1498\n  ≤29 2537    8747\n\n\nThe dimnames() code above sets the row names (first) and the column names (second) via a list object.\nThen just plug it into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                                           \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                            \n1      77.9 1.09e-18         1 Pearson's Chi-squared test with Yates' continuit…\n\n\n\n\nExerciseSolution\n\n\nFrom OC exercise above, insert these data into a 2x2 contingency table. Then run a chi-squared test for homogeneity. Verify that your results are the same as above.\n\n\n\ntab &lt;- matrix(c(13, 5000 - 13, 7, 10000 - 7), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(OC = c(\"Yes\", \"No\"), MI = c(\"Yes\", \"No\"))\ntab\n\n     MI\nOC    Yes   No\n  Yes  13 4987\n  No    7 9993\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1      7.67 0.00563         1 Pearson's Chi-squared test with Yates' continuity…\n\n\np-value is the same as above\n\n\n\n\n\nFisher’s Exact Test\n\nThe above methods are only valid for large \\(n\\) (expected counts at least 5 in every cell).\nIf this is not a valid assumption, then you can use fisher.test() to run an exact test (controls Type I error for all \\(n\\), not just large \\(n\\)).\nThe syntax is the exact same as chisq.test()\n\ntab\n\n     MI\nOC    Yes   No\n  Yes  13 4987\n  No    7 9993\n\nfisher.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1     3.72 0.00400     1.38      11.0 Fisher's Exact Test for Count… two.sided  \n\n\nFor large \\(n\\), the chi-squared and Fisher tests will provide about the same values. So why use chisq.test()? Sometimes, approximate methods can be better. But my opinion is that the stated benefits are minor compared to the benefit of controlling type I error exactly. So I would always use the Fisher test.\n\n\nExerciseSolution\n\n\nResearchers collected information on salt diet versus cardiovascular death. Run a Fisher’s exact test using the below table to evaluate if diet is associated with cardiovascular death. How does it compare to the chi-squared test?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCause of death\n\nType of diet\n\nTotal\n\n\nHigh salt\nLow salt\n\n\n\n\nNon-CVD\n2\n23\n25\n\n\nCVD\n5\n30\n35\n\n\nTotal\n7\n53\n60\n\n\n\n\n\n\n\n\n\n\ntab &lt;- matrix(c(2, 23, 5, 30), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(cause = c(\"Non-CVD\", \"CVD\"), diet = c(\"High salt\", \"Low salt\"))\ntab\n\n         diet\ncause     High salt Low salt\n  Non-CVD         2       23\n  CVD             5       30\n\nfisher.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1    0.527   0.688   0.0463      3.58 Fisher's Exact Test for Count… two.sided  \n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1     0.116   0.734         1 Pearson's Chi-squared test with Yates' continuity…\n\n\nSame conclusion. No evidence of an association. But p-value is smaller in Fisher’s exact test."
  },
  {
    "objectID": "07_cat/07_mcnemar.html",
    "href": "07_cat/07_mcnemar.html",
    "title": "McNemar’s Test",
    "section": "",
    "text": "library(broom)\n\n\nContingency Table Approach\n\nWomen were matched into pairs based on age and clinical characteristics. One woman of each pair was given treatment A and the other treatment B. The doctors then followed the women to see which survived and which died within 5 years.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\nB\n\nTotal\n\n\nSurvived\nDied\n\n\n\n\nSurvived\n510\n16\n526\n\n\nDied\n5\n90\n95\n\n\nTotal\n515\n106\n621\n\n\n\n\n\n\n\nIf we have a 2x2 contingency taable with matched pairs as the sampling unit, we can put it into R using matrix(), as with other contingency tables.\n\ntab &lt;- matrix(c(510, 16, 5, 90), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(A = c(\"Survived\", \"Died\"), B = c(\"Survived\", \"Died\"))\ntab\n\n          B\nA          Survived Died\n  Survived      510   16\n  Died            5   90\n\n\nTo run McNemar’s test, we can run mcnemar.test() for the large-sample approach\n\nmcnemar.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      4.76  0.0291         1 McNemar's Chi-squared test with continuity correc…\n\n\nThis is the exact same as running prop.test() on the discordant pairs.\n\nprop.test(x = 16, n = 5 + 16, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.762      4.76  0.0291         1    0.525     0.909 1-sample … two.sided  \n\n\nFor an exact test, we can run binom.test() on the discordant pairs.\n\nbinom.test(x = 16, n = 5 + 16, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.762        16  0.0266        21    0.528     0.918 Exact bin… two.sided  \n\n\nThis is best for small sample sizes.\nRule of thumb: 20 or more discordant pairs is enough for McNemar’s test. Fewer than that and use the binomial method.\nBut for this sample size, asymptotic approaches are fine.\n\n\n\nRaw Data Approach\n\nNow suppose we don’t have a 2x2 contingency table of pairs, but just two binary variables.\nE.g., a mall device and a trained observer assess if a person is hypertensive. The data are as follows\n\ndf &lt;- data.frame(\n  mall = c(\"-\", \"-\", \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"-\", \n           \"+\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"-\"),\n  trained = c(\"-\", \"-\", \"-\", \"+\", \"-\", \"-\", \"-\", \"+\", \"+\", \"-\", \n              \"-\", \"-\", \"-\", \"-\", \"+\", \"-\", \"-\", \"-\", \"-\", \"-\")\n)\ndf\n\n   mall trained\n1     -       -\n2     -       -\n3     +       -\n4     +       +\n5     -       -\n6     +       -\n7     -       -\n8     +       +\n9     +       +\n10    -       -\n11    +       -\n12    +       -\n13    -       -\n14    +       -\n15    -       +\n16    +       -\n17    +       -\n18    -       -\n19    -       -\n20    -       -\n\n\nYou can create the contingency table with table() and then run mcnemar.test().\n\ntab &lt;- table(df$mall, df$trained)\ntab\n\n\n    - +\n  - 9 1\n  + 7 3\n\nmcnemar.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      3.12  0.0771         1 McNemar's Chi-squared test with continuity correc…\n\n\nOr run the exact test, which you should here since there are 8 discordant pairs, which is less than our rule of thumb of 20.\n\nbinom.test(x = 7, n = 7 + 1) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.875         7  0.0703         8    0.473     0.997 Exact bin… two.sided  \n\n\nAn alternative to first creating the contingency table is to just put each variable in mcnemar.test() (as long as the sample size is large enough.\n\nmcnemar.test(x = df$mall, y = df$trained) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      3.12  0.0771         1 McNemar's Chi-squared test with continuity correc…\n\n\n\n\nExercise (from Rosner):Solution\n\n\nA twin design is used to study age-related macular degeneration (AMD), a common eye disease of the elderly that results in substantial losses in vision. Suppose we contact 66 twinships in which one twin has AMD and the other twin does not. The twins are given a dietary questionnaire to report their usual diet. We find that in 10 twinships the AMD twin takes multivitamin supplements and the normal twin does not. In 8 twinships the normal twin takes multivitamin supplements and the AMD twin does not. In 3 twinships both twins take multivitamin supplements, and in 45 twinships neither twin takes multivitamin supplements.\n\nWhat test can be used to assess whether there is an association between AMD and taking multivitamin supplements?\nAre AMD and taking multivitamin supplements significantly associated based on these data?\n\n\n\n\ntab &lt;- matrix(c(3, 10, 8, 45), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(AMD = c(\"Yes\", \"No\"), normal = c(\"Yes\", \"No\"))\ntab\n\n     normal\nAMD   Yes No\n  Yes   3 10\n  No    8 45\n\n\nThis has 18 discordant pairs, so we should use the exact method.\n\nbinom.test(x = 10, n = 10 + 8, p = 0.5) |&gt;\n  tidy()\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    0.556        10   0.815        18    0.308     0.785 Exact bin… two.sided  \n\n\np-value = 0.8145, so no evidence of an association"
  },
  {
    "objectID": "07_cat/07_larger.html",
    "href": "07_cat/07_larger.html",
    "title": "Larger Contingency Tables",
    "section": "",
    "text": "library(broom)\n\n\nSuppose we measure case and control status (for breast cancer) for various ages at first birth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge at First Birth\n\nAge at First Birth\n\nTotal\n\n\n&lt;20\n20-24\n25-29\n30-34\n≥35\n\n\n\n\nCase\n320\n1206\n1011\n463\n220\n3220\n\n\nControl\n1422\n4432\n2893\n1092\n406\n10245\n\n\nTotal\n1742\n5638\n3904\n1555\n626\n13465\n\n\n\n\n\n\n\nAs in the 2x2 case, we use matrix() to insert the data. Just change the nrow and ncol arguments to represent the number of rows and columns\n\ntab &lt;- matrix(\n  c(320, 1206, 1011, 463, 220,\n    1422, 4432, 2893, 1092, 406),\n  nrow = 2, ncol = 5, byrow = TRUE)\ndimnames(tab) &lt;- list(status = c(\"Case\", \"Control\"),\n                      age = c(\"&lt;20\", \"20-24\", \"25-29\", \"30-34\", \"≥35\"))\ntab\n\n         age\nstatus     &lt;20 20-24 25-29 30-34 ≥35\n  Case     320  1206  1011   463 220\n  Control 1422  4432  2893  1092 406\n\n\nYou then just plug this into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                    \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      130. 3.30e-27         4 Pearson's Chi-squared test\n\n\nAs in the 2x2 case, you can generate a contingency table from raw data using table().\nE.g., from the lead data, suppose that we are interested in testing if there is an association between lead_grp and sex.\n\nlibrary(tidyverse)\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\")\n\nWe can create this contingency table by table()\n\ntab &lt;- table(lead$sex, lead$lead_grp)\ntab\n\n\n         control current exposed previous exposed\n  female      32               7                9\n  male        46              17               13\n\n\nAnd we can plug this into chisq.test().\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.14   0.565         2 Pearson's Chi-squared test\n\n\nAlternatively, we could plug the two variables under consideration from the raw data frame directly into chisq.test().\n\nchisq.test(x = lead$sex, y = lead$lead_grp) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.14   0.565         2 Pearson's Chi-squared test\n\n\n\n\nExercise (from Rosner)Solution\n\n\nWe are interested in studying the relationship between the prevalence of hypertension in adolescents and ethnic group, where hypertension is defined as being above the 90th percentile for a child’s age, sex, and height, based on national norms.\n\nSuppose that 8 of 100 Caucasian adolescent girls, 12 out of 95 African-American adolescent girls, and 10 of 90 Hispanic adolescent girls are above the 90th percentile for blood pressure. What test can be used to assess whether there is an association between adolescent hypertension and ethnic group?\nImplement this test and report a \\(p\\)-value.\n\n\n\n\nObviously, the chi-squared test for homogeneity, since that is the only test we talked about in these notes.\nLet’s create this table:\n\ntab &lt;- matrix(c(8, 100 - 8, 12, 95 - 12, 10, 90 - 10), nrow = 2, byrow = FALSE)\ndimnames(tab) &lt;- list(Status = c(\"hypertensive\", \"normotensive\"),\n                      Ethnicity = c(\"Caucasion\", \"African-American\", \"Hispanic\"))\ntab\n\n              Ethnicity\nStatus         Caucasion African-American Hispanic\n  hypertensive         8               12       10\n  normotensive        92               83       80\n\n\nLet’s run the \\(\\chi^2\\) test:\n\nchisq.test(tab) |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      1.16   0.561         2 Pearson's Chi-squared test\n\n\nWe have no evidence of an association (p = 0.5606)."
  },
  {
    "objectID": "07_cat/07_kappa.html",
    "href": "07_cat/07_kappa.html",
    "title": "Cohen’s Kappa in R",
    "section": "",
    "text": "Some women ate beef and wrote down in two repeat surveys how much beef they ate. We are interested in how reliable this survey is. The data look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey 1\n\nSurvey 2\n\nTotal\n\n\n≤1 Serving/Week\n&gt;1 Serving/Week\n\n\n\n\n≤1 Serving/Week\n136\n92\n228\n\n\n&gt;1 Serving/Week\n69\n240\n309\n\n\nTotal\n205\n332\n537\n\n\n\n\n\n\n\nThere is no base R function to calculate Cohen’s kappa (though there are some third party packages). I made a function that will do it for you:\n\n#' @param tab The 2x2 contingency table\n#' \n#' @return A list with the following elements\n#' \\itemize{\n#'   \\item{kappa: Cohen's kappa}\n#'   \\item{se: (estimated) standard error}\n#'   \\item{z: test statistic}\n#'   \\item{p_value: upper one-sided p-value against the null of kappa = 0}\n#' }\n#' \n#' @author David Gerard\ncohen_kappa &lt;- function(tab) {\n  stopifnot(nrow(tab) == ncol(tab))\n  n &lt;- sum(tab)\n  po &lt;- sum(diag(tab)) / n\n  a &lt;- rowSums(tab) / n\n  b &lt;- colSums(tab) / n\n  pe &lt;- sum(a * b)\n  kappa &lt;- (po - pe) / (1 - pe)\n  se &lt;- sqrt((pe + pe^2 - sum(a * b * (a + b))) / (n * (1 - pe)^2))\n  z &lt;- kappa / se\n  p_value &lt;- stats::pnorm(z, lower.tail = FALSE)\n  return(list(kappa = kappa, se = se, z = z, p_value = p_value))\n}\n\nFirst, put in contingency table in R as before\n\ntab &lt;- matrix(c(136, 92, 69, 240), nrow = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(survey1 = c(\"low\", \"high\"), survey2 = c(\"low\", \"high\"))\ntab\n\n       survey2\nsurvey1 low high\n   low  136   92\n   high  69  240\n\n\nThen use this function I wrote:\n\ncohen_kappa(tab = tab)\n\n$kappa\n[1] 0.3782\n\n$se\n[1] 0.04298\n\n$z\n[1] 8.799\n\n$p_value\n[1] 6.921e-19"
  },
  {
    "objectID": "02_descriptive/02_notes.html",
    "href": "02_descriptive/02_notes.html",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Observe \\(X_1, X_2, \\dots, X_n\\)\n\nSample of numeric values\nSubscript indexes the units\n\nExample: \\(X_i =\\) Birthweight for baby \\(i\\)\nMeasure of location = center of a sample (statistic) or a population (parameter)\nArithmetic Mean \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} (X_1 + X_2 + \\dots + X_n)\n\\]\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\) \\[\n  \\sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 5 + (-4)\n  \\]\n\\[\n  \\sum_{i=1}^2 X_i = X_1 + X_2 = 2 + 5\n  \\]\n\\[\n  \\sum_{i=2}^2 X_i = X_2 = 5\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{3} \\sum_{i=1}^3 X_i = \\frac{1}{3} (2 + 5 - 4) = \\frac{3}{3} = 1\n  \\]\n\\(\\bar{X}\\) is sensitive to extreme observations.\nExample with extreme value:\n\\[\n  X_4 = 3997\n  \\]\n\\[\n  \\frac{1}{4} \\sum_{i=1}^4 X_i = \\frac{1}{4} (2 + 5 - 4 + 3997) = \\frac{4000}{4} = 1000\n  \\]\nMedian\n\nFor \\(n\\) odd \\(\\Rightarrow \\left(\\frac{n+1}{2}\\right)\\)th largest observation.\nFor \\(n\\) even \\(\\Rightarrow\\) Average of \\(\\left(\\frac{n}{2}\\right)\\)th and \\(\\left(\\frac{n}{2} + 1\\right)\\)th largest observations.\n\nExample:\n\n\\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4 \\Rightarrow -4, 2, 5\\)\n\\[\n\\text{Median}(X) = 2\n\\]\nIf \\(X_4 = 3997\\)\n\\[\n\\text{Median}(X) = \\frac{2 + 5}{2} = 3.5\n\\]\n\nIf distribution is symmetric, \\(\\text{median}(X) \\approx \\bar{X}\\).\nMean follows the skew of distribution (dashed is median, dotted is mean):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse mean if total is important.\nUse median if lots of skew.\nA mode is a frequently occurring value.\nTypes of Modalities:\n\nUnimodal:\n\n\n\n\n\n\n\n\n\nBimodal:\n\n\n\n\n\n\n\n\n\nTrimodal:\n\n\n\n\n\n\n\n\n\n\nThe mode is typically not used as a real measure of center but rather as a way to describe distribution.\n\n\n\n\nSuppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\n\n\nExerciseSolution\n\n\nWhat is the mean menstrual cycle deviation from 4 weeks?\n\n\n\\[\n3.92 - 4 = -0.08\n\\]"
  },
  {
    "objectID": "02_descriptive/02_notes.html#arithmetic-mean",
    "href": "02_descriptive/02_notes.html#arithmetic-mean",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "The arithmetic mean ( {X} ) is defined as:\n[ {X} = _{i=1}^n X_i = (X_1 + X_2 + + X_n) ]\n\n\nGiven ( X_1 = 2 ), ( X_2 = 3 ), ( X_3 = 4 ):\n\nSum: ( _{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 3 + 4 )\nMean:\n\n[ {X} = _{i=1}^3 X_i = (2 + 3 + 4) = = 3 ]\nThe arithmetic mean ( {X} ) is sensitive to extreme observations.\n\n\n\nIf ( X_3 = 3997 ):\n[ {X} = _{i=1}^3 X_i = (2 + 3 + 3997) = = 1334 ]\nAs shown, the mean increases significantly due to the extreme value."
  },
  {
    "objectID": "02_descriptive/02_notes.html#mean-and-distribution-skew",
    "href": "02_descriptive/02_notes.html#mean-and-distribution-skew",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "Mean and Distribution Skew",
    "text": "Mean and Distribution Skew\n\nThe mean is affected by the skew of the distribution.\n\n\nRight Skew\nIn a right-skewed distribution:\n\n( &gt; )\nExample: Years of oral contraception use.\n\n\nlibrary(ggplot2)\nset.seed(123)\nright_skew &lt;- data.frame(value = rexp(1000, rate = 0.5))\n\nggplot(right_skew, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(value)), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nLeft Skew\nIn a left-skewed distribution:\nMean&lt;MedianMean&lt;Median\nExample: Relative humidity in summer in Ohio.\n\nset.seed(123)\nleft_skew &lt;- data.frame(value = -rexp(1000, rate = 0.5) + 5)\n\nggplot(left_skew, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = median(value)), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Left-Skewed Distribution\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nChoosing Mean or Median\nUse the mean if the total is important.\nUse the median if there is a lot of skew."
  },
  {
    "objectID": "02_descriptive/02_notes.html#measures-of-location",
    "href": "02_descriptive/02_notes.html#measures-of-location",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Observe \\(X_1, X_2, \\dots, X_n\\)\n\nSample of numeric values\nSubscript indexes the units\n\nExample: \\(X_i =\\) Birthweight for baby \\(i\\)\nMeasure of location = center of a sample (statistic) or a population (parameter)\nArithmetic Mean \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} (X_1 + X_2 + \\dots + X_n)\n\\]\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\) \\[\n  \\sum_{i=1}^3 X_i = X_1 + X_2 + X_3 = 2 + 5 + (-4)\n  \\]\n\\[\n  \\sum_{i=1}^2 X_i = X_1 + X_2 = 2 + 5\n  \\]\n\\[\n  \\sum_{i=2}^2 X_i = X_2 = 5\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{3} \\sum_{i=1}^3 X_i = \\frac{1}{3} (2 + 5 - 4) = \\frac{3}{3} = 1\n  \\]\n\\(\\bar{X}\\) is sensitive to extreme observations.\nExample with extreme value:\n\\[\n  X_4 = 3997\n  \\]\n\\[\n  \\frac{1}{4} \\sum_{i=1}^4 X_i = \\frac{1}{4} (2 + 5 - 4 + 3997) = \\frac{4000}{4} = 1000\n  \\]\nMedian\n\nFor \\(n\\) odd \\(\\Rightarrow \\left(\\frac{n+1}{2}\\right)\\)th largest observation.\nFor \\(n\\) even \\(\\Rightarrow\\) Average of \\(\\left(\\frac{n}{2}\\right)\\)th and \\(\\left(\\frac{n}{2} + 1\\right)\\)th largest observations.\n\nExample:\n\n\\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4 \\Rightarrow -4, 2, 5\\)\n\\[\n\\text{Median}(X) = 2\n\\]\nIf \\(X_4 = 3997\\)\n\\[\n\\text{Median}(X) = \\frac{2 + 5}{2} = 3.5\n\\]\n\nIf distribution is symmetric, \\(\\text{median}(X) \\approx \\bar{X}\\).\nMean follows the skew of distribution (dashed is median, dotted is mean):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse mean if total is important.\nUse median if lots of skew.\nA mode is a frequently occurring value.\nTypes of Modalities:\n\nUnimodal:\n\n\n\n\n\n\n\n\n\nBimodal:\n\n\n\n\n\n\n\n\n\nTrimodal:\n\n\n\n\n\n\n\n\n\n\nThe mode is typically not used as a real measure of center but rather as a way to describe distribution.\n\n\n\n\nSuppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\nExercise: What is the mean menstrual cycle deviation from 4 weeks?"
  },
  {
    "objectID": "03_prob/03_notes.html",
    "href": "03_prob/03_notes.html",
    "title": "Chapter 3 Notes: Probability",
    "section": "",
    "text": "Sample Space: Set of all possible outcomes.\nEvent: Any set of outcomes (subset of sample space).\nProbability of Event: Frequency of the event over a large number of trials.\nExample: Tuberculin skin test to detect tuberculosis\n\n\n\nOutcome\nProb\n\n\n\n\nPositive\n0.1\n\n\nNegative\n0.7\n\n\nUncertain\n0.2\n\n\n\nSample Space: \\(\\{ \\text{Positive}, \\text{Negative}, \\text{Uncertain} \\}\\)\nPossible Events:\n\n\n\nEvent\nProb\n\n\n\n\n\\(\\{ \\text{Positive} \\}\\)\n0.1\n\n\n\\(\\{ \\text{Negative} \\}\\)\n0.7\n\n\n\\(\\{ \\text{Uncertain} \\}\\)\n0.2\n\n\n\\(\\{ \\text{Positive, Negative} \\}\\)\n0.8\n\n\n\\(\\{ \\text{Positive, Uncertain} \\}\\)\n0.3\n\n\n\\(\\{ \\text{Negative, Uncertain} \\}\\)\n0.9\n\n\n\\(\\{ \\text{Positive, Negative, Uncertain} \\}\\)\n1\n\n\n\nNotation:\n\n\\(P(E) =\\) Probability of event \\(E\\)\nE.g., if \\(E = \\{\\text{Positive, Negative}\\}\\), then \\(P(E) = 0.8\\)\n\nTwo events are mutually exclusive if they cannot both happen at the same time.\nExample:\n\n\\(E_1 = \\{\\text{Positive, Negative}\\}\\)\n\\(E_2 = \\{\\text{Uncertain}\\}\\)\n\\(E_3 = \\{\\text{Negative, Uncertain}\\}\\)\n\\(E_1\\) and \\(E_2\\) are mutually exclusive.\n\\(E_1\\) and \\(E_3\\) can both happen if the outcome is “Negative.”\n\\(E_2\\) and \\(E_3\\) can both happen if the outcome is “Uncertain.”"
  },
  {
    "objectID": "03_prob/04_notes.html",
    "href": "03_prob/04_notes.html",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "A random variable assigns numbers to outcomes in the sample space.\nExample:\n\nNumber of children with retinitis pigmentosa\nNumber of individuals with leukemia\netc…\n\nBasically, an event that is a number.\nDiscrete random variable: Can count them (but may be infinite).\n\nTypically \\(0, 1, 2, 3, \\dots\\)\n\nContinuous random variable: Cannot count them.\n\nTypically some interval \\((-\\infty, \\infty)\\), \\([0,1]\\), etc.\n\nDenote random variables with capital letters \\(X, Y, Z\\), etc.\n\n\n\n\n\n\n\nTipProbability Mass Function (PMF)\n\n\n\nAssigns a probability to a possible value \\(r\\). Denote this probability by \\(P(X = r)\\).\n\n\n\nA PMF is a function of \\(r\\), not \\(X\\). \\(X\\) is used to denote the random variable.\nHypertensive Example: Let \\(X =\\) number of patients in a trial of 4 who have improved blood pressure.\n\n\\(P(X = 0) = 0.008\\)\n\\(P(X = 1) = 0.076\\)\n\\(P(X = 2) = 0.265\\)\n\\(P(X = 3) = 0.411\\)\n\\(P(X = 4) = 0.240\\)\n\n\\(0 \\leq P(X = r) \\leq 1\\) for all \\(r\\)\n\\(\\sum_r P(X = r) = 1\\)\n\nSum over all possible \\(r\\) is 1.\n\n\n\n\n\n\n\n\nTipExpected Value or Mean\n\n\n\nMeasure of center of a PMF.\n\\[\nE(X) = \\sum_r r \\cdot P(X = r) = \\mu\n\\] - Again, summing over all possible \\(r\\).\n\n\n\nHypertensive Example:\n\\[\nE(X) = 0 \\cdot 0.008 + 1 \\cdot 0.076 + 2 \\cdot 0.265 + 3 \\cdot 0.411 + 4 \\cdot 0.240 = 2.8\n\\]\nCan be interpreted as the average value of \\(X\\) across many trials.\nNote: \\(\\mu\\) is a population parameter, not a statistic, which is a function of observed data.\nExample: Across some trials, we might see:\n\n\n\n\\(x\\)\nfrequency\n\n\n\n\n0\n0\n\n\n1\n9\n\n\n2\n24\n\n\n3\n48\n\n\n4\n19\n\n\n\n\\[\n\\bar{X} = 0 \\cdot 0 + 1 \\cdot \\frac{9}{100} + 2 \\cdot \\frac{24}{100} + 3 \\cdot \\frac{48}{100} + 4 \\cdot \\frac{19}{100} = 2.77\n\\]\n\n\n\n\n\n\n\nTipVariance\n\n\n\nMeasure of spread.\n\\[\n  \\text{Var}(X) = \\sum_r (r - \\mu)^2 \\, P(X = r) = \\sigma^2\n  \\]\n\n\n\n\\(SD(X) = \\sqrt{\\sigma^2} = \\sigma\\)\nNote: \\(\\text{Var}(X) = E(X^2) - E(X)^2 = \\sum_r r^2 \\, P(X = r) - \\left(\\sum_r r \\, P(X = r)\\right)^2\\)\nLarger \\(\\sigma\\) means more variable.\n\n\n\n\n\n\n\nTipCumulative Distribution Function (CDF)\n\n\n\n\\[\nF(x) = P(X \\leq x) = \\text{Probability } X \\text{ is less than or equal to } x\n\\]\n\n\n\nExample: Hypergeometric distribution\n\n\\(F(0) = 0.008\\)\n\\(F(1) = 0.008 + 0.076\\)\n\\(F(2) = 0.008 + 0.076 + 0.265\\)\n\\(F(3) = 0.008 + 0.076 + 0.265 + 0.411\\)\n\\(F(4) = 0.008 + 0.076 + 0.265 + 0.411 + 0.240\\)\n\nUseful for probability calculation:\n\\[\nP(1 \\leq X \\leq 3) = P(X \\leq 3) - P(X \\leq 0)\n\\]\n\n\nExerciseSolution\n\n\n\\(X =\\) number of boys in a family of 4\n\n\n\n\\(r\\)\n\\(P(X = r)\\)\n\n\n\n\n0\n\\(\\frac{1}{16}\\)\n\n\n1\n\\(\\frac{1}{4}\\)\n\n\n2\n\\(\\frac{3}{8}\\)\n\n\n3\n\\(\\frac{1}{4}\\)\n\n\n4\n\\(\\frac{1}{16}\\)\n\n\n\nCalculate \\(E(X)\\), \\(SD(X)\\), \\(F(X)\\)\n\n\n\n\\(E[X] = 0 \\cdot \\frac{1}{16} + 1 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{4} + 4 \\cdot \\frac{1}{16} = 2\\)\n\\(E[X^2] = 0^2 \\cdot \\frac{1}{16} + 1^2 \\cdot \\frac{1}{4} + 2^2 \\cdot \\frac{3}{8} + 3^2 \\cdot \\frac{1}{4} + 4^2 \\cdot \\frac{1}{16} = 5\\)\n\\(Var(X) = E[X^2] - E[X]^2 = 5 - 2^2 = 1\\)\n\\(SD(X) = \\sqrt{Var(X)} = \\sqrt{1} = 1\\)\nCDF:\n\n\\(F(0) = \\frac{1}{16}\\)\n\\(F(1) = \\frac{1}{16} + \\frac{1}{4}\\)\n\\(F(2) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8}\\)\n\\(F(3) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8} + \\frac{1}{4}\\)\n\\(F(4) = \\frac{1}{16} + \\frac{1}{4} + \\frac{3}{8} + \\frac{1}{4} + \\frac{1}{16}\\)\n\n\n\n\n\n\nSome distributions are seen in real data over and over again:\n\nBinomial: count out of \\(n\\)\nPoisson: count during some time interval, or count in some area of space.\n\nThese have specific PDFs/CDFs.\nWe need to know about permutations/combinations to understand them.\nNumber of permutations of \\(n\\) things taken \\(k\\) times:\n\\[\n{}_nP_k = n(n-1)(n-2) \\dots (n-k+1) = \\frac{n!}{(n-k)!}\n\\]\nExample: Individuals \\(= A, B, C, D\\)\n\\[\n{}_4P_2 = \\frac{4!}{2!} = \\frac{4 \\cdot 3}{1} = 12\n\\]\nPossible permutations:\n\n\\(A, B\\)\n\\(A, C\\)\n\\(A, D\\)\n\\(B, A\\)\n\\(B, C\\)\n\\(B, D\\)\n\\(C, A\\)\n\\(C, B\\)\n\\(C, D\\)\n\\(D, A\\)\n\\(D, B\\)\n\\(D, C\\)\n\n\\({}_4P_3\\) Example:\n\nTree diagram shows all possible arrangements of \\(A, B, C, D\\) taken 3 at a time:\n\n \nWhat if order does not matter? For example, \\(\\{A, B, C\\} = \\{C, B, A\\}\\).\nThe number of combinations of \\(n\\) things taken \\(k\\) at a time:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\nThere are \\(\\frac{n!}{(n-k)!}\\) permutations of size \\(k\\).\n\nEach of those shares elements with \\(k! = {}_kP_k\\) other permutations.\nExample: \\(A, B, C, D \\quad {}_4C_3\\)\n\nPossible combinations:\n\n\\(A, B, C = A, C, B = B, A, C = B, C, A = C, A, B = C, B, A = 3!\\)\n\n\nDivide by \\(k!\\) to get the number of combinations.\n\n\n\n\nIf given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\npmf\n\n# A tibble: 5 × 2\n      r    pr\n  &lt;int&gt; &lt;dbl&gt;\n1     0 0.008\n2     1 0.076\n3     2 0.265\n4     3 0.411\n5     4 0.24 \n\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/04_notes.html#binomial-distribution",
    "href": "03_prob/04_notes.html#binomial-distribution",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\\(n\\) trials\nOutcome of each trial is “success” or “failure”\n\\(P(\\text{success}) = p\\) for each trial\nTrials are independent\n\n\nLet $X = $ # of successes\nThen \\(X \\sim \\text{Bin}(n, p)\\) (Distributed as Binomial)\nExample: White blood count\n\nLet $X = $ # of neutrophils out of 100 white blood cells\n\\(P(\\text{neutrophile}) = 0.6\\)\n\\(\\Rightarrow X \\sim \\text{Bin}(100, 0.6)\\)\n\nIf \\(X \\sim \\text{Bin}(n, p)\\) then\n\\[\nP(X = r) = \\binom{n}{r} p^r (1 - p)^{n - r}\n\\]\nExample: Suppose \\(X \\sim \\text{Bin}(3, 0.3)\\)\n\n\\(P(X = 2) = P(\\text{2 successes and 1 failure})\\)\n\\(P(SSF) = P(SF S) = P(F SS) = p^2 (1 - p)\\)\nSo \\(P(X = 2) = 3 \\, p^2 (1 - p) = \\binom{3}{2} p^2 (1 - p)\\)\n\nClaim: # of ways to order \\(r\\) successes and \\(n - r\\) failures is \\(\\binom{n}{r}\\)\n\nProof: Position \\(1, 2, \\dots, n\\)\nChoose \\(r\\) out of these to be \\(S\\), rest are \\(F\\)\n\nExample: $X = $ # of boys out of 5 children, \\(p = 0.51\\)\n\n\\(P(X = 2) = \\binom{5}{2} (0.51)^2 (0.49)^3 = 0.306\\)\n\n\\[\n\\binom{5}{2} = \\frac{5 \\cdot 4}{2 \\cdot 1} = 10\n\\]\nCDF:\n\\[\nP(X \\leq x) = \\sum_{r=0}^{x} P(X = r) = \\sum_{r=0}^{x} \\binom{n}{r} p^r (1 - p)^{n - r}\n\\]\n\nNo simpler form.\n\nMean:\n\\[\nE(X) = \\sum_{r=0}^{n} r \\binom{n}{r} p^r (1 - p)^{n - r} = n \\, p\n\\]\n\nExpected # of successes $= $ # of trials $ P()$\n\nVariance:\n\\[\n\\text{Var}(X) = n \\, p (1 - p)\n\\]\n\n\\(n \\uparrow \\Rightarrow \\text{Var} \\uparrow\\)\n\\(p \\uparrow \\Rightarrow \\text{Var} \\uparrow\\)\nVariance is highest at \\(p = 0.5\\), smallest at \\(p = 0\\) or \\(1\\).\n\nBinomial functions in R:\n\ndbinom() = \\(P(X = r)\\)\npbinom() = \\(P(X \\leq x)\\)\nqbinom() = quantile\nrbinom() = random generation"
  },
  {
    "objectID": "03_prob/04_notes.html#poisson-distribution",
    "href": "03_prob/04_notes.html#poisson-distribution",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nCounts of rare events over some period of time or space.\nExample: # of typhoid cases in a year.\nExample: # of bacterial colonies on an agar plate.\nAssume:\n\nFor small time interval \\(\\Delta t\\), \\(P(\\text{success in } \\Delta t)\\) is about \\(\\lambda \\Delta t\\) (for some \\(\\lambda\\)).\n\\(P(\\text{more than 2 successes in } \\Delta t) \\approx 0\\)\nStationarity: \\(P(\\text{success})\\) about the same for all time intervals.\nIndependence: One success has no bearing on any other success.\n\nViolated, e.g., in epidemics.\n\n\nThen $X = $ # of “successes” in time \\(t\\)\n\n\\(X \\sim \\text{Pois}(\\mu)\\) such that \\(\\mu = \\lambda t\\)\n\n\\[\nP(X = k) = \\frac{e^{-\\mu} \\, \\mu^k}{k!}\n\\]\nNote:\n\nIf \\(X \\sim \\text{Pois}(\\mu)\\) over time \\(t\\) then \\(X \\sim \\text{Pois}(c \\mu)\\) over time \\(ct\\).\n\nExample:\n\n$X = $ # of typhoid deaths in 1 year\n\\(X \\sim \\text{Pois}(4.6)\\)\nLet $Y = $ # of typhoid deaths in half a year\n\\(Y \\sim \\text{Pois}\\left(\\frac{4.6}{2}\\right) = \\text{Pois}(2.3)\\)\n\nExample:\n\n$X = $ # of bacteria colonies in 100 cm²\n\\(X \\sim \\text{Pois}(2)\\)\n$Y = $ # of bacteria colonies in 1000 cm²\n\\(Y \\sim \\text{Pois}(20)\\)\n\nMean: If \\(X \\sim \\text{Pois}(\\mu)\\), \\(E(X) = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\mu\\)\nIf \\(X \\sim \\text{Pois}(\\lambda_1)\\) and \\(Y \\sim \\text{Pois}(\\lambda_2)\\) (and are independent), then \\(X + Y \\sim \\text{Pois}(\\lambda_1 + \\lambda_2)\\)\n\nNot generally true for other distributions (e.g., not for binomial).\n\nRelation to Binomial:\n\nIf \\(X \\sim \\text{Bin}(n, p)\\)\n\n\\(n\\) large \\((&gt; 100)\\)\n\\(p\\) small \\((&lt; 0.01)\\)\n\\(np\\) intermediate\n\nThen \\(X \\approx \\text{Pois}(np)\\)\n\n(Approximate)\n\n\nThis is used to justify Poisson in cases where we know \\(n\\) is large, but we don’t know it exactly.\nExample: $X = $ # of RNA molecules of a gene observed (on the order of 100)\n\nWe don’t know \\(n\\) but know it’s large.\nWe don’t know \\(p\\) but we know it’s small (because \\(X \\approx 100\\)).\nUse Poisson to model \\(X\\)!\n\nR functions\nExercises (4.24–4.29)\n\nof episodes for 1 child to have otitis media (ear disease) in 1 year is \\(\\text{Pois}(1.6)\\)\n\n4.24 What is the probability of getting 3 or more episodes in the first 2 years of life?\n\nSolution: $X = $ # in 2 years \\(\\sim \\text{Pois}(3.2)\\)\n\n\\[\n1 - \\text{ppois}(2, \\, \\text{lambda} = 3.2)\n\\] \\[\n= 0.6201\n\\]\n4.25 What is the probability of not getting any in the 1st year?\n\n$X = $ # in first year \\(\\sim \\text{Pois}(1.6)\\)\n\n\\[\n\\text{dpois}(x = 0, \\, \\text{lambda} = 1.6)\n\\] \\[\n= 0.2019\n\\]\n4.26 Probability two siblings will both have 3 or more episodes in the first year of life?\n\nAssumes independence.\n\n\\[\n= P(\\text{Sib 1 has } 2 \\text{ or more}) \\times P(\\text{Sib 2 has } 2 \\text{ or more})\n\\] \\[\n= 0.6201 \\times 0.6201 = 0.3845\n\\]\n\n4.27 Probability exactly 1 sibling will have 3 or more episodes (out of 2).\n\nLet $Y = $ # of siblings\n\n\\[\n  Y \\sim \\text{Bin}(2, 0.6201)\n  \\]\n\\[\n  P(Y = 1) = \\text{dbinom}(1, \\text{size} = 2, \\text{prob} = 0.6201)\n  \\]\n\\[\n  = 2 \\cdot 0.6201 \\cdot (1 - 0.6201)\n  \\]\n\\[\n  = 0.4712\n  \\]\n4.28 Probability neither will have 3 or more episodes in the first 2 years?\n\\[\n  (1 - 0.6201)^2 = 0.1443\n  \\]\n4.29 Expected number of siblings in a 2-sibling household who will have 3 or more episodes in the first two years.\n\\[\n  E(Y) = 2 \\cdot 0.6201 = 1.24\n  \\]"
  },
  {
    "objectID": "03_prob/05_notes.html",
    "href": "03_prob/05_notes.html",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "",
    "text": "Continous Distributions and the Normal\n\nA continuous random variable “takes on decimal values.”\nFor such random variables, the probability at any specific value is 0.\nExample:\n\nObviously, \\(\\Pr(\\text{a man is exactly } 6', 2.357921784123'') \\approx 0\\)\nBy the same logic, \\(\\Pr(\\text{a man is exactly } 6'2'') \\approx 0\\)\nMen are always a little above or a little below.\n\nBut, we know some regions are more likely than others.\n\n\\(\\Pr(5' \\leq X \\leq 7') &gt; \\Pr(0' \\leq X \\leq 1')\\)\n\nWe describe this intuition with a PDF.\n\n\n\n\n\n\n\nTipProbability Density Function\n\n\n\nA Probability Density Function (PDF) of a random variable \\(X\\) is a function \\(f\\) such that: \\[\n\\Pr(a \\leq X \\leq b) = \\text{area below curve between } a \\text{ and } b\n\\]\n\n\n\n\n\n\n\n\nTipCumulative Distribution Function\n\n\n\nThe Cumulative Distribution Function (CDF) is a function \\(F(x) = \\Pr(X \\leq x)\\).\n\n\n\nExample: \\(X =\\) Serum triglyceride level\n\n\\(\\Pr(50 \\leq X \\leq 100)\\)\n\n\n\n\n\n\n\n\n\n\\(\\Pr(X \\leq 100)\\)\n\n\n\n\n\n\n\n\n\n\nExpected value: \\(\\mu\\), average \\(X\\) over many trials. \\[\n\\mu = \\mathrm{E}[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\\] where \\(f(x)\\) = density.\nVariance: Average squared distance. \\[\n\\sigma^2 = \\mathrm{E}\\left[(X - \\mu)^2\\right] = \\mathrm{E}(X^2) - \\mu^2\n\\] \\[\n\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx\n\\]\nMost common continuous distribution: Normal distribution\n\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right]\n\\]\nDensity depends on \\(\\sigma^2\\) (variance) and \\(\\mu\\) (mean).\n\nAlso, if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(\\mathrm{E}(X) = \\mu\\), \\(\\mathrm{Var}(X) = \\sigma^2\\).\n\n\n\n\n\n\n\n\n\n\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different variances\n\n\n\n\n\n\n\n\n\nThe standard normal distribution is \\(N(0,1)\\).\nProperties:\n\n68–95–99.7 rule:\n\n68% of area within \\(\\pm 1\\sigma\\)\n95% of area within \\(\\pm 2\\sigma\\)\n99.7% of area within \\(\\pm 3\\sigma\\)\n\nSymmetric: \\(f(\\mu - x) = f(\\mu + x)\\)\n\\(\\mu =\\) median\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\)\nIf \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) are independent, then \\[\nZ = X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\n\\]\n\nWe denote PDF of standard normal by \\(\\phi(x)\\)\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is \\(\\Phi(x) = \\Pr(X \\leq x)\\)\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nset.seed(1)\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\nBlood Pressure \\(\\sim N(80, 144)\\)\nMild hypertension is \\(90 \\leq \\text{DBP} \\leq 100\\)\n\nUnits are in mmHg\n\nIndividuals are randomly sampled\nWhat is \\(\\Pr(\\text{mild hypertensive})\\)?\n\n\n\\(\\Pr(90 \\leq X \\leq 100)\\)  =   =  -  =\\(\\Pr(X \\leq 100) - \\Pr(X &lt; 90)\\) =pnorm(100, mean = 80, sd = sqrt(144))} - \\texttt{pnorm(90, mean = 80, sd = sqrt(144)) = 0.1545\n\n\n\nExerciseSolution\n\n\nTree diameter \\(\\sim N(8, 2^2)\\) (in inches). What is the probability that the tree has diameter \\(&gt; 12\\) in?\n\n\n\n1 - pnorm(q = 12, mean = 8, sd = 2)\n\n[1] 0.02275\n\n\n\n\n\n\nIf \\(X_1, \\dots, X_n\\) are random variables and\n\\[\nL = \\sum_{i=1}^n c_i X_i \\quad \\text{for } c_i \\text{ constants (not r.v.s)}\n\\] then\n\\[\n\\mathrm{E}[L] = \\sum_{i=1}^n c_i \\mathrm{E}[X_i]\n\\]\nIf the \\(X_i\\)’s are also independent, then \\[\n\\mathrm{Var}(L) = \\sum_{i=1}^n c_i^2 \\mathrm{Var}(X_i)\n\\]\nIf the \\(X_i\\) are also normally distributed, then\n\\[\nL \\sim N(\\mathrm{E}[L], \\mathrm{Var}(L))\n\\]\nExample:\nLet \\(X\\) = serum creatinine level for a Caucasian individual\nLet \\(Y\\) = serum creatinine level for a Black individual\nAssume: \\[\nX \\sim N(1.3, 0.25), \\quad Y \\sim N(1.5, 0.25)\n\\]\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\[\nZ = \\frac{1}{2}X + \\frac{1}{2}Y \\Rightarrow Z \\sim N(1.4, 0.125)\n\\]\n\n\\(\\mathrm{E}(Z) = \\frac{1}{2}(1.3 + 1.5) = 1.4\\)\n\\(\\mathrm{Var}(Z) = \\frac{1}{4}(0.25 + 0.25) = 0.125\\)\n\n\n\nNormal Approximation to Binomial\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\) and \\(np(1-p) \\geq 5\\) (rule of thumb), then \\[\n  X \\approx N(np, np(1-p))\n  \\]\n\nLet \\(X \\sim \\mathrm{Bin}(n, p)\\), and let \\(Y \\sim N(np, np(1 - p))\\).\nThen with continuity correction: \\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\n\\]\nMore generally, the continuity correction says that, if you are approximating a discrete random variable \\(X\\) with a normal distribution \\(Y\\), then\n\n\\(P(X \\geq a) \\approx P(Y &gt; a - 1/2)\\)\n\\(P(X &gt; a) \\approx P(Y &gt; a + 1/2)\\)\n\\(P(X \\leq b) \\approx P(Y &lt; b + 1/2)\\)\n\\(P(X &lt; b) \\approx P(Y &lt; b - 1/2)\\)\n\\(P(a \\leq X \\leq b) \\approx P(a - 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a &lt; X \\leq b) \\approx P(a + 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a \\leq X &lt; b) \\approx P(a - 1/2 &lt; Y &lt; b - 1/2)\\)\n\\(P(a &lt; X &lt; b) \\approx P(a + 1/2 &lt; Y &lt; b - 1/2)\\)\n\nLet’s demonstrate the continuity correction\n\np &lt;- 0.5\nn &lt;- 20\nmu &lt;- n * p\nsig &lt;- sqrt(n * p * (1 - p))\na &lt;- 8\nb &lt;- 12\n\npbinom(q = b, size = n, prob = p)\n\n[1] 0.8684\n\npnorm(q = b, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.8145\n\npnorm(q = b + 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.8682\n\npbinom(q = b, size = n, prob = p) - pbinom(q = a - 1, size = n, prob = p)\n\n[1] 0.7368\n\npnorm(q = b, mean = mu, sd = sig) - pnorm(q = a, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.6289\n\npnorm(q = b + 1/2, mean = mu, sd = sig) - pnorm(q = a - 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.7364\n\n\nWe will use the normal approximation for 2-sample binomial tests.\nWhy does the normal approximation work?\nLet \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials: \\[\nT_i =\n\\begin{cases}\n1 & \\text{w.p. } p \\\\\n0 & \\text{w.p. } 1 - p\n\\end{cases}\n\\]\nLet \\[\nX = T_1 + T_2 + \\dots + T_n = \\sum T_i\n\\]\nThe Central Limit Theorem says \\(X\\) is normal for large \\(n\\).\n\n\n\nNormal Approximation to Poisson\nIf \\(X \\sim \\mathrm{Poisson}(\\mu)\\), \\(Y \\sim N(\\mu, \\mu)\\), and \\(\\mu \\geq 10\\) (rule of thumb), then\n\\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq Y \\leq b + \\frac{1}{2}\\right)\n\\]\n\n\n\nExercise 5.12 – 5.13 of Rosner\nThe duration of cigarette smoking has been linked to many diseases, including lung cancer and various forms of heart disease. Suppose we know that among men ages 30−34 who have ever smoked, the mean number of years they smoked is 12.8 with a standard deviation of 5.1 years. For women in this age group, the mean number of years they smoked is 9.3 with a standard deviation of 3.2.\n\nExercise 5.12Solution\n\n\nAssuming that the duration of smoking is normally distributed, what proportion of men in this age group have smoked for more than 20 years?\n\n\nLet \\(X\\) be the number of years a randomly sampled smoking man has smoked. Then \\(X \\sim N(12.8, 5.1^2)\\). We want \\(P(X &gt; 20)\\) which we can get via pnorm().\n\n\n\n\n\n\n\n\n\n\n1 - pnorm(q = 20, mean = 12.8, sd = 5.1)\n\n[1] 0.07901\n\n\n\n\n\n\nExercise 5.11Solution\n\n\nAnswer Problem 5.12 for women.\n\n\nLet \\(Y\\) be the number of years a randomly sampled smoking woman has smoked. Then \\(Y \\sim N(9.3, 3.2^2)\\). We want \\(P(Y &gt; 20)\\).\n\n1 - pnorm(q = 20, mean = 9.3, sd = 3.2)\n\n[1] 0.0004133\n\n\n\n\n\n\n\nExercise 5.126 – 5.130 of Rosner\nThe Christmas Bird Count (CBC) is an annual tradition in Lexington, Massachusetts. A group of volunteers counts the number of birds of different species over a 1-day period. Each year, there are approximately 30–35 hours of observation time split among multiple volunteers. The following counts were obtained for the Northern Cardinal (or cardinal, in brief) for the period 2005–2011.\n\n\n\nYear\n\\(x_i\\)\n\n\n\n\n2005\n76\n\n\n2006\n47\n\n\n2007\n63\n\n\n2008\n53\n\n\n2009\n62\n\n\n2010\n64\n\n\n2011\n67\n\n\n\nNote that: \\[\n\\sum_{i=1}^7 x_i = 432, \\quad \\sum_{i=1}^7 x_i^2 = 27,212\n\\]\n\nExercise 5.126Solution\n\n\nWhat is the mean number of cardinal birds per year observed from 2005 to 2011?\n\n\nThe mean is \\[\n\\bar{x} = \\frac{1}{7}\\sum_{i=1}^7 x_i = \\frac{1}{7}432 = 61.71\n\\]\n\n\n\n\nExercise 5.127HintSolution\n\n\nWhat is the standard deviation (sd) of the number of cardinal birds observed?\n\n\nUse the sample version of \\(\\mathrm{Var}(X) = \\mathrm{E}(X^2) - \\mathrm{E}(X)^2\\)\n\n\n\\[\\begin{align*}\ns^2(x) &= \\frac{1}{7}\\sum_{i=1}^7 x_i^2 - \\bar{x}^2\\\\\n&= \\frac{1}{7}27,212 - 61.71^2\\\\\n&= 79.3\n\\end{align*}\\]\nThus, \\[\ns(x) = \\sqrt{s^2(x)} = \\sqrt{79.3} = 8.9\n\\]\nIf you don’t round until the very end, you get:\n\nsqrt(27212 / 7 - (432/7)^2)\n\n[1] 8.876\n\n\n\n\n\nSuppose we assume that the distribution of the number of cardinal birds observed per year is normally distributed and that the true mean and sd are the same as the sample mean and sd calculated in Problems 5.126 and 5.127.\n\nExercise 5.128Solution\n\n\nWhat is the probability of observing at least 60 cardinal birds in 2012?\nTry to apply a continuity correction to get a more accurate answer.\n\n\nLet \\(X\\) be the numer of cardinal birds observed in a given year. Then we assume that, approximately \\(X \\sim N(61.71, 8.876^2)\\). We want \\(P(X &gt;= 60)\\).\nIt’s fine if you wrote\n\n1 - pnorm(q = 60, mean = 61.71, sd = 8.876)\n\n[1] 0.5764\n\n\n\nplt_norm(mu = 61.71, sig = 8.876, lb = 61.71)\n\n\n\n\n\n\n\n\nApplying the continuity correction strategy, a more accurate probability would be to subtract 1/2 from 60 first.\n\n1 - pnorm(q = 60 - 0.5, mean = 61.71, sd = 8.876)\n\n[1] 0.5983\n\n\n\n\n\nThe observers wish to identify a normal range for the number of cardinal birds observed per year. The normal range will be defined as the interval (L, U), where L is the largest integer \\(\\leq\\) 15th percentile and U is the smallest integer \\(\\geq\\) 85th percentile .\n\nExercise 5.129HintSolution\n\n\nIf we make the same assumptions as in Problem 5.128, then what is L? What is U?\n\n\nUse qnorm()\n\n\nL is 52:\n\nqnorm(p = 0.15, mean = 61.71, sd = 8.876)\n\n[1] 52.51\n\n\nU is 71.\n\nqnorm(p = 0.85, mean = 61.71, sd = 8.876)\n\n[1] 70.91\n\n\n\n\n\n\nExercise 5.130HintSolution\n\n\nWhat is the probability that the number of cardinal birds will be \\(\\geq\\) U at least once on Christmas day during the 10-year period 2012–2021? Make the same assumptions as in Problem 5.128.\n\n\nUse the binomial distribution with success probability determined by \\(P(X \\geq 71)\\), where \\(X\\) is the number of birds in a year.\n\n\nIf \\(X\\) is the number of birds in a year, then \\(X \\sim N(61.71, 8.876^2)\\). Since U = 71, \\(P(X \\geq U)\\) is\n\n1 - pnorm(q = 70.5, mean = 61.71, sd = 8.876)\n\n[1] 0.161\n\n\nLet \\(Y\\) be the number of years, out of 10, that have at least U birds. Then \\(Y \\sim \\mathrm{Binom}(10, 0.161)\\). We want \\(P(Y \\geq 1)\\).\n\n1 - pbinom(q = 0, size = 10, prob = 0.161)\n\n[1] 0.8272"
  },
  {
    "objectID": "03_prob/05_notes_old.html",
    "href": "03_prob/05_notes_old.html",
    "title": "Chapter 5 Notes: Continuous Distributions",
    "section": "",
    "text": "A continuous random variable “takes on decimal values.”\nFor such random variables, the probability at any specific value is \\(0\\).\nExample:\n\n\\(P(\\text{a man is exactly 6'2.35792471613\"}) \\approx 0\\)?\n\\(P(\\text{a man is exactly 6'2\"}) \\approx 0\\)\nMen are a little above or a little below.\n\nHowever, we know some regions are more likely than others.\n\n\\(P(5' \\leq X \\leq 7') &gt; P(0' \\leq X \\leq 1')\\)\n\nWe describe this intuition with a PDF (Probability Density Function).\nA Probability Density Function of a random variable \\(X\\) is a function, \\(f\\), where:\n\n\\(P(a \\leq X \\leq b) = \\text{area below curve between } a \\text{ and } b\\)\n\nThe CDF (Cumulative Distribution Function) is again the \\(F(x) = P(X \\leq x)\\).\nExample: Let \\(X = \\text{Serum triglyceride level}\\)\n::: {.cell} ::: {.cell-output-display}  ::: :::\n::: {.cell} ::: {.cell-output-display}  ::: :::\nExpected value, \\(\\mu\\), the average \\(X\\) over many trials:\n\n\\(\\mu = \\int_{-\\infty}^{\\infty} x f(x) \\, dx \\quad \\text{where } f(x) = \\text{density}\\)\n\nVariance: Average squared distance\n\n\\(\\sigma^2 = E\\left((X - \\mu)^2\\right) = E(X^2) - \\mu^2\\)\n\\(\\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx\\)\n\nMost common continuous distribution: Normal distribution\n\n\\(f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2} \\frac{(x - \\mu)^2}{\\sigma^2}\\right) \\quad \\text{if } X \\sim N(\\mu, \\sigma^2)\\)\nFunction of \\(\\sigma^2\\) (variance) at \\(\\mu\\) (mean)\nAlso, if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(E(X) = \\mu\\), \\(Var(X) = \\sigma^2\\)\n\nNormal distribution curve:\n\nA typical bell-shaped curve centered at \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n\nThe standard normal distribution is \\(N(0, 1)\\).\nProperties:\n\n68-95-99.7 rule:\n\n68% of area within \\(\\pm 1\\sigma\\)\n95% of area within \\(\\pm 2\\sigma\\)\n99.7% of area within \\(\\pm 3\\sigma\\)\n\nSymmetric: \\(f(\\mu - x) = f(\\mu + x)\\)\nMean = median = \\(\\mu\\)\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma} \\Rightarrow Z \\sim N(0, 1)\\)\nIf \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\), then \\(Z = X + Y\\) is also normally distributed:\n\n\\(Z \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\)\n\n\nWe denote the PDF of the standard normal by \\(\\phi(x)\\).\nThe CDF is \\(\\Phi(x) = P(X \\leq x)\\).\nExample: Blood Pressure is \\(N(80, 144)\\)\n\nMild hypertension is \\(90 \\leq DBP \\leq 100\\)\nDuring a random day, what is \\(P(\\text{mild hypertensive})\\)?\nSolution:\n\n\\(P(90 \\leq X \\leq 100)\\)\n\\(= P(X \\leq 100) - P(X \\leq 90)\\)\n\\(= \\text{pnorm}(100, \\text{mean} = 80, \\text{sd} = \\sqrt{144}) - \\text{pnorm}(90, \\text{mean} = 80, \\text{sd} = \\sqrt{144})\\)\n\\(\\approx 0.1545\\)\nXYZ Finish this figure:\n\n\n\n\n\n\n\n\n\n\n\nExample: Tree diameter \\(\\sim N(8, 2^2)\\) (in inches)\n\nWhat is the probability that a tree has a diameter &gt; 12 inches?\nSolution: \\(1 - \\text{pnorm}(12, 8, 2) \\approx 0.02275\\)\n\nIf \\(X_1, \\dots, X_n\\) are random variables and \\[ L = \\sum_{i=1}^n c_i X_i \\] for \\(c_i\\) constants (not random variables), then\n\n\\(E(L) = \\sum_{i=1}^n c_i E(X_i)\\)\n\\(\\text{Var}(L) = \\sum_{i=1}^n c_i^2 \\text{Var}(X_i)\\)\n\nIf the \\(X_i\\) are also normally distributed, then \\(L \\sim N(E(L), \\text{Var}(L))\\)\nExample:\n\n\\(X =\\) serum creatinine level for Caucasian individual\n\\(Y =\\) serum creatinine level for Black individual\n\\(X \\sim N(1.3, 0.25)\\)\n\\(Y \\sim N(1.5, 0.25)\\)\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\(Z = \\frac{1}{2} X + \\frac{1}{2} Y\\)\nThen \\(Z \\sim N(1.4, 0.125)\\)\nCalculations:\n\n\\(E(Z) = \\frac{1}{2}(1.3) + \\frac{1}{2}(1.5) = 1.4\\)\n\\(\\text{Var}(Z) = \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) = 0.125\\)\n\n\nNormal Approximation to Binomial (rule of thumb):\n\nIf \\(X \\sim \\text{Bin}(n, p)\\) and \\(np(1 - p) \\geq 5\\), then \\(X \\approx N(np, np(1 - p))\\)\n\nLet \\(X \\sim \\text{Bin}(n, p)\\), \\(Y \\sim N(np, np(1 - p))\\).\n\nThen \\(P(a \\leq X \\leq b) \\approx P\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\\) (continuity correction).\n\nWe will use this for 2-sample binomial tests.\nWhy? Let \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials.\n\n\\(T_i = \\begin{cases} 1 & \\text{with probability } p \\\\ 0 & \\text{with probability } 1 - p \\end{cases}\\)\n\\(X = T_1 + T_2 + \\dots + T_n = \\sum_{i=1}^n T_i\\)\n\nCentral Limit Theorem says normal for large \\(n\\).\nNormal Approximation to Poisson:\n\nIf \\(X \\sim \\text{Pois}(\\mu)\\), then \\(Y \\sim N(\\mu, \\mu)\\).\nRule of thumb: For \\(\\mu \\geq 10\\), \\(P(a \\leq X \\leq b) \\approx P(a - 1 \\leq Y \\leq b + 1)\\)\n\nExercise 5.12 – 5.13 of Rosner:\n\nOf men 30-34 who have smoked:\n\n$X = $ # years a man has smoked\n$Y = $ # of years smoked by women in age group\n\\(X \\sim N(12.8, 5.1^2)\\)\n\\(Y \\sim N(9.3, 3.2^2)\\)\n\nQ1: What proportion of men have smoked for more than 20 years? Women?\n\nMen: \\(1 - \\text{pnorm}(20, \\text{mean} = 12.8, \\text{sd} = 5.1) \\approx 0.0794\\)\n\n\n\n\n\n\n\n\n\n\n\nWomen: \\(1 - \\text{pnorm}(20, \\text{mean} = 9.3, \\text{sd} = 3.2) \\approx 0.0041\\)\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.12 – 5.13:\n\nThe Christmas Bird Count is a holiday tradition in a boring part of Massachusetts.\nData:\n\nYear and Number of Birds (\\(x_i\\)):\n\n2006: 76\n2007: 47\n2008: 63\n2009: 53\n2010: 62\n2011: 69\n2012: 62\n\n\\(\\sum x_i = 432\\)\n\\(\\sum x_i^2 = 27,717\\)\n\nQuestions:\n\nWhat is the mean number of birds?\n\n\\[ \\bar{x} = \\frac{\\sum x_i}{n} = \\frac{432}{7} \\approx 61.71 \\]\n\nWhat is the standard deviation?\n\nCalculate the variance: \\[ \\text{Variance} = \\frac{\\sum x_i^2}{n} - \\left(\\frac{\\sum x_i}{n}\\right)^2 \\] \\[ = \\frac{27,717}{7} - \\left(\\frac{432}{7}\\right)^2 \\] \\[ \\approx 78.78 \\]\nStandard deviation: \\[ \\text{SD} = \\sqrt{\\text{Variance}} = \\sqrt{78.78} \\approx 8.876 \\]\n\nSuppose the number of birds is normally distributed with the same mean and SD as parts 1 and 2. What is the probability of at least 60 birds? Apply the continuity correction.\n\n\\[ 1 - \\text{pnorm}(59.5, \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx 0.5983 \\]\n\nFind the “normal range” \\((L, U)\\) (integers) such that:\n\n\\(L\\) = 15th percentile\n\\(U\\) = 85th percentile\n\\[ \\text{qnorm}(c(0.15, 0.85), \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx (52.51, 70.91) \\]\nThus, 52 to 70 is the “normal range.”\n\nWhat is the probability of having a count \\(\\geq U\\) at least once during a 10-year period?\n\n\\[ P(X \\geq U) \\approx 1 - \\text{pnorm}(79.5, \\text{mean} = 61.71, \\text{sd} = 8.876) \\]\n\\[ \\approx 0.02252 \\]\nLet $Y = $ # of years \\(\\geq U \\sim \\text{Bin}(10, 0.02252)\\).\n\\[ P(Y \\geq 1) = 1 - P(Y = 0) = 1 - \\text{dbinom}(0, 10, 0.02252) \\]\n\\[ \\approx 0.2037 \\]"
  },
  {
    "objectID": "04_est/06_notes.html",
    "href": "04_est/06_notes.html",
    "title": "Chapter 6 Notes: Estimation",
    "section": "",
    "text": "stateDiagram-v2\n    state \"Data Generating Process\" as Process\n    Process --&gt; Data: Probability\n    Data --&gt; Process: Inference\n\n\n\n\n\n\n\n\nProbability (Chapters 3, 4, and 5):\n\nWe know that \\(X \\sim N(80, 12)\\)\n\nWhat is \\(\\Pr(X &gt; 90)\\)?\n\nInference (Chapters 6 through 14):\n\nWe observe \\(X_i = 81, 78, 77, 89, \\ldots\\)\n\nWe assume \\(X_i \\sim N(\\mu, \\sigma^2)\\)\n\nBut the values of \\(\\mu\\) and \\(\\sigma^2\\) are unknown.\n\nWhat are \\(\\mu\\) and \\(\\sigma^2\\)?\n\nEstimation: Guess parameter values from data\n\nPoint estimation: A single number is your best guess\n\nE.g., estimate \\(\\mu\\) with \\(\\bar{X}\\)\n\nInterval estimation: Get a range of likely values of a parameter\n\nE.g., confidence intervals\n\n\nHypothesis testing: How sure are we a parameter is different from some value?\n\nE.g., \\(H_0: \\mu = 0\\)\n\n\n \n\n\n\n\n\n\nTipReference / Target / Study Population\n\n\n\nGroup we are interested in\n\n\n\n\n\n\n\n\nTipSample\n\n\n\nGroup we have data about\n\n\n\n\n\n\n\n\nTipParameter\n\n\n\nNumeric summary of population\nE.g.: \\(\\mu\\), \\(\\sigma^2\\), \\(p\\)\n\n\n\n\n\n\n\n\nTipStatistic\n\n\n\nNumeric summary of sample\nE.g. \\(\\bar{X}\\), \\(s^2\\), \\(\\hat{p}\\)\n\n\n\nSimplest way to get a sample is by a simple random sample\n\nEach unit has an equal chance of being in sample\n\nRandom selection (e.g., via SRS) is distinct from Random Assignment.\nRandom assignment: randomly assign units to different groups (e.g., treatment vs. control)\nRandom selection: results generalizable to target population\n\nBecause sample is similar to population in terms of demographic variables\n\nRandom assignment: allows for claims of causality\n\nBecause all possible confounders are equal (on average) in the groups\n\nRandomized Clinical Trial (RCT): Random assignment of treatments to compare them.\nNo causal claims without random assignment\nExample: tobramycin and gentamicin are antibiotics.\n\nTobramycin is more aggressive and has more side effects.\n\nEarly studies were not randomized and showed tobramycin performed worse. Why?\n\nDoctors gave sicker patients tobramycin\n\nRandomization guarantees equal number of sicker and less sick in each group (on average)\n\nDouble blind: neither doctor nor patient know treatment\n\nGuards against placebo effect\n\nSingle blind: doctor knows\nUnblinded: both know\n\n\nSampling in R"
  },
  {
    "objectID": "02_descriptive/02_notes.html#measures-of-spread",
    "href": "02_descriptive/02_notes.html#measures-of-spread",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "Measures of Spread",
    "text": "Measures of Spread\n\nSpread: How far apart numbers are.\nRange: \\(\\text{Max} - \\text{Min}\\) (sensitive to extreme values).\nInter-quartile Range (IQR): \\(75^{\\text{th}}\\) percentile - \\(25^{\\text{th}}\\) percentile.\n\\(p^{\\text{th}}\\) percentile = value \\(V_p\\) such that \\(p\\%\\) of points are at or below \\(V_p\\).\n\nExample: Median = \\(50^{\\text{th}}\\) percentile.\n\nQuantile: in units of proportions instead of percents.\n\n\\(0.75\\) quantile = \\(75^{\\text{th}}\\) percentile.\n\nExample: \\(X_1 = 2\\), \\(X_2 = 5\\), \\(X_3 = -4\\)\n\n\\(\\frac{1}{3}\\) quantile = \\(-4\\)\n\\(\\frac{2}{3}\\) quantile = \\(2\\)\n\\(1\\) quantile = \\(5\\)\n\nWhat about the \\(40^{\\text{th}}\\) percentile?\n\nUse some average of \\(-4\\) and \\(2\\), but definition varies.\nThe quantile() function in R has 9 different definitions of how this imputation works. See ?quantile.\n\nVariance: Average of squared deviations.\n\\[\n  s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\n  \\]\n\n\\(n-1\\) because we lose some information by estimating \\(\\bar{X}\\).\n\nStandard Deviation: Square root of variance.\n\\[\n  s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2}\n  \\]\n\nPuts this measure of spread on the same scale as units (e.g., \\(oz\\) instead of \\(oz^2\\)).\n\nLet \\(y_i = c_1 x_i + c_2\\), then \\(s^2(y) = c_1^2 s^2(x)\\) and \\(s(y) = c_1 s(x)\\)\n\nOnly scaling affects variance and standard deviation.\n\nWhy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: What is \\(s^2(y)\\) when \\(y_i = c_1 x_i\\)?\nR Notebook on Mean / Median / SD / Variance"
  },
  {
    "objectID": "02_descriptive/02_notes.html#properties-of-barx",
    "href": "02_descriptive/02_notes.html#properties-of-barx",
    "title": "Chapter 2 Notes: Descriptive Statistics",
    "section": "",
    "text": "Suppose you have a frequency table.\nThe intervals between menstrual periods (days):\n\n\n\nValue\nFreq\n\n\n\n\n24\n5\n\n\n25\n10\n\n\n26\n28\n\n\n27\n64\n\n\n28\n185\n\n\n\nMean of \\(X\\):\n\\[\n  n = 5 + 10 + 28 + 64 + 185 = 292\n  \\]\n\\[\n  \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{292} (5 \\cdot 24 + 10 \\cdot 25 + 28 \\cdot 26 + 64 \\cdot 27 + 185 \\cdot 28) = 27.42\n  \\]\nMedian of \\(X\\)\n\\[\n\\text{Median}(X) = \\frac{146^{\\text{th}} \\text{ and } 147^{\\text{th}} \\text{ values}}{2}\n\\]\n\\[\n= \\frac{28 + 28}{2} = 28\n\\]\nLet \\(y_i = x_i + C\\), then \\(\\bar{y} = \\bar{X} + C\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n (x_i + C) = \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n C = \\bar{X} + \\frac{1}{n}nC = \\bar{X} + C\n\\]\n\nExample: Let \\(y_i\\) = deviation from 28 days cycle.\n\\[\n  y_i = x_i - 28\n  \\]\n\\[\n  \\bar{y} = 27.42 - 28 = -0.58\n  \\]\nAlso true for Median: \\[\n  \\text{Median}(y) = \\text{Median}(x) + C\n  \\]\nLet \\(y_i = C x_i\\), then \\(\\bar{y} = C \\bar{X}\\)\n\nProof:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n C x_i = C \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i = C \\bar{X}\n\\]\n\nExample: Change units from days to weeks.\n\\[\n  y_i = \\frac{1}{7} x_i\n  \\]\n\\[\n  \\bar{y} = \\frac{1}{7} \\cdot 27.42 \\approx 3.92\n  \\]\nIf \\(y_i = a x_i + C\\), then \\(\\bar{y} = a \\bar{X} + C\\)\n\n\nExerciseSolution\n\n\nWhat is the mean menstrual cycle deviation from 4 weeks?\n\n\n\\[\n3.92 - 4 = -0.08\n\\]"
  },
  {
    "objectID": "03_prob/03_notes.html#exercises-3.53---3.63-of-rosner",
    "href": "03_prob/03_notes.html#exercises-3.53---3.63-of-rosner",
    "title": "Chapter 3 Notes: Probability",
    "section": "Exercises 3.53 - 3.63 of Rosner",
    "text": "Exercises 3.53 - 3.63 of Rosner\n\nWord problemConvert to math notation\n\n\nThe familial aggregation of respiratory disease is a well-established clinical phenomenon. However, whether this aggregation is due to genetic or environmental factors or both is somewhat controversial. An investigator wishes to study a particular environmental factor, namely the relationship of cigarette-smoking habits in the parents to the presence or absence of asthma in their oldest child age 5 to 9 years living in the household (referred to below as their offspring). Suppose the investigator finds that (1) if both the mother and father are current smokers, then the probability of their offspring having asthma is .15; (2) if the mother is a current smoker and the father is not, then the probability of their offspring having asthma is .13; (3) if the father is a current smoker and the mother is not, then the probability of their offspring having asthma is .05; and (4) if neither parent is a current smoker, then the probability of their offspring having asthma is .04.\n\n\n\nDefine:\n\n\\(M^+ =\\) Mother smokes\n\\(F^+ =\\) Father smokes\n\\(O^+ =\\) Offspring has asthma\n\nGiven probabilities:\n\n\n\nScenario\n\\(P(O^+ | \\text{Scenario})\\)\n\n\n\n\n\\(M^+ \\cap F^+\\)\n0.15\n\n\n\\(M^+ \\cap F^-\\)\n0.13\n\n\n\\(M^- \\cap F^+\\)\n0.05\n\n\n\\(M^- \\cap F^-\\)\n0.04\n\n\n\n\n\n\n\n\nExercise 1.Solution\n\n\nSuppose the smoking habits of the parents are independent and the probability that the mother is a current smoker is .4, whereas the probability that the father is a current smoker is .5. What is the probability that both the father and mother are current smokers?\n\n\nWe want to calculate \\(P(M^+ \\cap F^+)\\) given: \\(P(M^+) = 0.4\\), \\(P(F^+) = 0.5\\), \\(M^+ \\perp F^+\\) \\[\nP(M^+ \\cap F^+) = P(M^+) \\cdot P(F^+) = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 2.Solution\n\n\nConsider the subgroup of families in which the mother is not a current smoker. What is the probability that the father is a current smoker among such families? How does this probability differ from that calculated in Problem 1?\n\n\nWe want to calculate \\(P(F^+ | M^-)\\). However, since \\(M^+\\) and \\(F^+\\) are independent, \\(P(F^+ | M^-) = P(F^+) = 0.5\\).\n\n\n\n\nExercise 3.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 4.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 5.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 6.Solution\n\n\n\n\n\n\n\n\n\n\nExercise 7.Solution\n\n\n\n\n\n\n\n\n\n\nGiven:\n\n\\(P(M^+ | F^+) = 0.6\\)\n\\(P(M^+ | F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\nCalculate \\(P(F^+ \\cap M^-)\\):\n\n\n\nIs \\(F^+ \\perp M^+\\)?\n\n\nFind \\(P(O^+)\\):\n\n\nCalculate \\(P(F^+ | O^+)\\):\n\n\nCalculate \\(P(M^+ | O^+)\\):"
  },
  {
    "objectID": "03_prob/03_notes.html#exercises-3.53---3.59-of-rosner",
    "href": "03_prob/03_notes.html#exercises-3.53---3.59-of-rosner",
    "title": "Chapter 3 Notes: Probability",
    "section": "Exercises 3.53 - 3.59 of Rosner",
    "text": "Exercises 3.53 - 3.59 of Rosner\n\nExercise 0Solution\n\n\nThe familial aggregation of respiratory disease is a well-established clinical phenomenon. However, whether this aggregation is due to genetic or environmental factors or both is somewhat controversial. An investigator wishes to study a particular environmental factor, namely the relationship of cigarette-smoking habits in the parents to the presence or absence of asthma in their oldest child age 5 to 9 years living in the household (referred to below as their offspring). Suppose the investigator finds that (1) if both the mother and father are current smokers, then the probability of their offspring having asthma is 0.15; (2) if the mother is a current smoker and the father is not, then the probability of their offspring having asthma is 0.13; (3) if the father is a current smoker and the mother is not, then the probability of their offspring having asthma is 0.05; and (4) if neither parent is a current smoker, then the probability of their offspring having asthma is 0.04.\nConvert this word problem to mathematical notation amenable to analysis.\n\n\n\nDefine:\n\n\\(M^+ =\\) Mother smokes\n\\(F^+ =\\) Father smokes\n\\(O^+ =\\) Offspring has asthma\n\nGiven probabilities:\n\n\n\nScenario\n\\(P(O^+ | \\text{Scenario})\\)\n\n\n\n\n\\(M^+ \\cap F^+\\)\n0.15\n\n\n\\(M^+ \\cap F^-\\)\n0.13\n\n\n\\(M^- \\cap F^+\\)\n0.05\n\n\n\\(M^- \\cap F^-\\)\n0.04\n\n\n\n\n\n\n\n\nExercise 1HintSolution\n\n\nSuppose the smoking habits of the parents are independent and the probability that the mother is a current smoker is 0.4, whereas the probability that the father is a current smoker is 0.5. What is the probability that both the father and mother are current smokers?\n\n\nUse the “and” rule.\n\n\nWe want to calculate \\(P(M^+ \\cap F^+)\\) given: \\(P(M^+) = 0.4\\), \\(P(F^+) = 0.5\\), \\(M^+ \\perp F^+\\) \\[\nP(M^+ \\cap F^+) = P(M^+) \\cdot P(F^+) = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 2HintSolution\n\n\nConsider the subgroup of families in which the mother is not a current smoker. What is the probability that the father is a current smoker among such families? How does this probability differ from that calculated in Problem 1?\n\n\nUse independence.\n\n\nWe want to calculate \\(P(F^+ | M^-)\\). However, since \\(M^+\\) and \\(F^+\\) are independent, \\(P(F^+ | M^-) = P(F^+) = 0.5\\). This is just the probability that the father is a smoker, as opposed to part 1 where we calculated the probability that both father and mother are smokers.\n\n\n\nSuppose, alternatively, that if the father is a current smoker, then the probability that the mother is a current smoker is 0.6; whereas if the father is not a current smoker, then the probability that the mother is a current smoker is 0.2. Also assume that statements 1, 2, 3, and 4 above hold.\n\nExercise 3HintSolution\n\n\nIf the probability that the father is a current smoker is 0.5, what is the probability that the father is a current smoker and that the mother is not a current smoker?\n\n\nUse \\(P(F^+ \\cap M^-) = P(M^- | F^+) \\cdot P(F^+)\\).\n\n\nConverting the word problem to math notation, we have::\n\n\\(P(M^+ | F^+) = 0.6\\)\n\\(P(M^+ | F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\nOur goal is to calculate \\(P(F^+ \\cap M^-)\\).\nBy the complement rule, we have \\(P(M^- | F^+) = 1 - 0.6\\). Using this, we have:\n\n\\[\nP(F^+ \\cap M^-) = P(M^- | F^+) \\cdot P(F^+) = (1 - 0.6) \\cdot 0.5 = 0.4 \\cdot 0.5 = 0.2\n\\]\n\n\n\n\nExercise 4HintSolution\n\n\nAre the current smoking habits of the father and the mother independent? Why or why not?\n\n\nWhat is \\(P(M^+|F^+)\\)? What is \\(P(M^+|F^-)\\)?\n\n\nThere are a couple ways to check this:\n\nSee if \\(P(M^+|F^+) = P(M^+|F^-) = P(M^+)\\) (value of \\(F\\) does not matter). Any inequality here indicates non-independence.\nSee if \\(P(M^+ \\cap F^+) = P(M^+)P(F^+)\\), or any other combination, such as \\(P(M^- \\cap F^+) = P(M^-)P(F^+)\\). An inequality here indicates non-independence.\n\nThe easiest way is just the first. We know that \\(P(M^+|F^+) =  0.6 \\neq 0.2 = P(M^+|F^-)\\). So no, they are not independent.\n\n\n\n\nExercise 5HintSolution\n\n\nUnder the assumptions made in Problems 3 and 4, find the unconditional probability that the offspring will have asthma.\n\n\nUse the law of total probability. You’ll be using \\(P(O^+ | M^+ \\cap F^+)\\), \\(P(O^+ | M^+ \\cap F^-)\\), \\(P(O^+ | M^- \\cap F^+)\\), \\(P(O^+ | M^- \\cap F^-)\\), \\(P(M^+ \\cap F^+)\\), \\(P(M^+ \\cap F^-)\\), \\(P(M^- \\cap F^+)\\), and \\(P(M^- \\cap F^-)\\).\n\n\nWe want to calculate \\(P(O^+)\\):\nUsing the law of total probability: \\[\\begin{align*}\nP(O^+) &= P(O^+ | M^+ \\cap F^+) P(M^+ \\cap F^+) + P(O^+ | M^+ \\cap F^-) P(M^+ \\cap F^-)\\\\\n&+ P(O^+ | M^- \\cap F^+) P(M^- \\cap F^+) + P(O^+ | M^- \\cap F^-) P(M^- \\cap F^-)\n\\end{align*}\\]\nWe were given:\n\n\\(P(O^+ | M^+ \\cap F^+) = 0.15\\),\n\\(P(O^+ | M^+ \\cap F^-) = 0.13\\),\n\\(P(O^+ | M^- \\cap F^+) = 0.05\\), and\n\\(P(O^+ | M^- \\cap F^-) = 0.04\\),\n\\(P(M^+|F^+) = 0.6\\)\n\\(P(M^+|F^-) = 0.2\\)\n\\(P(F^+) = 0.5\\)\n\nWe have\n\n\\(P(M^+ \\cap F^+) = P(M^+|F^+)P(F^+) = 0.6 * 0.5 = 0.3\\)\n\\(P(M^+ \\cap F^-) = P(M^+|F^-)P(F^+) = 0.2 * 0.5 = 0.1\\)\n\\(P(M^- \\cap F^+) = P(M^-|F^+)P(F^+) = (1 - 0.6) * 0.5 = 0.2\\)\n\\(P(M^- \\cap F^-) = P(M^-|F^-)P(F^+) = (1 - 0.2) * 0.5 = 0.4\\)\n\nSubstituting the values: \\[\nP(O^+) = (0.15 \\cdot 0.3) + (0.13 \\cdot 0.1) + (0.05 \\cdot 0.2) + (0.04 \\cdot 0.4) = 0.084\n\\]\n\n\n\n\nExercise 6HintSolution\n\n\nSuppose a child has asthma. What is the posterior probability that the father is a current smoker?\n\n\nUse Bayes rule.\nAlso note that \\(P(O^+ \\cap F^+) = P(O^+ \\cap F^+ \\cap M^+) + P(O^+ \\cap F^+ \\cap M^-)\\).\n\n\nWe want to calculate \\(P(F^+ | O^+)\\). Using Bayes rule we have\n\\[\nP(F^+ | O^+) = \\frac{P(F^+ \\cap O^+)}{P(O^+)}\n\\] We already calculated \\(P(O^+) = 0.084\\) from Exercise 5. The hard part is \\(P(F^+ \\cap O^+)\\). But we can use the law of total probability to get that. \\[\\begin{align*}\nP(F^+ \\cap O^+) &= P(F^+ \\cap O^+ \\cap M^+) + P(F^+ \\cap O^+ \\cap M^-)\\\\\n&= P(O^+ | F^+ \\cap M^+)P(F^+ \\cap M^+) + P(O^+ | F^+ \\cap M^-)P(F^+ \\cap M^-)\\\\\n&= 0.15 \\cdot 0.3 + 0.05 \\cdot 0.2\\\\\n&= 0.055,\n\\end{align*}\\] where we used \\(P(F^+ \\cap M^+) = 0.3\\) and \\(P(F^+ \\cap M^-) = 0.2\\) from Exercise 5.\nSubstituting values we have \\[\nP(F^+ | O^+) = \\frac{0.055}{0.084} = 0.6548.\n\\]\n\n\n\n\nExercise 7HintSolution\n\n\nWhat is the posterior probability that the mother is a current smoker if the child has asthma?\n\n\nUse the same strategy as in Exercise 6.\n\n\nWe want \\(P(M^+ | O^+)\\). Using Bayes rule we have \\[\nP(M^+ | O^+) = \\frac{P(O^+ \\cap M^+)}{P(O^+)}\n\\] We can use the exact same strategy as in Exercise 6.\n\\[\\begin{align*}\nP(M^+ \\cap O^+) &= P(F^+ \\cap M^+ \\cap O^+) + P(F^- \\cap M^+ \\cap O^+) \\text{ (law of total probability)}\\\\\n&= P(O^+ | F^+ \\cap M^+) P(F^+ \\cap M^+) + P(O^+ | F^- \\cap M^+) P(F^- \\cap M^+) \\text{ (conditional probability)}\\\\\n&= 0.15 \\cdot 0.3 + 0.13 \\cdot 0.1 \\text{ (given/calculated previously)}\\\\\n&= 0.058\n\\end{align*}\\]\nSubstituting in values we have \\[\nP(M^+ | O^+) = \\frac{0.058}{0.084} = 0.6905.\n\\]"
  },
  {
    "objectID": "03_prob/04_notes.html#provided-distribution-in-r",
    "href": "03_prob/04_notes.html#provided-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "If given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/04_notes.html#binomial-distribution-in-r",
    "href": "03_prob/04_notes.html#binomial-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Binomial Distribution in R",
    "text": "Binomial Distribution in R\n\nThe PMF is dbinom().\nAbout 60% of all white blood cells are neutrophils. If we observe 10 white blood cells, the probability of seeing 4 neutrophils is\n\\[\n\\binom{10}{4} 0.6^4 0.4^6\n\\]\n\ndbinom(x = 4, size = 10, prob = 0.6)\n\n[1] 0.1115\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is pbinom():\nThe probability of seeing 4 or fewer neutrophiles out of 10 white blood cells is\n\\[\nPr(X \\leq x) = \\sum_{r=0}^x\\binom{10}{r} 0.6^r 0.4^{n-r}\n\\]\n\npbinom(q = 4, size = 10, prob = 0.6)\n\n[1] 0.1662\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qbinom().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 6\n\nqbinom(p = 0.55, size = 10, prob = 0.6)\n\n[1] 6\n\n\nbecause the CDF at 6 is above 0.55 and the CDF at 5 is below 0.55.\n\npbinom(q = 5, size = 10, prob = 0.6)\n\n[1] 0.3669\n\npbinom(q = 6, size = 10, prob = 0.6)\n\n[1] 0.6177\n\n\nYou generate random samples from the binomial distribution with rbinom()\n\nx &lt;- rbinom(n = 100, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rbinom(n = 10000, size = 10, prob = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nThe underlying incidence rate of chronic bronchitis in the first year of life is 0.05. What is the probability of obtaining at least 75 cases of chronic bronchitis in the first year of life among 1500 families?\n\n\n\n1 - pbinom(q = 74, size = 1500, prob = 0.05)\n\n[1] 0.5165\n\n\nor\n\npbinom(q = 74, size = 1500, prob = 0.05, lower.tail = FALSE)\n\n[1] 0.5165\n\n\n\n\n\n\nExerciseSolution\n\n\nSuppose a group of 100 women ages 60–64 received a new flu vaccine in 2004, and 5 of them died within the next year. Is this event unusual? According to life tables, the probability of death for this age group in the next year is 0.009. Calculate the probability that five or more such women would die under normal circumstances if the flu vaccine had no effect.\nWhat is the expected number of such women (out of 100) that we would expect to die in the next year?\n\n\n\n1 - stats::pbinom(q = 4, size = 100, prob = 0.009)\n\n[1] 0.002191\n\n\nYes, very unusual.\n100 * 0.009 = 0.9. So about 1 woman out of 100."
  },
  {
    "objectID": "03_prob/04_notes.html#poisson-distribution-in-r",
    "href": "03_prob/04_notes.html#poisson-distribution-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "Poisson Distribution in R",
    "text": "Poisson Distribution in R\n\nThe PMF is dpois().\nNumber of deaths from typhoid-fever is over a 1-year period approximately Poisson with rate \\(\\lambda = 4.6\\). The probability of exactly 3 deaths is\n\\[\ne^{-4.6}\\frac{4.6^3}{3!}\n\\]\n\ndpois(x = 3, lambda = 4.6)\n\n[1] 0.1631\n\n\n\n\n\n\n\n\n\n\n\nThe CDF is ppois():\n\\[\nPr(X \\leq x) = \\sum_{k=0}^{x}e^{-4.6}\\frac{4.6^k}{k!}\n\\]\n\nppois(q = 3, lambda = 4.6)\n\n[1] 0.3257\n\n\n\n\n\n\n\n\n\n\n\nThe quantile function is qpois().\n\\[\nf(p) = \\min(x) \\text{ such that } p \\leq Pr(X \\leq x)\n\\]\nE.g., the quantile function applied at 0.55 is 5\n\nqpois(p = 0.55, lambda = 4.6)\n\n[1] 5\n\n\nbecause the CDF at 5 is above 0.55 and the CDF at 4 is below 0.55.\n\nppois(q = 4, lambda = 4.6)\n\n[1] 0.5132\n\nppois(q = 5, lambda = 4.6)\n\n[1] 0.6858\n\n\nYou generate random samples from the poisson distribution with rpois()\n\nx &lt;- rpois(n = 100, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- rpois(n = 10000, lambda = 4.6)\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Approximation to Binomial\n\nFor \\(n\\) large, \\(p\\) small, and \\(np\\) intermediate, we have that if \\(X \\sim Binom(n, p)\\) then we also have approximately that \\(X \\sim Pois(np)\\).\nRule of thumb: \\(n \\geq 100\\) and \\(p \\leq 0.01\\)\nExample:\n\nn &lt;- 100\np &lt;- 0.01\ntibble(\n  Binom = dbinom(x = 0:5, size = n, prob = p),\n  Pois = dpois(x = 0:5, lambda = n * p)\n)\n\n\n\n\n\n\n\n\n\nBinom\nPois\n\n\n\n\n0.37\n0.37\n\n\n0.37\n0.37\n\n\n0.18\n0.18\n\n\n0.06\n0.06\n\n\n0.01\n0.02\n\n\n0.00\n0.00\n\n\n\n\n\n\n\nYou don’t use this anymore to actually calculate binomial probabilities, since computers do that efficiently without resorting to an approximation.\nThis is mostly useful in cases to justify using the Poisson.\nE.g., we see monthly number of cases of Guillain-Barré syndrome in Finland\n\nApril 1984: 3\nMay 1984: 7\n\nJune 1984: 0\n\nJuly 1984: 3\n\nAugust 1984: 4\n\nSeptember 1984: 4\n\nOctober 1984: 2\n\nThe distribution of the number of cases during a month is likely well approximated by a binomial, with \\(n\\) equaling the population of Finland. But we don’t know \\(n\\), so we can use a Poisson distribution to model these counts.\nIt is also useful if you want to add two Binomial counts together. If \\(X \\sim \\mathrm{Binom}(n_1, p_1)\\) and \\(Y \\sim \\mathrm{Binom}(n_2, p_2)\\), suppose we want to calculated the distribution of \\(Z = X + Y\\).\n\nTo do this exactly, we need to use a discrete linear convolution, which is annoying.\n\n(unless \\(p_1 = p_2\\))\n\nBut if we can approximate \\(X \\sim \\mathrm{Poi}(n_1p_1)\\) and \\(Y \\sim \\mathrm{Poi}(n_2p_2)\\), then \\(Z \\sim \\mathrm{Poi}(n_1p_1 + n_1p_2)\\), which is super nice.\nE.g. Population 1 has 100 individuals and each individual has a 0.002 probability of getting a disease. Population 2 has 200 individuals and each individual has a 0.004 probability of getting the disease. What is the distribution of the total number of folks who get the disease?\n\nIt’s about Poisson(0.2 + 0.8) = Poisson(1)."
  },
  {
    "objectID": "03_prob/04_notes.html#provided-distributions-in-r",
    "href": "03_prob/04_notes.html#provided-distributions-in-r",
    "title": "Chapter 4 Notes: Discrete Distributions",
    "section": "",
    "text": "If given a probability mass function, can create a data frame of it\n\nlibrary(tidyverse)\npmf &lt;- tibble(\n  r = 0:4,\n  pr = c(0.008, 0.076, 0.265, 0.411, 0.240)\n)\npmf\n\n# A tibble: 5 × 2\n      r    pr\n  &lt;int&gt; &lt;dbl&gt;\n1     0 0.008\n2     1 0.076\n3     2 0.265\n4     3 0.411\n5     4 0.24 \n\n\nWe can verify that the PMF sums to 1\n\nsum(pmf$pr)\n\n[1] 1\n\n\nWe can calculate the mean and variance like so\n\nmu &lt;- sum(pmf$r * pmf$pr)\nmu\n\n[1] 2.799\n\nsigma2 &lt;- sum((pmf$r - mu)^2 * pmf$pr)\nsigma2\n\n[1] 0.8406\n\n\nYou can get the CDF via cumsum()\n\npmf |&gt;\n  mutate(cdf = cumsum(pr))\n\n# A tibble: 5 × 3\n      r    pr   cdf\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.008 0.008\n2     1 0.076 0.084\n3     2 0.265 0.349\n4     3 0.411 0.76 \n5     4 0.24  1    \n\n\nWe can plot it\n\nggplot(pmf, aes(x = r, y = pr)) +\n  geom_col(fill = \"black\")"
  },
  {
    "objectID": "03_prob/05_notes.html#solution-3",
    "href": "03_prob/05_notes.html#solution-3",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "Solution",
    "text": "Solution\nThe mean is \\[\n\\bar{x} = \\frac{1}{7}\\sum_{i=1}^7 x_i = \\frac{1}{7}432 = 61.71\n\\] :::\n\nExercise 5.127HintSolution\n\n\nWhat is the standard deviation (sd) of the number of cardinal birds observed?\n\n\nUse the sample version of \\(\\mathrm{Var}(X) = \\mathrm{E}(X^2) - \\mathrm{E}(X)^2\\)\n\n\n\\[\\begin{align*}\ns^2(x) &= \\frac{1}{7}\\sum_{i=1}^7 x_i^2 - \\bar{x}^2\\\\\n&= \\frac{1}{7}27,212 - 61.71^2\\\\\n&= 79.3\n\\end{align*}\\]\nThus, \\[\ns(x) = \\sqrt{s^2(x)} = \\sqrt{79.3} = 8.9\n\\]\nIf you don’t round until the very end, you get:\n\nsqrt(27212 / 7 - (432/7)^2)\n\n[1] 8.876\n\n\n\n\n\nSuppose we assume that the distribution of the number of cardinal birds observed per year is normally distributed and that the true mean and sd are the same as the sample mean and sd calculated in Problems 5.126 and 5.127.\n\nExercise 5.128Solution\n\n\nWhat is the probability of observing at least 60 cardinal birds in 2012? (Hint: Apply a continuity correction where appropriate.)\n\n\n\n\n\n\nThe observers wish to identify a normal range for the number of cardinal birds observed per year. The normal range will be defined as the interval (L, U), where L is the largest integer ≤ 15th percentile and U is the smallest integer \\(\\geq\\) 85th percentile .\n\nExercise 5.129Solution\n\n\nIf we make the same assumptions as in Problem 5.128, then what is L? What is U?\n\n\n\n\n\n\n\nExercise 5.130Solution\n\n\nWhat is the probability that the number of cardinal birds will be \\(\\geq\\) U at least once on Christmas day during the 10-year period 2012–2021? (Hint: Make the same assumptions as in Problem 5.128.)\n\n\n\n\n\n\n\nWhat is the mean number of birds?\n\\[\n\\bar{x} = \\frac{432}{7} = 61.71\n\\]\nWhat is the standard deviation?\n\\[\n\\frac{1}{7}(27717) - \\left(\\frac{432}{7}\\right)^2 = 78.78\n\\]\n\\[\n\\mathrm{SD} = \\sqrt{78.78} = 8.876\n\\]\nSuppose number of birds is normal with same mean and SD as previous years.\nWhat is the probability of at least 60 birds? Apply continuity correction.\n\\[\n1 - \\texttt{pnorm}(59.5, \\text{mean} = 61.71, \\text{sd} = 8.876) = \\boxed{0.5983}\n\\]\nFind “normal range” \\((L, U)\\) (integers) such that:\n\n\\(L\\) = 5th percentile\n\n\\(U\\) = 95th percentile\n\n\\[\n\\texttt{qnorm}(c(0.05, 0.95), \\text{mean} = 61.71, \\text{sd} = 8.876) = (52.51, 70.91)\n\\]\n\nSo the normal range is 52 to 80\n\nWhat is the probability that \\(X \\geq U\\) at least once during a 10-year period?\n\\[\n\\Pr(X \\geq U) = 1 - \\texttt{pnorm}(79.5, \\text{mean} = 61.71, \\text{sd} = 8.876) = 0.02252\n\\]\nLet \\(Y\\) = number of years \\(\\geq U\\)\n\\(Y \\sim \\mathrm{Bin}(10, 0.02252)\\)\n\\[\n\\Pr(Y \\geq 1) = 1 - \\Pr(Y = 0) = 1 - \\texttt{dbinom}(0, 10, 0.02252) = \\boxed{0.2037}\n\\]"
  },
  {
    "objectID": "03_prob/05_notes.html#normal-distribution-in-r",
    "href": "03_prob/05_notes.html#normal-distribution-in-r",
    "title": "Chapter 5: Continuous Probability Distributions",
    "section": "",
    "text": "Density Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nset.seed(1)\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nTree diameter \\(\\sim N(8, 2^2)\\) (in inches). What is the probability that the tree has diameter \\(&gt; 12\\) in?\n\n\n\n1 - pnorm(q = 12, mean = 8, sd = 2)\n\n[1] 0.02275\n\n\n\n\n\n\nIf \\(X_1, \\dots, X_n\\) are random variables and\n\\[\nL = \\sum_{i=1}^n c_i X_i \\quad \\text{for } c_i \\text{ constants (not r.v.s)}\n\\] then\n\\[\n\\mathrm{E}[L] = \\sum_{i=1}^n c_i \\mathrm{E}[X_i]\n\\]\nIf the \\(X_i\\)’s are also independent, then \\[\n\\mathrm{Var}(L) = \\sum_{i=1}^n c_i^2 \\mathrm{Var}(X_i)\n\\]\nIf the \\(X_i\\) are also normally distributed, then\n\\[\nL \\sim N(\\mathrm{E}[L], \\mathrm{Var}(L))\n\\]\nExample:\nLet \\(X\\) = serum creatinine level for a Caucasian individual\nLet \\(Y\\) = serum creatinine level for a Black individual\nAssume: \\[\nX \\sim N(1.3, 0.25), \\quad Y \\sim N(1.5, 0.25)\n\\]\nWhat is the distribution of the average level for one Caucasian and one Black individual chosen at random?\nLet \\[\nZ = \\frac{1}{2}X + \\frac{1}{2}Y \\Rightarrow Z \\sim N(1.4, 0.125)\n\\]\n\n\\(\\mathrm{E}(Z) = \\frac{1}{2}(1.3 + 1.5) = 1.4\\)\n\\(\\mathrm{Var}(Z) = \\frac{1}{4}(0.25 + 0.25) = 0.125\\)\n\n\n\n\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\) and \\(np(1-p) \\geq 5\\) (rule of thumb), then \\[\n  X \\approx N(np, np(1-p))\n  \\]\n\nLet \\(X \\sim \\mathrm{Bin}(n, p)\\), and let \\(Y \\sim N(np, np(1 - p))\\).\nThen with continuity correction: \\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq X \\leq b + \\frac{1}{2}\\right)\n\\]\nMore generally, the continuity correction says that, if you are approximating a discrete random variable \\(X\\) with a normal distribution \\(Y\\), then\n\n\\(P(X \\geq a) \\approx P(Y &gt; a - 1/2)\\)\n\\(P(X &gt; a) \\approx P(Y &gt; a + 1/2)\\)\n\\(P(X \\leq b) \\approx P(Y &lt; b + 1/2)\\)\n\\(P(X &lt; b) \\approx P(Y &lt; b - 1/2)\\)\n\\(P(a \\leq X \\leq b) \\approx P(a - 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a &lt; X \\leq b) \\approx P(a + 1/2 &lt; Y &lt; b + 1/2)\\)\n\\(P(a \\leq X &lt; b) \\approx P(a - 1/2 &lt; Y &lt; b - 1/2)\\)\n\\(P(a &lt; X &lt; b) \\approx P(a + 1/2 &lt; Y &lt; b - 1/2)\\)\n\nLet’s demonstrate the continuity correction\n\np &lt;- 0.5\nn &lt;- 20\nmu &lt;- n * p\nsig &lt;- sqrt(n * p * (1 - p))\na &lt;- 8\nb &lt;- 12\n\npbinom(q = b, size = n, prob = p)\n\n[1] 0.8684\n\npnorm(q = b, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.8145\n\npnorm(q = b + 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.8682\n\npbinom(q = b, size = n, prob = p) - pbinom(q = a - 1, size = n, prob = p)\n\n[1] 0.7368\n\npnorm(q = b, mean = mu, sd = sig) - pnorm(q = a, mean = mu, sd = sig) ## no continuity correction\n\n[1] 0.6289\n\npnorm(q = b + 1/2, mean = mu, sd = sig) - pnorm(q = a - 1/2, mean = mu, sd = sig) ## with continuity correction\n\n[1] 0.7364\n\n\nWe will use the normal approximation for 2-sample binomial tests.\nWhy does the normal approximation work?\nLet \\(T_1, T_2, \\dots, T_n\\) be \\(n\\) independent Bernoulli trials: \\[\nT_i =\n\\begin{cases}\n1 & \\text{w.p. } p \\\\\n0 & \\text{w.p. } 1 - p\n\\end{cases}\n\\]\nLet \\[\nX = T_1 + T_2 + \\dots + T_n = \\sum T_i\n\\]\nThe Central Limit Theorem says \\(X\\) is normal for large \\(n\\).\n\n\n\n\nIf \\(X \\sim \\mathrm{Poisson}(\\mu)\\), \\(Y \\sim N(\\mu, \\mu)\\), and \\(\\mu \\geq 10\\) (rule of thumb), then\n\\[\n\\Pr(a \\leq X \\leq b) \\approx \\Pr\\left(a - \\frac{1}{2} \\leq Y \\leq b + \\frac{1}{2}\\right)\n\\]"
  },
  {
    "objectID": "04_est/06_notes.html#when-variance-is-not-known",
    "href": "04_est/06_notes.html#when-variance-is-not-known",
    "title": "Chapter 6 Notes: Estimation",
    "section": "When Variance is Not Known",
    "text": "When Variance is Not Known\n\nThe above only works when \\(\\sigma^2\\) is known\n\n\\(\\sigma^2\\) is never known\n\n\\(\\frac{\\bar{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}\\) (not \\(N(0, 1)\\))\n\n\\(t\\)-distribution with \\(n - 1\\) degrees of freedom\n\nOnly an exact result when \\(X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\)\n\nBut \\(t\\)-distribution tends to work better in small samples even when \\(X_i\\) are not normal\n\nFor large \\(n\\), \\(t_{n-1} \\approx N(0, 1)\\), so CLT is OK\nBell-shaped, centered at 0\nAs \\(\\text{df} \\downarrow\\), extreme values more likely\nAs \\(\\text{df} \\uparrow\\), extreme values less likely\nUse \\(t\\) because of added variability from using \\(s^2\\) instead of \\(\\sigma^2\\)\n\n\nR Code for t-distribution\n\n\nRosner uses notation \\(t_{\\text{df}, p}\\) for the \\(p\\) quantile of a \\(t_{\\text{df}}\\) distribution\n\n\\[\nt_{\\text{df}, p} = \\texttt{qt(p, df)}\n\\]\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n&\\Pr\\left(-t_{n-1, 1 - \\alpha/2} \\leq \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} \\leq t_{n-1, 1 - \\alpha/2} \\right) = 1 - \\alpha\\\\\n&\\Leftrightarrow \\Pr\\left(-t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\right) = 1 - \\alpha\\\\\n&\\Leftrightarrow \\Pr\\left(\\bar{X} - t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\\]\n\nThus, the following is a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma^2\\) is not known: \\[\\begin{align}\n&\\bar{X} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\text{ or, equivalently}\\\\\n&\\bar{X} \\pm \\texttt{qt(1 - }\\alpha/2\\texttt{, n-1)} \\cdot \\frac{s}{\\sqrt{n}}\n\\end{align}\\]\n\n\nExerciseHintSolution\n\n\nSuppose \\(n = 10\\), \\(\\bar{X} = 116.9\\), \\(s = 21.70\\). Calculate 90%, 95%, 99% CIs\n\n\nThe degrees of freedom of the appropriate \\(t\\) distribution is \\(10 - 1 = 9\\).\nE.g., for a 90% CI, \\(\\alpha = 1 - 0.9 = 0.1\\), and you’ll need the \\(1 - \\alpha/2 = 1 - 0.1/2 = 0.95\\) quantile of that \\(t\\) distribution.\n\n\nSince \\(n = 10\\), the degrees of freedom of the appropriate \\(t\\) distribution is \\(10 - 1 = 9\\)., For 90% CI’s, \\(\\alpha = 0.1\\) and the appropriate quantile is\n\nqt(p = 1 - 0.1/2, df = 9)\n\n[1] 1.833\n\n\nWe can thus get a 90% CI by\n\n116.9 - 1.833 * 21.70 / sqrt(10)\n\n[1] 104.3\n\n116.9 + 1.833 * 21.70 / sqrt(10)\n\n[1] 129.5\n\n\nSimilarly, for a 95% CI we have\n\n116.9 - qt(p = 1 - 0.05/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 101.4\n\n116.9 + qt(p = 1 - 0.05/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 132.4\n\n\nAnd, for a 99% CI we have\n\n116.9 - qt(p = 1 - 0.01/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 94.6\n\n116.9 + qt(p = 1 - 0.01/2, df = 9) * 21.70 / sqrt(10)\n\n[1] 139.2\n\n\n\n\n\n\nNote:\n\nCI level \\(\\uparrow\\) (so \\(\\alpha \\downarrow\\)) \\(\\Rightarrow\\) larger intervals\n\\(n \\uparrow\\) \\(\\Rightarrow\\) smaller intervals\n\\(s^2 \\uparrow\\) \\(\\Rightarrow\\) larger intervals\n\n\n\nBone Density Case Study"
  },
  {
    "objectID": "05_tests/07_notes.html",
    "href": "05_tests/07_notes.html",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "",
    "text": "Two hypotheses\n\n\\(H_0\\): Null\n\nBaseline, to be disproved\n\n\n\\(H_1\\): Alternative\n\nOpposite of null (contradicts \\(H_0\\))\n\n\nExample: Want to test if low socioeconomic status (SES) mothers have babies with lower birthweight\n\nKnow: national average is 120 oz\n\nFor \\(n = 10\\) low SES mothers, observe\n\n\\(\\bar{X} = 115\\) oz, \\(S = 24\\) oz\n\n\nLet \\(\\mu\\) = average birthweight of low SES mothers\n\n\\(H_0\\): \\(\\mu = 120\\)\n\n\\(H_1\\): \\(\\mu &lt; 120\\)\n\n\nWe use data (e.g., \\(\\bar{X}\\) and \\(s^2\\)) to make a decision (\\(H_0\\) vs. \\(H_1\\))\n\n\\[\n\\begin{array}{c|cc}\n& H_0 \\text{ true} & H_1 \\text{ true} \\\\\n\\hline\n\\text{Fail to Reject } H_0 & \\text{True negative} & \\text{Type II error} \\\\\n\\text{Reject } H_0 & \\text{Type I error} & \\text{True positive} \\\\\n\\end{array}\n\\]\n\nExample: If the truth is \\(\\mu = 120\\) (\\(H_0\\) is true) but we say \\(\\mu &lt; 120\\) (reject \\(H_0\\)), this is a Type I error\nTypically,\n\nNull Hypothesis: parameter = some value\n\nAlternative Hypothesis: One of:\n\nparameter ≠ value,\nparameter &gt; value,\nparameter &lt; value,\n\n\n\n\n\n\n\n\n\nTipType I error rate\n\n\n\nProbability of a Type I error \\[\\begin{align*}\n\\Pr(\\text{Reject } H_0 \\mid H_0 \\text{ True}) &= \\text{significance level} \\\\\n&= \\alpha\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nTipType II error rate\n\n\n\nProbability of a Type II error\n\\[\\begin{align*}\n\\Pr(\\text{Fail to reject } H_0 \\mid H_1 \\text{ True}) &= \\beta\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nTipPower\n\n\n\nProbability of correctly rejecting the null. \\[\\begin{align*}\n\\Pr(\\text{Reject } H_0 \\mid H_1 \\text{ True})\n&= 1 - \\Pr(\\text{Fail to reject } H_0 \\mid H_1 \\text{ True}) \\\\\n&= 1 - \\beta\n\\end{align*}\\]\n\n\n\nThese probabilities are all in terms of repeated samples.\n\n\nExerciseSolution:\n\n\nThere is a new pain relief drug for osteoarthritis (OA).\n\n50 OA patients take drug; measure \\(X\\) = % decline in pain level.\n\nReported:\n\nIf \\(X &gt; 0\\): less pain\nIf \\(X &lt; 0\\): more pain\n\n\nWe want to test if the drug is effective (average \\(X &gt; 0\\))\n\n\nWhat hypotheses are being tested?\n\nWhat do Type I error, Type II error, and power mean?\n\n\n\n\nLet \\(\\mu\\) = mean % decline in pain\n\\(H_0\\): \\(\\mu = 0\\)\n\\(H_1\\): \\(\\mu &gt; 0\\)\nType I: Say drug reduces pain on average, but it does not\n\nType II: Say drug does not reduce pain when it does\n\nPower: Probability we say it works when it truly works\n\n\n\n\n\nThe goal of a good test is to make both \\(\\alpha\\) and \\(\\beta\\) as small as possible\nHowever, typically, as \\(\\alpha \\downarrow\\), \\(\\beta \\uparrow\\) and as \\(\\beta \\downarrow\\), \\(\\alpha \\uparrow\\).\n\nThese are conflicting criteria\n\nCommon strategy: Fix \\(\\alpha = 0.05\\) (or similar), and use a test that has small \\(\\beta\\)\nAll hypothesis testing asks: How weird is our data if \\(H_0\\) were true?\n\n \n\nIf data are very weird, reject \\(H_0\\)\nIf data are not very weird, fail to reject \\(H_0\\)"
  },
  {
    "objectID": "05_tests/07_notes.html#power-calculations",
    "href": "05_tests/07_notes.html#power-calculations",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Power Calculations",
    "text": "Power Calculations\n\nPower = Probability of correctly rejecting the null:\n\\[\n\\text{Power} = \\Pr(\\text{Reject } H_0 \\mid H_1) = 1 - \\beta\n\\]\nIn experiments/surveys, it is common to guess power prior to collecting data:\n\nCalculate \\(n\\) needed\nSee what effect sizes we can detect\nEstimate likelihood our study will be successful\n\nTo do this, we need:\n\nGuess of effect size \\(\\mu_1 - \\mu_0\\) (from pilot study or wild guess)\nGuess of standard deviation \\(\\sigma\\)\nSample size \\(n\\)\nSignificance level \\(\\alpha\\) (provided by researcher)\n\nIf \\(\\sigma\\) is known, use z-test instead of t-test to calculate power\n\n\\[\\begin{align*}\nH_0 &: \\mu = \\mu_0 \\\\\nH_1 &: \\mu &lt; \\mu_0\n\\end{align*}\\]\n\nReject \\(H_0\\) if:\n\\[\n\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha\n\\]\n(This follows a standard normal under \\(H_0\\))\n\n\nPower calculation under \\(\\mu = \\mu_1\\):\n\\[\\begin{align*}\n\\Pr(\\text{Reject } H_0 \\mid \\mu = \\mu_1)\n&= \\Pr\\left( \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha \\ \\Big| \\ \\mu = \\mu_1 \\right) \\\\\n&= \\Pr\\left( \\frac{\\bar{X} - \\mu_1 + \\mu_1 - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha \\right)\n\\end{align*}\\]\nContinuing from the previous derivation:\n\\[\\begin{align*}\n\\Pr\\left( \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} &lt; z_\\alpha \\ \\Big| \\ \\mu = \\mu_1 \\right)\n&= \\Pr\\left( \\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} &lt; \\frac{\\mu_0 - \\mu_1}{\\sigma/\\sqrt{n}} + z_\\alpha \\right) \\\\\n&= \\Phi\\left( \\frac{\\mu_0 - \\mu_1}{\\sigma/\\sqrt{n}} + z_\\alpha \\right)\n\\end{align*}\\]\n\n\\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution\n\nThis gives us an estimate of power\n\nRewriting:\n\\[\n\\text{Power} = \\Phi\\left( z_\\alpha + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma} \\right)\n\\]\nPlot: \\(\\Phi(x)\\) is an S-shaped curve ranging from 0 to 1\nXYZ IMAGE HERE\n(Standard normal CDF)\n\n\nSummary of how parameters affect power:\n\n\n\n\n\n\n\n\n\nParameter\nChange\nEffect on Power\nReason\n\n\n\n\n\\(\\alpha\\)\n↓\nPower ↓\nMore stringent test\n\n\n\\(n\\)\n↓\nPower ↓\nLess data\n\n\n\\(|\\mu_0 - \\mu_1|\\)\n↓\nPower ↓\nSmaller effect size\n\n\n\\(\\sigma\\)\n↓\nPower ↑\nMore precise measurements\n\n\n\nXYZ IMAGE HERE\n(Overlapping normal curves showing the distribution of \\(\\bar{X}\\) under \\(H_0\\) and \\(H_1\\), with \\(\\alpha\\) and power labeled)\n\nLeft curve: distribution under \\(\\mu = \\mu_0\\)\n\nRight curve: distribution under \\(\\mu = \\mu_1\\)\n\nArea to the left of the critical value under the \\(H_1\\) curve is power = \\(1 - \\beta\\)\n\n\n\nExample:\nPilot study using 10 individuals gave:\n\\(\\bar{X} = -5\\), \\(s = 10\\)\nAssume \\(\\mu = 0\\) is the null (no effect)\nNow propose a new study with \\(n = 30\\)\nAt \\(\\alpha = 0.05\\), what is the power?\n\\[\\begin{align*}\n\\mu_1 &= -5 \\\\\n\\sigma &= 10 \\\\\nn &= 30 \\\\\n\\alpha &= 0.05 \\Rightarrow z_\\alpha = -1.645 \\quad (\\text{i.e., } \\texttt{qnorm(0.05)})\n\\end{align*}\\]\nCompute power: \\[\\begin{align*}\n\\Phi\\left(-1.645 + \\frac{0 - (-5)}{10/\\sqrt{30}} \\right)\n&= \\Phi(1.094) \\\\\n&= \\texttt{pnorm(1.094)} \\approx 0.863\n\\end{align*}\\]\n\n\\(\\Rightarrow\\) Estimated power is 0.863\n\n\n\nExercise:\nMean birthweight in the US is 120 oz.\nWhat is the power to detect low birthweight in a study with:\n- \\(n = 100\\)\n- \\(\\mu_1 = 115\\)\n- \\(\\alpha = 0.05\\)\n- \\(\\sigma = 24\\)\n\\[\\begin{align*}\n\\text{Power} &= \\Phi\\left(-1.645 + \\frac{120 - 115}{24 / \\sqrt{100}} \\right) \\\\\n&= \\Phi(0.438) = \\texttt{pnorm(0.438)} \\approx 0.669\n\\end{align*}\\]\n\n\nFor right-tailed test:\nIf\n\\[\\begin{align*}\nH_0 &: \\mu = \\mu_0 \\\\\nH_1 &: \\mu &gt; \\mu_0\n\\end{align*}\\]\nThen\n\\[\n\\text{Power} = \\Phi\\left( z_\\alpha + \\frac{\\sqrt{n}(\\mu_1 - \\mu_0)}{\\sigma} \\right)\n\\]\n(Just swap \\(\\mu_0\\) and \\(\\mu_1\\) from the left-tailed version.)\n\n\nFor two-tailed test:\nIf\n\\[\\begin{align*}\nH_0 &: \\mu = \\mu_0 \\\\\nH_1 &: \\mu \\ne \\mu_0\n\\end{align*}\\]\nThen\n\\[\n\\text{Power} = \\Phi\\left( z_{\\alpha/2} + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma} \\right)\n+ \\Phi\\left( z_{\\alpha/2} + \\frac{\\sqrt{n}(\\mu_1 - \\mu_0)}{\\sigma} \\right)\n\\]\n\nAdd both tails\n\nUse \\(\\alpha/2\\) critical value"
  },
  {
    "objectID": "05_tests/07_notes.html#sample-size-calculation",
    "href": "05_tests/07_notes.html#sample-size-calculation",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Sample Size Calculation",
    "text": "Sample Size Calculation\n\nTypically, you want a power of at least 0.8\n\nSo, what \\(n\\) will give us power of 0.8?\n\n\nAssume:\n\nGuess of effect size: \\(\\mu_1 - \\mu_0\\) (from pilot study or wild guess)\n\nGuess of standard deviation \\(\\sigma\\)\n\nDesired power = \\(1 - \\beta\\) (chosen by researcher)\n\nSignificance level \\(\\alpha\\)\n\n\n\nOne-sided test (e.g., \\(H_0\\!: \\mu = \\mu_0\\) vs. \\(H_1\\!: \\mu &lt; \\mu_0\\)):\nStart from power formula: \\[\n1 - \\beta = \\Phi\\left( z_\\alpha + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma} \\right)\n\\]\nNow solve for \\(n\\) (either numerically or algebraically):\nTake inverse: \\[\n\\Phi^{-1}(1 - \\beta) = z_\\alpha + \\frac{\\sqrt{n}(\\mu_0 - \\mu_1)}{\\sigma}\n\\]\nSolve: \\[\n\\sqrt{n} = \\frac{\\Phi^{-1}(1 - \\beta) - z_\\alpha}{(\\mu_0 - \\mu_1)/\\sigma}\n\\]\nSquare both sides: \\[\nn = \\frac{(\\Phi^{-1}(1 - \\beta) - z_\\alpha)^2 \\cdot \\sigma^2}{(\\mu_0 - \\mu_1)^2}\n\\]\n\n\nAlternate form (for \\(H_0\\!: \\mu = \\mu_0\\) vs. \\(H_1\\!: \\mu &gt; \\mu_0\\)):\n\\[\nn = \\frac{(z_{1-\\beta} + z_{1-\\alpha})^2 \\cdot \\sigma^2}{(\\mu_1 - \\mu_0)^2}\n\\]\n\n\nExercise: What is the effect on \\(n\\) of increasing:\n\n\\(\\sigma\\)\n\n\\(\\beta\\)\n\n\\(\\alpha\\)\n\n\\(|\\mu_0 - \\mu_1|\\)\n\n\nEffects:\n\n\\(\\sigma \\uparrow \\ \\Rightarrow\\ n \\uparrow\\)\n(Less precise measurements, so need more data)\n\\(\\beta \\uparrow \\ \\Rightarrow\\ 1 - \\beta \\downarrow \\ \\Rightarrow\\ z_{1 - \\beta} \\downarrow \\ \\Rightarrow\\ n \\downarrow\\)\n(Less power needed)\n\\(\\alpha \\uparrow \\ \\Rightarrow\\ z_{1 - \\alpha} \\downarrow \\ \\Rightarrow\\ n \\downarrow\\)\n(Less stringent test)\n\\(|\\mu_0 - \\mu_1| \\uparrow \\ \\Rightarrow\\ n \\downarrow\\)\n(Larger effect sizes are easier to detect)\n\nXYZ IMAGE HERE\n(Standard normal with \\(\\alpha\\), \\(\\beta\\), \\(z_{1-\\alpha}\\), and \\(z_{1-\\beta}\\) labeled)\n\n\n\nFor two-sided test:\nIf \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu \\ne \\mu_0\\),\nsolve for \\(n\\) using:\n\\[\nn = \\frac{\\sigma^2 (z_{1-\\beta} + z_{1 - \\alpha/2})^2}{(\\mu_0 - \\mu_1)^2}\n\\]\n(The only difference from one-sided test is the use of \\(z_{1 - \\alpha/2}\\) instead of \\(z_{1 - \\alpha}\\).)\n\nPower Calculations in R\n\nSkip Section 7.8"
  },
  {
    "objectID": "05_tests/07_notes.html#one-sample-inference-for-binomial",
    "href": "05_tests/07_notes.html#one-sample-inference-for-binomial",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "One-Sample Inference for Binomial",
    "text": "One-Sample Inference for Binomial\n\nExample:\nPrevalence of breast cancer is 2%.\nOf 10,000 women whose mothers had breast cancer,\n400 of them got breast cancer in their lives.\nAre they at higher risk?\n\nLet \\(X\\) = number of those 10,000 who got breast cancer\n\n\\(X \\sim \\text{Binom}(10{,}000,\\ p)\\)\n\n\\[\\begin{align*}\nH_0 &: p = 0.02 \\\\\nH_1 &: p &gt; 0.02\n\\end{align*}\\]\n\n\nUse normal approximation:\n\n\\(\\hat{p} = \\frac{x}{n} \\sim N\\left(p_0,\\ \\frac{p_0(1 - p_0)}{n}\\right)\\) under \\(H_0\\)\n\nThen: \\[\n\\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} \\sim N(0, 1)\n\\]\nXYZ IMAGE HERE\n(Normal curve with right tail labeled as p-value)\n\n\np-value:\n\\[\n\\text{p-value} = 1 - \\Phi\\left( \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} \\right)\n\\]\n\nIf\n\\[\\begin{align*}\nH_0 &: p = p_0 \\\\\nH_1 &: p \\ne p_0\n\\end{align*}\\]\nThen calculate area in both tails:\n\nXYZ IMAGE HERE\n(Symmetric normal curve with shaded tails, showing two-sided test)\n\nTwo-tailed p-value: \\[\n\\text{p-value} = 2 \\cdot \\Phi\\left( -\\left| \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}} \\right| \\right)\n\\]\n\n\n\nContinuity correction:\n\nSubtract \\(1/n\\) in the numerator for upper-tail tests\n\nAdd \\(1/n\\) in the numerator for lower-tail tests\n\n\n\nExample: Back to breast cancer example\n\\[\\begin{align*}\n\\hat{p} &= \\frac{400.5}{10{,}000} = 0.04005 \\\\\nz &= \\frac{0.04005 - 0.02}{\\sqrt{0.02(1 - 0.02)/10{,}000}} \\approx 14.32 \\\\\n\\text{p-value} &= \\Phi(-14.32) \\approx 8.4 \\cdot 10^{-47} \\ll 0.0001\n\\end{align*}\\]\n\n\\(\\Rightarrow\\) Very highly significant"
  },
  {
    "objectID": "05_tests/07_notes.html#exact-test",
    "href": "05_tests/07_notes.html#exact-test",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Exact Test",
    "text": "Exact Test\n\nWe are still assuming that \\(X \\sim \\text{Bin}(n, p)\\)\n\nHypotheses:\n\\(H_0: p = p_0\\)\n\\(H_1: p \\ne p_0\\), or \\(p &gt; p_0\\), or \\(p &lt; p_0\\)\nThe normal method only works for large \\(n\\)\nRule of thumb: You can use the normal method if \\[\nn p_0(1 - p_0) \\ge 5\n\\]\nIf \\(n\\) is not large enough, use an exact test\n\nControls for \\(\\alpha\\) for all \\(n\\), not just large \\(n\\)\n\nUnder the null \\(X \\sim \\mathrm{Binom}(n, p_0)\\). We we observe \\(x\\) (lower-case \\(x\\)), then just calculate the probability from the binomial PMF of seeing values as weird or weirder.\n\\(H_1: p &lt; p_0\\): Calculate \\(\\Pr(X \\leq x \\mid p = p_0)\\) with pbinom(q = x, size = n, prob = p0)\nE.g., \\(X = 3\\), \\(H_0: p_0 = 0.5\\), \\(H_1: p &lt; 0.5\\). This the the PMF of \\(X\\) under \\(H_0\\) and the sum of the orange bars is the \\(p\\)-value.\n\n\n\n\n\n\n\n\n\n\\(H_1: p &gt; p_0\\): Calculate \\(\\Pr(X \\geq x \\mid p = p_0)\\) with 1 - pbinom(q = x - 1, size = n, prob = p0)\nE.g., \\(X = 3\\), \\(H_0: p_0 = 0.5\\), \\(H_1: p_1 &gt; 0.5\\). This the the PMF of \\(X\\) under \\(H_0\\) and the sum of the orange bars is the \\(p\\)-value.\n\n\n\n\n\n\n\n\n\n\\(H_1: p \\neq p_0\\): 2 times the smaller of \\(\\Pr(X \\leq x \\mid p = p_0)\\) or \\(\\Pr(X \\geq x \\mid p = p_0)\\).\nE.g., \\(X = 3\\), \\(H_0: p_0 = 0.5\\), \\(H_1: p &lt; 0.5\\). This the the PMF of \\(X\\) under \\(H_0\\) and the two times the sum of the orange bars is the \\(p\\)-value.\n\n\n\n\n\n\n\n\n\nThe exact \\(p\\)-value formula for \\(H_1: p \\neq p_0\\) is \\[\n\\text{p-value} =\n\\begin{cases}\n2 \\sum_{k = 0}^{x} \\binom{n}{k} p_0^k (1 - p_0)^{n - k} & \\text{ if } x \\leq np_0\\\\\n2 \\sum_{k = x}^{n} \\binom{n}{k} p_0^k (1 - p_0)^{n - k} & \\text{ if } x &gt; np_0\n\\end{cases}\n\\]\nFor \\(H_1: p &lt; p_0\\): \\[\n\\text{p-value} = \\sum_{k = 0}^{x} \\binom{n}{k} p_0^k (1 - p_0)^{n - k}\n\\]\nFor \\(H_1: p &gt; p_0\\): \\[\n\\text{p-value} = \\sum_{k = x}^{n} \\binom{n}{k} p_0^k (1 - p_0)^{n - k}\n\\]\nExample: In a nuclear facility, there were 13 deaths, of which 5 were caused by cancer. Cancer causes 20% of deaths in this age group. Are there actually more cancer deaths than would be expected?\nLet \\(X\\) be the number of deaths caused by cancer out of 13. Then \\(X \\sim \\text{Binom}(13, p)\\). We are testing \\(H_0: p = 0.2\\) versus \\(H_1: p &gt; 0.2\\). The \\(p\\)-value is the probability of seeing at least 5 cancer deaths (out of 13) if indeed \\(p = 0.2\\).\n\n1 - pbinom(q = 4, size = 13, prob = 0.2)\n\n[1] 0.09913\n\n\nThis is a pretty large \\(p\\)-value, so we do not have evidence that there are more cancer deaths than expected.\n\n\nBinomial Tests in R"
  },
  {
    "objectID": "05_tests/07_notes.html#power-calculations-for-binomial-tests",
    "href": "05_tests/07_notes.html#power-calculations-for-binomial-tests",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Power Calculations for Binomial Tests",
    "text": "Power Calculations for Binomial Tests\nWe want: \\[\n\\text{Power} = \\Pr(\\text{Reject } H_0 \\mid H_1)\n\\]\n\nNeed:\n\nTrue alternative value: \\(p_1\\)\n\nSample size: \\(n\\)\n\nSignificance level: \\(\\alpha\\)\n\nWe skip the exact distribution and use a normal approximation.\nXYZ IMAGE HERE\n(Overlapping normal distributions under \\(H_0: p = p_0\\) and \\(H_1: p = p_1\\) with shaded rejection region and labeled \\(\\alpha/2\\), \\(\\beta\\))\n\n\nUnder \\(H_0: p \\ne p_0\\), power is approximated as:\n\\[\n\\text{Power} = \\Phi\\left(\n\\frac{p_0 - p_1}{\\sqrt{p_1(1 - p_1)/n}}\n\\cdot z_{\\alpha/2}\n+ \\frac{|p_0 - p_1| \\cdot \\sqrt{n}}{\\sqrt{p_1(1 - p_1)}}\n\\right)\n\\]\n\n\nSummary of effects on power:\n\n\n\n\n\n\n\n\n\nParameter\nChange\nEffect on Power\nReason\n\n\n\n\n\\(n\\)\n↑\nPower ↑\nMore data\n\n\n\\(\\alpha\\)\n↑\nPower ↑\nLess stringent test\n\n\n\\(|p_0 - p_1|\\)\n↑\nPower ↑\nMore signal"
  },
  {
    "objectID": "05_tests/07_notes.html#sample-size-calculation-binomial",
    "href": "05_tests/07_notes.html#sample-size-calculation-binomial",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Sample Size Calculation (Binomial)",
    "text": "Sample Size Calculation (Binomial)\n\nNeed:\n\nTrue alternative \\(p_1\\)\n\nDesired power \\(1 - \\beta\\)\n\nSignificance level \\(\\alpha\\)\n\nWe set: \\[\n1 - \\beta = f(n,\\ p_1,\\ \\alpha)\n\\]\nThen solve for \\(n\\)\n\nPower Calculations in R"
  },
  {
    "objectID": "05_tests/07_notes.html#poisson-test",
    "href": "05_tests/07_notes.html#poisson-test",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Poisson Test",
    "text": "Poisson Test\n\nLet \\(X \\sim \\text{Poisson}(\\mu)\\)\n\n\nHypotheses:\n\n\\(H_0: \\mu = \\mu_0\\)\n\n\\(H_1\\): \\(\\mu \\ne \\mu_0\\), or \\(\\mu &gt; \\mu_0\\), or \\(\\mu &lt; \\mu_0\\)\n\n\n\np-value (two-sided):\nLet \\(x\\) be the observed count. Then:\n\\[\n\\text{p-value} = \\sum_{h : \\Pr(h) \\le \\Pr(x)} \\Pr(h)\n= \\sum_{h : \\Pr(h) \\le \\Pr(x)} \\frac{e^{-\\mu_0} \\mu_0^h}{h!}\n\\]\nXYZ IMAGE HERE\n(PMF showing observed \\(x\\) and shaded tails for exact p-value)\n\n\nExample:\n8446 rubber workers, ages 40–84\n4 deaths due to Hodgkin’s disease\nExpected deaths (based on U.S. rates): 3.3\nLet \\(X\\) = # with Hodgkin’s\nAssume \\(X \\sim \\text{Poisson}(\\mu)\\)\n\\[\\begin{align*}\nH_0 &: \\mu = 3.3 \\\\\nH_1 &: \\mu \\ne 3.3\n\\end{align*}\\]\nR code:\n\npoisson.test(x = 4, r = 3.3)\n\n\n    Exact Poisson test\n\ndata:  4 time base: 1\nnumber of events = 4, time base = 1, p-value = 0.6\nalternative hypothesis: true event rate is not equal to 3.3\n95 percent confidence interval:\n  1.09 10.24\nsample estimates:\nevent rate \n         4 \n\n\nResult:\n\\[\n\\text{p-value} = 0.578\n\\]\n\n\\(\\Rightarrow\\) No evidence of elevated risk\n\n\n\nNote:\n\nBinomial test is not appropriate here\nbecause each individual has a different \\(p_i\\).\nThe null hypothesis was based on the expected \\(p_i\\) for each individual."
  },
  {
    "objectID": "05_tests/07_notes.html#standardized-mortality-ratio-smr",
    "href": "05_tests/07_notes.html#standardized-mortality-ratio-smr",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Standardized Mortality Ratio (SMR)",
    "text": "Standardized Mortality Ratio (SMR)\n\\[\n\\text{SMR} = \\frac{\\text{Observed}}{\\text{Expected}} \\times 100\\%\n\\]\n\n\\(\\text{SMR} &gt; 100\\%\\) \\(\\Rightarrow\\) more deaths than expected in the study population\n\n\\(\\text{SMR} &lt; 100\\%\\) \\(\\Rightarrow\\) fewer deaths\n\n\\(\\text{SMR} = 100\\%\\) \\(\\Rightarrow\\) same number of deaths as expected\n\n\nExample:\nObserved = 4, Expected = 3.3\n\\[\n\\text{SMR} = \\frac{4}{3.3} \\times 100\\% \\approx 121\\%\n\\]"
  },
  {
    "objectID": "05_tests/07_notes.html#exercises-7.17.8",
    "href": "05_tests/07_notes.html#exercises-7.17.8",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Exercises 7.1–7.8",
    "text": "Exercises 7.1–7.8\n\n12 patients\nAfter 24 hours, mean serum creatinine is \\(1.2 \\text{ mg/dL}\\)\n\nGeneral population: \\(\\mu = 1\\), \\(\\sigma = 0.4\\)\n\n\n(1) Known \\(\\sigma\\)\n\n\\(H_0\\): \\(\\mu = 1\\)\n\n\\(H_1\\): \\(\\mu \\ne 1\\)\n\n\\[\nz = \\frac{1.2 - 1}{0.4 / \\sqrt{12}} = 1.732\n\\]\nUnder \\(H_0\\), \\(z \\sim N(0, 1)\\):\n\\[\n2 \\cdot \\text{pnorm}(-1.73) = 0.08326\n\\]\nFail to reject \\(H_0\\).\n\n\n(2.1) If instead \\(s = 0.4\\), \\(\\sigma\\) unknown\n\n\\(t = 1.73 \\sim t_{11}\\)\n\n\\[\n2 \\cdot \\text{pt}(-1.73, \\text{df} = 11) = 0.1112\n\\]\n\\(p\\)-value = 0.6376\nFail to reject \\(H_0\\).\n\n\n(2.2) \\(s = 0.6\\), \\(\\sigma\\) unknown\n\\[\nt = \\frac{1.2 - 1}{0.6 / \\sqrt{12}} = 1.155\n\\]\n\\[\n2 \\cdot \\text{pt}(-1.155, \\text{df} = 11) = 0.2777\n\\]\nFail to reject \\(H_0\\).\n\n\n7.4\n\\[\n1.2 \\pm t_{11, 0.975} \\cdot \\frac{0.6}{\\sqrt{12}} \\\\\nt_{0.975, \\text{df}=11} = 2.201 \\Rightarrow \\text{CI} = (0.82, 1.58)\n\\]\n\n\n7.5\n\n1 is included in the 99% CI \\(\\Rightarrow\\) \\(p\\)-value &gt; 0.05\n\n\n\n7.6\n\\[\n\\text{pt}(-1.52, \\text{df}=6) = 0.179\n\\]\n\n\n7.7\n\\[\n1 - \\text{pt}(2.5, \\text{df}=36) = 0.008557\n\\]\n\n\n7.8\n\\[\n\\text{qt}(0.1, \\text{df}=54) = -1.297\n\\]\n\n\n7.52–7.53\n\n\\(n = 200\\)\n\\(X\\) = number who develop cancer\nobserved \\(x = 4\\)\n\\(X \\sim \\text{Binom}(200, p)\\)\n\n\\(H_0\\): \\(p = 0.01\\) vs \\(H_1\\): \\(p \\ne 0.01\\)\n\nCheck:\n\\[\nnp_0(1 - p_0) = 0.01 \\cdot 0.99 \\cdot 200 = 1.98 &lt; 5\n\\]\n→ Use exact binomial test.\nR code:\n\nbinom.test(x = 4, n = 200, p = 0.01)\n\n\n    Exact binomial test\n\ndata:  4 and 200\nnumber of successes = 4, number of trials = 200, p-value = 0.1\nalternative hypothesis: true probability of success is not equal to 0.01\n95 percent confidence interval:\n 0.005476 0.050414\nsample estimates:\nprobability of success \n                  0.02 \n\n\n\n\nManual p-value computation:\n\\[\n\\text{p-value} = 2 \\cdot \\sum_{k=4}^{200} \\binom{200}{k} (0.01)^k (0.99)^{200-k}\n\\]\nR shortcut:\n\n2 * dbinom(3, size = 200, prob = 0.01)\n\n[1] 0.3627\n\n\nResult:\n\\[\n\\text{p-value} \\approx 0.2839\n\\]\n\nR output \\(p\\)-value ≈ 0.142\n\n\n\n7.54\n\n5-year period\n\\(X = 20\\), \\(n = 200\\)\n5-year incidence rate is \\(0.05\\)\n\nHypotheses: - \\(H_0\\): \\(p = 0.05\\) - \\(H_1\\): \\(p \\ne 0.05\\)\nSince \\(0.05 \\cdot 0.95 \\cdot 200 = 9.5\\), we can use the normal approximation.\n\\[\nz = \\frac{0.1 - 0.05}{\\sqrt{0.05 \\cdot 0.95 / 200}} = 3.082\n\\]\n\\[\n\\text{p-value} = 2(1 - \\text{pnorm}(3.082)) = 0.002055\n\\]\n→ Reject \\(H_0\\)\n\n\n7.55\nCompute 95% CI for \\(p\\):\n\\[\n\\hat{p} \\pm z_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\quad \\text{where } z_{0.975} = 1.96\n\\]\n\\[\n0.1 \\pm 1.96 \\cdot \\sqrt{\\frac{0.1 \\cdot 0.9}{200}} = (0.05842, 0.1416)\n\\]\nR code:\n\nprop.test(x = 20, n = 200, p = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  20 out of 200, null probability 0.05\nX-squared = 9.5, df = 1, p-value = 0.002\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.06366 0.15230\nsample estimates:\n  p \n0.1"
  },
  {
    "objectID": "05_tests/07_notes.html#two-sided-tests-for-the-mean",
    "href": "05_tests/07_notes.html#two-sided-tests-for-the-mean",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Two-sided Tests for the Mean",
    "text": "Two-sided Tests for the Mean\n\nSometimes Usually, our alternative is just that the mean is not equal to the null value.\n\nE.g., average birthweights for low SES families are different from the population average.\n\nWhat constitutes “more weird” in this case is the area in both tails.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantTwo-tailed Test\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\)\nHypotheses:\n\n\\(H_0\\): \\(\\mu = \\mu_0\\)\n\n\\(H_1\\): \\(\\mu \\ne \\mu_0\\)\n\nTest statistic:\n\\[\nt = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}}\n\\]\nTwo-tailed p-value:\n\\[\n\\text{p-value} = 2 \\cdot \\texttt{pt}(-|t|,\\ n-1)\n\\]\nCritical value technique: Reject \\(H_0\\) if\n\\[\n|t| &gt; \\texttt{qt}\\left(1 - \\frac{\\alpha}{2},\\ n-1\\right)\n\\]\n\n\n\nExercise:Solution\n\n\nCholesterol levels of US women are 190 mg/dL on average.\nWant to compare cholesterol levels of recent Asian immigrants.\n100 female Asian immigrants:\n- \\(\\bar{X} = 181.52\\) mg/dL\n- \\(s = 40\\) mg/dL\n\n\n\\[\\begin{align*}\nH_0 &: \\mu = 190 \\\\\nH_1 &: \\mu \\ne 190 \\\\\nt &= \\frac{181.52 - 190}{40 / \\sqrt{100}} = -2.12 \\\\\n\\text{p-value} &= 2 \\cdot pt(-2.12,\\ 99) \\approx 0.037\n\\end{align*}\\]\n\n\n\n\nUse two-sided tests by default, unless only one direction is clearly of interest\n\ne.g., drug efficacy\n\nBecause this is an introductory class, there will be a lot of one-sided tests, to evaluate your understanding. These are typically rare in practice.\n\n\nOne-sample t-tests in R"
  },
  {
    "objectID": "05_tests/07_notes.html#confidence-intervals-and-hypothesis-tests",
    "href": "05_tests/07_notes.html#confidence-intervals-and-hypothesis-tests",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nSuppose \\(X_1, X_2, \\ldots, X_n \\overset{\\text{iid}}{\\sim} N(\\mu, \\sigma^2)\\) and\n\n\\(H_0 : \\mu = \\mu_0\\)\n\\(H_1 : \\mu \\ne \\mu_0\\)\n\nThen, we reject \\(H_0\\) at level \\(\\alpha\\) if and only if the \\(100(1 - \\alpha)\\%\\) confidence interval does not contain \\(\\mu_0\\)\n\nI.e., if \\(\\mu_0\\) is outside the \\((1-\\alpha)100\\%\\) confidence interval, then reject \\(H_0\\) at level \\(\\alpha\\) and the \\(p\\)-value is less than \\(\\alpha\\)\nGeneral result: All tests correspond to some confidence interval, and vice versa\nSo, a \\(100(1 - \\alpha)\\%\\) CI contains all values of \\(\\mu_0\\) that would fail to reject \\(H_0: \\mu = \\mu_0\\)\nOne-sided confidence intervalss correspond to one-sided tests"
  },
  {
    "objectID": "05_tests/07_notes.html#sample-size-for-z-tests",
    "href": "05_tests/07_notes.html#sample-size-for-z-tests",
    "title": "Chapter 7: One Sample Hypothesis Testing",
    "section": "Sample Size for z-tests",
    "text": "Sample Size for z-tests\nSuppose \\(H_0: \\mu = \\mu_0\\).\n\nIf \\(H_1: \\mu &gt; \\mu_0\\) or \\(H_1: \\mu &lt; \\mu_0\\) then \\[\nn = \\frac{(z_{1-\\beta} + z_{1-\\alpha})^2\\sigma^2}{(\\mu_0 - \\mu_1)^2}\n\\]\nIf \\(H_1: \\mu \\neq \\mu_0\\) then \\[\nn = \\frac{ (z_{1-\\beta} + z_{1 - \\alpha/2})^2\\sigma^2}{(\\mu_0 - \\mu_1)^2}\n\\]"
  },
  {
    "objectID": "05_tests/08_notes.html",
    "href": "05_tests/08_notes.html",
    "title": "Chapter 8: Two-sample Inference",
    "section": "",
    "text": "Paired t-test\n\nCompare 2 populations where parameters are not known.\nA paired sample is where observations in each population are matched.\n \nExample: Twin study where one twin smokes more than the other.\n\nPopulation 1: lighter smoking twins\nPopulation 2: heavier smoking twins\nMatched pair: each pair of twins\n\nExample: We measure blood pressure on the same individual at 2 time points\n\nPopulation 1: pre oral contraceptive (OC)\nPopulation 2: post OC\nMatched pair: the same individual\n\nThis second example is one of a longitudinal study, where we follow the same people over time\nLet\n\n\\(X_i \\sim N(\\mu_i, \\sigma^2)\\)\n\nFor example, pre-OC\n\n\n\\(Y_i \\sim N(\\mu_i + \\Delta, \\sigma^2)\\)\n\nFor example, post-OC\n\n\nHypotheses:\n\\(H_0\\): \\(\\Delta = 0\\)\n\\(H_1\\): \\(\\Delta \\ne 0\\)\nThis tests if there is a difference between populations while allowing each pair to have their own baseline mean \\(\\mu_i\\).\nDefine differences:\n\\(D_i = Y_i - X_i \\sim N(\\Delta, \\sigma_D^2)\\)\nVariance of differences:\n\\(\\sigma_D^2 = \\text{var}(X) + \\text{var}(Y) - 2\\ \\text{cov}(X, Y)\\)\n\nBut this is a nuisance parameter, so just call it \\(\\sigma_D^2\\)\n\nSo, just use a one-sample t-test on \\(D_i\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nA paired t-test is just a one-sample t-test on differences.\n\n\n\n\n\n\n\n\nTipPaired t-test\n\n\n\nInput: \\(X_1,X_2,\\ldots,X_n\\) and \\(Y_1,Y_2,\\ldots,Y_n\\) where \\(X_i\\) and \\(Y_i\\) are matched pairs.\n\nWe assume \\(X_i \\sim N(\\mu_i, \\sigma_x^2)\\) and \\(Y_i \\sim N(\\mu_i + \\Delta, \\sigma_y^2)\\).\nWe test:\n\n\\(H_0: \\Delta = d_0\\)\n\\(H_A: \\Delta \\neq d_0\\) or \\(\\Delta &gt; d_0\\) or \\(\\Delta &lt; d_0\\).\nwhere \\(d_0\\) = null mean difference between populatoins (e.g. 0)\n\nWe calculate:\n\n\\(D_i = X_i - Y_i\\)\n\\(s_D\\) = standard deviation of \\(D_i\\)’s\n\n\\(\\bar{D}\\) = mean of \\(D_i\\)’s\n\nThe \\(t\\) statistic follows a \\(t_{n-1}\\) distribution if \\(H_0\\) is true: \\[\nt = \\frac{\\bar{D} - d_0}{s_D / \\sqrt{n}}\n\\]\n\\(H_1: \\Delta \\neq d_0\\): \\[\n\\text{p-value} = 2\\texttt{pt}(-|t|, n-1)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\Delta &gt; d_0\\): \\[\n\\text{p-value} = 1 - \\texttt{pt}(t, n-1)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\Delta &lt; d_0\\): \\[\n\\text{p-value} = \\texttt{pt}(t, n-1)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha) \\cdot 100\\%\\) confidence interval for \\(\\Delta\\) is: \\[\n\\bar{D} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s_D}{\\sqrt{n}}\n\\]\n\n\n\n\nPaired t-tests in R\n\n\n\nTwo-sample t-tests with equal variances\n\nMore commonly, studies have 2 independent samples.\nExample:\n\nCollect one group of OC users\n\nCollect a separate group of non-OC users\n\nCross-sectional study: data collected at one point in time (units under different conditions)\nAssume: \\[\nX_1, X_2, \\ldots, X_{n_1} \\overset{\\text{iid}}{\\sim} N(\\mu_1, \\sigma_1^2)\n\\] \\[\nY_1, Y_2, \\ldots, Y_{n_2} \\overset{\\text{iid}}{\\sim} N(\\mu_2, \\sigma_2^2)\n\\]\nNote: different sample sizes are possible, and the observations are not paired.\nHypotheses:\n\n\\(H_0\\): \\(\\mu_1 = \\mu_2\\)\n\\(H_1\\): \\(\\mu_1 \\ne \\mu_2\\), or \\(\\mu_1 &lt; \\mu_2\\), or \\(\\mu_1 &gt; \\mu_2\\)\n\nFor now, assume \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\)\n\nAssumes the two populations have the same variability, which is often not valid.\nWe will relax this later\n\nWe observe:\n\n\\(\\bar{X} =\\) mean of \\(X_i\\)’s\n\n\\(\\bar{Y} =\\) mean of \\(Y_i\\)’s\n\n\\(s_1^2 =\\) sample variance of \\(X_i\\)’s\n\n\\(s_2^2 =\\) sample variance of \\(Y_i\\)’s\n\n\\(n_1 =\\) sample size 1\n\n\\(n_2 =\\) sample size 2\n\nConsider \\(\\bar{X} - \\bar{Y}\\)\n\\[\nE[\\bar{X} - \\bar{Y}] = E[\\bar{X}] - E[\\bar{Y}] = \\mu_1 - \\mu_2\n\\]\n\nEquals 0 under \\(H_0\\), not 0 under \\(H_1\\)\n\nVariance of difference:\n\\[\\begin{align*}\n\\text{Var}(\\bar{X} - \\bar{Y}) &= \\text{Var}(\\bar{X}) + \\text{Var}(\\bar{Y}) - 2\\,\\text{Cov}(\\bar{X}, \\bar{Y})\\\\\n&= \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2} \\\\\n&= \\sigma^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\n\\end{align*}\\] (Covariance term is 0 due to independence)\nTherefore, by properties of the normal distribution: \\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\right)\n\\]\nIf \\(\\sigma^2\\) were known, then could base our test on the distribution of the mean divided by the standard deviation:\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\n\\]\nWe could then compare this statistic to a \\(N(0,1)\\) distribution to get p-value.\nHowever, \\(\\sigma^2\\) is never known in practice, so we need to estimate it.\n\n\n\n\n\n\n\nTipPooled Sample Variance\n\n\n\n\nAssuming \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\), then we estimate \\(\\sigma^2\\) with the pooled sample variance:\n\n\\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\n\n\nThe pooled sample variance can equivalently be written as:\n\\[\ns^2 = \\frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \\frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2\n\\]\n\nThis should show you that higher weight goes to the sample with larger \\(n\\)\n\nOur test statistic becomes:\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nThis follows a \\(t_{n_1 + n_2 - 2}\\) distribution only if \\(H_0\\) is true.\n\nIt follows something else if \\(H_1\\) is true.\n\n\n\n\n\n\n\n\nImportantTwo-sample t-test with Equal Variances\n\n\n\nInput: \\(X_1, X_2, \\ldots, X_{n_1}\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) (sample sizes might be different)\n\nWe assume \\(X_i \\sim N(\\mu_1, \\sigma^2)\\) and \\(Y_i \\sim N(\\mu_2, \\sigma^2)\\).\n\nEqual variances, possibly different means\n\nWe test:\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(H_A: \\mu_1 \\neq \\mu_2\\) or \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\).\n\nWe calculate:\n\n\\(\\bar{X}\\), \\(\\bar{Y}\\), and the pooled sample variance \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nThe \\(t\\) statistic follows a \\(t_{n_1 + n_2 - 2}\\) distribution if \\(H_0\\) is true: \\[\nt = \\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t_{n_1 + n_2 - 2}\n\\]\n\\(H_1: \\mu_1 \\neq \\mu_2\\): \\[\n\\text{p-value} = 2\\texttt{pt}(-|t|, n_1 + n_2 - 2)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &gt; \\mu_2\\): \\[\n\\text{p-value} = 1 - \\texttt{pt}(t, n_1 + n_2 - 2)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &lt; \\mu_2\\): \\[\n\\text{p-value} = \\texttt{pt}(t, n_1 + n_2 - 2)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha)100\\%\\) confidence interval is\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\n\n\n\n\nTwo-sample t-test in R, equal variance\n\n\n\nTest for Equal Variances\n\nBecause of the equal variance assumption above, folks have developed statistical tests for whether the variances are indeed equal.\nLet: \\[\nX_i \\sim N(\\mu_1, \\sigma_1^2)\n\\]\n\\[\nY_i \\sim N(\\mu_2, \\sigma_2^2)\n\\]\nHypotheses:\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\n\\(H_1\\): \\(\\sigma_1^2 \\ne \\sigma_2^2\\)\n\nThe test is based on \\(s_1^2\\) and \\(s_2^2\\).\n\nIf they are very different, this provides evidence that \\(\\sigma_1^2 \\neq \\sigma_2^2\\).\n\nNobody does this in real life because:\n\nVery sensitive to non-normality. In contrast, the \\(t\\)-test is not sensitive because of the CLT\nThe equal variance \\(t\\)-test is robust to violations in equal variance assumption.\nNobody assumes equal variances anyway because they all use Welch’s  2-sample \\(t\\)-test (§8.7)\n\nIf your boss asks you to test for equal variances, use var.test()\n\n\nTest for equal variance in R\n\n\n\nTwo-sample t-test with unequal variances\n\nWe’ll now relax the equal variance assumption\n\nThis is a mathy way to say that we won’t assume equal variances.\n\nOur approach is called Welch’s  t-test\nAlways use this unless you know for sure that the variances are equal.\nLet:\n\n\\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) with sample size \\(n_1\\)\n\n\\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\) with sample size \\(n_2\\)\n\nThen:\n\n\\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2,\\ \\frac{1}{n_1} \\sigma_1^2 + \\frac{1}{n_2} \\sigma_2^2\\right)\n\\]\n\nThe test statistic is the mean divided by the estimated standard error\n\\[\nt = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}}\n\\]\nThis is approximately \\(t_\\nu\\) if \\(H_0\\) is true\nThe degrees of freedom \\(\\nu\\) for the null distribution is a weird thing called the Satterthwaite approximation:\n\\[\n\\nu = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1 - 1} + \\frac{(s_2^2/n_2)^2}{n_2 - 1}}\n\\]\n\nYou don’t need to remember this.\nThis \\(\\nu\\) is just to make the \\(t_\\nu\\) distribution as close as possible to the actual null distribution of test statistic.\n\n\n\n\n\n\n\n\nImportantTwo-sample t-test with Equal Variances\n\n\n\nInput: \\(X_1, X_2, \\ldots, X_{n_1}\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) (sample sizes might be different)\n\nWe assume \\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\).\n\nPossibly unequal variances, possibly different means\n\nWe test:\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(H_A: \\mu_1 \\neq \\mu_2\\) or \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\).\n\nWe calculate:\n\n\\(\\bar{X}\\), \\(\\bar{Y}\\), \\(s_1^2\\), and \\(s_2^2\\)\n\nThe \\(t\\) statistic follows a \\(t_{\\nu}\\) distribution if \\(H_0\\) is true: \\[\nt = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}}\n\\]\n\\(\\nu\\) is derived from Satterthwaite’s Equation.\n\\(H_1: \\mu_1 \\neq \\mu_2\\): \\[\n\\text{p-value} = 2\\texttt{pt}(-|t|, \\nu)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &gt; \\mu_2\\): \\[\n\\text{p-value} = 1 - \\texttt{pt}(t, \\nu)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: \\mu_1 &lt; \\mu_2\\): \\[\n\\text{p-value} = \\texttt{pt}(t, \\nu)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha)100\\%\\) confidence interval is\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{\\nu,\\ 1 - \\alpha/2} \\cdot \\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}\n\\]\n\n\n\n\nTwo-sample t-test in R, equal variance\n\n\n\nSample Size and Power for 2-Sample t-tests\n\nIdea: Given\n\n\\(\\mu_1 - \\mu_2\\) (effect size)\n\n\\(\\sigma_1^2\\) (variance of sample 1)\n\n\\(\\sigma_2^2\\) (variance of sample 2)\n\n\\(\\alpha\\) (significance level)\n\n\\(n_1\\) (sample size 1)\n\n\\(n_2\\) (sample size 2)\n\nThen we can calculate power \\(1 - \\beta\\) using similar methods as before.\nTo get \\(n_1\\), \\(n_2\\), assume \\(n_2 = k n_1\\) for known \\(k\\)\n\nE.g., we know we have equal sample sizes, or we know group 1 will have twice as many folks, etc.\nTotal sample size: \\(n_1 + k n_1 = n_1 \\left(1 + k\\right)\\)\nYou then have an equation of the form \\(g(n_1) = 1-\\beta\\), where \\(g(\\cdot)\\) is a function that gets the power given a known sample size.\nTo get the sample size given power, you can solve for \\(n_1\\).\n\n\n\nPower in R\n\nSkip Section 8.10\n\n\nAssumptions of t-methods\nThere are three assumptions, in decreasing order of importance (first is most important):\n\nIndependence\n\nCheck by thinking about sampling design\n\nDid you measure units in clusters (e.g. all from the same family)\nDid you measure the same units over time\n\nIf violated, use more complicated methods\n\nAnova, multiple linear regression, longitudinal approaches\n\n\nEqual variance\n\nNot required for Welch’s  t-test\n\nJust use Welch’s  t-test\n\nNormality\n\nViolated if there are outliers or skew\nCheck using:\n\nHistograms\nBoxplots\nQQ-plots (more on this later)\n\nOnly a big deal if \\(n\\) is small (e.g., \\(&lt; 50\\)) and there are lots of skew/outliers\n\n\n\nNote: Need normality within each group, not for the pooled or marginal distribution.\n\n\n\n\n\n\n\n\n\n\nMarginal distribution is not normal → that’s okay\n\nCheck by histograms and Q–Q plots in each group.\n\n\nIf normality is violated:\n\nIf all values are \\(&gt; 0\\), try logging the \\(X_i\\)’s\nIf all values are \\(\\geq 0\\), try taking the square root of the \\(X_i\\)’s\nRemove outliers and report both results\nUse a nonparametric method (see Chapter 9)\n\n\n\n\nExercises 8.3–8.11\nThe mean ±1 sd of ln [calcium intake (mg)] among 25 females, 12 to 14 years of age, below the poverty level is 6.56 ± 0.64. Similarly, the mean ± 1 sd of ln [calcium intake (mg)] among 40 females, 12 to 14 years of age, above the poverty level is 6.80 ± 0.76.\n\nExercise 8.3Solution\n\n\nWhat is the appropriate procedure to test for a significant difference in means between the two groups?\n\n\nA two-sample t-test. Equal variance would probably be OK, but why not just use Welch ?\nI think Rosner has you run a test for equal variance to determine if we assume equal variances or not. But don’t do this. In real life, just run Welch .\nBut it’s easier for me to test the equal variance approach. So in subsequent problems, assume the variances are equal.\n\n\n\n\nExercise 8.4HintSolution\n\n\nImplement the procedure in Problem 8.3 using the critical-value method at significance level 0.1.\n\n\n\n\\(\\bar{X} = 6.56\\)\n\\(\\bar{Y} = 6.80\\)\n\\(s_1^2 = 0.64^2 = 0.4096\\)\n\\(s_2^2 = 0.76^2 - 0.5776\\)\n\\(n_1 = 25\\)\n\\(n_2 = 40\\)\n\nFirst calculate the pooled sample variance.\n\n\nWe assume\n\n\\(X_1,\\ldots,X_{25} \\sim N(\\mu_1,\\sigma^2)\\)\n\\(Y_1,\\ldots,Y_{40} \\sim N(\\mu_2,\\sigma^2)\\)\n\nWe are testing\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(H_1: \\mu_1 \\neq \\mu_2\\)\n\nWe need the the pooled sample variance:\n\n((25 - 1) * 0.64^2 + (40 - 1) * 0.76^2) / (25 + 40 - 2)\n\n[1] 0.5136\n\n\nThe \\(t\\)-statistic is then\n\n(6.56 - 6.8) / (sqrt(.5136) * sqrt(1 / 25 + 1/40))\n\n[1] -1.314\n\n\nWe compare this to a \\(t\\) distribution with 25 + 40 - 2 = 63 degrees of freedom. We calculate the \\(1-\\alpha/2 = 1 - 0.1 / 2 = 0.95\\) quantile of the \\(t\\) distribution (it’s \\(\\alpha/2\\) because it’s a two-sided test) to get\n\nqt(p = 0.95, df = 63)\n\n[1] 1.669\n\n\nSince \\(|-1.314| = 1.314 &lt; 1.669\\), we fail to reject the null hypothesis at significance level 0.1 and conclude that we do not have evidence to say that females above and below the poverty level have different mean calcium intakes.\n\n\n\n\nExercise 8.5HintSolution\n\n\nWhat is the p-value corresponding to your answer to Problem 8.4?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe compare the t-statistic of -1.314 for a \\(t\\) distribution with 25 + 40 - 2 = 63 degrees of freedom. Since this is a two-sided test, we need the area in both tails.\n\n2 * pt(-1.314, df = 63)\n\n[1] 0.1936\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.6HintSolution\n\n\nCompute a 95% CI for the difference in means between the two groups.\n\n\nEstimate \\(\\pm\\) Multiplier \\(\\times\\) Standard Error\nWe already calculate the pooled sample variance. You need the \\(1 - \\alpha/2\\) quantile of the \\(t\\) distribution with \\(n_1 + n_2 - 2\\) degrees of freedom, which you can get with qt().\n\n\nWe calculate, for \\(\\alpha = 0.05\\): \\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nWe already calculated the pooled sample variance as \\(s^2 = 0.5136\\). Thus We also have that the 1 - 0.05/2 = 0.975 quantile of the t distribution with 25 + 40 -2 = 63 degrees of freedom is\n\nqt(0.975, df = 63)\n\n[1] 1.998\n\n\nThus, our interval is:\n\n6.56 - 6.8 - 1.998 * sqrt(0.5136) * sqrt(1/25 + 1/40)\n\n[1] -0.6051\n\n6.56 - 6.8 + 1.998 * sqrt(0.5136) * sqrt(1/25 + 1/40)\n\n[1] 0.1251\n\n\n\n\n\n\nExercise 8.7HintSolution\n\n\nSuppose an equal number of 12- to 14-year-old girls below and above the poverty level are recruited to study differences in calcium intake. How many girls should be recruited to have an 80% chance of detecting a significant difference using a two-sided test with α = .05?\n\n\nUse power.t.test(). Use the estimates above for your effect size and standard deviation.\n\n\nWe’ll use the estimates above as our “wild guess” for the effect size \\[\n\\bar{X} - \\bar{Y} = 6.56 - 6.8 = -0.24\n\\] and the estimated standard deviation comes from the pooled sample variance \\[\n\\sqrt{0.5136} = 0.7167\n\\]\n\npower.t.test(\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  power = 0.8,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 141\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThat means we need at least 141 individual per group.\n\n\n\n\nExercise 8.8HintSolution\n\n\nAnswer Problem 8.7 if a one-sided rather than a two-sided test is used.\n\n\nIt’s just one change in power.t.test() from 8.7.\n\n\n\npower.t.test(\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  power = 0.8,\n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 111\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nSo we need at least 111 individuals.\n\n\n\n\nExercise 8.10HintSolution\n\n\nSuppose 50 girls above the poverty level and 50 girls below the poverty level are recruited for the study. How much power will the study have of finding a significant difference using a two-sided test with α = .05, assuming that the population parameters are the same as the sample estimates in Problem 8.2?\n\n\nJust give power.t.test() n instead of power.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.3813\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power will be 0.3813.\n\n\n\n\nExercise 8.11HintSolution\n\n\nAnswer Problem 8.10 assuming a one-sided rather than a two-sided test is used.\n\n\nJust change one thing from power.t.test() in problem 8.10.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 0.24,\n  sd = 0.7167, \n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 0.24\n             sd = 0.7167\n      sig.level = 0.05\n          power = 0.5071\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nThe power will be 0.5071."
  },
  {
    "objectID": "05_tests/08_notes.html#paired-t-test",
    "href": "05_tests/08_notes.html#paired-t-test",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Paired t-test",
    "text": "Paired t-test\n\nLet\n\\(X_i \\sim N(\\mu_i, \\sigma^2)\\) for example, pre-OC\n\\(Y_i \\sim N(\\mu_i + \\Delta, \\sigma^2)\\) for example, post-OC\nHypotheses:\n\\(H_0\\): \\(\\Delta = 0\\)\n\\(H_1\\): \\(\\Delta \\ne 0\\)\nThis tests if there is a difference between populations while allowing each pair to have their own baseline mean \\(\\mu_i\\).\nDefine differences:\n\\(D_i = Y_i - X_i \\sim N(\\Delta, \\sigma_D^2)\\)\nVariance of differences:\n\\(\\sigma_D^2 = \\text{var}(X) + \\text{var}(Y) - 2\\ \\text{cov}(X, Y)\\)\n\nBut nuisance parameters, so just call it \\(\\sigma_D^2\\)\n\nSo, just use a one-sample t-test on \\(D_i\\)\nA paired t-test is just a one-sample t-test on differences.\n\n\\[\nt = \\frac{\\bar{D} - d_0}{s_D / \\sqrt{n}} \\sim t_{n-1}\n\\]\nXYZ IMAGE HERE\n\n\\(d_0\\) = null value\n\n\\(s_D\\) = SD of \\(D_i\\)’s\n\n\\(\\bar{D}\\) = mean of \\(D_i\\)’s\nGet \\((1 - \\alpha) \\cdot 100\\%\\) confidence interval by\n\n\\[\n\\bar{D} \\pm t_{n-1, 1 - \\alpha/2} \\cdot \\frac{s_D}{\\sqrt{n}}\n\\]\nwhere\n\\(t_{n-1, 1 - \\alpha/2}\\) is the \\(t\\)-quantile with df = \\(n - 1\\)\n(typically \\(\\alpha = 0.05\\))"
  },
  {
    "objectID": "05_tests/08_notes.html#paired-t-tests-independent-samples",
    "href": "05_tests/08_notes.html#paired-t-tests-independent-samples",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Paired t-tests ≠ independent samples",
    "text": "Paired t-tests ≠ independent samples\n\nMore common studies have 2 independent samples.\nExample:\n\nCollect one group of OC users\n\nCollect a separate group of non-OC users\n\nCross-sectional study: data collected at one point in time (units under different conditions)\n\n\\[\nX_1, X_2, \\ldots, X_{n_1} \\overset{\\text{iid}}{\\sim} N(\\mu_1, \\sigma_1^2)\n\\]\n\\[\nY_1, Y_2, \\ldots, Y_{n_2} \\overset{\\text{iid}}{\\sim} N(\\mu_2, \\sigma_2^2)\n\\]\n\nDifferent sample sizes possible, not paired.\nHypotheses:\n\n\\(H_0\\): \\(\\mu_1 = \\mu_2\\)\n\\(H_1\\): \\(\\mu_1 \\ne \\mu_2\\), or \\(\\mu_1 &lt; \\mu_2\\), or \\(\\mu_1 &gt; \\mu_2\\)\n\nFor now, assume \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\) (i.e., equal/pooled variance)\nWe observe:\n\n\\(\\bar{X} =\\) mean of \\(X_i\\)’s\n\n\\(\\bar{Y} =\\) mean of \\(Y_i\\)’s\n\n\\(s_1^2 =\\) sample variance of \\(X_i\\)’s\n\n\\(s_2^2 =\\) sample variance of \\(Y_i\\)’s\n\n\\(n_1 =\\) sample size 1\n\n\\(n_2 =\\) sample size 2\n\nConsider \\(\\bar{X} - \\bar{Y}\\)\n\n\\[\n\\mathbb{E}[\\bar{X} - \\bar{Y}] = \\mathbb{E}[\\bar{X}] - \\mathbb{E}[\\bar{Y}] = \\mu_1 - \\mu_2\n\\]\n\nEquals 0 under \\(H_0\\), not 0 under \\(H_1\\)\nVariance of difference:\n\n\\[\n\\text{Var}(\\bar{X} - \\bar{Y}) = \\text{Var}(\\bar{X}) + \\text{Var}(\\bar{Y}) - 2\\,\\text{Cov}(\\bar{X}, \\bar{Y})\n\\]\n\nCovariance term is 0 due to independence\n\n\\[\n= \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2} = \\sigma^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\n\\]\n\nTherefore,\n\n\\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\right)\n\\]\n\nIf \\(\\sigma^2\\) were known, then could test using\n\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\n\\]\n\nIf \\(H_0\\) true, compare stat to \\(N(0,1)\\) to get p-value\nAssuming \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\), estimate \\(\\sigma^2\\) with the pooled sample variance:\n\n\\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nEquivalent to\n\n\\[\ns^2 = \\frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \\frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2\n\\]\n\nHigher weight goes to sample with larger \\(n\\)\nTest statistic becomes:\n\n\\[\n\\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t_{n_1 + n_2 - 2}\n\\]\n\nIf \\(H_0\\) true\nLet:\n\n\\[\nX_1, X_2, \\ldots, X_{n_1} \\sim N(\\mu_1, \\sigma^2)\n\\]\n\\[\nY_1, Y_2, \\ldots, Y_{n_2} \\sim N(\\mu_2, \\sigma^2)\n\\]\n\nPooled variance:\n\n\\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\nTest statistic:\n\n\\[\nt = \\frac{\\bar{X} - \\bar{Y}}{s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t_{n_1 + n_2 - 2}\n\\]\nXYZ IMAGE HERE\n\nCalculate area in only one tail if alternative is one-sided.\n\\((1 - \\alpha) \\cdot 100\\%\\) confidence interval:\n\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\cdot s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nwhere\n\\(t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2}\\) is the \\(t\\)-quantile with df = \\(n_1 + n_2 - 2\\)\n\nt-test in R\n\n\n§8.6: Test for Equal Variances\n\n\\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\)\n\\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\)\nHypotheses:\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\n\\(H_1\\): \\(\\sigma_1^2 \\ne \\sigma_2^2\\)\n\nRun a test based on \\(s_1^2\\), \\(s_2^2\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nNobody does this in real life because:\n\n\n\nVery sensitive to non-normality — \\(t\\)-test is not sensitive because of the CLT\nEqual variance \\(t\\)-test is robust to violations in equal variance assumption\nNobody assumes equal variances anyway because they all use Welch’s 2-sample \\(t\\)-test (§8.7)\n\n\nIf your boss asks you to test for equal variances, use var.test()\n\n\nF-stat in R"
  },
  {
    "objectID": "05_tests/08_notes.html#two-sample-t-test-with-unequal-variances",
    "href": "05_tests/08_notes.html#two-sample-t-test-with-unequal-variances",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Two-sample t-test with unequal variances",
    "text": "Two-sample t-test with unequal variances\n\nAlways use this unless you know for sure that the variances are equal.\nLet:\n\n\\(X_i \\sim N(\\mu_1, \\sigma_1^2)\\) with sample size \\(n_1\\)\n\n\\(Y_i \\sim N(\\mu_2, \\sigma_2^2)\\) with sample size \\(n_2\\)\n\nThen:\n\n\\[\n\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_1 - \\mu_2,\\ \\frac{1}{n_1} \\sigma_1^2 + \\frac{1}{n_2} \\sigma_2^2\\right)\n\\]\n\nTest statistic:\n\n\\[\nt = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}} = \\frac{\\text{effect}}{\\text{SE}}\n\\]\n\nApproximately \\(t_\\nu\\) if \\(H_0\\) is true\nDegrees of freedom \\(\\nu\\) is a weird thing called the Satterthwaite approximation:\n\n\\[\n\\nu = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1 - 1} + \\frac{(s_2^2/n_2)^2}{n_2 - 1}}\n\\]\n\nYou don’t need to remember this.\n\nXYZ IMAGE HERE\n\nUse this \\(\\nu\\) to make the \\(t\\) as close to the actual distribution of \\(t\\) as possible.\nConfidence interval:\n\n\\[\n(\\bar{X} - \\bar{Y}) \\pm t_{\\nu,\\ 1 - \\alpha/2} \\cdot \\sqrt{\\frac{1}{n_1} s_1^2 + \\frac{1}{n_2} s_2^2}\n\\]\n\nt-tests in R"
  },
  {
    "objectID": "05_tests/08_notes.html#sample-size-and-power-for-2-sample-t-tests",
    "href": "05_tests/08_notes.html#sample-size-and-power-for-2-sample-t-tests",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Sample Size and Power for 2-Sample t-tests",
    "text": "Sample Size and Power for 2-Sample t-tests\n\nIdea: Given\n\n\\(\\mu_1 - \\mu_2\\) (effect)\n\n\\(\\sigma_1^2\\) (variance of sample 1)\n\n\\(\\sigma_2^2\\) (variance of sample 2)\n\n\\(\\alpha\\) (significance level)\n\n\\(n_1\\) (sample size 1)\n\n\\(n_2\\) (sample size 2)\n\nCan calculate power \\(1 - \\beta\\) using similar methods as before.\nTo get \\(n_1\\), \\(n_2\\), assume \\(n_2 = k n_1\\) for known \\(k\\)\n(i.e., known equal sample sizes or group 1 will have twice as many folks)\nThen solve for \\(n_1\\) given a fixed power.\nTotal sample size:\n\n\\[\n\\text{Sample Size} = n_1 + k n_1 = n_1 \\left(1 + k\\right)\n\\]\n\nPower in R\n\nSkip Section 8.10"
  },
  {
    "objectID": "05_tests/08_notes.html#assumptions-of-t-methods",
    "href": "05_tests/08_notes.html#assumptions-of-t-methods",
    "title": "Chapter 8: Two-sample Inference",
    "section": "Assumptions of t-methods",
    "text": "Assumptions of t-methods\n\nIndependence\n\nCheck by thinking about sampling design\n\nIf violated, use more complicated methods\n\nEqual variance\n\nNot required for Welch’s t-test\n\nJust use Welch’s t-test\n\nNormality\n\nCheck using:\n\nHistograms\nBoxplots\nShapiro test\n\nOnly a big deal if \\(n\\) is small (e.g., \\(&lt; 50\\)) and there are lots of skew/outliers\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote: Need normality within each group, not for the pooled or marginal distribution.\n\n\nXYZ IMAGE HERE (Boxplots and density plots for group 1 and group 2)\nMarginal distribution is not normal → that’s okay\n\nCheck by histograms and Q–Q plots (more later) in each group.\nIf violated:\n\n\nIf all \\(&gt; 0\\), try logging \\(X_i\\)’s\nRemove outliers and report both results\nUse a nonparametric method (see Chapter 9)"
  },
  {
    "objectID": "06_nonparametric/09_notes.html",
    "href": "06_nonparametric/09_notes.html",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "",
    "text": "\\(t\\)-tests assume Normality\n\nMaking distributional assumpitons is called “parametric”, because it uses parameters (like \\(\\mu\\) and \\(\\sigma^2\\))\nBut it only makes this assumption for small \\(n\\) (we can rely on the CLT for large \\(n\\))\n\nWhat if you have small \\(n\\) and non-normal data?\n\nUse non-parametric methods (work for all data, not just normal)\n\nAlso, some variables are not on a numeric scale where differences are meaningful, and so the \\(t\\) methods cannot be meaningfully applied.\n\n\n\n\n\n\n\nTipCardinal variable\n\n\n\nA numeric variable where distances between points make sense\n\n\n\nExamples:\n\nBody weight\nSerum creatine levels\nSystolic blood pressure\n\n\n\n\n\n\n\n\nTipOrdinal Variable\n\n\n\nA variable where order matters, but not the specific numbers\n\n\n\nExamples:\n\nVisual acuity: 20–20 vs. 20–30 vs. 20–40\nLikert scales (1 = very strongly disagree, 2 = disagree, …)\n\nMeans and variances not meaningful for ordinal data.\n\n\n\n\n\n\n\nTipNominal Variable\n\n\n\nA categorical variable with no ordering\n\n\n\nExamples:\n\nDeath classification (cancer, cardiovascular disease, …)\nEthnicity\nHair color\n\nMethods described in Chapter 9 are for\n\nOrdinal data, or\nCardinal data with normality violations.\n\nWe’ll talk about analyzing nominal data in Chapter 10."
  },
  {
    "objectID": "06_nonparametric/09_notes.html#sign-test",
    "href": "06_nonparametric/09_notes.html#sign-test",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "Sign Test",
    "text": "Sign Test\n\nThe Sign test is a non-parametric version of the paired \\(t\\)-test\nWherever you would use a paired \\(t\\)-test, you can instead use a sign test.\n\nIt is very robust to assumptions\nIt has very low power, so use a one-sample t-test unless your data don’t allow for a normality assumption.\n\nSuppose we just know (or use) that for two paired observations \\((A, B)\\), that either\n\\(A &gt; B\\), \\(A &lt; B\\), or \\(A = B\\)\nExample: Two ointments (A and B). Randomly apply one to left arm and the other to right. See which ointment produces more redness for each person.\nFor \\(n = 45\\), suppose we saw:\n\n22 with \\(A &lt; B\\)\n\n18 with \\(A &gt; B\\)\n\n5 with \\(A = B\\)\n\n\\(X_i\\) = redness on arm A\n\\(Y_i\\) = redness on arm B\n\\(d_i = X_i - Y_i\\)\n\\(\\Delta = \\text{median}(d_i)\\)\n\npopulation median, not sample median\n\n\\(H_0\\): \\(\\Delta = 0\\)\n\n\\(H_A\\): \\(\\Delta \\ne 0\\)\n\\(d_i\\) not observed, only observe whether \\(d_i &gt; 0\\) (A &gt; B), \\(d_i &lt; 0\\) (A &lt; B), or \\(d_i = 0\\) (A = B)\nLet \\(n = \\#(d_i &gt; 0) + \\#(d_i &lt; 0)\\)\n\nexclude \\(d_i = 0\\)\n\nLet \\(X = \\#(d_i &gt; 0)\\)\nIf \\(H_0\\) were true, \\(X \\sim \\text{Binom}\\left(n, \\frac{1}{2}\\right)\\)\nWhy: “success” = \\(d_i &gt; 0\\)?\n\\[\\begin{align*}\n\\mathbb{P}(\\text{success}) &= \\mathbb{P}(d_i &gt; 0) \\\\\n&= \\mathbb{P}(d_i &gt; \\text{median}(d_i)) \\text{ (if $H_0: \\Delta = 0$ true)}\\\\\n&= \\frac{1}{2} \\text{ (definition of median)}\n\\end{align*}\\]\nSo just use binomial methods on the sign of the differences (normal or exact)\n\n\n\n\n\n\n\nImportantSign Test\n\n\n\nLet \\(X = \\#(A &gt; B)\\) and \\(n = \\#(A&gt;B) + \\#(A&lt;B)\\)\n\n\\(H_0\\): \\(\\Pr(A &gt; B) = \\Pr(A &lt; B)\\)\nIf \\(H_0\\) is true, then \\[\nX | n \\sim \\mathrm{Binom(n, 1/2)}\n\\]\nThe sign test is equivalent to assuming \\(X \\sim \\mathrm{Binom}(n, p)\\) and testing \\(H_0: p = 1/2\\).\nUse Binomial methods (exact or normal).\n\n\n\n\nExample: If \\(X \\sim \\text{Binom}\\left(n, \\frac{1}{2}\\right)\\) (the null is true), then \\[\n\\frac{X}{n} \\sim N(p, p(1-p)/n) = N\\left(\\frac{1}{2}, \\frac{1/2(1-1/2)}{n}\\right) = N\\left(\\frac{1}{2}, \\frac{1}{4n}\\right)\n\\]\nPlugging in \\(X = 22\\) and \\(n = 40\\), we calculate our \\(z\\) statistic:\n\\[\nz = \\frac{\\hat{p} - 1/2 - 1/(2n)}{\\sqrt{1/2(1-1/2)/n}} = \\frac{22/40 - 1/2 - 1/80}{\\sqrt{1/(40 \\times 4)}}\n\\]\n\n(22/40 - 1/2 - 1/80) / sqrt(1 / (40 * 4))\n\n[1] 0.4743\n\n\nWe compare this \\(z\\)-statistic to a standard normal distribution\n\n2 * (1 - pnorm(0.4743))\n\n[1] 0.6353\n\n\n\n\n\n\n\n\n\n\n\nThis agrees with the value from prop.test()\n\nprop.test(x = 22, n = 40, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.635\n\n\n\nOr, we can do exact method with binom.test(). Sum all probabilities less probabe than or equally probable to our observed value of \\(X = 22\\):\n\n\n\n\n\n\n\n\n\n\n\nbinom.test(x = 22, n = 40, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.636"
  },
  {
    "objectID": "06_nonparametric/09_notes.html#wilcoxon-signed-rank-test",
    "href": "06_nonparametric/09_notes.html#wilcoxon-signed-rank-test",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "Wilcoxon Signed-Rank Test",
    "text": "Wilcoxon Signed-Rank Test\n\nThe Wilcoxin Signed-Rank test is also an alternative to the paired $t$-test\nIt’s more powerful than the sign test, but makes some additional assumptions, making it slightly less robust.\nIdea: still use \\(d_i\\), but take into account the rank of the magnitudes\nIntution: suppose we observed \\(d_i\\): -10, -7, -6, -5, 1, 2, 3, 4\n\nSince \\(\\#(d_i &gt; 0) = \\#(d_i &lt; 0)\\), so the sign test would fail to reject.\nBut, the negative differences are way more negative. This makes it seem that the median is less than 0.\n\nIdea:\n\nRank observations from \\(1\\) to \\(n\\) in terms of \\(|d_i|\\) (smallest to largest of the absolute values)\nSum ranks such that \\(d_i &gt; 0\\)\n\nExample:\n\n\n\n\\(d_i\\)\n\\(|d_i|\\)\nRank \\(|d_i|\\)\n\n\n\n\n-10\n10\n8\n\n\n-7\n7\n7\n\n\n-6\n6\n6\n\n\n-5\n5\n5\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n3\n3\n\n\n4\n4\n4\n\n\n\n\nYou now sum the ranks of the positive numbers: 1 + 2 + 3 + 4 = 10\n\nLet \\(R\\) be the rank sum (sum of the ranks of the positive numbers).\nIf \\(H_0: \\Delta = 0\\) is true, then, theoretical results are that:\n\\[\\begin{align*}\nE[R] &= \\frac{n(n+1)}{4}\\\\\n\\text{Var}(R) &= \\frac{n(n+1)(2n+1)}{24}\n\\end{align*}\\]\n\\(R \\sim N\\left(E[R], \\text{Var}(R)\\right)\\) for large \\(n\\)\nCompare \\(R\\) to null distribution to get \\(p\\)-value\nExact methods exist when \\(n\\) is small\nVariations exist when there are ties in \\(d_i\\)\nNote: The null is really that \\(\\Delta = 0\\) and \\(d_i\\) are symmetric (though possibly non-normal).\n\nMight reject \\(H_0\\) if \\(\\Delta = 0\\) but \\(d_i\\) are skewed\nSo really only testing \\(H_0: \\Delta = 0\\) if \\(d_i\\) are symmetric (checkable via histograms)\nIf symmatry is not a good assumption, use the sign test.\n\n\n\nWilcoxon signed-rank test in R"
  },
  {
    "objectID": "06_nonparametric/09_notes.html#wilcoxon-rank-sum-test",
    "href": "06_nonparametric/09_notes.html#wilcoxon-rank-sum-test",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "Wilcoxon Rank-Sum Test",
    "text": "Wilcoxon Rank-Sum Test\n\nThis is also called the Mann–Whitney \\(U\\) test.\nIt’s a nonparametric alternative to two-sample \\(t\\)-test\nWe want to test if the distribution is shifted in one group or the other\nLet \\(F_1\\) be the CDF of group 1\n\nLet \\(F_2\\) be the CDF of group 2\nHypotheses:\n\n\\(H_0\\): \\(F_1 = F_2\\)\n\\(H_A\\): \\(F_1(x) = F_2(x + \\Delta)\\) for some \\(\\Delta \\ne 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssume distribution is same in each group except one is shifted over (for \\(H_1\\))\nProcedure: Rank all values (not magnitudes like before)\n\nAdd up ranks in one group\nLet \\(R_1\\) = sum of ranks in group 1\nUnder \\(H_0\\):\n\\[\\begin{align*}\nE[R_1] &= \\frac{n_1(n_1 + n_2 + 1)}{2}\\\\\n\\text{Var}(R_1) &= \\frac{n_1 n_2 (n_1 + n_2 + 1)}{12}\n\\end{align*}\\]\nBy CLT, for large \\(n\\), \\(R_1 \\sim N(E[R_1], \\text{Var}(R_1))\\)\nSo compare to this distribution to get a \\(p\\)-value\n\nExample: \\(X\\) = -3, -1, 0, 1, 3 \\(Y\\) = -2, 2, 4\n\n\n\nSample\nValue\nRank\n\n\n\n\nX\n-3\n1\n\n\nY\n-2\n2\n\n\nX\n-1\n3\n\n\nX\n0\n4\n\n\nX\n1\n5\n\n\nY\n2\n6\n\n\nX\n3\n7\n\n\nY\n4\n8\n\n\n\nSum ranks for group X: \\[\nR_1 = 1 + 3 + 4 + 5 + 7 = 20\n\\]\n\n\n\n\n\n\n\n\n\n\n\nIntuition for when \\(n_1 = n_2\\):\n\nWe expect \\(R_1 \\approx R_2\\) = So if \\(R_1 \\gg R_2\\) or \\(R_1 \\ll R_2\\), then reject \\(H_0\\)\n\nSince \\(R_1 + R_2 = \\sum_{i=1}^{n_1 + n_2} i = \\frac{(n_1 + n_2)(n_1 + n_2 + 1)}{2}\\) just need to look at distribution of \\(R_1\\).\n\nThe sum of \\(R_1\\) and \\(R_2\\) is equal to some number which does not change from sample to sample\nSo if we know \\(R_1\\) then we automatically know \\(R_2\\)\nSo we only need to see one of these.\n\nFor small \\(n\\), an exact distribution of \\(R_1\\) (when \\(H_0\\) is true) is available\nModifications exist when there are ties\nCan use this to compare ordinal data\nExample: Visual acuity for individuals with dominant form of retinitis pigmentosa (RP) vs. visual acuity for sex-linked RP\n\nVisual acuity: 20–20, 20–25, 20–30, …\n\n\n\nWilcoxon Rank-Sum in R"
  },
  {
    "objectID": "06_nonparametric/09_notes.html#introduction",
    "href": "06_nonparametric/09_notes.html#introduction",
    "title": "Chapter 9: Nonparametric Methods",
    "section": "",
    "text": "\\(t\\)-tests assume Normality\n\nMaking distributional assumpitons is called “parametric”, because it uses parameters (like \\(\\mu\\) and \\(\\sigma^2\\))\nBut it only makes this assumption for small \\(n\\) (we can rely on the CLT for large \\(n\\))\n\nWhat if you have small \\(n\\) and non-normal data?\n\nUse non-parametric methods (work for all data, not just normal)\n\nAlso, some variables are not on a numeric scale where differences are meaningful, and so the \\(t\\) methods cannot be meaningfully applied.\n\n\n\n\n\n\n\nTipCardinal variable\n\n\n\nA numeric variable where distances between points make sense\n\n\n\nExamples:\n\nBody weight\nSerum creatine levels\nSystolic blood pressure\n\n\n\n\n\n\n\n\nTipOrdinal Variable\n\n\n\nA variable where order matters, but not the specific numbers\n\n\n\nExamples:\n\nVisual acuity: 20–20 vs. 20–30 vs. 20–40\nLikert scales (1 = very strongly disagree, 2 = disagree, …)\n\nMeans and variances not meaningful for ordinal data.\n\n\n\n\n\n\n\nTipNominal Variable\n\n\n\nA categorical variable with no ordering\n\n\n\nExamples:\n\nDeath classification (cancer, cardiovascular disease, …)\nEthnicity\nHair color\n\nMethods described in Chapter 9 are for\n\nOrdinal data, or\nCardinal data with normality violations.\n\nWe’ll talk about analyzing nominal data in Chapter 10."
  },
  {
    "objectID": "07_cat/10_notes.html",
    "href": "07_cat/10_notes.html",
    "title": "Chapter 10: Categorical Data",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\nTipCategorical Variable\n\n\n\nA variable that groups units into different categories.\n\n\n\nExamples:\n\nOral contraceptive user vs. Non-OC-user\nHas cancer vs. Does not have cancer\n\nThere are lots of inference problems that are interesting using categorical variables:\nExample 1: Test if cancer incidence is the same between OC users and non-OC users.\n\nTest the association between two categorical variables (cancer status and OC status).\n\nExample 2: Test if heavy OC users, light OC users, and non-OC users have the same cancer rates.\n\nSame as example 1 above, but now OC status has three categories instead of 2.\n\nExample 3: Are the observed frequencies below consistent with the theoretical probabilities?\n\n\n\nDiastolic Blood Pressure\nFrequency\nExpected Probability\n\n\n\n\n&lt; 50\n57\n1%\n\n\n≥ 50, &lt; 60\n330\n7%\n\n\n≥ 60, &lt; 70\n2132\n30%\n\n\n≥ 70, &lt; 80\n4584\n62%\n\n\n\n\n\n\nTwo-sample Test for Binomial Proportions\n\nExample: Age at first birth vs. breast cancer incidence.\n\n\n\n\nControl (No Cancer)\nCase (Cancer)\n\n\n\n\nAge at 1st birth ≤ 29 years\n8747\n2537\n\n\nAge at 1st birth ≥ 30 years\n1498\n683\n\n\nTotal\n10245\n3220\n\n\n\nData collection scheme: Chose women with cancer and women without cancer, then measured age of their first births\nGoal: Test if cancer is associated with age.\nTwo populations:\n\nThose with cancer\nThose without cancer\n\nLet:\n\n\\(p_1 = P(\\text{age} \\geq 30 \\mid \\text{cancer})\\)\n\\(p_2 = P(\\text{age} \\geq 30 \\mid \\text{no cancer})\\)\n\nHypotheses:\n\n\\(H_0\\): \\(p_1 = p_2\\) vs. \\(H_1\\): \\(p_1 \\neq p_2\\)\n\nSample proportions:\n\n\\(\\hat{p}_1 = \\frac{683}{3220} = 0.212\\)\n\\(\\hat{p}_2 = \\frac{1498}{10245} = 0.146\\)\n\nIdea: Base test on \\(\\hat{p}_1 - \\hat{p}_2\\).\nLet:\n\n\\(X_1\\) = number of women age \\(\\geq 30\\) with cancer\n\\(X_2\\) = number of women age \\(\\geq 30\\) without cancer\n\n\\(X_1 \\sim \\text{Binom}(n_1, p_1)\\), where \\(n_1 = 3220\\)\n\n\\(\\hat{p}_1 = X_1 / n_1\\)\n\n\\(X_2 \\sim \\text{Binom}(n_2, p_2)\\), where \\(n_2 = 10245\\)\n\n\\(\\hat{p}_2 = X_2 / n_2\\)\n\nWe have very large sample sizes, so we can use normal theory.\n\n\\(\\hat{p}_1 \\sim N\\left(p_1, \\frac{1}{n_1} p_1 (1 - p_1) \\right)\\)\n\\(\\hat{p}_2 \\sim N\\left(p_2, \\frac{1}{n_2} p_2 (1 - p_2) \\right)\\)\n\nUnder, \\(H_0\\), \\(p_1 = p_2 = p\\), so\n\n\\(\\hat{p}_1 \\sim N\\left(p, \\frac{1}{n_1} p (1 - p) \\right)\\)\n\\(\\hat{p}_2 \\sim N\\left(p, \\frac{1}{n_2} p (1 - p) \\right)\\)\n\nThus, \\[\n\\hat{p}_1 - \\hat{p}_2 \\sim N\\left(0, \\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)p(1 - p)\\right)\n\\]\nSo we divide by the standard error to get a \\(z\\) statistic that follows a standard normal distribution (but only if \\(H_0\\) is true): \\[\n\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)p(1 - p)}} \\sim N(0,1)\n\\]\nWe estimate \\(p\\) under \\(H_0\\) by noting that, if \\(H_0\\) were true, then \\[\nX_1 + X_2 \\sim \\text{Binom}(n_1 + n_2, p)\n\\] Thus, \\[\n\\hat{p} = \\frac{X_1 + X_2}{n_1 + n_2}\n\\]\nPlugging in \\(\\hat{p}\\) for \\(p\\) we get something that is asymptotically standard normal \\[\n\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\hat{p}(1 - \\hat{p})}} \\sim N(0,1)\n\\]\nNote: don’t use the \\(t\\)-distribution here. That depends on the data actually being normal.\nIn practice, we need to do a continuity correction for better performance.\n\n\n\n\n\n\n\nImportantTwo-sample Binomial Test\n\n\n\nObserve \\(X_1 \\sim \\mathrm{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(n_2, p_2)\\)\n\nNull Hypothesis: \\(H_0: p_1 = p_2\\)\n\nCalculate - \\(\\hat{p}_1 = X_1 / n_1\\) - \\(\\hat{p}_2 = X_2 / n_2\\) - \\(\\hat{p} = (X_1 + X_2) / (n_1 + n_2)\\)\n\n\\(z\\)-statistic \\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\hat{p}(1 - \\hat{p})}}\n\\]\n\\(H_1: p_1 \\neq p_2\\) \\[\n\\text{p-value} = 2 \\times \\texttt{pnorm}(-|z|)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: p_1 &gt; p_2\\) \\[\n\\text{p-value} = 1 - \\texttt{pnorm}(z)\n\\]\n\n\n\n\n\n\n\n\n\n\\(H_1: p_1 &lt; p_2\\) \\[\n\\text{p-value} = \\texttt{pnorm}(z)\n\\]\n\n\n\n\n\n\n\n\n\nA \\((1 - \\alpha)100\\%\\) confidence interval for the difference is \\[\n\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\alpha/2}\\sqrt{ \\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\]\nRule of thumb: The normal approximation works if \\(n_1\\hat{p}(1-\\hat{p}) \\geq 5\\) and \\(n_2\\hat{p}(1-\\hat{p}) \\geq 5\\)\n\n\n\n\nAge at first birth vs cancer example continued:\n\n\\(X_1 = 683\\), \\(n_1 = 3220\\)\n\\(X_2 = 1498\\), \\(n_2 = 10245\\)\n\\(\\hat{p}_1 = 683/3220 = 0.2121\\)\n\\(\\hat{p}_2 = 1498 / 10245 = 0.1462\\)\n\\(\\hat{p} = (683 + 1498) / (3220 + 10245) = 0.162\\)\nTest statistic: \\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\\hat{p}(1 - \\hat{p})}}\n= \\frac{0.2121 - 0.1462}{\\sqrt{\\left( \\frac{1}{3220} + \\frac{1}{10245} \\right)(0.162)(1 - 0.162)}}\n= 8.866\n\\]\np-value:\n\n2 * pnorm(-8.866)\n\n[1] 7.582e-19\n\n\nWe have strong evidence that women with cancer are more likely to have had their first child after age 30.\n\n\n\n2-sample binomial tests in R\n\n\n\nTest for Homogeneity in 2 \\(\\times\\) 2 Tables\n\nWhat I showed before was a \\(2 \\times 2\\) contingency table:\n\n\n\nStatus\nAge ≥ 30\nAge &lt; 30\nRow Total\n\n\n\n\nCase\n683\n2537\n3220\n\n\nControl\n1498\n8747\n10245\n\n\nCol Total\n2181\n11284\n13465\n\n\n\nRow margins and column margins are shown above. The total is the grand total.\nAn equivalent (exact same \\(p\\)-value) method is to test for homogeneity or independence between the two variables.\nSuppose our table looks like this:\n\n\n\n\nZ = A\nZ = B\nRow Total\n\n\n\n\nW = C\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(m_1\\)\n\n\nW = D\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(m_2\\)\n\n\nCol Total\n\\(n_1\\)\n\\(n_2\\)\n\\(N\\)\n\n\n\nDefinitions:\n\n\\(Z\\) is a variable with possible values of \\(A\\) and \\(B\\)\n\\(W\\) is a variable with possible values \\(C\\) and \\(D\\).\n\\(n_1 = X_{11} + X_{21}\\)\n\\(n_2 = X_{12} + X_{22}\\)\n\\(m_1 = X_{11} + X_{12}\\)\n\\(m_2 = X_{21} + X_{22}\\)\n\\(N = n_1 + n_2 = m_1 + m_2\\)\n\nIf \\(Z \\perp W\\), then:\n\n\\(P(Z = A \\cap W = C) = P(Z = A) \\cdot P(W = C)\\)\n\\(P(Z = A \\cap W = D) = P(Z = A) \\cdot P(W = D)\\)\n\\(P(Z = b \\cap W = C) = P(Z = b) \\cdot P(W = C)\\)\n\\(P(Z = b \\cap W = D) = P(Z = b) \\cdot P(W = D)\\)\n\nIdea: Calculate expected counts from the assumption of independence based on the margin totals.\n\n\\(P(Z = A) = \\frac{n_1}{N}\\)\n\\(P(Z = B) = \\frac{n_2}{N}\\)\n\\(P(W = C) = \\frac{m_1}{N}\\)\n\\(P(W = D) = \\frac{m_2}{N}\\)\n\nThe expected count under independence:\n\n\\(E[X_{11} \\mid Z \\perp W] = N \\Pr(Z = A \\cap W = C) = N\\Pr(Z = A)\\Pr(W=C) =  N \\cdot \\frac{n_1}{N} \\cdot \\frac{m_1}{N} = \\frac{n_1 m_1}{N}\\)\n\\(E[X_{12} \\mid Z \\perp W] = N \\Pr(Z = B \\cap W = C) = N\\Pr(Z = B)\\Pr(W=C) =  N \\cdot \\frac{n_2}{N} \\cdot \\frac{m_1}{N} = \\frac{n_2 m_1}{N}\\)\n\\(E[X_{21} \\mid Z \\perp W] = N \\Pr(Z = A \\cap W = D) = N\\Pr(Z = A)\\Pr(W=D) =  N \\cdot \\frac{n_1}{N} \\cdot \\frac{m_2}{N} = \\frac{n_1 m_2}{N}\\)\n\\(E[X_{22} \\mid Z \\perp W] = N \\Pr(Z = B \\cap W = D) = N\\Pr(Z = B)\\Pr(W=D) =  N \\cdot \\frac{n_2}{N} \\cdot \\frac{m_2}{N} = \\frac{n_2 m_2}{N}\\)\n\nCompare \\(X_{ij}\\) to the expected value of \\(X_{ij}\\) if variables were independent.\nExpected counts from age at first birth example:\n\n\n\n\n\n\n\n\n\n≥ 30\n&lt; 30\n\n\n\n\nCase\n\\(\\frac{2181 \\cdot 3220}{13465}\\)\n\\(\\frac{11284 \\cdot 3220}{13465}\\)\n\n\nControl\n\\(\\frac{2181 \\cdot 10245}{13465}\\)\n\\(\\frac{11284 \\cdot 10245}{13465}\\)\n\n\n\nThis results in:\n\n\n\n\n≥ 30\n&lt; 30\n\n\n\n\nCase\n521.6\n2698.4\n\n\nControl\n1659.4\n8585.6\n\n\n\nHow do these compare to observed counts?\nWhenever you compare observed counts to expected counts, do a Pearson \\(\\chi^2\\) test.\n\\[\nX^2 = \\sum_{\\text{categories}} \\frac{(O - E)^2}{E}\n\\]\n\n\\(O\\) = observed count\n\\(E\\) = expected count\n\nIf \\(H_0\\) is true, then \\(X^2 \\sim \\chi^2_\\nu\\)\n\n\\(\\nu\\) = degrees of freedom\n\nFor a test of homogeneity in \\(2 \\times 2\\) tables, \\(\\nu = 1\\)\nBirth example:\n\\[\nX^2 = \\frac{(683 - 521.6)^2}{521.6} + \\frac{(2537 - 2698.4)^2}{2698.4}\n+ \\frac{(1498 - 1659.4)^2}{1659.4} + \\frac{(8747 - 8585.6)^2}{8585.6}\n= 77.89\n\\]\n\n\n\n\n\n\n\n\n\nIn R:\n\n1 - pchisq(77.89, df = 1)\n\n[1] 0\n\n\nIt’s more numerically accurate to use lower.tail = FALSE because R has better precision for numbers close to 0 than for numbers close to 1.\n\npchisq(77.89, df = 1, lower.tail = FALSE)\n\n[1] 1.089e-18\n\n\n\n\nExerciseHintSolution\n\n\nWhat are the expected counts of the following OC use vs. MI table?\n\n\n\nOC Use\nMI Yes\nMI No\nRow Total\n\n\n\n\nYes\n13\n4987\n5000\n\n\nNo\n7\n9993\n10000\n\n\nCol Total\n20\n14980\n15000\n\n\n\n\n\nUse:\n\\[\nE = \\frac{\\text{row total} \\cdot \\text{column total}}{\\text{grand total}}\n\\]\nApply for each cell.\n\n\n\n\n\n\n\n\n\n\nOC Use\nMI Yes\nMI No\n\n\n\n\nYes\n\\(\\frac{20 \\cdot 5000}{15000}\\)\n\\(\\frac{14980 \\cdot 5000}{15000}\\)\n\n\nNo\n\\(\\frac{20 \\cdot 10000}{15000}\\)\n\\(\\frac{14980 \\cdot 10000}{15000}\\)\n\n\n\n\n\n\nOC Use\nMI Yes\nMI No\n\n\n\n\nYes\n6.7\n4993.3\n\n\nNo\n13.3\n9986.7\n\n\n\n\n\n\n\nExerciseSolution:\n\n\nCalculate the \\(\\chi^2\\) statistic from the previous exercise.\n\n\nChi-squared test statistic:\n\\[\nX^2 =\n\\frac{(13 - 6.7)^2}{6.7} +\n\\frac{(4987 - 4993.3)^2}{4993.3} +\n\\frac{(7 - 13.3)^2}{13.3} +\n\\frac{(9993 - 9986.7)^2}{9986.7} = 8.92\n\\]\n\n\n\n\nContingency test perspective in R\n\n\n\nFisher’s Exact Test\n\nWhat if \\(n\\) is small?\nRule of thumb for “small”: Any expected count \\(&lt; 5\\)\n\nIt’s OK if the observed counts are \\(&lt; 5\\).\n\nExample: Salt diet vs. cardiovascular disease (CVD) death\n\n\n\nCause of Death\nHigh Salt\nLow Salt\nRow Total\n\n\n\n\nNon-CVD\n2\n23\n25\n\n\nCVD\n5\n30\n35\n\n\nCol Total\n7\n53\n60\n\n\n\nExpected count for top-left cell: \\[\nE_{11} = \\frac{7 \\cdot 25}{60} = 2.92 &lt; 5\n\\]\n\nUse \\(2.92\\) (not \\(2\\)) to determine normality approximation validity.\n\n\n\n\n\n\n\n\nTipExact Test\n\n\n\nAn exact test is one that controls Type I error for any \\(n\\), not just large \\(n\\).\n\n\n\n\n\n\n\n\nImportantFisher’s Exact Test\n\n\n\n\nFix the margin totals.\nEnumerate all possible tables with those same margin totals.\nEach table has a known probability (under \\(H_0\\)).\nFind how likely our observed table is (if \\(H_0\\) true) by summing over all tables less than or as probable as our observed table.\n\n\n\n\nTo find all tables that maintain margin totals, you can start from your observed table and perform one of these two operations until the operations become impossible (because doing so would create negative counts).\n\\[\n\\pmatrix{x_{11} - 1 & x_{12} + 1 \\\\ x_{21} + 1 & x_{22} - 1}\n\\] or \\[\n\\pmatrix{x_{11} + 1 & x_{12} - 1 \\\\ x_{21} - 1 & x_{22} + 1}\n\\]\nOur observed table is\n\\[\n\\pmatrix{2 & 23 \\\\ 5 & 30}\n\\]\nAll possible \\(2 \\times 2\\) tables with the exact same margins\n\n\n\nTable\nProbability Under Null\n\n\n\n\n\\(\\pmatrix{0 & 25 \\\\ 7 & 28}\\)\n0.017\n\n\n\\(\\pmatrix{1 & 24 \\\\ 6 & 29}\\)\n0.105\n\n\n\\(\\pmatrix{2 & 23 \\\\ 5 & 30}\\)\n0.252 (observed table)\n\n\n\\(\\pmatrix{3 & 22 \\\\ 4 & 31}\\)\n0.312\n\n\n\\(\\pmatrix{4 & 21 \\\\ 3 & 32}\\)\n0.214\n\n\n\\(\\pmatrix{5 & 20 \\\\ 2 & 33}\\)\n0.082\n\n\n\\(\\pmatrix{6 & 19 \\\\ 1 & 34}\\)\n0.016\n\n\n\\(\\pmatrix{7 & 18 \\\\ 0 & 35}\\)\n0.001\n\n\n\nSum all probabilities \\(\\leq\\) the observed one (in bold) to get the p-value.\nThese probabilities come from the hypergeometric distribution (we won’t cover the exact formula).\n\n\nExerciseSolution\n\n\nWhat are the possible tables with fixed margins from the following?\n\\[\n\\pmatrix{2 & 1 \\\\ 2 & 2}\n\\]\n\n\n\nValid tables with these margins:\n\n\\(\\pmatrix{3 & 0 \\\\ 1 & 3}\\)\n\\(\\pmatrix{2 & 1 \\\\ 2 & 2}\\)\n\\(\\pmatrix{1 & 2 \\\\ 3 & 1}\\)\n\\(\\pmatrix{0 & 3 \\\\ 4 & 0}\\)\n\n\n\n\n\n\nFisher Exact Test in R\n\n\n\nMcNemar’s Test\n\nStudy design: Have matched samples, where each unit has two binary variables.\nExample:\n\nVariable 1: Treatment A vs. Treatment B\n\nVariable 2: Survive vs. Not\nChoose a woman to go into A.\nMatch another woman with similar characteristics to go into B.\nObserve survival for both.\n\n“Similar characteristics”: age, weight, clinical condition, etc.\nNaive Way: Treat each individual as an independent unit and put these in a 2 \\(\\times\\) 2 table:\n\n\n\nTreatment\nSurvive\nDie\nRow Total\n\n\n\n\nA\n526\n95\n621\n\n\nB\n515\n106\n621\n\n\nCol Total\n1041\n201\n1242\n\n\n\n\nYou could run a \\(\\chi^2\\) test for homogeneity, but this is wrong — the data are matched, not independent.\n\nInstead, use a matched-pair contingency table.\n\nAbove, each count is a person.\nInstead, each count should be a pair.\n\nCorrect way: Each matched pair is a unit\n\n\n\n\nB Survive\nB Die\nRow Total\n\n\n\n\nA Survive\n510\n16\n526\n\n\nA Die\n5\n90\n95\n\n\nCol Total\n515\n106\n621\n\n\n\n\n\n\n\n\n\n\nImportantNote\n\n\n\nDon’t use \\(\\chi^2\\) on this table because we know observations are associated (matched).\nWe only want to know which treatment does better.\n\n\n\n\n\n\n\n\nTipConcordant pair\n\n\n\nSame outcome (e.g., both survive or both die)\n\n\n\n\n\n\n\n\nTipDiscordant pair\n\n\n\nDifferent outcomes (e.g. one dies, one survives)\n\n\n\nConcordant pairs tell you nothing about which treatment is better.\nIdea: Treatments are the same if the discordant cells have approximately the same counts.\nLet:\n\n\\(X\\) = number of pairs where A survives and B dies\n\\(n\\) = number of discordant pairs\n\n\\[\nX \\sim \\text{Binom}(n, p)\n\\]\nHypotheses:\n\n\\(H_0\\): \\(p = \\frac{1}{2}\\) (treatments A and B equally effective)\n\\(H_1\\): \\(p \\ne \\frac{1}{2}\\)\n\nJust use binomial methods on discordant pairs.\n\nUse normal approximation for large counts\nUse exact test for small counts\n\n\n\nMcNemar’s Test in R\n\n\nExerciseHintSolution\n\n\nHypertension diagnosis: Each person is assessed (i) by trained observer and (ii) by a machine. Each person is a unit and the assessments are matched. Test if they give the same result (on average).\n\n\n\n\nTrained +\nTrained −\n\n\n\n\nMachine +\n3\n7\n\n\nMachine −\n1\n9\n\n\n\n\n\nOnly discordant pairs (7 and 1) matter.\n\n\nLet: - \\(X \\sim \\text{Binom}(8, p)\\) (discordant total = 8) - \\(H_0: p = \\frac{1}{2}\\) vs. \\(H_1: p \\ne \\frac{1}{2}\\)\nRun an exact binomial test in R:\n\nbinom.test(x = 7, n = 8, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  7 and 8\nnumber of successes = 7, number of trials = 8, p-value = 0.07\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4735 0.9968\nsample estimates:\nprobability of success \n                 0.875 \n\n\nThe p-value is 0.07, so we only have weak evidence that the machines and technicians differ.\nCannot use mcnemar.test() here because it uses a normal approximation.\n\n\n\n\nSummary: If you can group individuals into concordant vs. discordant pairs, then use McNemar’s test.\n\nSkip Power calculations §10.5\n\n\nLarger Contingency Tables\n\nInstead of 2 binary variables, we now have 2 categorical variables.\n\nVariable 1 has \\(R\\) levels\n\nVariable 2 has \\(C\\) levels\n\nExample: Cancer vs. Age at First Birth\n\n\n\nAge\n&lt; 20\n20–24\n25–29\n30–34\n≥ 35\nCol Total\n\n\n\n\nCase\n320\n1206\n1011\n463\n220\n3220\n\n\nControl\n1422\n4432\n2893\n1092\n406\n10245\n\n\nRow Total\n1742\n5638\n3904\n1555\n626\n13465\n\n\n\n\\(H_0\\): Cancer status ⟂ Age at first birth\n\n\\(H_1\\): The two variables are related\nUse the same contingency table approach as for \\(2 \\times 2\\) tables:\n\n\nCalculate expected counts assuming independence\n\nCompare to observed counts using \\(\\chi^2\\) statistic: \\[\nX^2 = \\sum_{\\text{cells}} \\frac{(O - E)^2}{E}\n\\]\n\\(X^2 \\sim \\chi^2_\\nu\\) if \\(H_0\\) is true where \\[\n\\nu = (R - 1)(C - 1)\n\\]\n\\(p\\)-value is the upper tail area\n\n\n\n\n\n\n\n\n\n\n\nFor example, from the table above we have\n\n\\(E_{11} = \\frac{1742 \\cdot 3220}{13465} = 416.6\\)\n\\(E_{12} = \\frac{1742 \\cdot 10245}{13465} = 1348.3\\)\nEtc…\n\n\n\nExerciseSolution\n\n\nCompute \\(E_{25}\\):\n\n\n\\[\nE_{25} = \\frac{626 \\cdot 10245}{13465} = 476.3\n\\]\n\n\n\n\nExerciseSolution\n\n\nIt turns out that the \\(\\chi^2\\) statistic is:\n\\[\nX^2 = \\sum_{\\text{cell}} \\frac{(O - E)^2}{E}\n= \\frac{(416.6 - 320)^2}{416.6} + \\cdots + \\frac{(476.3 - 406)^2}{476.3} = 130.3\n\\]\nWhat is the \\(p\\)-value for the test for independence?\n\n\nThe degrees of freedom of the \\(\\chi^2\\) distribution is: \\[\n\\nu = (2 - 1)(5 - 1) = 4\n\\]\nSo the \\(p\\)-value is\n\npchisq(130.3, df = 4, lower.tail = FALSE)\n\n[1] 3.359e-27\n\n\n\n\n\n\nLarger contingency tables in R\n\nSkip \\(\\chi^2\\) test for trend\n\n\n\\(\\chi^2\\) Goodness-of-Fit Test\n\nThe \\(\\chi^2\\) tests of homogeneity are special cases of \\(\\chi^2\\) goodness-of-fit tests.\nLet \\(e\\) be the expected counts under the null hypothesis. Let \\(o\\) be the observed count. Let \\[\nX^2 = \\sum_{\\text{categories}} \\frac{(e - o)^2}{e}.\n\\]\nUnder \\(H_0\\), \\(\\chi^2 \\sim \\chi^2_\\nu\\)\nDegrees of freedom: \\[\n\\nu = \\text{\\# parameters under } H_1 - \\text{\\# parameters under } H_0\n\\]\n\\(H_1\\) has more parameters (is more complex).\nExample: \\(2 \\times 2\\) Table\n\n\n\n\nC\nD\n\n\n\n\nA\n\\(x_{11}\\)\n\\(x_{12}\\)\n\n\nB\n\\(x_{21}\\)\n\\(x_{22}\\)\n\n\n\n\n\\(H_0\\): Independence\n\n2 parameters: \\(P(A)\\) and \\(P(C)\\)\nSince \\(P(B) = 1 - P(A)\\), it is redundant (know \\(P(A)\\) then know \\(P(B)\\)).\nSince \\(P(D) = 1 - P(C)\\), it is redundant (know \\(P(C)\\) then know \\(P(D)\\)).\n\n\\(H_1\\): Association\n\n3 parameters: \\(P(A \\cap C)\\), \\(P(A \\cap D)\\), and \\(P(B \\cap C)\\)\nSince \\(P(B \\cap D) = 1 - \\left[ P(A \\cap C) + P(A \\cap D) + P(B \\cap C) \\right]\\), it is redundant.\n\nSo: \\[\n\\nu = 3 - 2 = 1\n\\]\n\n\n\n\nCohen’s Kappa\n\nWant a measure of how reliable a test is\nOr measure how similar two judges rate something\nExample: 2 surveys measuring beef consumption\n\n\n\n\n\n\n\n\n\n\nSurvey 2: ≤ 1 serving/week\nSurvey 2: &gt; 1 serving/week\nRow Total\n\n\n\n\nSurvey 1: ≤ 1 serving/week\n136\n92\n228\n\n\nSurvey 1: &gt; 1 serving/week\n69\n240\n304\n\n\nCol Total\n205\n332\n537\n\n\n\nIdea: The “amount” of concordance reflects the reliability of the survey\nObserved proportion concordant: \\[\np_o = \\frac{136 + 240}{537} = 0.7\n\\]\nExpected concordance under independence: \\[\np_e = \\frac{205}{537} \\cdot \\frac{228}{537} + \\frac{332}{537} \\cdot \\frac{304}{537} = 0.518\n\\]\n\n\n\n\n\n\n\nTipCohen’s kappa\n\n\n\n\\[\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n\\]\n\n\n\nProperties:\n\nBounds: \\[\n\\frac{-p_e}{1 - p_e} \\leq \\kappa \\leq 1\n\\]\n\nSet \\(p_o = 0\\) or \\(1\\) to prove this.\n\n\\(\\kappa = 1\\) implies perfect concordance.\nRules of thumb:\n\n\\(\\kappa &gt; 0.75\\) → excellent reproducibility\n\\(0.4 &lt; \\kappa \\leq 0.75\\) → good reproducibility\n\\(0 \\leq \\kappa \\leq 0.4\\) → marginal reproducibility\n\n\nConfidence intervals and tests for \\(\\kappa\\) are possible.\nUse \\(\\kappa\\) for repeated measures of the same variable.\nFor two different variables, use sensitivity and specificity, not \\(\\kappa\\)\n\n\nCohen’s Kappa in R\n\n\n\nExercises 10.8–10.12\nTwo drugs (A, B) are compared for the medical treatment of duodenal ulcer. For this purpose, patients are carefully matched with regard to age, gender, and clinical condition. The treatment results based on 200 matched pairs show that for 89 matched pairs both treatments are effective; for 90 matched pairs both treatments are ineffective; for 5 matched pairs drug A is effective, whereas drug B is ineffective; and for 16 matched pairs drug B is effective, whereas drug A is ineffective.\n\nExercise 10.8Solution\n\n\nWhat test procedure can be used to assess the results?\n\n\nMcNemar’s test. Since we have 5 + 16 = 21 discordant pairs, and 21/4 = 5.25 \\(\\geq\\) 5, we can use either the normal approximation approach or the exact approach.\n\n\n\n\nExercise 10.9Solution\n\n\nPerform the test in Problem 10.8, and report a p-value.\n\n\n\nprop.test(x = 5, n = 21, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0291\n\n\nWe have evidence that there is a difference between drugs (p = 0.0291).\n\n\n\nIn the same study, if the focus is on the 100 matched pairs consisting of male patients, then the following results are obtained: for 52 matched pairs both drugs are effective; for 35 matched pairs both drugs are ineffective; for 4 matched pairs drug A is effective, whereas drug B is ineffective; and for 9 matched pairs drug B is effective, whereas drug A is ineffective.\n\nExercise 10.10Solution\n\n\nHow many concordant pairs are there among the male matched pairs?\n\n\n52 + 35 = 87 concordant pairs\n\n\n\n\nExercise 10.11Solution\n\n\nHow many discordant pairs are there among the male matched pairs?\n\n\n9 + 4 = 13 discordant pairs.\n\n\n\n\nExercise 10.12Solution\n\n\nPerform a significance test to assess any differences in effectiveness between the drugs among males. Report a p-value.\n\n\nSince 13/4 = 3.25 &lt; 5, we should use the exact approach.\n\nbinom.test(x = 4, n = 13, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.267\n\n\nWe do not have any evidence that the two drugs differ in males (p = 0.2668).\n\n\n\n\n\nExercise 10.13\nSuppose researchers do an epidemiologic investigation of people entering a sexually transmitted disease clinic. They find that 160 of 200 patients who are diagnosed as having gonorrhea and 50 of 105 patients who are diagnosed as having nongonococcal urethritis have had previous episodes of urethritis.\nTry doing it “by hand” and in R.\n\nExercise 10.13Solution\n\n\nAre the present diagnosis and prior episodes of urethritis associated? Use the two-sample binomial approach.\n\n\nLet \\(X_1\\) be the number of gonorrhea patients (out of 200) who have had previous episodes of urethritis.\nLet \\(X_2\\) be the number of nongonococcal urethritis patients (out of 105) who have had previous episodes of urethritis.\nWe have \\(X_1 \\sim \\mathrm{Binom}(200, p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(105, p_2)\\). We are testing \\(H_0: p_1 = p_2\\) versus \\(H_1: p_1 \\neq p_2\\).\n\\(\\hat{p}_1 = 160 / 200 = 0.8\\), \\(\\hat{p}_2 = 50/105 = 0.4762\\), \\(\\hat{p} = (160 + 50) / (200 + 105) = 0.6885\\)\nWe can run this test using the two-sample binomial approach since the normal conditions are met.\n\n200 * 0.6885 * (1 - 0.6885)\n\n[1] 42.89\n\n105 * 0.6885 * (1 - 0.6885)\n\n[1] 22.52\n\n\nOur \\(z\\) statistic is\n\n(0.8 - 0.4762) / sqrt(0.6885 * (1 - 0.6885) * (1/200 + 1/105))\n\n[1] 5.802\n\n\nWe compare this to a standard normal distribution to get a \\(p\\)-value\n\n2 * pnorm(-5.802)\n\n[1] 6.553e-09\n\n\nWe can also just use prop.test()\n\nprop.test(x = c(160, 50), n = c(200, 105)) |&gt;\n  tidy()  |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n       p.value\n         &lt;dbl&gt;\n1 0.0000000141\n\n\nThe \\(p\\)-values differ because we didn’t use the continuity correction. If we did, then we would get equivalent results:\n\np1 &lt;- 160/200\np2 &lt;- 50/105\np &lt;- (160 + 50) / (200 + 105)\nzstat &lt;- (p1 - p2 - 1/(2 * 200) - 1/(2 * 105)) / sqrt(p * (1 - p) * (1/200 + 1/105))\n2 * pnorm(-zstat)\n\n[1] 1.412e-08\n\n\nEither way, we have very strong evidence of an association between present diagnosis and prior episodes of urethritis (p &lt; 0.001)\n\n\n\n\nExercise 10.13bSolution\n\n\nRepeat Exercise 10.13 using the chi-squared approach.\n\n\nSetting up our contingency table, we have:\n\ntab &lt;- matrix(\n  c(160, 50, \n    40 , 55), \n  nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(tab) &lt;- list(Prior = c(\"Yes\", \"No\"), Gonorrhea = c(\"Yes\", \"No\"))\ntab\n\n     Gonorrhea\nPrior Yes No\n  Yes 160 50\n  No   40 55\n\n\nLet’s get the expected counts:\n\n## Column margins\ncolSums(tab)\n\nYes  No \n200 105 \n\n## Row margins\nrowSums(tab)\n\nYes  No \n210  95 \n\n## Grand total\nsum(tab)\n\n[1] 305\n\n\nExpected counts\n\netab &lt;- matrix(\n  c(200 * 210 / 305, 105 * 210 / 305,\n    200 * 95 / 305 , 105 * 95 / 305),\n  nrow = 2, ncol = 2, byrow = TRUE)\ndimnames(etab) &lt;- list(Prior = c(\"Yes\", \"No\"), Gonorrhea = c(\"Yes\", \"No\"))\netab\n\n     Gonorrhea\nPrior   Yes   No\n  Yes 137.7 72.3\n  No   62.3 32.7\n\n\nLet’s get the \\(\\chi^2\\) statistic:\n\n(160 - 137.7)^2 / 137.7 +\n  (50 - 72.3)^2 / 72.3 +\n  (40 - 62.3)^2 / 62.3 +\n  (55 - 32.7)^2 / 32.7\n\n[1] 33.68\n\n\nWe compare this to a \\(\\chi^2_1\\) distribution to get the \\(p\\)-value\n\npchisq(q = 33.68, df = 1, lower.tail = FALSE)\n\n[1] 6.497e-09\n\n\nIn R, this is:\n\nchisq.test(x = tab) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n       p.value\n         &lt;dbl&gt;\n1 0.0000000141\n\n\nThe p-value is different because we didn’t do the continuity correction. The continuity correction is a little weirder for \\(\\chi^2\\) tests. You subtract off a half value from the absolute value of the differences inside the square.\n\netab &lt;- outer(rowSums(tab), colSums(tab)) / 305\netab\n\n      Yes   No\nYes 137.7 72.3\nNo   62.3 32.7\n\nchstat &lt;- sum((abs(etab - tab) - 0.5)^2 / etab)\npchisq(q = chstat, df = 1, lower.tail = FALSE)\n\n[1] 1.412e-08"
  },
  {
    "objectID": "07_cat/10_notes.html#properties-of-cohens-kappa",
    "href": "07_cat/10_notes.html#properties-of-cohens-kappa",
    "title": "Chapter 10: Categorical Data",
    "section": "Properties of Cohen’s Kappa",
    "text": "Properties of Cohen’s Kappa\n\nBounds: \\[\n\\frac{-p_e}{1 - p_e} \\leq \\kappa \\leq 1\n\\]\n\nSet \\(p_o = 0\\) or \\(1\\) for bounds.\n\n\\(\\kappa = 1\\) implies perfect concordance.\nRules of thumb:\n\n\\(\\kappa &gt; 0.75\\) → excellent reproducibility\n\\(0.4 &lt; \\kappa \\leq 0.75\\) → good reproducibility\n\\(0 \\leq \\kappa \\leq 0.4\\) → marginal reproducibility\n\n\n\nConfidence intervals and tests for \\(\\kappa\\) are possible.\nUse \\(\\kappa\\) for repeated measures of the same variable.\nFor two different variables, use sensitivity and specificity.\n\n\nCohen’s Kappa in R"
  },
  {
    "objectID": "07_cat/10_notes.html#exercise-1",
    "href": "07_cat/10_notes.html#exercise-1",
    "title": "Chapter 10: Categorical Data",
    "section": "Exercise",
    "text": "Exercise\nCalculate the \\(\\chi^2\\) statistic from the previous exercise"
  },
  {
    "objectID": "07_cat/10_notes.html#solution-1",
    "href": "07_cat/10_notes.html#solution-1",
    "title": "Chapter 10: Categorical Data",
    "section": "Solution:",
    "text": "Solution:\nChi-squared test statistic:\n\\[\n\\chi^2 =\n\\frac{(13 - 6.7)^2}{6.7} +\n\\frac{(4987 - 4993.3)^2}{4993.3} +\n\\frac{(7 - 13.3)^2}{13.3} +\n\\frac{(9993 - 9986.7)^2}{9986.7}\n\\]"
  },
  {
    "objectID": "review/practice_problems.html",
    "href": "review/practice_problems.html",
    "title": "Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions.\nI have only kept the problems most relevant for this course.\n\nChapter 2\nChapter 3\nChapter 4\nChapter 5\nChapter 6\nChapter 7\nChapter 8\nChapter 9\nChapter 10"
  },
  {
    "objectID": "review/pp_02.html",
    "href": "review/pp_02.html",
    "title": "Chapter 2 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_02.html#effect-on-statistics",
    "href": "review/pp_02.html#effect-on-statistics",
    "title": "Chapter 2 Practice Problems",
    "section": "Effect on statistics",
    "text": "Effect on statistics\nSuppose the origin for a data set is changed by adding a constant to each observation.\n\n2.1Solution\n\n\nWhat is the effect on the median?\n\n\nIt is shifted by the same constant.\n\n\n\n\n2.2Solution\n\n\nWhat is the effect on the mode?\n\n\nIt is shifted by the same constant.\n\n\n\n\n2.3Solution\n\n\nWhat is the effect on the arithmetic mean?\n\n\nIt is shifted by the same constant.\n\n\n\n\n2.4Solution\n\n\nWhat is the effect on the range?\n\n\nIt is unchanged."
  },
  {
    "objectID": "review/pp_02.html#renal-disease",
    "href": "review/pp_02.html#renal-disease",
    "title": "Chapter 2 Practice Problems",
    "section": "Renal Disease",
    "text": "Renal Disease\nFor a study of kidney disease, the following measurements were made on a sample of women working in several factories in Switzerland. They represent concentrations of bacteria in a standard-size urine specimen. High concentrations of these bacteria may indicate possible kidney pathology. The data are presented in the following data frame\n\nlibrary(tidyverse)\nrenal &lt;- tibble(\n  Concentration = 10^(0:10),\n  Frequency = c(521, 230, 115, 74, 69, 62, 43, 30, 21, 10, 2)\n)\n\n\n\n\n\n\n\n\n\nConcentration\nFrequency\n\n\n\n\n1e+00\n521\n\n\n1e+01\n230\n\n\n1e+02\n115\n\n\n1e+03\n74\n\n\n1e+04\n69\n\n\n1e+05\n62\n\n\n1e+06\n43\n\n\n1e+07\n30\n\n\n1e+08\n21\n\n\n1e+09\n10\n\n\n1e+10\n2\n\n\n\n\n\n\n\nHere is a barplot\n\nggplot(renal, aes(x = as.factor(Concentration), y = Frequency)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n2.5Solution\n\n\nCompute the arithmetic mean for this sample.\n\n\nBy hand:\n\nn &lt;- 521 + 230 + 115 + 74 + 69 + 62 + 43 + 30 + 21 + 10 + 2\nsum_x &lt;- (1e0 * 521 + 1e1 * 230 + 1e2 * 115 + 1e3 * 74 + 1e4 * 69 +\n            1e5 * 62 + 1e6 * 43 + 1e7 * 30 + 1e8 * 21 + 1e9 * 10 + 1e10 * 2)\nsum_x / n\n\n[1] 27570075\n\n\nVerify using R:\n\nsum(renal$Concentration * renal$Frequency) / sum(renal$Frequency)\n\n[1] 27570075\n\n\n\n\n\n\n2.6Solution\n\n\nCompute the median for this sample.\n\n\nThere are \\(n = 1177\\) individuals\n\nn\n\n[1] 1177\n\n\nThis is odd, so we need the \\((n + 1) / 2\\) = 589th value. This is just 10.\n\n\n\n\n2.7Solution\n\n\nWhich do you think is a more appropriate measure of location?\n\n\nObviously the median. This is a super skewed distribution."
  },
  {
    "objectID": "review/pp_02.html#cardiovascular-disease",
    "href": "review/pp_02.html#cardiovascular-disease",
    "title": "Chapter 2 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nThe mortality rates from heart disease (per 100,000 population) for each of the 50 states and the District of Columbia in 1973 are given in descending order in the data frame below.\n\nstate_data &lt;- tibble(\n  State = c(\"West Virginia\", \"Pennsylvania\", \"Maine\", \"Missouri\", \"Illinois\",\n            \"Florida\", \"Rhode Island\", \"Kentucky\", \"New York\", \"Iowa\",\n            \"Arkansas\", \"New Jersey\", \"Massachusetts\", \"Kansas\", \"Oklahoma\",\n            \"Ohio\", \"South Dakota\", \"Wisconsin\", \"Vermont\", \"Nebraska\",\n            \"Tennessee\", \"New Hampshire\", \"Indiana\", \"North Dakota\", \"Delaware\",\n            \"Mississippi\", \"Louisiana\", \"Connecticut\", \"Oregon\", \"Washington\",\n            \"Minnesota\", \"Michigan\", \"Alabama\", \"North Carolina\", \"DC\",\n            \"South Carolina\", \"Montana\", \"Maryland\", \"Georgia\", \"Virginia\",\n            \"California\", \"Wyoming\", \"Texas\", \"Idaho\", \"Colorado\", \"Arizona\",\n            \"Nevada\", \"Utah\", \"New Mexico\", \"Hawaii\", \"Alaska\"),\n  Rate = c(445.4, 442.7, 427.3, 422.9, 420.8, 417.4, 414.4, 407.6, 406.7, 396.9,\n            396.8, 395.2, 394.0, 391.7, 391.0, 377.7, 376.2, 369.8, 369.2, 368.9,\n            361.4, 358.2, 356.4, 353.3, 351.6, 351.6, 349.4, 340.3, 338.7, 334.2,\n            332.7, 330.2, 329.1, 328.4, 327.1, 322.4, 319.1, 315.9, 311.8, 311.2,\n            310.6, 306.8, 300.6, 297.4, 274.6, 265.4, 236.9, 214.2, 194.0, 169.0, \n            83.9)\n)\n\nConsider this data set as a sample of size 51. \\[\n(X_1, X_2, \\ldots, x_{51})\n\\] If \\(\\sum_{i=1}^{51}x_i = 17409\\) and \\(\\sum_{i=1}^{51}(x_i - \\bar{x})^2 = 249063.65\\), then do the following:\n\n2.8Solution\n\n\nCompute the arithmetic mean of this sample.\n\n\n\\[\n\\sum_{i=1}^{51}x_i / n = 17409 / 51\n\\]\n\n17409 / 51\n\n[1] 341.4\n\n\n\n\n\n\n2.9Solution\n\n\nCompute the median of this sample.\n\n\nWe need the (51 + 1) / 2 = 26th largest value. You can count, but you’ll end up with Delaware (or Mississippi), whose rate is\n\nmedian(state_data$Rate)\n\n[1] 351.6\n\n\n\n\n\n\n2.10Solution\n\n\nCompute the standard deviation of this sample.\n\n\n\\[\n\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{51}(x_i - \\bar{x})^2} = \\sqrt{249063.65 / 50}\n\\]\n\nsqrt(249063.65 / 50)\n\n[1] 70.58\n\n\nVerifying in R, we have\n\nsd(state_data$Rate)\n\n[1] 70.58\n\n\n\n\n\n\n2.11Solution\n\n\nThe national mortality rate for heart disease in 1973 was 360.8 per 100,000. Why does this figure not correspond to your answer for Problem 2.8?\n\n\nIn 2.8, we calculated the mean rates of the states. The national mortality rate is by person. To get the same number from the state data, we would need to weight by the state population.\n\n\n\n\n2.12Solution\n\n\nDoes the differential in raw rates between Florida (417.4) and Georgia (311.8) actually imply that the risk of dying from heart disease is greater in Florida than in Georgia? Why or why not?\n\n\nNo. You need to control for demographic variables. E.g., age of the population."
  },
  {
    "objectID": "review/pp_02.html#nutrition",
    "href": "review/pp_02.html#nutrition",
    "title": "Chapter 2 Practice Problems",
    "section": "Nutrition",
    "text": "Nutrition\nThe data frame below shows the distribution of dietary vitamin-A intake as reported by 14 students who filled out a dietary questionnaire in class. The total intake is a combination of intake from individual food items and from vitamin pills. The units are in IU/100 (International Units/100).\n\nvita &lt;- tibble(\n  Student_number = 1:14,\n  Intake_IU_100 = c(31.1, 21.5, 74.7, 95.5, 19.4, 64.8, 108.7,\n                    48.1, 24.4, 13.4, 37.1, 21.3, 78.5, 17.7)\n)\n\n\n\n\n\n\n\n\n\nStudent_number\nIntake_IU_100\n\n\n\n\n1\n31.1\n\n\n2\n21.5\n\n\n3\n74.7\n\n\n4\n95.5\n\n\n5\n19.4\n\n\n6\n64.8\n\n\n7\n108.7\n\n\n8\n48.1\n\n\n9\n24.4\n\n\n10\n13.4\n\n\n11\n37.1\n\n\n12\n21.3\n\n\n13\n78.5\n\n\n14\n17.7\n\n\n\n\n\n\n\n\n2.13Solution\n\n\nCompute the mean and median from these data.\n\n\nThe mean\n\nsumx &lt;- 31.1 + 21.5 + 74.7 + 95.5 + 19.4 + 64.8 + 108.7 +\n  48.1 + 24.4 + 13.4 + 37.1 + 21.3 + 78.5 + 17.7\nn &lt;- 14\nxbar &lt;- sumx / n\nxbar\n\n[1] 46.87\n\n\nThe median is the mean of the 7th and 8th largest observations. Ordering the observations we get \\[\n13.4, 17.7, 19.4, 21.3, 21.5, 24.4, 31.1, 37.1, 48.1, 64.8, 74.7, 78.5, 95.5, 108\n\\] So the median is\n\n(31.1 + 37.1) / 2\n\n[1] 34.1\n\n\nWe can verify in R\n\nmedian(vita$Intake_IU_100)\n\n[1] 34.1\n\n\n\n\n\n\n2.14Solution\n\n\nCompute the standard deviation and coefficient of variation from these data.\n\n\nSum of the squared values\n\nsumx2 &lt;- 31.1^2 + 21.5^2 + 74.7^2 + 95.5^2 + 19.4^2 + 64.8^2 + 108.7^2 +\n  48.1^2 + 24.4^2 + 13.4^2 + 37.1^2 + 21.3^2 + 78.5^2 + 17.7^2\n\nSince \\(\\frac{1}{n}\\sum (x_i - \\bar{x})^2 = \\frac{1}{n}\\sum x_i^2 - \\bar{x}^2\\), we get variance is\n\n(sumx2 / n - xbar^2) * n / (n-1)\n\n[1] 1012\n\n\nAnd so the standard deviation is\n\nsqrt(1012)\n\n[1] 31.81\n\n\nWe can verify in R\n\nsd(vita$Intake_IU_100)\n\n[1] 31.81\n\n\nThe coefficient of variation is the standard deviation divided by the mean. So\n\n31.8 / 46.9\n\n[1] 0.678\n\n\n\n\n\n\n2.15Solution\n\n\nSuppose the data are expressed in IU rather than IU/100. What are the mean, standard deviation, and coefficient of variation in the new units?\n\n\nJust multiply the mean and sd by 100.\n\n46.9 * 100 ## mean\n\n[1] 4690\n\n31.8 * 100 ## sd\n\n[1] 3180\n\n\nThe CV is unchanged.\n\n(31.8 * 100) / (46.9 * 100)\n\n[1] 0.678\n\n\n\n\n\n\n2.17Solution\n\n\nDo you think the mean or median is a more appropriate measure of location for this data set? Here is a histogram\n\nggplot(vita, aes(x = Intake_IU_100)) + \n  geom_histogram(bins = 5)\n\n\n\n\n\n\n\n\n\n\nHard to tell. Maybe the median because of the slight skew? But there is too few data to say for sure."
  },
  {
    "objectID": "review/pp_03.html",
    "href": "review/pp_03.html",
    "title": "Chapter 3 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_03.html#basic-probability",
    "href": "review/pp_03.html#basic-probability",
    "title": "Chapter 3 Practice Problems",
    "section": "Basic Probability",
    "text": "Basic Probability\nLet\n\n\\(A\\) = {serum cholesterol \\(=\\) 250–299},\n\\(B\\) = {serum cholesterol \\(\\geq\\) 300},\n\\(C\\) = {serum cholesterol \\(\\leq\\) 280}.\n\n\n3.1Solution\n\n\nAre the events \\(A\\) and \\(B\\) mutually exclusive?\n\n\nYes. You cannot both have a value between 250 and 299 AND have a value greater than 300.\n\n\n\n\n3.2Solution\n\n\nAre the events \\(A\\) and \\(C\\) mutually exclusive?\n\n\nNo. Both \\(A\\) and \\(C\\) occur if the serum colesterol level is anywhere between 250 and 280.\n\n\n\n\n3.3Solution\n\n\nSuppose \\(P(A) = 0.2\\), \\(P(B) = 0.1\\). What is \\(P(\\text{serum cholesteroal} \\geq 250)\\)?\n\n\nThis is \\(P(A \\cup B) = P(A) + P(B) = 0.2 + 0.1 = 0.3\\)\n\n\n\n\n3.4Solution\n\n\nWhat does \\(A \\cup C\\) mean?\n\n\nThe serum cholesterol level is less than or equal to 299.\n\n\n\n\n3.5Solution\n\n\nWhat does \\(A \\cap C\\) mean?\n\n\nThe serum cholesterol level is between 250 and 280 (inclusive).\n\n\n\n\n3.6Solution\n\n\nWhat does \\(B \\cup C\\) mean?\n\n\nThe serum cholesterol level is either less than or equal to 280, or greater than or equal to 300.\n\n\n\n\n3.7Solution\n\n\nWhat does \\(B \\cap C\\) mean?\n\n\nThe null set. Both events cannot happen.\n\n\n\n\n3.8Solution\n\n\nAre the events \\(B\\) and \\(C\\) mutually exclusive?\n\n\nYes. They cannot both happen.\n\n\n\n\n3.9Solution\n\n\nWhat does the event \\(\\bar{B}\\) mean? What is its probability?\n\n\nThe serum cholesterol level is less than or equal to 299. \\[\nP(\\bar{B}) = 1 - P(B) = 1 - 0.1 = 0.9\n\\]\n\n\n\nSuppose that the gender of successive offspring in the same family are independent events and that the probability of a male or female offspring is 0.5.\n\n3.10Solution\n\n\nWhat is the probability of two successive female offspring?\n\n\nLet \\(A\\) = first female and \\(B\\) = second female. Then \\[\nP(A \\cap B) = P(A)P(B) = 0.5 \\times 0.5  = 0.25\n\\]\n\n\n\n\n3.11Solution\n\n\nWhat is the probability that exactly one of two successive children will be female?\n\n\n\\[\\begin{align*}\nP[(\\bar{A} \\cap B) \\cup (A \\cap \\bar{B})] &= P(\\bar{A} \\cap B) + P(A \\cap \\bar{B}) \\text{ (mutually exclusive events)}\\\\\n&= P(\\bar{A})P(B) + P(A)P(\\bar{B}) \\text{ (independent events)}\\\\\n&= 0.5 \\times 0.5 + 0.5 \\times 0.5 \\text{ (substituting in probabilities)}\\\\\n&= 0.5.\n\\end{align*}\\]\n\n\n\n\n3.12Solution\n\n\nSuppose that three successive offspring are male. What is the probability that a fourth child will be male?\n\n\nStill 0.5. These events are all independent."
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease",
    "href": "review/pp_03.html#cardiovascular-disease",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nA survey was performed among people 65 years of age and older who underwent open-heart surgery. It was found that 30% of patients died within 90 days of the operation, whereas an additional 25% of those who survived 90 days died within 5 years after the operation.\n\n3.13Solution\n\n\nWhat is the probability that a patient undergoing open-heart surgery will die within 5 years?\n\n\n\n\n\n\n\n3.14Solution\n\n\nWhat is the mortality incidence (per patient month) in patients receiving this operation in the first 90 days after the operation? (Assume that 90 days = 3 months.)\n\n\n\n\n\n\n\n3.15Solution\n\n\nAnswer the same question as in Problem 3.14 for the period from 90 days to 5 years after the operation.\n\n\n\n\n\n\n\n3.16Solution\n\n\nCan you tell if the operation prolongs life from the data presented? If not, then what additional data do you need?\n\n\n\n\n\n\nA study relating smoking history to several measures of cardiopulmonary disability was recently reported. The data in the table below were presented relating the number of people with different disabilities according to cigarette-smoking status.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1 Number of people with selected cardiopulmonary disabilities versus cigarette-smoking status\n\n\nDisability\nNone(n=656)\nEx(n=826)\nCurrent&lt; 15 g/day (n=955)\nCurrent&gt;= 15 g/day (n=654)\n\n\n\n\nShortness of breath\n7\n15\n18\n13\n\n\nAngina\n15\n19\n19\n16\n\n\nPossible infarction\n3\n7\n8\n6\n\n\n\n\n\n\n\n\n3.17Solution\n\n\nWhat is the prevalence of angina among light current smokers (&lt; 15g/day)?\n\n\n\n\n\n\n\n3.18Solution\n\n\nWhat is the relative risk of ex-smokers, light current smokers, and heavy current smokers, respectively, for shortness of breath as compared with nonsmokers?\n\n\n\n\n\n\n\n3.19Solution\n\n\nAnswer Problem 3.18 for angina.\n\n\n\n\n\n\n\n3.20Solution\n\n\nAnswer Problem 3.18 for possible infarction"
  },
  {
    "objectID": "review/pp_03.html#pulmonary-disease",
    "href": "review/pp_03.html#pulmonary-disease",
    "title": "Chapter 3 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\nPulmonary embolism is a relatively common condition that necessitates hospitalization and also often occurs in patients hospitalized for other reasons. An oxygen tension (arterial Po\\(_{2}\\)) &lt; 90 mm Hg is one of the important criteria used in diagnosing this condition. Suppose that the sensitivity of this test is 95%, the specificity is 75%, and the estimated prevalence is 20% (i.e., a doctor estimates that a patient has a 20% chance of pulmonary embolism before performing the test).\n\n3.21HintSolution\n\n\nWhat is the predictive value positive of this test? What does it mean in words?\n\n\n\nSensitivity: \\(P(T^+|D^+) = 0.95\\)\nSpecificity: \\(P(T^-|D^-) = 0.75\\)\nPrevalence: \\(P(D^+) = 0.2\\)\n\nWe want\n\n\\(PV^+ = P(D^+|T^+)\\)\n\n\n\n\nSensitivity: \\(P(T^+|D^+) = 0.95\\)\nSpecificity: \\(P(T^-|D^-) = 0.75\\)\nPrevalence: \\(P(D^+) = 0.2\\)\n\nWe want\n\n\\(PV^+ = P(D^+|T^+)\\)\n\nUsing Bayes rule, we have \\[\nP(D^+|T^+) = \\frac{P(T^+|D^+) P(D+^)}{P(T^+)}\n\\] So we can use the law of total probability to get \\[\\begin{align*}\nP(T^+) &= P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-)\\\\\n&= P(T^+|D^+)P(D^+) + [1 - P(T^-|D^-)][1 - P(D^+)]\\\\\n&= 0.95 \\times 0.2 + (1 - 0.75)\\times(1 - 0.2)\n\\end{align*}\\]\nSo we have \\[\nP(D^+|T^+) = \\frac{0.95 \\times 0.2}{0.95 \\times 0.2 + (1 - 0.75)\\times(1 - 0.2)} = 0.4872\n\\]\nIt means the chance of having Pulmonary embolism given that oxygen tension &lt; 90 mm Hg is about 0.487.\n\n\n\n\n3.22Solution\n\n\nWhat is the predictive value negative of this test? What does it mean in words?\n\n\nWe need \\(PV^- = P(D^-|T^-)\\). Using Bayes rule we have \\[\\begin{align*}\nP(D^-|T^-) &= \\frac{P(T^-|D^-)P(D^-)}{P(T^-)}\\\\\n&= \\frac{P(T^-|D^-)P(D^-)}{1 - P(T^+)}\\\\\n&= \\frac{0.75 \\times (1 - 0.2)}{1 - [0.95 \\times 0.2 + (1 - 0.75)\\times(1 - 0.2)]}\\\\\n&= 0.9836\n\\end{align*}\\]\nIt means the chance of not having Pulmonary embolism given that oxygen tension &lt;&gt;90 mm Hg is about 0.9836"
  },
  {
    "objectID": "review/pp_03.html#environmental-health-pediatrics",
    "href": "review/pp_03.html#environmental-health-pediatrics",
    "title": "Chapter 3 Practice Problems",
    "section": "Environmental Health, Pediatrics",
    "text": "Environmental Health, Pediatrics\n\n3.25HintSolution\n\n\nSuppose that a company plans to build a lead smelter in a community and that the city council wishes to assess the health effects of the smelter. In particular, there is concern from previous literature that children living very close to the smelter will experience unusually high rates of lead poisoning in the first 3 years of life. The projected rates of lead poisoning over this time period are 50 per 100,000 for those children living within 2 km of the smelter, 20 per 100,000 for children living &gt; 2 km but ≤ 5 km from the smelter, and 5 per 100,000 for children living &gt; 5 km from the smelter. If 80% of the children live more than 5 km from the smelter, 15% live &gt; 2 km but ≤ 5 km from the smelter, and the remainder live ≤ 2 km from the smelter, then what is the overall probability that a child from this community will get lead poisoning?\n\n\nLet\n\n\\(A_1\\): living within 2 km\n\\(A_2\\): living &gt; 2 km but ≤ 5 km\n\\(A_3\\): living &gt; 5 km\n\\(B\\): Lead poisened.\n\nThen we are given\n\n\\(P(B|A_1) = 50/100000\\)\n\\(P(B|A_2) = 20/100000\\)\n\\(P(B|A_3) = 5/100000\\)\n\\(P(A_1) = 1 - (0.15 + 0.8) = 0.05\\)\n\\(P(A_2) = 0.15\\)\n\\(P(A_3) = 0.8\\)\n\n\n\nLet\n\n\\(A_1\\): living within 2 km\n\\(A_2\\): living &gt; 2 km but ≤ 5 km\n\\(A_3\\): living &gt; 5 km\n\\(B\\): Lead poisened.\n\nThen we are given\n\n\\(P(B|A_1) = 50/100000\\)\n\\(P(B|A_2) = 20/100000\\)\n\\(P(B|A_3) = 5/100000\\)\n\\(P(A_1) = 1 - (0.15 + 0.8) = 0.05\\)\n\\(P(A_2) = 0.15\\)\n\\(P(A_3) = 0.8\\)\n\nWe want \\(P(B)\\), which we can get by the law of total probability \\[\\begin{align*}\nP(B) &= P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + P(B|A_3)P(A_3)\\\\\n&= 50/100000 \\times 0.05 + 20/100000 \\times 0.15 + 5/100000 \\times 0.8\\\\\n&= 0.000095\n\\end{align*}\\]"
  },
  {
    "objectID": "review/pp_03.html#diabetes",
    "href": "review/pp_03.html#diabetes",
    "title": "Chapter 3 Practice Problems",
    "section": "Diabetes",
    "text": "Diabetes\nThe prevalence of diabetes in adults at least 20 years old has been studied in Tecumseh, Michigan. The age-sex-specific prevalence (per 1000) is given in Table 3.2.\n\n\n\n\n\n\n\n\nTable 3.2 Age-sex-specific prevalence of diabetes in Tecumseh, MI (per 1000)\n\n\nAge group (years)\nMale\nFemale\n\n\n\n\n20–39\n5\n7\n\n\n40–54\n23\n31\n\n\n55+\n57\n89\n\n\n\n\n\n\n\n\n3.26HintSolution\n\n\nSuppose we plan a new study in a town that consists of 48% males and 52% females. Of the males, 40% are ages 20–39, 32% are 40–54, and 28% are 55+. Of the females, 44% are ages 20–39, 37% are 40–54, and 19% are 55+. Assuming that the Tecumseh prevalence rates hold, what is the expected prevalence of diabetes in the new study?\n\n\nLet\n\n\\(A\\) be male.\n\\(B_1\\) be 20–39\n\\(B_2\\) be 40–54\n\\(B_3\\) be 55+\n\\(C\\) be Diabetes.\n\nWe are given\n\n\\(P(A) = 0.48\\) and \\(P(\\bar{A}) = 0.52\\)\n\\(P(B_1|A) = 0.4\\), \\(P(B_2|A) = 0.32\\), and \\(P(B_3|A) = 0.28\\)\n\\(P(B_1|\\bar{A}) = 0.44\\), \\(P(B_2|\\bar{A}) = 0.37\\), and \\(P(B_3|\\bar{A}) = 0.19\\)\n\\(P(C|A \\cap B_1) = 5/1000\\)\n\\(P(C|A \\cap B_2) = 23/1000\\)\n\\(P(C|A \\cap B_3) = 57/1000\\)\n\\(P(C|\\bar{A} \\cap B_1) = 7/1000\\)\n\\(P(C|\\bar{A} \\cap B_2) = 31/1000\\)\n\\(P(C|\\bar{A} \\cap B_3) = 89/1000\\)\n\nWe are asked to calculate \\(P(C)\\).\n\n\nLet\n\n\\(A\\) be male.\n\\(B_1\\) be 20–39\n\\(B_2\\) be 40–54\n\\(B_3\\) be 55+\n\\(C\\) be Diabetes.\n\nWe are given\n\n\\(P(A) = 0.48\\) and \\(P(\\bar{A}) = 0.52\\)\n\\(P(B_1|A) = 0.4\\), \\(P(B_2|A) = 0.32\\), and \\(P(B_3|A) = 0.28\\)\n\\(P(B_1|\\bar{A}) = 0.44\\), \\(P(B_2|\\bar{A}) = 0.37\\), and \\(P(B_3|\\bar{A}) = 0.19\\)\n\\(P(C|A \\cap B_1) = 5/1000\\)\n\\(P(C|A \\cap B_2) = 23/1000\\)\n\\(P(C|A \\cap B_3) = 57/1000\\)\n\\(P(C|\\bar{A} \\cap B_1) = 7/1000\\)\n\\(P(C|\\bar{A} \\cap B_2) = 31/1000\\)\n\\(P(C|\\bar{A} \\cap B_3) = 89/1000\\)\n\nWe are asked to calculate \\(P(C)\\).\n\\[\\begin{align*}\nP(C) &= P(C|A\\cap B_1)P(B_1|A)P(A) + P(C|A\\cap B_2)P(B_2|A)P(A) + P(C|A\\cap B_3)P(B_3|A)P(A) + \\\\\n&P(C|\\bar{A}\\cap B_1)P(B_1|\\bar{A})P(\\bar{A}) + P(C|\\bar{A}\\cap B_2)P(B_2|\\bar{A})P(\\bar{A}) + P(C|\\bar{A}\\cap B_3)P(B_3|\\bar{A})P(\\bar{A})\n\\end{align*}\\]\nPlugging in values, we get: \\[\\begin{align*}\nP(C) &= \\frac{5}{1000} \\times 0.4 \\times 0.48 + \\frac{23}{1000} \\times 0.32 \\times 0.48 + \\frac{57}{1000} \\times 0.28 \\times 0.48 +\\\\\n&\\frac{7}{1000} \\times 0.44 \\times 0.52 + \\frac{31}{1000} \\times 0.37 \\times 0.52 + \\frac{89}{1000} \\times 0.19 \\times 0.52\n\\end{align*}\\]\n\n\n\n\n3.27Solution\n\n\nWhat proportion of diabetics in the new study would be expected in each of the six age-sex groups?\n\n\nE.g., we want \\(P(A \\cap B_1 | C)\\) for the proportion of diabetics that are men between 20 and 29. This is obtained by Bayes theorem \\[\nP(A \\cap B_1 | C) = \\frac{P(C|A\\cap B_1)P(A\\cap B_1)}{P(C)} = \\frac{P(C|A\\cap B_1)P(B_1| A)P(A)}{P(C)}.\n\\] The numerator values are given, the denominator was calculated in 3.26."
  },
  {
    "objectID": "review/pp_03.html#cancer",
    "href": "review/pp_03.html#cancer",
    "title": "Chapter 3 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\nTable 3.3 shows the annual incidence rates for colon cancer, lung cancer, and stomach cancer in males ages 50 years and older from the Connecticut Tumor Registry, 1963–1965.\n\n\n\n\n\n\n\n\nTable 3.3\n\n\nAverage annual incidence per 100,000 males for colon, lung, and stomach cancer from the Connecticut Tumor Registry 1963–1965\n\n\nType of cancer\nAges 50-54\nAges 55-59\nAges 60-64\n\n\n\n\nColon\n35.7\n60.3\n98.9\n\n\nLung\n76.1\n137.5\n231.7\n\n\nStomach\n20.8\n39.1\n46.0\n\n\n\n\n\n\n\n\n3.28Solution\n\n\nWhat is the probability that a 57-year-old, disease-free male will develop lung cancer over the next year?\n\n\nThis is just 137.5 / 100000\n\n\n\n\n3.29Solution\n\n\nWhat is the probability that a 55-year-old, disease-free male will develop colon cancer over the next 5 years?\n\n\nThis is one minus the probability that he does not develop cancer over the next five years, which is \\[\n1 - (1 - 60.3 / 100000)^5\n\\]\n\n\n\n\n3.30Solution\n\n\nSuppose there is a cohort of 1000 50-year-old men who have never had cancer. How many colon cancers would be expected to develop in this cohort over a 15-year period?\n\n\nThis is the probability of having colon cancer during that period times 1000.\nIt’s easier to calculate the proportion that do not get colon cancer.\n\nProbability don’t have it from 50–54: (1 - 35.7/100000)^5\nProbability don’t have it from 55–59: (1 - 60.3/100000)^5\nProbability don’t have it from 60–64: (1 - 98.9/100000)^5\n\nMultiply these together:\n\n(1 - 35.7/100000)^5 * (1 - 60.3/100000)^5 * (1 - 98.9/100000)^5\n\n[1] 0.9903\n\n\nThis is the probability of getting colon cancer. So the probability of not getting it is one minus this, or\n\n1 - (1 - 35.7/100000)^5 * (1 - 60.3/100000)^5 * (1 - 98.9/100000)^5\n\n[1] 0.009701\n\n\nMultiply this by 1000 to get the expected number. Or, about 9.7 cases."
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease-1",
    "href": "review/pp_03.html#cardiovascular-disease-1",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease 1",
    "text": "Cardiovascular Disease 1\nA survey was performed among people 65 years of age and older who underwent open-heart surgery. It was found that 30% of patients died within 90 days of the operation, whereas an additional 25% of those who survived 90 days died within 5 years after the operation.\n\n3.13HintSolution\n\n\nWhat is the probability that a patient undergoing open-heart surgery will die within 5 years?\n\n\nLet \\(A\\) be died in first 90 days. Let \\(B\\) be died between 90 days and 5 years. We are given \\[\nP(A) = 0.3\\\\\nP(B|\\bar{A}) = 0.25\n\\] We are asked to calculate \\(P(A\\cup B)\\)\n\n\nLet \\(A\\) be died in first 90 days. Let \\(B\\) be died between 90 days and 5 years. We are given \\[\nP(A) = 0.3\\\\\nP(B|\\bar{A}) = 0.25\n\\] We are asked to calculate \\(P(A\\cup B)\\)\nNote that \\(A\\) and \\(B\\) are mutually exclusive events. This is because you can’t die twice (some eastern religions excepted). This means that \\[\nP(A \\cup B) = P(A) + P(B)\n\\] \\(P(A)\\) was given to us. Let’s get \\(P(B)\\) from the law of total probability. \\[\\begin{align*}\nP(B) = P(B|A)P(A) + P(B|\\bar{A})P(\\bar{A})\\\\\n&= 0 * 0.3 + 0.25 * (1 \\times 0.3)\\\\\n&= 0.25 \\times (1 - 0.3)\n\\end{align*}\\] (\\(P(B|A) = 0\\) because, again, you cannot die if you already dead).\nSo, putting these together, we get \\[\nP(A \\cup B) = 0.3 + 0.25 \\times (1 - 0.3) = 0.475\n\\]\n\n\n\n\n3.16Solution\n\n\nCan you tell if the operation prolongs life from the data presented? If not, then what additional data do you need?\n\n\nNo. We need a control group to see if folks doing the operation do better than those without doing the operation.\n\n\n\nA study relating smoking history to several measures of cardiopulmonary disability was recently reported. The data in the table below were presented relating the number of people with different disabilities according to cigarette-smoking status.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1 Number of people with selected cardiopulmonary disabilities versus cigarette-smoking status\n\n\nDisability\nNone(n=656)\nEx(n=826)\nCurrent&lt; 15 g/day (n=955)\nCurrent&gt;= 15 g/day (n=654)\n\n\n\n\nShortness of breath\n7\n15\n18\n13\n\n\nAngina\n15\n19\n19\n16\n\n\nPossible infarction\n3\n7\n8\n6\n\n\n\n\n\n\n\n\n3.17Solution\n\n\nWhat is the prevalence of angina among light current smokers (&lt; 15g/day)?\n\n\n\\[\n19 / 955 = 0.0199\n\\]\n\n\n\n\n3.18Solution\n\n\nWhat is the relative risk of ex-smokers, light current smokers, and heavy current smokers, respectively, for shortness of breath as compared with nonsmokers?\n\n\nLet \\(B\\) be shortness of breath. Let \\(A_1\\) be ex-smoker, \\(A_2\\) be light current smoker, \\(A_3\\) be heavy current smoker, and \\(A_0\\) be nonsmoker. Then\n\nRR of ex-smoker vs non-smoker = \\(\\frac{P(B|A_1)}{P(B|A_0)} = \\frac{15/826}{7/656}\\)\nRR of light current smoker vs non-smoker = \\(\\frac{P(B|A_2)}{P(B|A_0)} = \\frac{18/955}{7/656}\\)\nRR of heavy current smoker vs non-smoker = \\(\\frac{P(B|A_3)}{P(B|A_0)} = \\frac{13/654}{7/656}\\)\n\n\n\n\n\n3.19Solution\n\n\nAnswer Problem 3.18 for angina.\n\n\nLet \\(B\\) be angina. Let \\(A_1\\) be ex-smoker, \\(A_2\\) be light current smoker, \\(A_3\\) be heavy current smoker, and \\(A_0\\) be nonsmoker. Then\n\nRR of ex-smoker vs non-smoker = \\(\\frac{P(B|A_1)}{P(B|A_0)} = \\frac{19/826}{15/656}\\)\nRR of light current smoker vs non-smoker = \\(\\frac{P(B|A_2)}{P(B|A_0)} = \\frac{19/955}{15/656}\\)\nRR of heavy current smoker vs non-smoker = \\(\\frac{P(B|A_3)}{P(B|A_0)} = \\frac{16/654}{15/656}\\)\n\n\n\n\n\n3.20Solution\n\n\nAnswer Problem 3.18 for possible infarction\n\n\nLet \\(B\\) be possible infarction Let \\(A_1\\) be ex-smoker, \\(A_2\\) be light current smoker, \\(A_3\\) be heavy current smoker, and \\(A_0\\) be nonsmoker. Then\n\nRR of ex-smoker vs non-smoker = \\(\\frac{P(B|A_1)}{P(B|A_0)} = \\frac{7/826}{3/656}\\)\nRR of light current smoker vs non-smoker = \\(\\frac{P(B|A_2)}{P(B|A_0)} = \\frac{8/955}{3/656}\\)\nRR of heavy current smoker vs non-smoker = \\(\\frac{P(B|A_3)}{P(B|A_0)} = \\frac{6/654}{3/656}\\)"
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease-2",
    "href": "review/pp_03.html#cardiovascular-disease-2",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease 2",
    "text": "Cardiovascular Disease 2\nThe relationship between physical fitness and cardiovascular-disease mortality was recently studied in a group of railroad working men, ages 22–79. Data were presented relating baseline exercise-test heart rate and coronary heart-disease mortality (Table 3.4).\n\n\n\n\n\n\n\n\nTable 3.4\n\n\nRelationship between baseline exercise-test heart rate and coronary heart-disease mortality\n\n\nExercise-test heart rate (beats/min)\nCoronary heart-disease mortality (20 years) (per 100)\n\n\n\n\n≤ 105\n9.1\n\n\n106–115\n8.7\n\n\n116–127\n11.6\n\n\n&gt; 127\n13.2\n\n\n\n\n\n\n\nSuppose that 20, 30, 30, and 20% of the population, respectively, have exercise-test heart rates of ≤ 105, 106–115, 116–127, &gt; 127 beats/minute. Suppose a test is positive if the exercise-test heart rate is &gt; 127 beats/min and negative otherwise.\n\n3.33HintSolution\n\n\nWhat is the probability of a positive test among men who have died over the 20-year period? Is there a name for this quantity?\n\n\nLet\n\n\\(A_1\\) be ≤ 105\n\\(A_2\\) be 106–115\n\\(A_3\\) be 116–127\n\\(A_4\\) be &gt; 127\n\\(D^+\\) be death\n\nWe are given\n\n\\(P(A_1) = 0.2\\)\n\\(P(A_2) = 0.3\\)\n\\(P(A_3) = 0.3\\)\n\\(P(A_4) = 0.2\\)\n\\(P(D^+|A_1) = 9.1/100\\)\n\\(P(D^+|A_2) = 8.7/100\\)\n\\(P(D^+|A_3) = 11.6/100\\)\n\\(P(D^+|A_4) = 13.2/100\\)\n\n\n\nLet\n\n\\(A_1\\) be ≤ 105\n\\(A_2\\) be 106–115\n\\(A_3\\) be 116–127\n\\(A_4\\) be &gt; 127\n\\(D^+\\) be death\n\nWe are given\n\n\\(P(A_1) = 0.2\\)\n\\(P(A_2) = 0.3\\)\n\\(P(A_3) = 0.3\\)\n\\(P(A_4) = 0.2\\)\n\\(P(D^+|A_1) = 9.1/100\\)\n\\(P(D^+|A_2) = 8.7/100\\)\n\\(P(D^+|A_3) = 11.6/100\\)\n\\(P(D^+|A_4) = 13.2/100\\)\n\nWe are told that \\(A_4 = T^+\\) (a positive test result). We want \\(P(T^+|D^+) = P(A_4|D^+)\\). We can use Bayes rule \\[\nP(A_4|D^+) = \\frac{P(D^+|A_4)P(A_4)}{P(D^+)}\n\\]\nThe numerator terms are given, the denominator can be found by the law of total probability. \\[\\begin{align*}\nP(D^+) &= P(D^+|A_1)P(A_1) + P(D^+|A_2)P(A_2) + P(D^+|A_3)P(A_3) + P(D^+|A_4)P(A_4)\\\\\n&= 0.091 \\times 0.2 + 0.087 \\times 0.3 + 0.116 \\times 0.3 + 0.132 \\times 0.2\\\\\n&= 0.1055\n\\end{align*}\\] So, the solution is \\[\nP(A_4|D^+) = \\frac{0.132 \\times 0.2}{0.1055} = 0.2502\n\\] This is called the sensitivity.\n\n\n\n\n3.34Solution\n\n\nWhat is the probability of a positive test among men who survived the 20-year period? Is there a name for this quantity?\n\n\nUsing the same notation from 3.33, we want \\(P(A_4|D^-)\\). We can use Bayes rule \\[\nP(A_4|D^-) = \\frac{P(D^-|A_4)P(A_4)}{P(D^-)} = \\frac{(1 - 0.132) \\times 0.2}{1 - 0.1055} = 0.1941\n\\] \\(P(T^+|D^-)\\) is the false positive rate, but you don’t need to know this term.\n\n\n\n\n3.35HintSolution\n\n\nWhat is the probability of death among men with a negative test? Is there a name for this quantity?\n\n\n\\[\nD^+ \\cap (A_1 \\cup A_2 \\cup A_3) = (D^+\\cap A_1) \\cup (D^+\\cap A_2) \\cup (D^+\\cap A_3)\n\\]\n\n\nUsing the same notation from 3.33, we want \\(P(D^+|\\bar{A_4})\\). This is \\[\\begin{align*}\nP(D^+|\\bar{A_4}) &= P(D^+|A_1 \\cup A_2 \\cup A_3)\\\\\n&= \\frac{P[D^+ \\cap (A_1 \\cup A_2 \\cup A_3)]}{P(A_1 \\cup A_2 \\cup A_3)}\\\\\n&= \\frac{P[(D^+\\cap A_1) \\cup (D^+\\cap A_2) \\cup (D^+\\cap A_3)}{P(A_1) + P(A_2) + P(A_3)}\\\\\n&= \\frac{P(D^+\\cap A_1) + P(D^+\\cap A_2) + P(D^+\\cap A_3)}{P(A_1) + P(A_2) + P(A_3)}\\\\\n&= \\frac{P(D^+|A_1)P(A_1) + P(D^+| A_2)P(A_2) + P(D^+| A_3)P(A_3)}{P(A_1) + P(A_2) + P(A_3)}\\\\\n&= \\frac{0.091 \\times 0.2 + 0.087 \\times 0.3 + 0.116 \\times 0.3}{0.2 + 0.3 + 0.3}\\\\\n&= 0.09888\n\\end{align*}\\]\n\\(P(D^+|T^-)\\) is the false omission rate. But you don’t need to know this term."
  },
  {
    "objectID": "review/pp_03.html#nutrition",
    "href": "review/pp_03.html#nutrition",
    "title": "Chapter 3 Practice Problems",
    "section": "Nutrition",
    "text": "Nutrition\nThe food-frequency questionnaire (FFQ) is a commonly used method for assessing dietary intake, where individuals are asked to record the number of times per week they usually eat for each of about 100 food items over the previous year. It has the advantage of being easy to administer to large groups of people, but has the disadvantage of being subject to recall error. The gold standard instrument for assessing dietary intake is the diet record (DR), where people are asked to record each food item eaten on a daily basis over a 1-week period. To investigate the accuracy of the FFQ, both the FFQ and the DR were administered to 173 participants in the United States. The reporting of alcohol consumption with each instrument is given in Table 3.5, where alcohol is coded as alcohol = some drinking, versus no alcohol = no drinking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.5\n\n\nActual drinking habits as determined from the diet record cross-classified by self-reported drinking status from the food-frequency questionnaire\n\n\n\n\nFood-frequency questionnaire\n\n\n\nAlcohol\nNo alcohol\nTotal\n\n\n\n\nAlcohol\n138\n18\n156\n\n\nNo alcohol\n1\n16\n17\n\n\nTotal\n139\n34\n173\n\n\n\n\n\n\n\n\n3.40Solution\n\n\nWhat is the sensitivity of the FFQ?\n\n\nHere, if the FFQ says alcohol, this is a “positive test”. If the DR says alcohol, then this is a “positive disease”. So, in this context, the sensitivity of FFQ is the proportion of DR+ that are also FFQ+. Or, \\[\n138 / 156 = 0.8846\n\\]\n\n\n\n\n3.41Solution\n\n\nWhat is the specificity of the FFQ?\n\n\nThis is the proportion of DR- that are also FFQ-, or \\[\n16 / 17 = 0.9412\n\\]\n\n\n\nLet us treat the 173 participants in this study as representative of people who would use the FFQ.\n\n3.42Solution\n\n\nWhat is the predictive value positive of the FFQ?\n\n\nThis is the proportion of FFQ+ that are also DR+. So \\[\n138 / 139 = 0.9928\n\\]\n\n\n\n\n3.43Solution\n\n\nWhat is the predictive value negative of the FFQ?\n\n\nThis is the proportion of FFQ- that are also DR-. So \\[\n16 / 34 = 0.4706\n\\]\n\n\n\n\n3.44Solution\n\n\nSuppose the questionnaire is to be administered in a different country where the true proportion of drinkers is 80%. What would be the predictive value positive if administered in this setting?\n\n\nWe are told that \\(P(D^+) = 0.8\\). We will also assume, from the questionnaire, that the sensitivity and specificty are known: \\(P(T^+|D^+) = 138 / 156 = 0.8846\\) and \\(P(T^-|D^-) = 16 / 17 = 0.9412\\). We want \\(P(D^+|T^+)\\), which we can get using Bayes rule \\[\\begin{align*}\nP(D^+|T^+) &= \\frac{P(T^+|D^+)P(D^+)}{P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-)} \\\\\n&= \\frac{0.8846 \\times 0.8}{0.8846 \\times 0.8 + (1 - 0.9412) \\times (1 - 0.8)}\\\\\n&= 0.9837\n\\end{align*}\\]"
  },
  {
    "objectID": "review/pp_03.html#genetics",
    "href": "review/pp_03.html#genetics",
    "title": "Chapter 3 Practice Problems",
    "section": "Genetics",
    "text": "Genetics\nTwo healthy parents have a child with a severe autosomal recessive condition that cannot be identified by prenatal diagnosis. They realize that the risk of this condition for subsequent offspring is 1/4, but wish to embark on a second pregnancy. During the early stages of the pregnancy, an ultrasound test determines that there are twins.\n\n3.45Solution\n\n\nSuppose that there are monozygotic, or MZ (identical) twins. What is the probability that both twins are affected? one twin affected? neither twin affected? Are the outcomes for the two MZ twins independent or dependent events?\n\n\nLet \\(A\\) be the first is affected. Let \\(B\\) be the second is affected. Note that for monozygotic twins, we have \\(P(A|B) = 1\\) and \\(P(\\bar{A}|\\bar{B}) = 1\\), since if one is affected then the other must also be affected. Using this, we have\n\\[\nP(\\text{both}) = P(A \\cap B) = P(A|B)P(B) = 1 \\times 0.25 = 0.25\n\\] \\[\nP(\\text{one}) = P(\\bar{A}|B)P(B) + P(A|\\bar{B})P(\\bar{B}) = 0 \\times 0.25 + 0 \\times (1 - 0.25) = 0\n\\] \\[\nP(\\text{neither}) = P(\\bar{A} \\cap \\bar{B}) = P(\\bar{A}|\\bar{B})P(\\bar{B}) = 1 \\times (1 - 0.25) = 0.75\n\\]\nThese are not independent events because \\(P(A|B) = 1\\) but \\(P(A|\\bar{B}) = 0\\).\n\n\n\n\n3.46Solution\n\n\nSuppose that there are dizygotic, or DZ (fraternal) twins. What is the probability that both twins are affected? one twin affected? neither twin affected? Are the outcomes for the two DZ twins independent or dependent events?\n\n\nBy the assumption of Mendelian segregation, these are independent events. Thus, we have \\[\nP(\\text{both}) = P(A\\cap B) = P(A)P(B) = 0.25^2 = 0.0625\n\\] \\[\nP(\\text{one}) = P(A\\cap \\bar{B}) + P(\\bar{A} \\cap B) = P(A)P(\\bar{B}) + P(\\bar{A})P(B) = 2 \\times 0.25 \\times (1 - 0.25) = 0.375\n\\] \\[\nP(\\text{neither}) = P(\\bar{A} \\cap \\bar{B}) = P(\\bar{A})P(\\bar{B}) = (1 - 0.25)^2 = 0.5625\n\\]\n\n\n\n\n3.47Solution\n\n\nSuppose there is a 1/3 probability of MZ twins and a 2/3 probability of DZ twins. What is the overall probability that both twins are affected? One twin affected? Neither affected?\n\n\n\\[\nP(\\text{both}) = P(\\text{both}|MZ)P(MZ) + P(\\text{both}|DZ)P(DZ) = 0.25 \\times 1/3 + 0.0625 \\times 2/3 = 0.125\n\\] \\[\nP(\\text{one}) = P(\\text{one}|MZ)P(MZ) + P(\\text{one}|DZ)P(DZ) = 0 \\times 1/3 + 0.375 \\times 2/3 = 0.25\n\\] \\[\nP(\\text{neither}) = P(\\text{neither}|MZ)P(MZ) + P(\\text{neither}|DZ)P(DZ) = 0.75 \\times 1/3 + 0.5625 \\times 2/3 = 0.625\n\\]\n\n\n\n\n3.48Solution\n\n\nSuppose we learn that both twins are affected but don’t know whether they are MZ or DZ twins. What is the probability that they are MZ twins given this additional information?\n\n\nWe use Bayes rule.\n\\[\nP(MZ|\\text{both}) = \\frac{P(\\text{both}|MZ)P(MZ)}{P(\\text{both})} = \\frac{0.25 \\times 1/3}{0.125} = 2/3\n\\]"
  },
  {
    "objectID": "review/pp_03.html#cerebrovascular-disease",
    "href": "review/pp_03.html#cerebrovascular-disease",
    "title": "Chapter 3 Practice Problems",
    "section": "Cerebrovascular Disease",
    "text": "Cerebrovascular Disease\nAtrial fibrillation (AF) is a common cardiac condition in the elderly (e.g., former President George H.W. Bush has this condition) characterized by an abnormal heart rhythm that greatly increases the risk of stroke. The following estimates of the prevalence rate of AF and the incidence rate of stroke for people with and without AF by age from the Framingham Heart Study are given in Table 3.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.6: Relationship between atrial fibrillation and stroke\n\n\nAge group\nPrevalence of AF (%)\n\nIncidence rate of stroke per 1000 person-years\n\n\n\nNo AF\nAF\n\n\n\n\n60–69\n1.8\n4.5\n21.2\n\n\n70–79\n4.7\n9.0\n48.9\n\n\n80–89\n10.2\n14.3\n71.4\n\n\n\n\n\n\n\n\n3.49Solution\n\n\nWhat does an incidence rate of 48.9 strokes per 1000 person-years among 70–79-year-olds with AF mean in Table 3.6?\n\n\n\n\n\n\n\n3.50Solution\n\n\nWhat is the relative risk of stroke for people with AF compared with people without AF in each age group? Does the relative risk seem to be the same for different age groups\n\n\n\n\n\n\nSuppose we screen 500 subjects from the general population of age 60–89, of whom 200 are 60–69, 200 are 70–79, and 100 are 80–89.\n\n3.51Solution\n\n\nWhat is the incidence rate of stroke in the screened population over a 1-year period?\n\n\n\n\n\n\n\n3.52Solution\n\n\nSuppose the study of 500 subjects is a “pilot study” for a larger study. How many 60-89 year old subjects need to be screened if we wish to observe an average of 50 strokes in the larger study over 1 year?"
  },
  {
    "objectID": "review/pp_03.html#radiology",
    "href": "review/pp_03.html#radiology",
    "title": "Chapter 3 Practice Problems",
    "section": "Radiology",
    "text": "Radiology\nIt is well-known that there is variation among readers in evaluating radiologic images. For this purpose, we present evaluations from a 2nd reader of the same 100 images for the study described in Section 3.6.1 of the Study Guide. The results are given in Table 3.7.\n\n\n\n\n\n\n\n\nTable 3.7\n\n\nRatings from Reader 2 using PACS film for the study described in Section 3.6.1\n\n\nTrue Status\n1\n2\n3\n4\n5\nTotal\n\n\n\n\nNegative\n0\n3\n7\n14\n9\n33\n\n\nPositive\n0\n49\n11\n4\n3\n67\n\n\nTotal\n0\n52\n18\n18\n12\n100\n\n\n\n\n\n\n\nSuppose different cutpoints are considered for positivity, viz, ≤ 0 , ≤ 1, ≤ 2, ≤ 3, ≤ 4 and ≤ 5.\n\n3.53Solution\n\n\nCompute the sensitivity of the test for each cutpoint for Reader 2.\n\n\n\n\n\n\n\n3.54Solution\n\n\nCompute the specificity of the test for each cutpoint for Reader 2.\n\n\n\n\n\n\n\n3.55Solution\n\n\nDraw the ROC curve for Reader 2.\n\n\n\n\n\n\n\n3.56Solution\n\n\nCompare the accuracy of the test for Reader 2 vs Reader 1."
  },
  {
    "objectID": "review/pp_03.html#cardiovascular-disease-3",
    "href": "review/pp_03.html#cardiovascular-disease-3",
    "title": "Chapter 3 Practice Problems",
    "section": "Cardiovascular Disease 3",
    "text": "Cardiovascular Disease 3\nExercise testing has sometimes been used to diagnose patients with coronary-artery disease. One test criterion that has been used to identify those with disease is the abnormal ejection-fraction criterion; that is, an absolute rise of less than 0.05 with exercise. The validity of this noninvasive test was assessed in 196 patients versus coronary angiography, the gold standard, a procedure that can unequivocally diagnose the disease but the administration of which carries some risk for the patient. A sensitivity of 79% and a specificity of 68% were found for the exercise test in this group.\n\n3.36Solution\n\n\nWhat does the sensitivity mean in words in this setting?\n\n\nThe probability of abnormal ejection-fraction given coronary-artery disease (as determined by coronary angiography).\n\n\n\n\n3.37Solution\n\n\nWhat does the specificity mean in words in this setting?\n\n\nThe probability of a normal ejection-fraction given no coronoary-artery disease (as determined by coronary angiography).\n\n\n\nSuppose a new patient undergoes exercise testing and a physician feels before seeing the exercise-test results that the patient has a 20% chance of having coronary-artery disease.\n\n3.38Solution\n\n\nIf the exercise test is positive, then what is the probability that such a patient has disease?\n\n\nFrom the sensitivity and specificity, we have\n\n\\(P(T^+|D^+) = 0.79\\)\n\\(P(T^-|D^-) = 0.68\\)\n\nThe doctor told us that \\(P(D^+) = 0.2\\).\nWe want \\(P(D^+|T^+)\\). We can do this by Bayes rule: \\[\nP(D^+|T^+) = \\frac{P(T^+|D^+)P(D^+)}{P(T^+)}\n\\] The numerator terms are given. We can calculate the denominator by the law of total probability \\[\nP(T^+) = P(T^+|D^+)P(D^+) + P(T^+|D^-)P(D^-) = 0.79 \\times 0.2 + (1 - 0.68) \\times (1 - 0.2) = 0.414\n\\]\nSo, \\[\nP(D^+|T^+) = \\frac{0.79 \\times 0.2}{0.414} = 0.3816\n\\]\n\n\n\n\n3.39Solution\n\n\nAnswer Problem 3.38 if the exercise test is negative.\n\n\nWe want \\(P(D^+|T^-)\\), which we can do by Bayes rule \\[\nP(D^+|T^-) = \\frac{P(T^-|D^+)P(D^+)}{P(T^-)}\n=\\frac{(1 - 0.79)\\times 0.2}{1 - 0.414}\n= 0.07167\n\\]"
  },
  {
    "objectID": "review/pp_04.html",
    "href": "review/pp_04.html",
    "title": "Chapter 4 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_04.html#health-services-administration",
    "href": "review/pp_04.html#health-services-administration",
    "title": "Chapter 4 Practice Problems",
    "section": "Health-Services Administration",
    "text": "Health-Services Administration\nThe in-hospital mortality rate for 16 clinical conditions at 981 hospitals was recently reported. It was reported that in-hospital mortality was 10.5% for coronary-bypass surgery and 5.0% for total hip replacement. Suppose an institution changes from an academic institution to a private for-profit institution. They find that after the change, of the first 20 patients receiving coronary-bypass surgery, 5 die, while of 20 patients receiving total hip replacement, 4 die.\n\n4.8Solution\n\n\nWhat is the probability that of 20 patients receiving coronary-bypass surgery, exactly 5 will die in-hospital, if this hospital is representative of the total pool of 981 hospitals?\n\n\nLet \\(X\\) be the number among the coronary-bypass patients that die out of 20. Then \\(X \\sim \\mathrm{Binom}(20, 0.105)\\). We want \\(P(X = 5)\\)\n\ndbinom(x = 5, size = 20, prob = 0.105)\n\n[1] 0.03747\n\n\n\n\n\n\n4.9Solution\n\n\nWhat is the probability of at least 5 deaths among the coronary-bypass patients?\n\n\nWe want \\(P(X \\geq 5) = 1 - P(X \\leq 4)\\)\n\n1 - pbinom(q = 4, size = 20, prob = 0.105)\n\n[1] 0.05162\n\n\n\n\n\n\n4.10Solution\n\n\nWhat is the probability of no more than 5 deaths among the coronary-bypass patients?\n\n\nWe want \\(P(X \\leq 5)\\)\n\npbinom(q = 5, size = 20, prob = 0.105)\n\n[1] 0.9859\n\n\n\n\n\n\n4.11Solution\n\n\nWhat is the probability that exactly 4 will die among the hip-replacement patients?\n\n\nLet \\(Y\\) be the number that die among the hip-replacement patients out of 20. Then \\(Y \\sim \\mathrm{Binom}(20, 0.05)\\). We want \\(P(X = 4)\\)\n\ndbinom(x = 4, size = 20, prob = 0.05)\n\n[1] 0.01333\n\n\n\n\n\n\n4.12Solution\n\n\nWhat is the probability that at least 4 will die among the hip-replacement patients?\n\n\nWe want \\(P(X \\geq 4) = 1 - P(X \\leq 3)\\)\n\n1 - pbinom(q = 3, size = 20, prob = 0.05)\n\n[1] 0.0159\n\n\n\n\n\n\n4.13Solution\n\n\nWhat is the probability of 4 or fewer deaths among the hip-replacement patients?\n\n\nWe want \\(P(X \\leq 4)\\)\n\npbinom(q = 4, size = 20, prob = 0.05)\n\n[1] 0.9974\n\n\n\n\n\n\n4.14Solution\n\n\nCan you draw any conclusions based on the results in Problems 4.8–4.13 regarding any effects of the change in hospital administration on in-hospital mortality rates?\n\n\nIt would be very unlikely that this many patients would die if this hospital was representative of the 981 hospitals. Assuming that it was representative, we would only have a 5.2% chance of seeing as many or more deaths among the coronary-artery patients, and we would only have a 1.6% chance of seeing as many or more hip-deaths."
  },
  {
    "objectID": "review/pp_04.html#cardiovascular-disease",
    "href": "review/pp_04.html#cardiovascular-disease",
    "title": "Chapter 4 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nThe rate of myocardial infarction (MI) in 50–59-year-old, disease-free women is approximately 2 per 1000 per year or 10 per 1000 over 5 years. Suppose that 3 MI’s are reported over 5 years among 1000 postmenopausal women initially disease free who have been taking postmenopausal hormones.\n4.15 Use the binomial distribution to see if this experience represents an unusually small number of events based on the overall rate.\n4.16 Answer Problem 4.15 using the Poisson approximation to the binomial distribution.\n4.17 Compare your answers in Problems 4.15 and 4.16."
  },
  {
    "objectID": "review/pp_04.html#cardiovascular-disease-1",
    "href": "review/pp_04.html#cardiovascular-disease-1",
    "title": "Chapter 4 Practice Problems",
    "section": "Cardiovascular Disease 1",
    "text": "Cardiovascular Disease 1\nThe rate of myocardial infarction (MI) in 50–59-year-old, disease-free women is approximately 2 per 1000 per year or 10 per 1000 over 5 years. Suppose that 3 MI’s are reported over 5 years among 1000 postmenopausal women initially disease free who have been taking postmenopausal hormones.\n\n4.15Solution\n\n\nUse the binomial distribution to see if this experience represents an unusually small number of events based on the overall rate.\n\n\nLet \\(X\\) be the number of MI’s out of 1000 over the 5 years. The probability of MI over that time is 10/1000 = 0.01. Then we have that \\(X \\sim \\mathrm{Binom}(1000, 0.01)\\). We want \\(P(X \\leq 3)\\), because this probability tells us if \\(X\\) is unusually small:\n\npbinom(q = 3, size = 1000, prob = 0.01)\n\n[1] 0.01007\n\n\nSo it is unusually small. We would only expect to see this 1% of the time.\n\n\n\n\n4.16Solution\n\n\nAnswer Problem 4.15 using the Poisson approximation to the binomial distribution.\n\n\n\\(X \\approx \\mathrm{Pois}(1000 \\times 0.01) = \\mathrm{Pois}(10)\\). Calculation \\(P(X\\leq 3)\\) we get:\n\nppois(q = 3, lambda = 10)\n\n[1] 0.01034\n\n\n\n\n\n\n4.17Solution\n\n\nCompare your answers in Problems 4.15 and 4.16.\n\n\nThey are close."
  },
  {
    "objectID": "review/pp_04.html#pediatrics",
    "href": "review/pp_04.html#pediatrics",
    "title": "Chapter 4 Practice Problems",
    "section": "Pediatrics",
    "text": "Pediatrics\nA hospital administrator wants to construct a special-care nursery for low-birthweight infants (≤ 2500 g) and wants to have some idea as to the number of beds she should allocate to the nursery. She is willing to assume that the recovery period of each baby is exactly 4 days and thus is interested in the expected number of premature births over the period.\n\n4.19Solution\n\n\nIf the number of premature births in any 4-day period is binomially distributed with parameters n = 25 and p = .1, then find the probability of 0, 1, 2, …, 7 premature births over this period.\n\n\nLet \\(X\\) be the number of premature births out of 25. Then \\(X \\sim \\mathrm{Binom}(25, 0.1)\\). We want \\(P(X = 0), P(X = 1),\\ldots,P(X = 7)\\). We can do this in R via:\n\npvec &lt;- dbinom(x = 0:7, size = 25, prob = 0.1)\npvec\n\n[1] 0.071790 0.199416 0.265888 0.226497 0.138415 0.064594 0.023924 0.007215\n\n\n\n\n\n\n4.20Solution\n\n\nThe administrator wishes to allocate \\(x\\) beds where the probability of having more than \\(x\\) premature births over a 4-day period is less than 5%. What is the smallest value of \\(x\\) that satisfies this criterion?\n\n\nThis is the same thing as finding the smallest value \\(x\\) such that \\(P(X \\leq x) &gt; 0.95\\). This is exactly the 0.95 quantile from the binomial distribution.\n\nqbinom(p = 0.95, size = 25, prob = 0.1)\n\n[1] 5\n\n\nSo, 5 beds.\nWe can confirm by seeing if \\(P(X \\geq 5)\\) is less than 0.05 and \\(P(X \\geq 4)\\) is greater than 0.05.\n\n1 - pbinom(q = 5, size = 25, prob = 0.1)\n\n[1] 0.0334\n\n1 - pbinom(q = 4, size = 25, prob = 0.1)\n\n[1] 0.09799\n\n\n\n\n\n\n4.21Solution\n\n\nAnswer Problem 4.20 for 1%.\n\n\nUsing the same logic, we want the 0.99 quantile of a \\(\\mathrm{Binom}(25, 0.1)\\) distribution\n\nqbinom(p = 0.99, size = 25, prob = 0.1)\n\n[1] 6\n\n\nSo, six beds. Verifying, we get\n\n1 - pbinom(q = 6, size = 25, prob = 0.1)\n\n[1] 0.009476\n\n1 - pbinom(q = 5, size = 25, prob = 0.1)\n\n[1] 0.0334"
  },
  {
    "objectID": "review/pp_04.html#cancer",
    "href": "review/pp_04.html#cancer",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\nThe incidence rate of malignant melanoma is suspected to be increasing over time. To document this increase, a questionnaire was mailed to 100,000 U.S. nurses in 1976 and 1978, asking about any current or previous tumors. Thirty new cases of malignant melanoma were found to have developed over the 2-year period among women with no previous cancers in 1976.\n4.22 If the annual incidence rate from previous cancer-registry data is 10 per 100,000, then what is the expected number of new cases over 2 years?\n4.23 Are the preceding results consistent or inconsistent with the cancer-registry data? Specifically, what is the prob- ability of observing at least 30 new cases over a 2-year period if the cancer-registry incidence rate is correct?"
  },
  {
    "objectID": "review/pp_04.html#accident-epidemiology",
    "href": "review/pp_04.html#accident-epidemiology",
    "title": "Chapter 4 Practice Problems",
    "section": "Accident Epidemiology",
    "text": "Accident Epidemiology\nSuppose the annual number of traffic fatalities at a given intersection follows a Poisson distribution with parameter µ = 10 .\n\n4.24Solution\n\n\nWhat is the probability of observing exactly 10 traffic fatalities in 1992?\n\n\n\ndpois(x = 10, lambda = 10)\n\n[1] 0.1251\n\n\n\n\n\n\n4.25Solution\n\n\nWhat is the probability of observing exactly 25 traffic fatalities over the 2-year period from January 1, 1990, to December 31, 1991?\n\n\nLet \\(X\\) be the number of fatalities over 2 years. Then \\(X \\sim \\mathrm{Pois}(20)\\). We want \\(P(X = 25)\\)\n\ndpois(x = 25, lambda = 20)\n\n[1] 0.04459\n\n\n\n\n\n\n4.26Solution\n\n\nSuppose that the traffic intersection is redesigned with better lighting, and 12 traffic fatalities are observed over the next 2 years. Is this rate a meaningful improvement over the previous rate of traffic fatalities?\n\n\nLet’s calculate \\(P(X \\leq 12)\\) when \\(X \\sim \\mathrm{Pois}(20)\\)\n\nppois(q = 12, lambda = 20)\n\n[1] 0.03901\n\n\nYes, it would be very unlikely to see so few deaths (0.03901) if nothing changed."
  },
  {
    "objectID": "review/pp_04.html#pulmonary-disease-environmental-health",
    "href": "review/pp_04.html#pulmonary-disease-environmental-health",
    "title": "Chapter 4 Practice Problems",
    "section": "Pulmonary Disease, Environmental Health",
    "text": "Pulmonary Disease, Environmental Health\nSuppose the number of people seen for violent asthma attacks in the emergency ward of a hospital over a 1-day period is usually Poisson distributed with parameter λ = 1.5\n\n4.27Solution\n\n\nWhat is the probability of observing 5 or more cases over a 2-day period?\n\n\nLet \\(X\\) be the number observed over 2 days. Then \\(X \\sim \\mathrm{Pois}(3)\\). We want \\(P(X \\geq 5)\\)\n\n1 - ppois(q = 4, lambda = 3)\n\n[1] 0.1847\n\n\n\n\n\nOn a particular 2-day period, the air-pollution levels increase dramatically and the distribution of attacks over a 1-day period is now estimated to be Poisson distributed with parameter λ = 3.\n\n4.28Solution\n\n\nAnswer Problem 4.27 under these assumptions.\n\n\nLet \\(Y\\) be the number observed over the 2 day period. Then \\(Y \\sim \\mathrm{Pois}(6)\\). We want \\(P(Y \\geq 5)\\)\n\n1 - ppois(q = 4, lambda = 6)\n\n[1] 0.7149\n\n\n\n\n\n\n4.29Solution\n\n\nIf 10 days out of every year are high-pollution days, then what is the expected number of asthma cases seen in the emergency ward over a 1-year period? (Assume there are 365 days in a year.)\n\n\n10 days are \\(\\mathrm{Pois}(3)\\) and 355 days are \\(\\mathrm{Pois}(1.5)\\). So if \\(Z\\)is number in a year, we have \\(Z \\sim \\mathrm{Pois}(10 \\times 3 + 355 \\times 1.5) = \\mathrm{Pois}(562.5)\\). The expected number is 562.5."
  },
  {
    "objectID": "review/pp_04.html#pulmonary-disease",
    "href": "review/pp_04.html#pulmonary-disease",
    "title": "Chapter 4 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\nEach year approximately 4% of current smokers attempt to quit smoking, and 50% of those who try to quit are successful in the sense that they abstain from smoking for at least 1 year from the date they quit.\n\n4.35Solution\n\n\nWhat is the probability that a current smoker will quit for at least 1 year?\n\n\n\\(P(\\mathrm{Quit}) = P(\\mathrm{Quit}|\\mathrm{Try})P(\\mathrm{Try}) = 0.5 \\times 0.04 = 0.02\\).\n\n\n\n\n4.36Solution\n\n\nWhat is the probability that among 100 current smokers, at least 5 will quit smoking for at least 1 year?\n\n\nLet \\(X\\) be the number that quit. Then \\(X \\sim \\mathrm{Binom}(100, 0.02)\\). We want \\(P(X \\geq 5)\\)\n\n1 - pbinom(q = 4, size = 100, prob = 0.02)\n\n[1] 0.05083\n\n\n\n\n\n\n4.37Solution\n\n\nAn educational program was conducted among smokers who attempt to quit to maximize the likelihood that such individuals would continue to abstain for the long term. Suppose that of 20 people who enter the program when they first stop smoking, 15 still abstain from smoking 1 year later. Can the program be considered successful?\n\n\nAssuming that it wasn’t, \\(X \\sim \\mathrm{Binom}(20, 0.5)\\). We use 0.5 instead of 0.02 because these are folks who have decided they want to quit. We want \\(P(X \\geq 15)\\)\n\n1 - pbinom(q = 14, size = 20, prob = 0.5)\n\n[1] 0.02069\n\n\nSo we are very sure that the program is successful, since if it weren’t we would only have a 2% probability of seeing as many or more successful quitters."
  },
  {
    "objectID": "review/pp_04.html#infectious-disease",
    "href": "review/pp_04.html#infectious-disease",
    "title": "Chapter 4 Practice Problems",
    "section": "Infectious Disease",
    "text": "Infectious Disease\nAn outbreak of acute gastroenteritis occurred at a nursing home in Baltimore, Maryland, in December 1980. A total of 46 out of 98 residents of the nursing home became ill. People living in the nursing home shared rooms: 13 rooms contained 2 occupants, 4 rooms contained 3 occupants, and 15 rooms contained 4 occupants. One question that arises is whether or not a geographical clustering of disease occurred for persons living in the same room.\nA summary of the number of affected people and the total number of people in a room is given in Table 4.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.3\n\n\nNumber of affected people and total number of people in a room for an outbreak of acute gastroenteritis in a nursing home in Baltimore, Maryland\n\n\nPeople in room\nTotal number of rooms\n\nNumber of rooms with\n\n\n\n0 affected people\n1 affected person\n2 affected people\n3 affected people\n4 affected people\n\n\n\n\n2\n13\n5\n4\n4\n0\n0\n\n\n3\n4\n1\n2\n0\n1\n0\n\n\n4\n15\n2\n4\n3\n5\n1\n\n\n\n\n\n\n\n\n4.47Solution\n\n\nIf the binomial distribution holds, what is the probability distribution of the number of affected people in rooms with 2 occupants? That is, what is the probability of finding zero affected people? One affected person? Two affected people?\n\n\nLet’s assume the probability of disease is the estimated probabiliy of disease 46/98 = 0.4694. Let \\(X\\) be the number of affected out of 2. Then \\(X \\sim \\mathrm{Binom}(2, 0.4694)\\). These probabilities are\n\ndbinom(x = 0:2, size = 2, prob = 0.4694)\n\n[1] 0.2815 0.4981 0.2203\n\n\n\n\n\n\n4.48Solution\n\n\nAnswer Problem 4.47 for the probability distribution of the number of affected people in rooms with 3 occupants.\n\n\nLet \\(Y\\) be the number affected out of 3. Then \\(Y \\sim \\mathrm{Binom}(3, 0.4694)\\). The probabilities are\n\ndbinom(x = 0:3, size = 3, prob = 0.4694)\n\n[1] 0.1494 0.3965 0.3507 0.1034\n\n\n\n\n\n\n4.49Solution\n\n\nAnswer Problem 4.47 for the probability distribution of the number of affected people in rooms with 4 occupants.\n\n\nLet \\(Z\\) be the number affected out of 4. Then \\(Z \\sim \\mathrm{Binom}(4, 0.4694)\\). The probabilities are\n\ndbinom(x = 0:4, size = 4, prob = 0.4694)\n\n[1] 0.07926 0.28048 0.37220 0.21951 0.04855\n\n\n\n\n\n\n4.50Solution\n\n\nOne useful summary measure of geographical clustering is the number of rooms with 2 or more affected occupants. If the binomial distribution holds, what is the expected number of rooms with 2 or more affected occupants over the entire nursing home?\n\n\nThere are 13 2-rooms, 4 3-rooms, and 15 4-rooms. The expected number of rooms with 2 or more affected folks is \\[\n13 P(X \\geq 2) + 4 P(Y \\geq 2) + 15 P(Z \\geq 2)\n\\] where the random variables were defined in 4.47, 4.48, and 4.49. We calculate these probabilities as\n\n13 * (1 - pbinom(q = 1, size = 2, prob = 0.4694)) +\n  4 * (1 - pbinom(q = 1, size = 3, prob = 0.4694)) +\n  15 * (1 - pbinom(q = 1, size = 4, prob = 0.4694))\n\n[1] 14.28"
  },
  {
    "objectID": "review/pp_04.html#cancer-1",
    "href": "review/pp_04.html#cancer-1",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer 1",
    "text": "Cancer 1\nThe incidence rate of malignant melanoma is suspected to be increasing over time. To document this increase, a questionnaire was mailed to 100,000 U.S. nurses in 1976 and 1978, asking about any current or previous tumors. Thirty new cases of malignant melanoma were found to have developed over the 2-year period among women with no previous cancers in 1976.\n\n4.22Solution\n\n\nIf the annual incidence rate from previous cancer-registry data is 10 per 100,000, then what is the expected number of new cases over 2 years?\n\n\n\\[\n100000 \\times \\frac{10}{100000} \\times 2 = 20\n\\]\n\n\n\n\n4.23Solution\n\n\nAre the preceding results consistent or inconsistent with the cancer-registry data? Specifically, what is the probability of observing at least 30 new cases over a 2-year period if the cancer-registry incidence rate is correct?\n\n\nThe exact distribution is a \\(\\mathrm{Binom}(100000, 20/100000)\\). We want \\(P(X \\geq 30)\\)\n\n1 - pbinom(q = 29, size = 100000, prob = 20/100000)\n\n[1] 0.02181\n\n\nYou can also use a \\(\\mathrm{Pois}(20)\\)\n\n1 - ppois(q = 29, lambda = 20)\n\n[1] 0.02182"
  },
  {
    "objectID": "review/pp_04.html#cancer-2",
    "href": "review/pp_04.html#cancer-2",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer 2",
    "text": "Cancer 2\nThe incidence rate of malignant melanoma in women ages 35–59 is approximately 11 new cases per 100,000 women per year. A study is planned to follow 10,000 women with excessive exposure to sunlight.\n\n4.52Solution\n\n\nWhat is the expected number of cases among 10,000 women over 4 years? (Assume no excess risk due to sunlight exposure.)\n\n\n\\[\n10000 \\text{ women} \\times 4 \\text{ years} \\times \\frac{11 \\text{ cases}}{100000 \\text{ women year}} = 4.4 \\text{ cases}\n\\]\n\n\n\n\n4.53Solution\n\n\nSuppose that 9 new cases are observed in this period. How unusual a finding is this?\n\n\nLet \\(X\\) be the number of new cases over 4 years. The exact distribution is Binomial with size 10000 and probability 4 \\(\\times\\) 11 / 100000 = 0.00044. We want \\(P(X \\geq 9)\\)\n\n1 - pbinom(q = 8, size = 10000, prob = 0.00044)\n\n[1] 0.03577\n\n\nWe can approximate this with a \\(\\mathrm{Pois}(4.4)\\)\n\n1 - ppois(q = 8, lambda = 4.4)\n\n[1] 0.0358\n\n\nSo, somewhat unusual."
  },
  {
    "objectID": "review/pp_04.html#cardiovascular-disease-2",
    "href": "review/pp_04.html#cardiovascular-disease-2",
    "title": "Chapter 4 Practice Problems",
    "section": "Cardiovascular Disease 2",
    "text": "Cardiovascular Disease 2\n\n4.18Solution\n\n\nAn accepted hypothesis in the etiology of heart disease is that aspirin intake of 325 mg per day reduces subsequent cardiovascular mortality in men with a prior heart attack. Suppose that in a pilot study of 50 women who received 1 tablet per day (325 mg), only 2 die over a 3-year period from cardiovascular disease. How likely is it that not more than 2 women will die if the underlying 3-year mortality rate is 10% in such women?\n\n\nLet \\(X\\) be the number of women that die out of 50. Then \\(X \\sim \\mathrm{Binom}(50, 0.1)\\). We want to calculate \\(P(X \\leq 2)\\):\n\npbinom(q = 2, size = 50, prob = 0.1)\n\n[1] 0.1117"
  },
  {
    "objectID": "review/pp_04.html#cancer-3",
    "href": "review/pp_04.html#cancer-3",
    "title": "Chapter 4 Practice Problems",
    "section": "Cancer 3",
    "text": "Cancer 3\nA study was performed in Woburn, MA, looking at the rate of leukemia among children (≤ age 19) in the community in comparison to statewide leukemia rates. Suppose there are 12,000 children in the community that have lived there for a 10-year period and 12 leukemias have occurred in 10 years.\n\n4.54Solution\n\n\nIf the statewide incidence rate of leukemia in children is 5 events per 100,000 children per year (i.e., per 100,000 person-years) then how many leukemias would be expected in Woburn over the 10-year period if the statewide rates were applicable?\n\n\n\\[\n12000 \\text{ children} \\times 10 \\text{ years} \\times \\frac{5 \\text{ cases}}{100000 \\text{ children year}} = 6\n\\]\n\n\n\n\n4.55Solution\n\n\nWhat is the probability of obtaining exactly 12 events over a 10-year period if statewide incidence rates were applicable?\n\n\nLet \\(X\\) be the number of cases of 10 years. Then \\(X\\) exactly follows a binomial with size 12000 and probability 10 \\(\\times\\) 5 / 100000 = 0.0005. We want \\(P(X = 12)\\)\n\ndbinom(x = 12, size = 12000, prob = 0.0005)\n\n[1] 0.01125\n\n\nWe can approximate this calculate with a \\(\\mathrm{Pois}(6)\\)\n\ndpois(x = 12, lambda = 6)\n\n[1] 0.01126\n\n\n\n\n\n\n4.56Solution\n\n\nWhat is the probability of obtaining at least 12 events over the 10-year period if the statewide incidence rates were applicable?\n\n\nWe want \\(P(X \\geq 12)\\). We can do this with binomial\n\n1 - pbinom(q = 11, size = 12000, prob = 0.0005)\n\n[1] 0.02006\n\n\nor the Poisson\n\n1 - ppois(q = 11, lambda = 6)\n\n[1] 0.02009\n\n\n\n\n\n\n4.57Solution\n\n\nHow do you interpret the results in Problem 4.56?\n\n\nIt’s rare to see so many. So there is likely extra risk in Woburn."
  },
  {
    "objectID": "review/pp_04.html#basic-discrete-probability",
    "href": "review/pp_04.html#basic-discrete-probability",
    "title": "Chapter 4 Practice Problems",
    "section": "Basic Discrete Probability",
    "text": "Basic Discrete Probability\n\n4.1Solution\n\n\nEvaluate the number of ways of selecting 4 objects out of 10 if the order of selection matters. What term is used to denote this quantity?\n\n\nThe book’s notation is \\(_{10}P_4\\). This is the number of permutations of 10 things taken 4 times. This is \\(10 \\times 9 \\times 8 \\times 7 = 5040\\).\n\n\n\n\n4.2Solution\n\n\nEvaluate the number of ways of selecting 4 objects out of 10 if the order of selection does not matter. What term is used to denote this quantity?\n\n\nThis is the number of combination sof 10 things taken 4 times. The book notation is \\(_{10}C_4\\), but everyone else uses \\(\\binom{10}{4}\\). \\[\n\\binom{10}{4} = \\frac{10 \\times 9 \\times 8 \\times 7}{4 \\times 3 \\times 2 \\times 1} = 210\n\\] Checking in R, we get:\n\nchoose(10, 4)\n\n[1] 210\n\n\n\n\n\n\n4.3Solution\n\n\nSuppose that the probability that a person will develop hypertension over a lifetime is 20%. What is the probability distribution of the number of hypertensives over a lifetime among 20 students graduating from the same high school class?\n\n\nBinomial(20, 0.2)\n\n\n\n\n4.4Solution\n\n\nWhat is the probability that exactly 4 people out of 50 aged 60–64 will die after receiving the flu vaccine if the probability that 1 person will die is .028?\n\n\nLet \\(X\\) be the number that die out of 50. Then \\(X \\sim \\mathrm{Binom}(50, 0.028)\\). We calculate \\(P(X = 4)\\) via\n\ndbinom(x = 4, size = 50, prob = 0.028)\n\n[1] 0.03833\n\n\n\n\n\n\n4.5Solution\n\n\nWhat is the probability that at least 4 people will die after receiving the vaccine?\n\n\n\n1 - pbinom(q = 3, size = 50, prob = 0.028)\n\n[1] 0.05116\n\n\n\n\n\n\n4.6Solution\n\n\nWhat is the expected number of deaths following the flu vaccine?\n\n\n\\(np\\) = 50 \\(\\times\\) 0.028 = 1.4.\n\n\n\n\n4.7Solution\n\n\nWhat is the standard deviation of the number of deaths following the flu vaccine?\n\n\n\\[\n\\sqrt{np(1-p)} = \\sqrt{50 \\times 0.028 \\times (1 - 0.028)}\n\\] Evalulating in R, we get\n\nsqrt(50 * 0.028 * (1 - 0.028))\n\n[1] 1.167"
  },
  {
    "objectID": "review/pp_05.html",
    "href": "review/pp_05.html",
    "title": "Chapter 5 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_05.html#basice",
    "href": "review/pp_05.html#basice",
    "title": "Chapter 5 Practice Problems",
    "section": "Basice",
    "text": "Basice\n\n5.1Solutions\n\n\nWhat are the deciles of the standard normal distribution; that is, the 10, 20, 30, … , 90 percentiles?\n\n\n\nqnorm(p = seq(0.1, 0.9, by = 0.1))\n\n[1] -1.2816 -0.8416 -0.5244 -0.2533  0.0000  0.2533  0.5244  0.8416  1.2816\n\n\n\n\n\n\n5.2Solutions\n\n\nWhat are the quartiles of the standard normal distribution?\n\n\n\nqnorm(p = c(0.25, 0.5, 0.75))\n\n[1] -0.6745  0.0000  0.6745"
  },
  {
    "objectID": "review/pp_05.html#hypertension",
    "href": "review/pp_05.html#hypertension",
    "title": "Chapter 5 Practice Problems",
    "section": "Hypertension",
    "text": "Hypertension\nBlood pressure (BP) in childhood tends to increase with age, but differently for boys and girls. Suppose that for both boys and girls, mean systolic blood pressure is 95 mm Hg at 3 years of age and increases 1.5 mm Hg per year up to the age of 13. Furthermore, starting at age 13, the mean increases by 2 mm Hg per year for boys and 1 mm Hg per year for girls up to the age of 18. Finally, assume that blood pressure is normally distributed and that the standard deviation is 12 mm Hg for all age-sex groups.\n5.3 What is the probability that an 11-year-old boy will have an SBP greater than 130 mm Hg?\n5.4 What is the probability that a 15-year-old girl will have an SBP between 100 and 120 mm Hg?\n5.5 What proportion of 17-year-old boys have an SBP between 120 and 140 mm Hg?\n5.6 What is the probability that of 200 15-year-old boys, at least 10 will have an SBP of 130 mm Hg or greater?\n5.7 What level of SBP is at the 80th percentile for 7-year- old boys?\n5.8 What level of SBP is at the 70th percentile for 12-year- old girls?\n5.9 Suppose that a task force of pediatricians decides that children over the 95th percentile, but not over the 99th percentile, for their age-sex group should be encouraged to take preventive nonpharmacologic measures to reduce their blood pressure, whereas those children over the 99th percentile should receive antihypertensive drug therapy. Construct a table giving the appropriate BP levels to identify these groups for boys and girls for each year of age from 3 to 18."
  },
  {
    "objectID": "review/pp_05.html#cancer",
    "href": "review/pp_05.html#cancer",
    "title": "Chapter 5 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\nThe incidence of breast cancer in 40–49-year-old women is approximately 1 new case per 1000 women per year.\n5.10 What is the incidence of breast cancer over 10 years in women initially 40 years old?\n5.11 Suppose we are planning a study based on an enrollment of 10,000 women. What is the probability of obtaining at least 120 new breast-cancer cases over a 10-year follow-up period?"
  },
  {
    "objectID": "review/pp_05.html#accident-epidemiology",
    "href": "review/pp_05.html#accident-epidemiology",
    "title": "Chapter 5 Practice Problems",
    "section": "Accident Epidemiology",
    "text": "Accident Epidemiology\nSuppose the annual death rate from motor-vehicle accidents in 1980 was 10 per 100,000 in urban areas of the United States.\n\n5.12Solutions\n\n\nIn an urban state with a population of 2 million, what is the probability of observing not more than 150 traffic fatalities in a given year?\n\n\nThe death rate is \\[\n2,000,000 \\text{ folks} \\times \\frac{10 \\text{ deaths}}{100,000 \\text{ folks year}} = 200 \\text{ deaths/year}\n\\] Let \\(X\\) be the number of deaths in a year. The exact distribution is \\(\\mathrm{Binom}(2,000,000, 10/100,000)\\) and we want \\(P(X \\leq 150)\\)\n\npbinom(q = 150, size = 2e6, prob = 1e-4)\n\n[1] 0.0001313\n\n\nThe book wants you to use the normal approximation with mean 200 and standard deviation\n\nsqrt(2e6 * 1e-4 * (1-1e-4))\n\n[1] 14.14\n\n\nUsing a continuity correction, we calculation \\(P(X \\leq 150.5)\\)\n\npnorm(q = 150.5, mean = 200, sd = 14.14)\n\n[1] 0.000232\n\n\nIt’s actually very different (relatively speaking)! You should use the exact binomial probabilities in this case.\n\n\n\nIn rural areas of the United States the annual death rate from motor-vehicle accidents is on the order of 100 per 100,000 population.\n\n5.13Solutions\n\n\nIn a rural state with a population of 100,000, what is the probability of observing not more than 80 traffic fatalities in a given year?\n\n\nLet \\(Y\\) be the number of fatalities. The exact distribution is Binomial with size 100,000 and probability 100/100,000. We want \\(P(Y \\leq 80)\\)\n\npbinom(q = 80, size = 1e5, prob = 0.001)\n\n[1] 0.0226\n\n\nUsing a normal approximation with mean \\(np = 100,000 \\times 0.001 = 100\\) and variance \\(np(1-p) = 100,000 \\times 0.001 \\times (1 - 0.001) = 99.9\\). Using a continuity correction, we calculation \\(P(Y &lt; 80.5)\\)\n\npnorm(80.5, mean = 100, sd = sqrt(99.9))\n\n[1] 0.02553\n\n\n\n\n\n\n5.14Solutions\n\n\nWhat is the largest \\(x\\) so that the probability of observing not more than \\(x\\) traffic fatalities in a given year in a rural state with population 100,000 is less than 5%?\n\n\nWe want \\(x\\) such that \\(P(X \\leq x) &lt; 0.05\\). This is one less than the 0.05 quantile of the binomial distribution with size 100,000 and probability 0.001. “One less” because the 0.05 quantile is defined as the smallest value such that \\(P(X \\leq x) \\geq 0.05\\)\n\nqbinom(p = 0.05, size = 100000, prob = 0.001)\n\n[1] 84\n\n\nSo, \\(x = 83\\). Let’s verify\n\npbinom(q = 83, size = 100000, prob = 0.001)\n\n[1] 0.04624\n\npbinom(q = 84, size = 100000, prob = 0.001)\n\n[1] 0.05746\n\n\n\n\n\n\n5.15Solutions\n\n\nWhat is the smallest \\(x\\) so that the probability of observing \\(x\\) or more traffic fatalities in a given year, in an urban state with population 500,000 is less than 10%?\n\n\nWe want the 0.9 quantile of the binomial of size 500,000 and success probability 0.0001. This is exactly the definition of the quantile, the smallest \\(x\\) such that \\(P(X \\geq x) &lt; 0.1\\) is the same as the smallest \\(x\\) such that \\(P(X \\leq x) \\geq 0.9\\), which is the 0.9 quantile.\n\nqbinom(p = 0.9, size = 500000, prob = 0.0001)\n\n[1] 59\n\n\nSo \\(x = 59\\). Let’s verify\n\npbinom(q = 59, size = 500000, prob = 0.0001)\n\n[1] 0.9077\n\npbinom(q = 58, size = 500000, prob = 0.0001)\n\n[1] 0.8836"
  },
  {
    "objectID": "review/pp_05.html#obstetrics",
    "href": "review/pp_05.html#obstetrics",
    "title": "Chapter 5 Practice Problems",
    "section": "Obstetrics",
    "text": "Obstetrics\nAssume that birthweights are normally distributed with a mean of 3400 g and a standard deviation of 700 g.\n\n5.16Solutions\n\n\nFind the probability of a low-birthweight child, where low birthweight is defined as ≤ 2500 g.\n\n\nWe want \\(P(X \\leq 2500)\\) when \\(X \\sim N(3400, 700^2)\\)\n\npnorm(q = 2500, mean = 3400, sd = 700)\n\n[1] 0.09927\n\n\n\n\n\n\n5.17Solutions\n\n\nFind the probability of a very low birthweight child, where very low birthweight is defined as ≤ 2000 g.\n\n\nSimilar to 5.16\n\npnorm(q = 2000, mean = 3400, sd = 700)\n\n[1] 0.02275\n\n\n\n\n\n\n5.18Solutions\n\n\nAssuming that successive deliveries by the same woman have the same probability of being low birthweight, what is the probability that a woman with exactly 3 deliveries will have 2 or more low-birthweight deliveries?\n\n\nLet \\(X\\) be the number of low-birthweights out of 3. Then \\(X \\sim \\mathrm{Binom}(3, 0.09927)\\), where the success probability was calculated in 5.16. We want \\(P(X \\geq 2)\\)\n\n1 - pbinom(q = 1, size = 3, prob = 0.09927)\n\n[1] 0.02761"
  },
  {
    "objectID": "review/pp_05.html#renal-disease",
    "href": "review/pp_05.html#renal-disease",
    "title": "Chapter 5 Practice Problems",
    "section": "Renal Disease",
    "text": "Renal Disease\nThe presence of bacteria in a urine sample (bacteriuria) is sometimes associated with symptoms of kidney disease. Assume that a determination of bacteriuria has been made over a large population at one point in time and that 5% of those sampled are positive for bacteriuria.\n\n5.19Solutions\n\n\nSuppose that 500 people from this population are sampled. What is the probability that 30 or more people would be positive for bacteriuria?\n\n\nLet \\(X\\) be the number out of 500 that are positive. Then \\(X \\sim \\mathrm{Binom}(500, 0.05)\\). We want \\(P(X \\geq 30)\\)\n\n1 - pbinom(q = 30, size = 500, prob = 0.05)\n\n[1] 0.1309\n\n\nWe can use the normal approximation here since \\(500 \\times 0.05 \\times (1 - 0.05) = 23.75 &gt; 5\\). Then \\(X\\) is about normal with mean \\(np = 500 \\times 0.05 = 25\\) and variance 23.75. Using a continutity correction, we calculation \\(P(X &gt; 29.5)\\)\n\n1 - pnorm(q = 29.5, mean = 25, sd = sqrt(23.75))\n\n[1] 0.1779"
  },
  {
    "objectID": "review/pp_05.html#ophthalmology",
    "href": "review/pp_05.html#ophthalmology",
    "title": "Chapter 5 Practice Problems",
    "section": "Ophthalmology",
    "text": "Ophthalmology\nA study was conducted among patients with retinitis pigmentosa, an ocular condition where pigment appears over the retina, resulting in substantial loss of vision in many cases. The study was based on 94 patients who were seen annually at a baseline visit and at three annual follow-up visits. In this study, 90 patients provided visual-field measurements at each of the four examinations and are the subjects of the following data analyses. Visual field was transformed to the ln scale to better approximate normality and yielded the data given in Table 5.1.\n\n\n\n\n\n\n\n\nTable 5.1 Visual-field measurements in retinitis-pigmentosa patients\n\n\nYear of examination\nMeana\nStandard deviation\nn\n\n\n\n\nYear 0 (baseline)\n8.15\n1.23\n90\n\n\nYear 3\n8.01\n1.33\n90\n\n\nYear 0–year 3\n0.14\n0.66\n90\n\n\n\n\n\n\n\n5.20 Assuming that change in visual field over 3 years is normally distributed when using the ln scale, what is the proportion of patients who showed a decline in visual field over 3 years?\n5.21 What percentage of patients would be expected to show a decline of at least 20% in visual field over 3 years? (Note: In the ln scale this is equivalent to a decline of at least log(1/0.8) = 0.223).\n5.22 Answer Problem 5.21 for a 50% decline over 3 years."
  },
  {
    "objectID": "review/pp_05.html#cancer-1",
    "href": "review/pp_05.html#cancer-1",
    "title": "Chapter 5 Practice Problems",
    "section": "Cancer 1",
    "text": "Cancer 1\nThe incidence of breast cancer in 40–49-year-old women is approximately 1 new case per 1000 women per year.\n\n5.10Solutions\n\n\nWhat is the incidence of breast cancer over 10 years in women initially 40 years old?\n\n\n10 new cases per 1000 women per decade, which is the same as 1 case per 100 women, or 0.01.\n\n\n\n\n5.11Solutions\n\n\nSuppose we are planning a study based on an enrollment of 10,000 women. What is the probability of obtaining at least 120 new breast-cancer cases over a 10-year follow-up period?\n\n\nLet \\(X\\) be the number of new breast-cancer cases. Then \\(X \\sim \\mathrm{Binom}(10000, 0.01)\\). We want \\(P(X \\geq 120)\\), which is\n\n1 - pbinom(q = 119, size = 10000, prob = 0.01)\n\n[1] 0.0276\n\n\nWe can use a normal approximation too since \\(10000 \\times 0.01 \\times (1 - 0.01) = 99 \\geq 5\\). So \\(X\\) is approximately normal with mean \\(np = 10000 \\times 0.01 = 100\\) and variance \\(10000 \\times 0.01 \\times (1 - 0.01) = 99\\). Using a continuity correction, we calculate \\(P(X &gt; 119.5)\\)\n\n1 - pnorm(q = 119.5, mean = 100, sd = sqrt(99))\n\n[1] 0.02501\n\n\nThese are pretty close."
  },
  {
    "objectID": "review/pp_05.html#hypertension-1",
    "href": "review/pp_05.html#hypertension-1",
    "title": "Chapter 5 Practice Problems",
    "section": "Hypertension 1",
    "text": "Hypertension 1\nBlood pressure (BP) in childhood tends to increase with age, but differently for boys and girls. Suppose that for both boys and girls, mean systolic blood pressure is 95 mm Hg at 3 years of age and increases 1.5 mm Hg per year up to the age of 13. Furthermore, starting at age 13, the mean increases by 2 mm Hg per year for boys and 1 mm Hg per year for girls up to the age of 18. Finally, assume that blood pressure is normally distributed and that the standard deviation is 12 mm Hg for all age-sex groups.\n\n5.3Solutions\n\n\nWhat is the probability that an 11-year-old boy will have an SBP greater than 130 mm Hg?\n\n\nLet \\(X\\) be the SBP of a randomly selected 11 year old boy. Then \\(X\\) is normal with mean 95 + 1.5 \\(\\times\\) 8 = 107 and standard deviation 12. We want \\(P(X &gt; 130)\\)\n\n1 - pnorm(q = 130, mean = 107, sd = 12)\n\n[1] 0.02764\n\n\n\n\n\n\n5.4Solutions\n\n\nWhat is the probability that a 15-year-old girl will have an SBP between 100 and 120 mm Hg?\n\n\nLet \\(Y\\) be the SBP of a randomly selected 15-year old girl. Then \\(Y\\) is normal with mean \\(95 + 1.5 \\times 10 + 1 \\times 2 = 112\\) and standard deviation 12. We want \\(P(100 &lt; Y &lt; 120) = P(Y &lt; 120) - P(Y &lt; 100)\\)\n\npnorm(q = 120, mean = 112, sd = 12) - pnorm(q = 100, mean = 112, sd = 12)\n\n[1] 0.5889\n\n\n\n\n\n\n5.5Solutions\n\n\nWhat proportion of 17-year-old boys have an SBP between 120 and 140 mm Hg?\n\n\nLet \\(Z\\) be the SBP of a randomly selected 17-year-old boy. Then \\(Z\\) is normal with mean \\(95 + 1.5 \\times 10 + 2 \\times 4 = 118\\) and standard deviation 12. We want \\(P(120 &lt; Z &lt; 140) = P(Z &lt; 140) - P(Z &lt; 120)\\)\n\npnorm(q = 140, mean = 118, sd = 12) - pnorm(q = 120, mean = 118, sd = 12)\n\n[1] 0.4004\n\n\n\n\n\n\n5.6HintSolutions\n\n\nWhat is the probability that of 200 15-year-old boys, at least 10 will have an SBP of 130 mm Hg or greater?\n\n\nLet’s first find the probability for a single 15-year-old boy.\n\n\nLet’s first find the probability for a single 15-year-old boy. Their SBP is normal with mean \\(95 + 1.5 \\times 10 + 2 \\times 2 = 114\\) and sd 12. So the probability of their SBP being greater than 130 is\n\n1 - pnorm(q = 130, mean = 114, sd = 12)\n\n[1] 0.09121\n\n\nLet \\(X\\) now be the number of 15-year-old boys, out of 200, that have an SBP greater than 130. Then \\(X \\sim \\mathrm{Binom}(200, 0.09121)\\). We want \\(P(X \\geq 10)\\)\n\n1 - pbinom(q = 9, size = 200, prob = 0.09121)\n\n[1] 0.9895\n\n\n\n\n\n\n5.7Solutions\n\n\nWhat level of SBP is at the 80th percentile for 7-year-old boys?\n\n\nThe SBP of a randomly sampled 7-year-old boy is normal with mean \\(95 + 1.5 \\times 4 = 101\\) and standard deviation 12. We want the 0.8 quantile of this distribution.\n\nqnorm(p = 0.8, mean = 101, sd = 12)\n\n[1] 111.1\n\n\n\n\n\n\n5.8Solutions\n\n\nWhat level of SBP is at the 70th percentile for 12-year-old girls?\n\n\nThe SBP of a randomly sampled 12-year-old girl is normal with mean \\(95 + 1.5 \\times 9 = 108.5\\) and standard deviation 12. We want the 0.7 quantile of this distribution.\n\nqnorm(p = 0.7, mean = 108.5, sd = 12)\n\n[1] 114.8\n\n\n\n\n\n\n5.9HintSolutions\n\n\nSuppose that a task force of pediatricians decides that children over the 95th percentile, but not over the 99th percentile, for their age-sex group should be encouraged to take preventive nonpharmacologic measures to reduce their blood pressure, whereas those children over the 99th percentile should receive antihypertensive drug therapy. Construct a table giving the appropriate BP levels to identify these groups for boys and girls for each year of age from 3 to 18.\n\n\nThis is R heavy. You can skip for the exam. But it is great practice for a practical application of real statistics.\n\n\nThere are fancier ways to do this with less code. But I’ll do it using just the techniques you know (except for gt(), which is a just function that makes a pretty table).\n\nlibrary(tidyverse)\ndf_boy_young &lt;- tibble(age = 3:13, sex = \"M\") |&gt;\n  mutate(mu = 95 + 1.5 * (age - 3), sd = 12)\ndf_boy_old &lt;- tibble(age = 14:18, sex = \"M\") |&gt;\n  mutate(mu = 95 + 1.5 * 10 + 2 * (age - 13), sd = 12)\ndf_girl_young &lt;- tibble(age = 3:13, sex = \"F\") |&gt;\n  mutate(mu = 95 + 1.5 * (age - 3), sd = 12)\ndf_girl_old &lt;- tibble(age = 14:18, sex = \"F\") |&gt;\n  mutate(mu = 95 + 1.5 * 10 + 1 * (age - 13), sd = 12)\ndf &lt;- bind_rows(df_boy_young, df_boy_old, df_girl_young, df_girl_old)\ndf |&gt;\n  mutate(\n    bp95 = qnorm(p = 0.95, mean = mu, sd = sd),\n    bp99 = qnorm(p = 0.99, mean = mu, sd = sd)\n    ) |&gt;\n  select(Age = age, Sex = sex, bp95, bp99) |&gt;\n  gt::gt()\n\n\n\n\n\n\n\nAge\nSex\nbp95\nbp99\n\n\n\n\n3\nM\n114.7\n122.9\n\n\n4\nM\n116.2\n124.4\n\n\n5\nM\n117.7\n125.9\n\n\n6\nM\n119.2\n127.4\n\n\n7\nM\n120.7\n128.9\n\n\n8\nM\n122.2\n130.4\n\n\n9\nM\n123.7\n131.9\n\n\n10\nM\n125.2\n133.4\n\n\n11\nM\n126.7\n134.9\n\n\n12\nM\n128.2\n136.4\n\n\n13\nM\n129.7\n137.9\n\n\n14\nM\n131.7\n139.9\n\n\n15\nM\n133.7\n141.9\n\n\n16\nM\n135.7\n143.9\n\n\n17\nM\n137.7\n145.9\n\n\n18\nM\n139.7\n147.9\n\n\n3\nF\n114.7\n122.9\n\n\n4\nF\n116.2\n124.4\n\n\n5\nF\n117.7\n125.9\n\n\n6\nF\n119.2\n127.4\n\n\n7\nF\n120.7\n128.9\n\n\n8\nF\n122.2\n130.4\n\n\n9\nF\n123.7\n131.9\n\n\n10\nF\n125.2\n133.4\n\n\n11\nF\n126.7\n134.9\n\n\n12\nF\n128.2\n136.4\n\n\n13\nF\n129.7\n137.9\n\n\n14\nF\n130.7\n138.9\n\n\n15\nF\n131.7\n139.9\n\n\n16\nF\n132.7\n140.9\n\n\n17\nF\n133.7\n141.9\n\n\n18\nF\n134.7\n142.9"
  },
  {
    "objectID": "review/pp_05.html#nutrition",
    "href": "review/pp_05.html#nutrition",
    "title": "Chapter 5 Practice Problems",
    "section": "Nutrition",
    "text": "Nutrition\nThe distribution of serum levels of alpha tocopherol (serum vitamin E) is approximately normal with mean 860 µg dL and standard deviation 340 µg dL.\n\n5.27Solutions\n\n\nWhat percentage of people have serum alpha tocopherol levels between 400 and 1000 µg dL ?\n\n\nLet \\(X\\) be normal with mean 860 and standard deviation 340. We want \\(P(400 &lt; X &lt; 1000)\\)\n\npnorm(1000, mean = 860, sd = 340) - pnorm(q = 400, mean = 860, sd = 340)\n\n[1] 0.5717\n\n\n\n\n\n\n5.28Solutions\n\n\nSuppose a person is identified as having toxic levels of alpha tocopherol if his or her serum level is &gt; 2000 µg dL. What percentage of people will be so identified?\n\n\nUsing the same notation as 5.27, we want \\(P(X &gt; 2000)\\)\n\n1 - pnorm(2000, mean = 860, sd = 340)\n\n[1] 0.0003998\n\n\n\n\n\n\n5.29Solutions\n\n\nA study is undertaken for evidence of toxicity among 2000 people who regularly take vitamin-E supplements. The investigators found that 4 people have serum alpha tocopherol levels &gt; 2000 µg dL. Is this an unusual number of people with toxic levels of serum alpha tocopherol?\n\n\nLet \\(Y\\) be the number out of 2000 with levels &gt; 2000 µg. Then \\(Y\\) is Binomial with size 2000 and probability 0.0003998 (from 5.28). We want \\(P(Y \\geq 4)\\)\n\n1 - pbinom(q = 3, size = 2000, prob = 0.0003998)\n\n[1] 0.009048\n\n\nThis is very unusual."
  },
  {
    "objectID": "review/pp_05.html#epidemiology",
    "href": "review/pp_05.html#epidemiology",
    "title": "Chapter 5 Practice Problems",
    "section": "Epidemiology",
    "text": "Epidemiology\nA major problem in performing longitudinal studies in medicine is that people initially entered into a study are lost to follow-up for various reasons.\n\n5.30Solutions\n\n\nSuppose we wish to evaluate our data after 2 years and anticipate that the probability a patient will be available for study after 2 years is 90%. How many patients should be entered into the study to be 80% sure of having at least 100 patients left at the end of this period?\n\n\n\n\n\n\n\n5.31Solutions\n\n\nHow many patients should be entered to be 90% sure of having at least 150 patients after 4 years if the probability of remaining in the study after 4 years is 80%?"
  },
  {
    "objectID": "review/pp_05.html#pulmonary-disease",
    "href": "review/pp_05.html#pulmonary-disease",
    "title": "Chapter 5 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\n\n5.32Solutions\n\n\nThe usual annual death rate from asthma in England over the period 1862–1962 for people aged 5–34 was approximately 1 per 100,000. Suppose that in 1963 twenty deaths were observed in a group of 1 million people in this age group living in Britain. Is this number of deaths inconsistent with the preceding 100-year rate? In particular, what is the probability of observing 20 or more deaths in 1 year in a group of 1 million people? Note: This finding is both statistically and medically interesting, since it was found that the excess risk could be attributed to certain aerosols used by asthmatics in Britain during the period 1963–1967. The rate returned to normal during the period 1968–1972, when these types of aerosols were no longer used.\n\n\nLet \\(X\\) be the number of deaths in a year. Then \\(X\\) is poisson with mean 1,000,000 \\(\\times\\) 1/100,000 = 10. We want \\(P(X \\geq 20)\\).\n\n1 - ppois(q = 19, lambda = 10)\n\n[1] 0.003454\n\n\nThis would be very rare.\nNote that the exact distribution is Binomial with size 1,000,000 and probability 1/100,000. But we get the same numbers:\n\n1 - pbinom(q = 19, size = 1e6, prob = 1e-5)\n\n[1] 0.003454"
  },
  {
    "objectID": "review/pp_05.html#hypertension-2",
    "href": "review/pp_05.html#hypertension-2",
    "title": "Chapter 5 Practice Problems",
    "section": "Hypertension 2",
    "text": "Hypertension 2\nBlood-pressure measurements are known to be variable, and repeated measurements are essential to accurately characterize a person’s BP status. Suppose a person is measured on \\(n\\) visits with \\(k\\) measurements per visit and the average of all \\(nk\\) diastolic blood pressure (DBP) measurements (\\(\\bar{x}\\)) is used to classify a person as to BP status. Specifically, if \\(\\bar{x}\\) ≥ 95 mm Hg, then the person is classified as hypertensive; if \\(\\bar{x}\\) &lt; 90 mm Hg, then the person is classified as normotensive; and if \\(\\bar{x}\\) ≥ 90 mm Hg, and &lt; 95 mm Hg, the person is classified as borderline. It is also assumed that a person’s “true” blood pressure is \\(\\mu\\) , representing an average over a large number of visits with a large number of measurements per visit, and that \\(\\bar{X}\\) is normally distributed with mean \\(\\mu\\) and variance = \\(27.7/n + 7.9/(nk)\\)\n\n5.33Solutions\n\n\nIf a person’s true diastolic blood pressure is 100 mm Hg, then what is the probability that the person will be classified accurately (as hypertensive) if a single measurement is taken at 1 visit?\n\n\nWe are given that \\(\\mu = 100\\) and \\(n = 1\\) and \\(k = 1\\), so the variance is \\(27.7/1 + 7.9/(1\\times 1) = 35.6\\). Let \\(X\\) be the person’s single measurement, then we want \\(P(X \\geq 95)\\)\n\n1 - pnorm(q = 95, mean = 100, sd = sqrt(35.6))\n\n[1] 0.799\n\n\nSo there is only a 79.9% chance of correctly classifying that person on one visit.\n\n\n\n\n5.34Solutions\n\n\nIs the probability in Problem 5.33 a measure of sensitivity, specificity, or predictive value?\n\n\nSensitifity. This is the probability of the test being positive given the disease is positive.\n\n\n\n\n5.35Solutions\n\n\nIf a person’s true blood pressure is 85 mm Hg, then what is the probability that the person will be accurately classified (as normotensive) if 3 measurements are taken at each of 2 visits?\n\n\nWe are given \\(\\mu = 85\\) and \\(k = 3\\) and \\(n = 2\\). So the variance is \\(27.7/2 + 7.9/(2\\times 3) = 15.17\\). So the probability of being accurately classified at is \\(P(X \\leq 90)\\)\n\npnorm(q = 90, mean = 85, sd = sqrt(15.17))\n\n[1] 0.9004\n\n\nSo there is a 90% chance of being correctly classified.\n\n\n\n\n5.36Solutions\n\n\nIs the probability in Problem 5.35 a measure of sensitivity, specificity, or predictive value?\n\n\nSpecificity. This is the probability of the test being negative given the disease is negative.\n\n\n\n\n5.37Solutions\n\n\nSuppose we decide to take 2 measurements per visit. How many visits are needed so that the sensitivity and specificity in Problems 5.33 and 5.35 would each be at least 95%?\n\n\nThe book’s solution is a lot of math, which is great, and something we’ll go through later. But we can do a quick and dirty version on R by just trying out some values of n:\n\nk &lt;- 2\nn &lt;- 3\nsigma &lt;- sqrt(27.7 / n + 7.9/(n *  k))\n1 - pnorm(q = 95, mean = 100, sd = sigma)\n\n[1] 0.9381\n\npnorm(q = 90, mean = 85, sd = sigma)\n\n[1] 0.9381\n\nn &lt;- 4\nsigma &lt;- sqrt(27.7 / n + 7.9/(n *  k))\n1 - pnorm(q = 95, mean = 100, sd = sigma)\n\n[1] 0.9623\n\npnorm(q = 90, mean = 85, sd = sigma)\n\n[1] 0.9623\n\n\nSo, 4 visits."
  },
  {
    "objectID": "review/pp_05.html#hypertension-3",
    "href": "review/pp_05.html#hypertension-3",
    "title": "Chapter 5 Practice Problems",
    "section": "Hypertension 3",
    "text": "Hypertension 3\nA study is planned to look at the effect of sodium restriction on lowering blood pressure. Nutritional counseling sessions are planned for the participants to encourage dietary sodium restriction. An important component in the study is validating the extent to which individuals comply with a sodium-restricted diet. This is usually accomplished by obtaining urine specimens and measuring sodium in the urine.\nAssume that in free-living individuals (individuals with no sodium restriction) 24-hour urinary sodium excretion is normally distributed with mean 160.5 mEq/24 hrs and standard deviation = 57.4 mEq/24 hrs.\n\n5.38Solutions\n\n\nIf 100 mEq/24 hr is the cutoff value chosen to measure compliance, then what percentage of noncompliant individuals—that is, individuals who do not restrict sodium—will have a 24-hour sodium level below this cutoff point?\n\n\nLet \\(X\\) be normal with mean 160.5 and standard deviation 57.4. Then we want \\(P(X &lt; 100)\\)\n\npnorm(q = 100, mean = 160.5, sd = 57.4)\n\n[1] 0.1459\n\n\n\n\n\nSuppose that in an experimental study it is found that people who are on a sodium-restricted diet have 24-hour sodium excretion that is normally distributed with mean 57.5 mEq/24 hrs and standard deviation of 11.3 mEq/24 hrs.\n\n5.39Solutions\n\n\nWhat is the probability that a person who is on a sodium-restricted diet will have a 24-hour urinary sodium level above the cutoff point (100 mEq/24 hrs)?\n\n\nLet \\(X\\) be normal with mean 57.5 and standard deviation 11.3. Then we want \\(P(X &gt; 100)\\).\n\n1 - pnorm(q = 100, mean = 57.5, sd = 11.3)\n\n[1] 8.46e-05\n\n\n\n\n\n\n5.40Solutions\n\n\nSuppose the investigators in the study wish to change the cutoff value (from 100 mEq/24 hrs) to another value such that the misclassification probabilities in Problems 5.38 and 5.39 are the same. What should the new cutoff value be, and what are the misclassification probabilities corresponding to your answers to Problems 5.38 and 5.39?\n\n\nLet \\(x\\) be the cutoff value. Then we can re-write 5.38 at \\[\n\\Phi\\left(\\frac{x - 160.5}{57.4} \\right),\n\\] where \\(\\Phi(\\cdot)\\) is the CDF of the standard normal. We can also re-write 5.39 as \\[\n1 - \\Phi\\left(\\frac{x - 57.5}{11.3} \\right) = \\Phi\\left(-\\frac{x - 57.5}{11.3} \\right)\n\\] So we want to solve for \\(x\\) in the equation \\[\n\\Phi\\left(\\frac{x - 160.5}{57.4} \\right) = \\Phi\\left(-\\frac{x - 57.5}{11.3}\\right)\n\\Leftrightarrow \\frac{x - 160.5}{57.4} = -\\frac{x - 57.5}{11.3}\n\\] This can be solved using basic algebra. You end up getting \\(x = 74.4\\).\nYou won’t get such lengthy algebra on an exam. But you should absolutely know that pnorm(x, μ, σ) is the exact same thing as \\(\\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\)"
  },
  {
    "objectID": "review/pp_05.html#hypertension-4",
    "href": "review/pp_05.html#hypertension-4",
    "title": "Chapter 5 Practice Problems",
    "section": "Hypertension 4",
    "text": "Hypertension 4\nThe Pediatric Task Force Report on Blood Pressure Control in Children reports blood-pressure norms for children by age and sex group. The mean ± standard deviation for 17-year-old boys for diastolic blood pressure (DBP) is 63.7 ± 11.4 mm Hg based on a large sample.\n\n5.46Solutions\n\n\nSuppose the 90th percentile of the distribution is the cutoff for elevated BP. If we assume that the distribution of blood pressure for 17-year-old boys is normally distributed, then what is the cutoff in mm Hg?\n\n\nThis is the 0.9 quantile of a normal distribution with mean 63.7 and standard deviation 11.4.\n\nqnorm(p = 0.9, mean = 63.7, sd = 11.4)\n\n[1] 78.31\n\n\n\n\n\n\n5.47Solutions\n\n\nAnother approach for defining elevated BP is to use 90 mm Hg as the cutoff (the standard for elevated adult DBP). What percentage of 17-year-old boys would have elevated BP using this approach?\n\n\nWe want the \\(P(X \\geq 90)\\) where \\(X\\) is normal with mean 63.7 and standard deviation 11.4.\n\n1 - pnorm(q = 90, mean = 63.7, sd = 11.4)\n\n[1] 0.01053\n\n\nOnly about 1% of 17-year-old boys.\n\n\n\n\n5.48Solutions\n\n\nSuppose there are 200 17-year-old boys in the 11th grade of whom 25 have elevated BP using the criteria in Problem 5.46. Are there an unusually large number of boys with elevated BP? Why or why not?\n\n\nLet \\(X\\) be the number of boys with elevated blood pressure out of 200. Then we would expect \\(X \\sim \\mathrm{Binom}(200, 0.1)\\) (only 10% have high blood pressure on average). We want \\(P(X \\geq 25)\\).\n\n1 - pbinom(q = 24, size = 200, prob = 0.1)\n\n[1] 0.1449\n\n\nNot that unusual."
  },
  {
    "objectID": "review/pp_05.html#environmental-health",
    "href": "review/pp_05.html#environmental-health",
    "title": "Chapter 5 Practice Problems",
    "section": "Environmental Health",
    "text": "Environmental Health\n\n5.49Solutions\n\n\nA study was conducted relating particulate air pollution and daily mortality in Steubenville, Ohio. On average over the last 10 years, there have been 3 deaths per day. Suppose that on 90 high-pollution days (where the total suspended particulates are in the highest quartile among all days) the death rate is 3.2 deaths per day or 288 deaths observed over the 90 high-pollution days. Are there an unusual number of deaths on high-pollution days?\n\n\nLet \\(X\\) be the number of deaths in 90 days. Then we expect \\(X \\sim \\mathrm{Pois}(90 \\times 3) = \\mathrm{Pois}(270)\\). We want \\(P(X \\geq 288)\\).\n\n1 - ppois(q = 287, lambda = 270)\n\n[1] 0.1437\n\n\nNot that unusual.\nWe can also use a normal approximate with mean 270 and variance 270. Using a continuity correction, we calculate \\(P(X \\geq 287.5)\\)\n\n1 - pnorm(q = 287.5, mean = 270, sd = sqrt(270))\n\n[1] 0.1434"
  },
  {
    "objectID": "review/pp_05.html#ophthalmology-1",
    "href": "review/pp_05.html#ophthalmology-1",
    "title": "Chapter 5 Practice Problems",
    "section": "Ophthalmology 1",
    "text": "Ophthalmology 1\nA study was conducted among patients with retinitis pigmentosa, an ocular condition where pigment appears over the retina, resulting in substantial loss of vision in many cases. The study was based on 94 patients who were seen annually at a baseline visit and at three annual follow-up visits. In this study, 90 patients provided visual-field measurements at each of the four examinations and are the subjects of the following data analyses. Visual field was transformed to the log scale to better approximate normality and yielded the data given in Table 5.1.\n\n\n\n\n\n\n\n\nTable 5.1 Visual-field measurements in retinitis-pigmentosa patients\n\n\nYear of examination\nMean\nStandard deviation\nn\n\n\n\n\nYear 0 (baseline)\n8.15\n1.23\n90\n\n\nYear 3\n8.01\n1.33\n90\n\n\nYear 0–year 3\n0.14\n0.66\n90\n\n\n\n\n\n\n\n\n5.20Solutions\n\n\nAssuming that change in visual field over 3 years is normally distributed when using the log scale, what is the proportion of patients who showed a decline in visual field over 3 years?\n\n\nLet \\(X\\) be the visual acuity change of a randomly selected patient. Then \\(X\\) is normal with mean 0.14 and standard deviation 0.66. We want \\(P(X &gt; 0)\\) (greater than because it’s year 0 minus year 3, so the decline happens if the \\(X\\) is positive).\n\n1 - pnorm(q = 0, mean = 0.14, sd = 0.66)\n\n[1] 0.584\n\n\n\n\n\n\n5.21Solutions\n\n\nWhat percentage of patients would be expected to show a decline of at least 20% in visual field over 3 years? (Note: In the ln scale this is equivalent to a decline of at least log(1/0.8) = 0.223).\n\n\nUsing the notation from 5.22, we want \\(P(X &gt; 0.223)\\)\n\n1 - pnorm(q = 0.223, mean = 0.14, sd = 0.66)\n\n[1] 0.45\n\n\n\n\n\n\n5.22Solutions\n\n\nAnswer Problem 5.21 for a 50% decline over 3 years.\n\n\nThe lower bound is\n\nlog(1 / 0.5)\n\n[1] 0.6931\n\n\nSo we want \\(P(X &gt; 0.6931)\\)\n\n1 - pnorm(q = 0.6931, mean = 0.14, sd = 0.66)\n\n[1] 0.201"
  },
  {
    "objectID": "review/pp_05.html#cancer-2",
    "href": "review/pp_05.html#cancer-2",
    "title": "Chapter 5 Practice Problems",
    "section": "Cancer 2",
    "text": "Cancer 2\nA study of the Massachusetts Department of Health found 46 deaths due to cancer among women in the city of Bedford, MA over the period 1974–1978, where 30 deaths had been expected from statewide rates.\n\n5.23Solutions\n\n\nWrite an expression for the probability of observing exactly \\(k\\) deaths due to cancer over this period if the statewide rates are correct.\n\n\nLet \\(X\\) be the number of deaths. Then \\(X\\) is Poisson with mean \\(30\\). You can write out the PMF here \\[\n\\frac{30^ke^{-30}}{k!}.\n\\]\n\n\n\n\n5.24Solutions\n\n\nCan the occurrence of 46 deaths due to cancer be attributed to chance? Specifically, what is the probability of observing at least 46 deaths due to cancer if the statewide rates are correct?\n\n\nUsing the same notation as in 5.23, we want \\(P(X \\geq 46)\\)\n\n1 - ppois(q = 45, lambda = 30)\n\n[1] 0.003958\n\n\nThis is very rare, and so we strong evidence that the rates are different than the specified state-wide rates.\nWe can also use the normal approximation of the Poisson with mean and variance of 30. Using a contintuity correction, we want \\(P(X &gt; 45.5)\\)\n\n1 - pnorm(q = 45.5, mean = 30, sd = sqrt(30))\n\n[1] 0.002328"
  },
  {
    "objectID": "review/pp_05.html#hypertension-5",
    "href": "review/pp_05.html#hypertension-5",
    "title": "Chapter 5 Practice Problems",
    "section": "Hypertension 5",
    "text": "Hypertension 5\nThe Pediatric Task Force Report on Blood Pressure Control in Children reports blood-pressure norms for children by age and sex group. The mean ± standard deviation for 17-year-old boys for diastolic blood pressure (DBP) is 63.7 ± 11.4 mm Hg based on a large sample.\n\n5.46Solutions\n\n\nSuppose the 90th percentile of the distribution is the cutoff for elevated BP. If we assume that the distribution of blood pressure for 17-year-old boys is normally distributed, then what is the cutoff in mm Hg?\n\n\n\n\n\n\n\n5.47Solutions\n\n\nAnother approach for defining elevated BP is to use 90 mm Hg as the cutoff (the standard for elevated adult DBP). What percentage of 17-year-old boys would have elevated BP using this approach?\n\n\n\n\n\n\n\n5.48Solutions\n\n\nSuppose there are 200 17-year-old boys in the 11th grade of whom 25 have elevated BP using the criteria in Problem 5.46. Are there an unusually large number of boys with elevated BP? Why or why not?"
  },
  {
    "objectID": "review/pp_05.html#ophthalmology-2",
    "href": "review/pp_05.html#ophthalmology-2",
    "title": "Chapter 5 Practice Problems",
    "section": "Ophthalmology 2",
    "text": "Ophthalmology 2\nIn ophthalmology, it is customary to obtain measurements on both the right eye (OD) and the left eye (OS). Suppose intraocular pressure (IOP) is measured on each eye and an average of the OD and OS values (denoted by the OU value) is computed. We wish to compute the variance of the IOP OU values for an individual person. Assume that the standard deviation is 4 mm Hg for the OD values and 4 mm Hg for the OS values (as estimated from the sample standard deviation of IOP from right and left eyes, respectively, from a sample of 100 people).\n\n5.53Solutions\n\n\nIf the OD and OS values are assumed to be independent, then what is the variance of the IOP OU value for individual persons?\n\n\nLet \\(X\\) be OD value with \\(\\mathrm{var}(X) = 4^2 = 16\\) and \\(Y\\) be the OS value wiht \\(\\mathrm{var}(Y) = 4^2 = 16\\). We have the OU value as \\(Z = (X + Y)/2)\\). We want \\[\n\\mathrm{Z} = \\frac{1}{4}\\mathrm{var}(X) + \\frac{1}{4}\\mathrm{var}(Y) = 16/4 + 16/4 = 8.\n\\]"
  },
  {
    "objectID": "review/pp_05.html#quantiles",
    "href": "review/pp_05.html#quantiles",
    "title": "Chapter 5 Practice Problems",
    "section": "Quantiles",
    "text": "Quantiles\n\n5.1Solutions\n\n\nWhat are the deciles of the standard normal distribution; that is, the 10, 20, 30, … , 90 percentiles?\n\n\n\nqnorm(p = seq(0.1, 0.9, by = 0.1))\n\n[1] -1.2816 -0.8416 -0.5244 -0.2533  0.0000  0.2533  0.5244  0.8416  1.2816\n\n\n\n\n\n\n5.2Solutions\n\n\nWhat are the quartiles of the standard normal distribution?\n\n\n\nqnorm(p = c(0.25, 0.5, 0.75))\n\n[1] -0.6745  0.0000  0.6745"
  },
  {
    "objectID": "review/pp_05.html#alcohol-and-hypertension.",
    "href": "review/pp_05.html#alcohol-and-hypertension.",
    "title": "Chapter 5 Practice Problems",
    "section": "Alcohol and Hypertension.",
    "text": "Alcohol and Hypertension.\nTable 5.2 shows data reported relating alcohol consumption and blood-pressure level among 18–39-year-old males. The “Elevated blood pressure” column means either SBP ≥ 140 mm Hg or DBP ≥ 90 mm Hg (or both).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTABLE 5.2 The relationship of blood pressure level to drinking behavior\n\n\n\n\nSystolic blood pressure (SBP)\n\n\nDiastolic blood pressure (DBP)\n\n\nElevated blood pressure\n\n\n\nMean\nsd\nn\nMean\nsd\nn\n \n\n\n\n\nNondrinker\n120.2\n10.7\n96\n74.8\n10.2\n96\n11.5%\n\n\nHeavy drinker\n123.5\n12.8\n124\n78.5\n9.2\n124\n17.7%\n\n\n\n\n\n\n\n\n5.42Solutions\n\n\nIf we assume that the distributions of SBP are normal, then what proportion of nondrinkers have SBP ≥ 140 mm Hg?\n\n\nLet \\(X\\) be a randomly sampled non-drinker. then \\(X\\) is normal with mean 120.2 and standard deviation 10.7. We want \\(P(X \\geq 140)\\)\n\n1 - pnorm(q = 140, mean = 120.2, sd = 10.7)\n\n[1] 0.03212\n\n\n\n\n\n\n5.43Solutions\n\n\nWhat proportion of heavy drinkers have SBP ≥ 140 mm Hg?\n\n\nLet \\(Y\\) be a randomly selected heavy drinker. Then \\(X\\) is normal with mean 123.5 and standard deviation 12.8. We want \\(P(Y \\geq 140)\\).\n\n1 - pnorm(q = 140, mean = 123.5, sd = 12.8)\n\n[1] 0.09869\n\n\n\n\n\n\n5.44HintSolutions\n\n\nWhat proportion of heavy drinkers have “isolated systolic hypertension”; i.e., SBP ≥ 140 mm Hg but DBP &lt; 90 mm Hg?\n\n\n\\[\nP(A\\cap \\bar{B}) = P(A \\cup B) - P(B)\n\\] Draw it out to convince yourself.\n\n\nLet \\(A\\) be SBP &gt; 140 and \\(B\\) be DBP &gt; 90. We want \\(P(A \\cap \\bar{B})\\). We are given that \\(P(A \\cup B) = 0.177\\). We can calculate \\(P(B)\\) from a normal with mean 78.5 and standard deviation 9.2 as\n\n1 - pnorm(q = 90, mean = 78.5, sd = 9.2)\n\n[1] 0.1056\n\n\nUsing the hint, we have \\[\nP(A\\cap \\bar{B}) = P(A \\cup B) - P(B) = 0.177 - 0.1056 = 0.0714\n\\]\n\n\n\n\n5.45Solutions\n\n\nWhat proportion of heavy drinkers have “isolated diastolic hypertension”; i.e., DBP ≥ 90 mm Hg but SBP &lt; 140 mm Hg?\n\n\nUsing the same notation as 5.44, we want \\(P(\\bar{A} \\cap B)\\). \\(P(A) = 0.09869\\) from 5.43. We are also given \\(P(A \\cup B) = 0.177\\). Using the same strategy as 5.44, we have \\[\nP(\\bar{A} \\cap B) = P(A \\cup B) - P(A) = 0.177 - 0.09869 = 0.07831\n\\]"
  },
  {
    "objectID": "review/pp_06.html",
    "href": "review/pp_06.html",
    "title": "Chapter 6 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_06.html#quantiles",
    "href": "review/pp_06.html#quantiles",
    "title": "Chapter 6 Practice Problems",
    "section": "Quantiles",
    "text": "Quantiles\n\n6.1Solution\n\n\nWhat is the upper 10th percentile of a chi-square distribution with 5 df?\n\n\n\nqchisq(p = 1 - 0.1, df = 5)\n\n[1] 9.236\n\n\n\n\n\n\n6.2Solution\n\n\nWhat is the upper 1st percentile of a chi-square distribution with 3 df?\n\n\n\nqchisq(p = 1 - 0.01, df = 3)\n\n[1] 11.34"
  },
  {
    "objectID": "review/pp_06.html#cancer",
    "href": "review/pp_06.html#cancer",
    "title": "Chapter 6 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\nA case-control study of the effectiveness of the Pap smear in preventing cervical cancer (by identifying precancerous lesions) was performed [1]. It was found that 28.1% of 153 cervical-cancer cases and 7.2% of 153 age-matched (within 5 years) controls had never had a Pap smear prior to the time of the case’s diagnosis.\n\n6.5Solution\n\n\nProvide a 95% CI for the percentage of cervical-cancer cases who never had a Pap test.\n\n\n\n\n\n\n\n6.6Solution\n\n\nProvide a 95% CI for the percentage of controls who never had a Pap test.\n\n\n\n\n\n\n\n6.7Solution\n\n\nDo you think the Pap test is helpful in preventing cervical cancer?"
  },
  {
    "objectID": "review/pp_06.html#hypertension",
    "href": "review/pp_06.html#hypertension",
    "title": "Chapter 6 Practice Problems",
    "section": "Hypertension",
    "text": "Hypertension\nHypertensive patients are screened at a neighborhood health clinic and are given methyl dopa, a strong antihypertensive medication for their condition. They are asked to come back 1 week later and have their blood pressures measured again. Suppose the initial and follow-up systolic blood pressures of the patients are given in the following data frame.\n\nsbp_data &lt;- tibble(\n  patient_number = 1:10,\n  initial_sbp = c(200.0, 194.0, 236.0, 163.0, 240.0, \n                  225.0, 203.0, 180.0, 177.0, 240.0),\n  followup_sbp = c(188.0, 212.0, 186.0, 150.0, 200.0, \n                   222.0, 190.0, 154.0, 180.0, 225.0)\n)\nsbp_data\n\n# A tibble: 10 × 3\n   patient_number initial_sbp followup_sbp\n            &lt;int&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1              1         200          188\n 2              2         194          212\n 3              3         236          186\n 4              4         163          150\n 5              5         240          200\n 6              6         225          222\n 7              7         203          190\n 8              8         180          154\n 9              9         177          180\n10             10         240          225\n\n\nTo test the effectiveness of the drug, we want to measure the difference (\\(D\\)) between initial and follow-up blood pressures for each person.\n\n6.8Solution\n\n\nWhat is the mean and sd of \\(D\\)?\n\n\nWe calculate \\(D\\) with\n\nsbp_data |&gt;\n  mutate(diff = initial_sbp - followup_sbp) -&gt;\n  sbp_data\n\nWe can now calculate the mean and sd\n\nsbp_data |&gt;\n  summarize(mean = mean(diff), sd = sd(diff))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  15.1  19.8\n\n\n\n\n\n\n6.9Solution\n\n\nWhat is the standard error of the mean?\n\n\nRosner means the estimated estimated standard error. This is \\(s / \\sqrt{n}\\) where \\(s\\) = 19.85 from 6.8 and \\(n\\) = 10.\n\n19.85 / sqrt(10)\n\n[1] 6.277\n\n\n\n\n\n\n6.10Solution\n\n\nAssume that \\(D\\) is normally distributed. Construct a 95% CI for \\(\\mu\\).\n\n\nThis is \\[\n\\bar{x} \\pm t_{n-1, 1-\\alpha/2}\\frac{s}{\\sqrt{n}}\n\\] Where \\(t_{n-1, 1-\\alpha/2}\\) with \\(\\alpha = 0.05\\) and \\(n = 10\\) is\n\nqt(1 - 0.05 / 2, df = 9)\n\n[1] 2.262\n\n\nPlugging in values \\[\n15.1 \\pm 2.262 \\times 6.277\n\\] Numerically\n\n15.1 - 2.262 * 6.277\n\n[1] 0.9014\n\n15.1 + 2.262 * 6.277\n\n[1] 29.3\n\n\nIn real-life, we would use t.test().\n\nt.test(diff ~ 1, data = sbp_data) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.903      29.3\n\n\n\n\n\n\n6.11Solution\n\n\nDo you have any opinion on the effectiveness of methyl dopa from the results of these 10 patients?\n\n\nSince 0 is not included in the confidence interval, it is not a likely value. So we have evidence that the mean decrease is positive. But to make claims on efficacy, we would need a control group to see if their mean decrease is also positive by the same amount."
  },
  {
    "objectID": "review/pp_06.html#cancer-1",
    "href": "review/pp_06.html#cancer-1",
    "title": "Chapter 6 Practice Problems",
    "section": "Cancer 1",
    "text": "Cancer 1\nA case-control study of the effectiveness of the Pap smear in preventing cervical cancer (by identifying precancerous lesions) was performed. It was found that 28.1% of 153 cervical-cancer cases and 7.2% of 153 age-matched (within 5 years) controls had never had a Pap smear prior to the time of the case’s diagnosis.\n\n6.5Solution\n\n\nProvide a 95% CI for the percentage of cervical-cancer cases who never had a Pap test.\n\n\nLet \\(X\\) be the number of cervical-cancer cases who had never had a Pap-smear. Then \\(X \\sim \\mathrm{Binom}(n, p)\\). We want a 95% confidence interval for \\(p\\). We are given that \\(\\hat{p} = 0.281\\) and \\(n = 153\\). We calculate \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We can get \\(z_{1-\\alpha/2}\\) where \\(\\alpha = 0.05\\) via\n\nqnorm(1 - 0.05/2)\n\n[1] 1.96\n\n\nPlugging in, we get \\[\n0.281 \\pm 1.96 \\cdot \\sqrt{ \\frac{ 0.281(1 - 0.281) }{153} }\n\\] Numerically, this is\n\n0.281 - 1.96 * sqrt(0.281 * (1 - 0.281) / 153)\n\n[1] 0.2098\n\n0.281 + 1.96 * sqrt(0.281 * (1 - 0.281) / 153)\n\n[1] 0.3522\n\n\n\n\n\n\n6.6Solution\n\n\nProvide a 95% CI for the percentage of controls who never had a Pap test.\n\n\nThis is the exact same thing as 6.5, but with \\(\\hat{p} = 0.072\\) and \\(n = 153\\). We get \\[\n0.072 \\pm 1.96 \\cdot \\sqrt{ \\frac{ 0.072(1 - 0.072) }{153} }\n\\] Numerically\n\n0.072 - 1.96 * sqrt(0.072 * (1 - 0.072) / 153)\n\n[1] 0.03104\n\n0.072 + 1.96 * sqrt(0.072 * (1 - 0.072) / 153)\n\n[1] 0.113\n\n\n\n\n\n\n6.7Solution\n\n\nDo you think the Pap test is helpful in preventing cervical cancer?\n\n\nYes. Let \\(p_1\\) is the proportion of cases who did not have a pap-smear and \\(p_2\\) be the proportion of controls who did not have a pap-smear. The confidence interval for \\(p_2\\) is entirely below that of \\(p_1\\). So it is very likely that controls are less likely to not have had a pap-smear than controls. In Chapter 7, we will make this more formal."
  },
  {
    "objectID": "review/pp_06.html#cardiovascular-disease",
    "href": "review/pp_06.html#cardiovascular-disease",
    "title": "Chapter 6 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nA recent hypothesis states that vigorous exercise is an effective preventive measure for subsequent cardiovascular death. To test this hypothesis, a sample of 750 men aged 50–75 who report that they jog at least 10 miles per week is ascertained. After 6 years, 64 have died of cardiovascular disease.\n\n6.13Solution\n\n\nCompute a 95% CI for the incidence of cardiovascular death in this group.\n\n\n\n\n\n\n\n6.14Solution\n\n\nIf the expected death rate from cardiovascular disease over 6 years in 50–75-year-old men based on large samples is 10%, then can a conclusion be drawn concerning this hypothesis from these data?"
  },
  {
    "objectID": "review/pp_06.html#cardiovascular-disease-1",
    "href": "review/pp_06.html#cardiovascular-disease-1",
    "title": "Chapter 6 Practice Problems",
    "section": "Cardiovascular Disease 1",
    "text": "Cardiovascular Disease 1\nA recent hypothesis states that vigorous exercise is an effective preventive measure for subsequent cardiovascular death. To test this hypothesis, a sample of 750 men aged 50–75 who report that they jog at least 10 miles per week is ascertained. After 6 years, 64 have died of cardiovascular disease.\n\n6.13Solution\n\n\nCompute a 95% CI for the incidence of cardiovascular death in this group.\n\n\nLet \\(X\\) be the number that died. Then \\(X \\sim \\mathrm{Binom}(n, p)\\). We are told that \\(n = 750\\) and \\(x = 64\\). So \\(\\hat{p} = 64 / 750\\). Or\n\n64 / 750\n\n[1] 0.08533\n\n\nWe want a 95% confidence interval for \\(p\\). The normal approximation should be fine since \\(np(1-p)\\geq 5\\).\n\n750 * 0.08533 * (1 - 0.08533)\n\n[1] 58.54\n\n\nThe form of the CI is \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We calculate \\(z_{1 - \\alpha/2}\\) with \\(\\alpha = 0.05\\) to get\n\nqnorm(p = 1 - 0.05 / 2)\n\n[1] 1.96\n\n\nPlugging in values, we get \\[\n0.08533 \\pm 1.96 \\cdot \\sqrt{ \\frac{ 0.08533(1 - 0.08533) }{750} }\n\\] Numerically\n\n0.08533 - 1.96 * sqrt(0.08533 * (1 - 0.08533) / 750)\n\n[1] 0.06534\n\n0.08533 + 1.96 * sqrt(0.08533 * (1 - 0.08533) / 750)\n\n[1] 0.1053\n\n\nThe real way in R would be\n\nprop.test(x = 64, n = 750) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0668     0.108\n\n\nThis differs slightly than ours because R uses Wilson intervals, while the way in class we learned are Wald intervals.\n\n\n\n\n6.14Solution\n\n\nIf the expected death rate from cardiovascular disease over 6 years in 50–75-year-old men based on large samples is 10%, then can a conclusion be drawn concerning this hypothesis from these data?\n\n\n10% is consistent with these data. So we cannot conclude that exercise is associated with lower death using these data. This holds even if we calculate an upper bound:\n\n0.08533 + qnorm(1 - 0.05) * sqrt(0.08533 * (1 - 0.08533) / 750)\n\n[1] 0.1021"
  },
  {
    "objectID": "review/pp_06.html#pulmonary-disease",
    "href": "review/pp_06.html#pulmonary-disease",
    "title": "Chapter 6 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\nA spirometric tracing is a standard technique used to measure pulmonary function. These tracings represent plots of the volume of air expelled over a 6-second period and tend to look like Figure 6.1. One quantity of interest is the slope at various points along the curve. The slopes are referred to as flow rates. A problem that arises is that the flow rates cannot be accurately measured, and some observer error is always introduced. To quantify the observer error, an observer measures the flow at 50% of forced vital capacity (volume as measured at 6 seconds) twice on tracings from 10 different people. A machine called a digitizer can trace the curves automatically and can estimate the flow mechanically. Suppose the digitizer is also used to measure the flow twice on these 10 tracings. The data are given in the data frame below.\n\n\n\n\n\n\n\n\n\n\nflow_rates &lt;- tibble(\n  Person = 1:10,\n  Manual_replicate1   = c(1.80, 2.01, 1.63, 1.54, 2.21, 4.16, 3.02, 2.75, 3.03, 2.68),\n  Manual_replicate2   = c(1.84, 2.09, 1.52, 1.49, 2.36, 4.08, 3.07, 2.80, 3.04, 2.71),\n  Digitizer_replicate1 = c(1.82, 2.05, 1.62, 1.49, 2.32, 4.21, 3.08, 2.78, 3.06, 2.70),\n  Digitizer_replicate2 = c(1.83, 2.04, 1.60, 1.45, 2.36, 4.27, 3.09, 2.79, 3.05, 2.70)\n)\n\n\n6.18Solution\n\n\nFind a 95% CI for the standard deviation of the difference between the first and second replicates using the manual method.\n\n\n\n\n\n\n\n6.19Solution\n\n\nAnswer Problem 6.18 for the difference between the first and second replicates using the digitizer method.\n\n\n\n\n\n\nSuppose we want to compare the variability of the two methods within the same person. Let \\(x_{i1}\\) , \\(x_{i2}\\) represent the 2 replicates for the \\(i\\)th person using the manual method, and let \\(y_{i1}, y_{i2}\\) represent the 2 replicates for the \\(i\\)th person using the digitizer method. Let \\[\nd_i = |x_{i1} - x_{i2}| - |y_{i1} - y_{i2}|\n\\] Then, \\(d_i\\) is a measure of the difference in variability using the two methods. Assume that \\(d_i\\) is normally distributed with mean \\(\\mu_d\\) and variance \\(\\sigma_d^2\\)\n\n6.20Solution\n\n\nFind a 95% CI for \\(\\mu_d\\)\n\n\n\n\n\n\n\n6.21Solution\n\n\nWhat is your opinion as to the relative variability of the two methods?"
  },
  {
    "objectID": "review/pp_06.html#bacteriology",
    "href": "review/pp_06.html#bacteriology",
    "title": "Chapter 6 Practice Problems",
    "section": "Bacteriology",
    "text": "Bacteriology\nSuppose a group of mice are inoculated with a uniform dose of a specific type of bacteria and all die within 24 days, with the distribution of survival times given in the following data frame.\n\nmouse_survival &lt;- tibble(\n  survival_time_days = 10:24,\n  number_of_mice     = c(5, 11, 29, 30, 40, 51, 71, 65, 48, 36, 21, 12, 7, 2, 1)\n)\n\nNote that the mean and standard deviation of survival time is\n\n\n\n\n\n\n\n\nMean\nVar\n\n\n\n\n16.13\n7.065\n\n\n\n\n\n\n\n\n6.27Solution\n\n\nAssume that the underlying distribution of survival times is normal. Estimate the probability \\(p\\) that a mouse will survive for 20 or more days.\n\n\nLet \\(X\\) be the number of day and that \\(X \\sim N(\\mu, \\sigma^2)\\). We want \\(P(X \\geq 20)\\). We don’t know \\(\\mu\\) and \\(\\sigma^2\\), but we can use their estimates \\(\\bar{X}\\) and \\(s^2\\):\n\n1 - pnorm(q = 20, mean = 16.13, sd = sqrt(7.065))\n\n[1] 0.0727\n\n\nSince there is discretezation in the days, it is technically more accurate to use a continuity correction. But don’t worry if you forgot this\n\n1 - pnorm(q = 19.5, mean = 16.13, sd = sqrt(7.065))\n\n[1] 0.1024\n\n\n\n\n\n\n6.28Solution\n\n\nSuppose we are not willing to assume that the underlying distribution is normal. Estimate the probability \\(p\\) that a mouse will survive for 20 or more days.\n\n\nThis is just the proportion of mice that survived for 20 or more days.\nThe total number of mice is\n\nmouse_survival |&gt;\n  summarize(total = sum(number_of_mice))\n\n# A tibble: 1 × 1\n  total\n  &lt;dbl&gt;\n1   429\n\n\nThe number who survived for at least 20 days is\n\nmouse_survival |&gt;\n  filter(survival_time_days &gt;= 20) |&gt;\n  summarize(x = sum(number_of_mice))\n\n# A tibble: 1 × 1\n      x\n  &lt;dbl&gt;\n1    43\n\n\nSo \\(\\hat{p} = 43 / 429\\) = 0.1002.\n\n\n\n\n6.29Solution\n\n\nCompute 95% confidence limits for the parameter estimated in Problem 6.28.\n\n\nThe interval is of the form \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] Where \\(z_{1 - 0.05/2}\\) is\n\nqnorm(p = 1 - 0.05 / 2)\n\n[1] 1.96\n\n\nPlugging in values we get \\[\n0.1002 \\pm 1.96 \\cdot \\sqrt{ \\frac{ 0.1002(1 - 0.1002) }{429} }\n\\] Numerically, this is\n\n0.1002 - 1.96 * sqrt(0.1002 * (1 - 0.1002) / 429)\n\n[1] 0.07179\n\n0.1002 + 1.96 * sqrt(0.1002 * (1  - 0.1002) / 429)\n\n[1] 0.1286\n\n\nThe real way in R would be\n\nprop.test(x = 43, n = 429) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0743     0.134\n\n\n\n\n\n\n6.30Solution\n\n\nCompute 99% confidence limits for the parameter estimated in Problem 6.28.\n\n\nNow \\(\\alpha = 0.01\\), so we need \\(z_{1 - 0.01 / 2}\\)\n\nqnorm(1 - 0.01 / 2)\n\n[1] 2.576\n\n\n\n0.1002 - 2.576 * sqrt(0.1002 * (1 - 0.1002) / 429)\n\n[1] 0.06286\n\n0.1002 + 2.576 * sqrt(0.1002 * (1  - 0.1002) / 429)\n\n[1] 0.1375\n\n\nOr, using R\n\nprop.test(x = 43, n = 429, conf.level = 0.99) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0678     0.145"
  },
  {
    "objectID": "review/pp_06.html#cardiovascular-disease-2",
    "href": "review/pp_06.html#cardiovascular-disease-2",
    "title": "Chapter 6 Practice Problems",
    "section": "Cardiovascular Disease 2",
    "text": "Cardiovascular Disease 2\nA group of 60 men under the age of 55 with a prior history of myocardial infarction are put on a strict vegetarian diet as part of an experimental program. After 5 years, 3 men from the group have died.\n\n6.15Solution\n\n\nWhat is the best point estimate of the 5-year mortality rate in this group of men?\n\n\nIf \\(X\\) the number who died out of 60, then \\(X \\sim \\mathrm{Binom}(60, p)\\). We observe \\(x = 3\\). So, \\[\n\\hat{p} = 3 / 60 = 0.05\n\\]\n\n\n\n\n6.16Solution\n\n\nDerive a 95% confidence interval for the 5-yearamortality rate.\n\n\nWe cannot use the normal approximation since \\(n\\hat{p}(1 - \\hat{p}) &lt; 5\\)\n\n60 * 0.05 * (1 - 0.05)\n\n[1] 2.85\n\n\nThe exact method is\n\nbinom.test(x = 3, n = 60) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0104     0.139\n\n\nThis is because if \\(X \\sim \\mathrm{Binom}(60, 0.01043)\\) then \\(P(X \\geq 3)\\) is \\(\\alpha/2 = 0.05/2 = 0.025\\):\n\n1 - pbinom(q = 2, size = 60, prob = 0.01043)\n\n[1] 0.02499\n\n\n\n\n\n\n\n\n\n\n\nand if \\(X \\sim \\mathrm{Binom}(60, 0.1392)\\) then \\(P(X \\leq 3)\\) is 0.025\n\npbinom(q = 3, size = 60, prob = 0.1392)\n\n[1] 0.02505\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.17Solution\n\n\nSuppose that from a large sample of men under age 55 with a prior history of myocardial infarction, we know that the 5-year mortality rate is 18%. How does the observed mortality rate obtained in Problem 6.15 compare with the large sample rate of 18%?\n\n\nThe upper bound of the 95% CI is below 0.18. So we have evidence that those on the vegetarian diet have a lower mortality rate."
  },
  {
    "objectID": "review/pp_06.html#pulmonary-disease-1",
    "href": "review/pp_06.html#pulmonary-disease-1",
    "title": "Chapter 6 Practice Problems",
    "section": "Pulmonary Disease 1",
    "text": "Pulmonary Disease 1\nA spirometric tracing is a standard technique used to measure pulmonary function. These tracings represent plots of the volume of air expelled over a 6-second period and tend to look like Figure 6.1. One quantity of interest is the slope at various points along the curve. The slopes are referred to as flow rates. A problem that arises is that the flow rates cannot be accurately measured, and some observer error is always introduced. To quantify the observer error, an observer measures the flow at 50% of forced vital capacity (volume as measured at 6 seconds) twice on tracings from 10 different people. A machine called a digitizer can trace the curves automatically and can estimate the flow mechanically. Suppose the digitizer is also used to measure the flow twice on these 10 tracings. The data are given in the data frame below.\n\n\n\n\n\n\n\n\n\n\nflow_rates &lt;- tibble(\n  Person = 1:10,\n  Manual_replicate1   = c(1.80, 2.01, 1.63, 1.54, 2.21, 4.16, 3.02, 2.75, 3.03, 2.68),\n  Manual_replicate2   = c(1.84, 2.09, 1.52, 1.49, 2.36, 4.08, 3.07, 2.80, 3.04, 2.71),\n  Digitizer_replicate1 = c(1.82, 2.05, 1.62, 1.49, 2.32, 4.21, 3.08, 2.78, 3.06, 2.70),\n  Digitizer_replicate2 = c(1.83, 2.04, 1.60, 1.45, 2.36, 4.27, 3.09, 2.79, 3.05, 2.70)\n)\n\n\n6.18Solution\n\n\nFind a 95% CI for the standard deviation of the difference between the first and second replicates using the manual method.\n\n\nWe first calculate the differences\n\nflow_rates |&gt;\n  mutate(\n    Manual_diff = Manual_replicate1 - Manual_replicate2,\n    Digitizer_diff = Digitizer_replicate1 - Digitizer_replicate2) -&gt;\n  flow_rates\n\nWe have to assume normality to use the standard variance CI. The following QQ plot seems to say this is an OK assumption:\n\nggplot(data = flow_rates, mapping = aes(sample = Manual_diff)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe variance CI is of the form \\[\n\\frac{(n - 1)s^2}{\\chi^2_{n-1, 1 - \\alpha/2}} \\leq \\sigma^2 \\leq \\frac{(n - 1)s^2}{\\chi^2_{n-1, \\alpha/2}}\n\\] \\(s^2\\) is the sample variance\n\nsummarize(flow_rates, var = var(Manual_diff))\n\n# A tibble: 1 × 1\n      var\n    &lt;dbl&gt;\n1 0.00607\n\n\n\\(n\\) is 10. \\(\\chi^2_{n-1, 1-\\alpha/2}\\) and \\(\\chi^2_{n-1,\\alpha/2}\\) are the \\(1 - \\alpha/2\\) and \\(\\alpha/2\\) quantiels of the chi-squared distribution with \\(n-1\\) degrees of freedom.\n\nqchisq(p = c(0.05/2, 1 - 0.05/2), df = 10 - 1)\n\n[1]  2.70 19.02\n\n\nPlugging in values we get \\[\n\\frac{(10 - 1)0.006068}{19.02} \\leq \\sigma^2 \\leq \\frac{(10 - 1)0.006068}{2.70}\n\\] Numerically, a 95% CI for the variance\n\n9 * 0.006068 / 19.02\n\n[1] 0.002871\n\n9 * 0.006068 / 2.70\n\n[1] 0.02023\n\n\nWe take the square root of these to get a 95% CI for the standard deviation\n\nsqrt(9 * 0.006068 / 19.02)\n\n[1] 0.05358\n\nsqrt(9 * 0.006068 / 2.70)\n\n[1] 0.1422\n\n\nIn real-life, folks would use a bootstrap because of the sensitivity of this CI to the normality assumption\n\nset.seed(787)\nsdout &lt;- replicate(n = 10000, expr = {\n  sd(sample(flow_rates$Manual_diff, replace = TRUE))\n})\nquantile(sdout, probs = c(0.025, 0.975))\n\n   2.5%   97.5% \n0.04007 0.10064 \n\n\n\n\n\n\n6.19Solution\n\n\nAnswer Problem 6.18 for the difference between the first and second replicates using the digitizer method.\n\n\nThe sample variance is\n\nsummarize(flow_rates, var = var(Digitizer_diff))\n\n# A tibble: 1 × 1\n       var\n     &lt;dbl&gt;\n1 0.000828\n\n\nWe can just plug in the value using the same equation from 6.18\n\nsqrt(9 * 0.0008278 / 19.02)\n\n[1] 0.01979\n\nsqrt(9 * 0.0008278 / 2.70)\n\n[1] 0.05253\n\n\nThe bootstrap CI is\n\nset.seed(787)\nsdout &lt;- replicate(n = 10000, expr = {\n  sd(sample(flow_rates$Digitizer_diff, replace = TRUE))\n})\nquantile(sdout, probs = c(0.025, 0.975))\n\n   2.5%   97.5% \n0.01287 0.03808 \n\n\n\n\n\nSuppose we want to compare the variability of the two methods within the same person. Let \\(x_{i1}\\) , \\(x_{i2}\\) represent the 2 replicates for the \\(i\\)th person using the manual method, and let \\(y_{i1}, y_{i2}\\) represent the 2 replicates for the \\(i\\)th person using the digitizer method. Let \\[\nd_i = |x_{i1} - x_{i2}| - |y_{i1} - y_{i2}|\n\\] Then, \\(d_i\\) is a measure of the difference in variability using the two methods. Assume that \\(d_i\\) is normally distributed with mean \\(\\mu_d\\) and variance \\(\\sigma_d^2\\)\n\n6.20Solution\n\n\nFind a 95% CI for \\(\\mu_d\\)\n\n\nWe need to calculate \\(d_i\\)\n\nflow_rates |&gt;\n  mutate(\n    d = abs(Manual_replicate1 - Manual_replicate2) - \n      abs(Digitizer_replicate1 - Digitizer_replicate2)\n    ) -&gt;\n  flow_rates\n\nA 95% CI is of the form \\[\n\\bar{x} \\pm t_{n-1, 1-\\alpha/2}\\frac{s}{\\sqrt{n}}\n\\] We can get all of these terms:\n\nflow_rates |&gt;\n  summarize(\n    mean = mean(d), \n    sd = sd(d), \n    n = n(),\n    t = qt(p = 1 - 0.05/2, df = n() - 1))\n\n# A tibble: 1 × 4\n   mean     sd     n     t\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 0.044 0.0353    10  2.26\n\n\nPlugging in values we get \\[\n0.044    \\pm 2.262\\frac{0.03534}{\\sqrt{10}}\n\\] Numerically\n\n0.044 - 2.262 * 0.03534 / sqrt(10)\n\n[1] 0.01872\n\n0.044 + 2.262 * 0.03534 / sqrt(10)\n\n[1] 0.06928\n\n\nThe real way in R\n\nt.test(d ~ 1, data = flow_rates) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0187    0.0693\n\n\n\n\n\n\n6.21Solution\n\n\nWhat is your opinion as to the relative variability of the two methods?\n\n\nThe confidence interval in part 20 does not include 0. So the average diffrence between replicates is non-zero. In fact, since it is above 0 we have evidence that the Manual way has larger absolute differences on average than the digitizer method."
  },
  {
    "objectID": "review/pp_06.html#cardiovascular-disease-3",
    "href": "review/pp_06.html#cardiovascular-disease-3",
    "title": "Chapter 6 Practice Problems",
    "section": "Cardiovascular Disease 3",
    "text": "Cardiovascular Disease 3\nIn Table 2.1 below, data on serum-cholesterol levels of 24 hospital employees before and after they adopted a vegetarian diet are provided.\n\n\n\n\n\n\n\n\nTable 2.1 Serum-Cholesterol levels before and after adopting a vegetarian diet (mg/dL)\n\n\nSubject\nBefore\nAfter\nBefore - After\n\n\n\n\n1\n195.00\n146.0\n49.00\n\n\n2\n145.00\n155.0\n-10.00\n\n\n3\n205.00\n178.0\n27.00\n\n\n4\n159.00\n146.0\n13.00\n\n\n5\n244.00\n208.0\n36.00\n\n\n6\n166.00\n147.0\n19.00\n\n\n7\n250.00\n202.0\n48.00\n\n\n8\n236.00\n215.0\n21.00\n\n\n9\n192.00\n184.0\n8.00\n\n\n10\n224.00\n208.0\n16.00\n\n\n11\n238.00\n206.0\n32.00\n\n\n12\n197.00\n169.0\n28.00\n\n\n13\n169.00\n182.0\n-13.00\n\n\n14\n158.00\n127.0\n31.00\n\n\n15\n151.00\n149.0\n2.00\n\n\n16\n197.00\n178.0\n19.00\n\n\n17\n180.00\n161.0\n19.00\n\n\n18\n222.00\n187.0\n35.00\n\n\n19\n168.00\n176.0\n-8.00\n\n\n20\n168.00\n145.0\n23.00\n\n\n21\n167.00\n154.0\n13.00\n\n\n22\n161.00\n153.0\n8.00\n\n\n23\n178.00\n137.0\n41.00\n\n\n24\n137.00\n125.0\n12.00\n\n\nMean\n187.79\n168.2\n19.54\n\n\nSD\n33.16\n26.8\n16.81\n\n\nn\n24.00\n24.0\n24.00\n\n\n\n\n\n\n\n\n6.31Solution\n\n\nWhat is your best estimate of the effect of adopting a vegetarian diet on change in serum-cholesterol levels?\n\n\n\\(\\bar{X}\\): An average difference of 19.54 mg/dL\n\n\n\n\n6.32Solution\n\n\nWhat is the standard error of the estimate given in Problem 6.31?\n\n\n\\[\ns / \\sqrt{n} = 16.81 / \\sqrt{24}\n\\]\n\n16.81 / sqrt(24)\n\n[1] 3.431\n\n\n\n\n\n\n6.33Solution\n\n\nProvide a 95% CI for the effect of adopting the diet.\n\n\nThe interval is of the form \\[\n\\bar{x} \\pm t_{n-1, 1-\\alpha/2}\\frac{s}{\\sqrt{n}}\n\\] We just need \\(t_{24 - 1, 1 - 0.05 / 2}\\)\n\nqt(p = 1 - 0.05 / 2, df = 24 - 1)\n\n[1] 2.069\n\n\nPlugging in numbers \\[\n19.54 \\pm 2.069 \\times 3.431\n\\] Numerically\n\n19.54 - 2.069 * 3.431\n\n[1] 12.44\n\n19.54 + 2.069 * 3.431\n\n[1] 26.64\n\n\n\n\n\n\n6.34Solution\n\n\nWhat can you conclude from your results in Problem 6.33?\n\n\nWe have evidence that cholesterol levels are lower after starting the diet since the 95% CI is above 0.\n\n\n\nSome physicians consider only changes of at least 10 mg/dL (the same units as in Table 2.1) to be clinically significant.\n\n6.35Solution\n\n\nAmong people with a clinically significant change in either direction, what is the best estimate of the proportion of subjects with a clinically significant decline in cholesterol?\n\n\nYou count the number of folks who are less than -10 and the number of folks who are greater than 10. You can do this manually. You end up with 18 greater than 10 and 2 less than -10. Positive numbers mean a decline, so 18 out of 20 clinically significant changes have been a decline, or \\[\n\\hat{p} = 18/20 = 0.9\n\\]\n\n\n\n\n6.36Solution\n\n\nProvide a 95% CI associated with the estimate in Problem 6.35.\n\n\nWe need to use an exact method since:\n\n20 * 0.9 * (1 - 0.9)\n\n[1] 1.8\n\n\nThe exact CI is\n\nbinom.test(x = 18, n = 20) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.683     0.988\n\n\nThis is because if \\(X \\sim \\mathrm{Binom}(20, 0.683)\\) then \\(P(X \\geq 18)\\) is \\(\\alpha/2 = 0.05/2 = 0.025\\):\n\n1 - pbinom(q = 17, size = 20, prob = 0.683)\n\n[1] 0.02499\n\n\n\n\n\n\n\n\n\n\n\nand if \\(X \\sim \\mathrm{Binom}(20, 0.9877)\\) then \\(P(X \\leq 18)\\) is 0.025\n\npbinom(q = 18, size = 20, prob = 0.9877)\n\n[1] 0.02482\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.37Solution\n\n\nWhat can you conclude from your results in Problem 6.36?\n\n\nSince the CI is well above 0.5, we can conclude it is more likely that a significant change will result in lower cholestoral than high cholesterol"
  },
  {
    "objectID": "review/pp_06.html#cancer-2",
    "href": "review/pp_06.html#cancer-2",
    "title": "Chapter 6 Practice Problems",
    "section": "Cancer 2",
    "text": "Cancer 2\nData from U.S. cancer-tumor registries suggest that of all people with the type of lung cancer where surgery is the recommended therapy, 40% survive for 3 years from the time of diagnosis and 33% survive for 5 years.\n\n6.12Solution\n\n\nSuppose that a group of patients who would have received standard surgery are assigned to a new type of surgery. Of 100 such patients, 55 survive for 3 years and 45 survive for 5 years. Can we say that the new form of surgery is better in any sense than the standard form of surgery?\n\n\nWe calculate two lower bounds on proportions (upper confidence intervals). One for the proportion that survive for 3 years (\\(p_1\\)) and one for the proportion that survive for 5 years (\\(p_2\\)). We will see if the lower bound for \\(p_1\\) is above 0.4 (the national proportion) and the lower bound for \\(p_2\\) is above 0.33 (the national proportion). We are told that \\(\\hat{p}_1 = 55/100 = 0.55\\) and \\(\\hat{p}_2 = 45 / 100 = 0.45\\). Also, \\(n = 100\\). Both intervals are of the form \\[\n\\hat{p} - z_{1 - \\alpha} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] Let’s use 95% for the level (the standard). Then \\(z_{1 - 0.05}\\) is\n\nqnorm(p = 1 - 0.05)\n\n[1] 1.645\n\n\nThe first lower bound is \\[\n0.55 - 1.645 \\sqrt{ \\frac{ 0.55(1 - 0.55) }{100} }\n\\] Numerically\n\n0.55 - 1.645 * sqrt(0.55 * (1 - 0.55) / 100)\n\n[1] 0.4682\n\n\nThe second lower bound is \\[\n0.45 - 1.645 \\sqrt{ \\frac{ 0.45(1 - 0.45) }{100} }\n\\] Numerically\n\n0.45 - 1.645 * sqrt(0.45 * (1 - 0.45) / 100)\n\n[1] 0.3682\n\n\nBoth lower bounds are above their national proportions. So it seems the new surgery is better."
  },
  {
    "objectID": "review/pp_06.html#pulmonary-disease-2",
    "href": "review/pp_06.html#pulmonary-disease-2",
    "title": "Chapter 6 Practice Problems",
    "section": "Pulmonary Disease 2",
    "text": "Pulmonary Disease 2\nWheezing is a common respiratory symptom reported by both children and adults. A study is conducted to assess whether either personal smoking or maternal smoking are associated with wheezing in children ages 6–19.\n\n6.38Solution\n\n\nSuppose that 400 children in this age group whose mothers smoke are assessed in 1985, and it is found that 60 report symptoms of wheezing. Provide a point estimate and a 95% CI for the underlying rate of wheezing in the population of children whose mothers smoke.\n\n\nLet \\(X\\) be the number wheezing out of 400. Then \\(X \\sim \\mathrm{Binom}(400, p)\\). We estimate \\(p\\) with \\[\n\\hat{p} = 60 / 400 = 0.15\n\\] We obtain a 95% CI of this form: \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We get \\(z_{1 - \\alpha/2}\\) with\n\nqnorm(1 - 0.05 / 2)\n\n[1] 1.96\n\n\nPlugging in numbers we get \\[\n0.15 \\pm 1.96 \\sqrt{ \\frac{ 0.15(1 - 0.15) }{400} }\n\\] Numerically, this is\n\n0.15 - 1.96 * sqrt(0.15 * (1 - 0.15) / 400)\n\n[1] 0.115\n\n0.15 + 1.96 * sqrt(0.15 * (1 - 0.15) / 400)\n\n[1] 0.185\n\n\nThe real way in R is\n\nprop.test(x = 60, n = 400) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.117     0.190\n\n\n\n\n\n\n6.39Solution\n\n\nSuppose the rate of wheezing among children ages 6–19 in the general population is 10%. What can you conclude from the results in Problem 6.38?\n\n\nWe have evidence that the rate is higher in children with smoking mothers. This is since the CI from part 6.38 is above 0.1.\n\n\n\n\n6.40Solution\n\n\nSuppose we have a subgroup of 30 children where both the mother and the child are smokers. Six of these 30 children have wheezing symptoms. Provide a 95% CI for the true rate of wheezing in this population.\n\n\nIn this case \\[\n\\hat{p} = 6 / 30 = 0.2\n\\] We need to use an exact method since \\(n\\hat{p}(1 - \\hat{p}) &lt; 5\\)\n\n30 * 0.2 * (1 - 0.2)\n\n[1] 4.8\n\n\nThe exact approach results in a 95% CI of\n\nbinom.test(x = 6, n = 30) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0771     0.386\n\n\nThis is because if \\(X \\sim \\mathrm{Binom}(30, 0.07714)\\) then \\(P(X \\geq 6)\\) is \\(\\alpha/2 = 0.05/2 = 0.025\\):\n\n1 - pbinom(q = 5, size = 30, prob = 0.07714 )\n\n[1] 0.02501\n\n\n\n\n\n\n\n\n\n\n\nand if \\(X \\sim \\mathrm{Binom}(30, 0.3857)\\) then \\(P(X \\leq 6)\\) is 0.025\n\npbinom(q = 6, size = 30, prob = 0.3857)\n\n[1] 0.02498\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.41Solution\n\n\nHow do you assess the results in Problem 6.40?\n\n\nWe do not have evidence that they have a different wheezing rate than the general population between the CI includes 0.1.\n\n\n\n\n6.42Solution\n\n\nSuppose 6 events are realized over a 1-year period for a random variable that is thought to be Poisson-distributed. Provide a 95% CI for the true expected number of events over 1 year.\n\n\n\npoisson.test(x = 6) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1     2.20      13.1"
  },
  {
    "objectID": "review/pp_07.html",
    "href": "review/pp_07.html",
    "title": "Chapter 7 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_07.html#nutrition",
    "href": "review/pp_07.html#nutrition",
    "title": "Chapter 7 Practice Problems",
    "section": "Nutrition",
    "text": "Nutrition\nAs part of a dietary-instruction program, ten 25–34-year-old males adopted a vegetarian diet for 1 month. While on the diet, the average daily intake of linoleic acid was 13 g with standard deviation = 4 g.\n\n7.1Solution\n\n\nIf the average daily intake among 25–34-year-old males in the general population is 15 g, then, using a significance level of .05, test the hypothesis that the intake of linoleic acid in this group is lower than that in the general population.\n\n\n\n\n\n\n\n7.2Solution\n\n\nCompute a p-value for the hypothesis test in Problem 7.1.\n\n\n\n\n\n\nAs part of the same program, eight 25–34-year-old females report an average daily intake of saturated fat of 11 g with standard deviation = 11 g while on a vegetarian diet.\n\n7.3Solution\n\n\nIf the average daily intake of saturated fat among 25–34-year-old females in the general population is 24 g, then, using a significance level of .01, test the hypothesis that the intake of saturated fat in this group is lower than that in the general population.\n\n\n\n\n\n\n\n7.4Solution\n\n\nCompute a p-value for the hypothesis test in Problem 7.3.\n\n\n\n\n\n\n\n7.5Solution\n\n\nWhat is the relationship between your answers to Problems 7.3 and 7.4?\n\n\n\n\n\n\nSuppose we are uncertain what effect a vegetarian diet will have on the level of linoleic-acid intake in Problem 7.1.\n\n7.6Solution\n\n\nWhat are the null and alternative hypotheses in this case?\n\n\n\n\n\n\n\n7.7Solution\n\n\nCompare the mean level of linoleic acid in the vegetarian population with that of the general population under the hypotheses in Problem 7.6. Report a p-value.\n\n\n\n\n\n\n\n7.8Solution\n\n\nSuppose \\(s\\) = 5 based on a sample of 20 subjects. Test the null hypothesis \\(H_0\\) : \\(\\sigma^2 = 16\\) versus \\(H_1\\) : \\(\\sigma^2 \\neq 16\\) using the critical-value method based on a significance level of .05.\n\n\n\n\n\n\n\n7.9Solution\n\n\nAnswer Problem 7.8 using the p-value method.\n\n\n\n\n\n\n\n7.10Solution\n\n\nSuppose we are uncertain what effect a vegetarian diet will have on saturated fat intake. Compute a two-sided p-value for the hypothesis testing situation mentioned in Problem 7.3.\n\n\n\n\n\n\n\n7.11Solution\n\n\nCompute a lower one-sided 95% CI for the true mean intake of linoleic acid in the vegetarian population depicted in Problem 7.1.\n\n\n\n\n\n\n\n7.12Solution\n\n\nHow does your answer to Problem 7.11 relate to your answer to Problem 7.1?"
  },
  {
    "objectID": "review/pp_07.html#nutrition-1",
    "href": "review/pp_07.html#nutrition-1",
    "title": "Chapter 7 Practice Problems",
    "section": "Nutrition 1",
    "text": "Nutrition 1\nAs part of a dietary-instruction program, ten 25–34-year-old males adopted a vegetarian diet for 1 month. While on the diet, the average daily intake of linoleic acid was 13 g with standard deviation = 4 g.\n\n7.1Solution\n\n\nIf the average daily intake among 25–34-year-old males in the general population is 15 g, then, using a significance level of .05, test the hypothesis that the intake of linoleic acid in this group is lower than that in the general population.\n\n\nLet \\(X_i\\) be the average daily intake of linoleic acid of individual \\(i\\). We assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 15\\) versus \\(H_A: \\mu &lt; 15\\). We are told that \\(\\bar{x} = 13\\), \\(s = 4\\), and \\(n = 10\\). We first calculate the \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{13 - 15}{4 / \\sqrt{10}}\n\\] Numerically\n\n(13 - 15) / (4 / sqrt(10))\n\n[1] -1.581\n\n\nFor the critical value method, we compare this to \\(t_{n-1, \\alpha}\\) where \\(\\alpha = 0.05\\)\n\nqt(p = 0.05, df = 10 - 1)\n\n[1] -1.833\n\n\nRecall that since we are testing if the mean is lower than some value, evidence against the null is for very negative \\(t\\)-statistics. Since \\(t &gt; t_{n-1, \\alpha}\\) (-1.581 &gt; -1.833), we fail to reject \\(H_0\\), and conclude that we do not have evidence that the average linoleic acid is lower than the general population.\n\n\n\n\n7.2Solution\n\n\nCompute a p-value for the hypothesis test in Problem 7.1.\n\n\nWe could also calculate the \\(p\\)-value, \\(P(T &lt; -1.581)\\) where \\(T \\sim t_{n-1}\\)\n\n\n\n\n\n\n\n\n\nNumerically\n\npt(q = -1.581, df = 9)\n\n[1] 0.07417\n\n\nSince the \\(p\\)-value is greater than 0.05, we again fail to reject at level 0.05 and conclude that we do not have evidence that the average linoleic acid is lower than the general population.\n\n\n\nAs part of the same program, eight 25–34-year-old females report an average daily intake of saturated fat of 11 g with standard deviation = 11 g while on a vegetarian diet.\n\n7.3Solution\n\n\nIf the average daily intake of saturated fat among 25–34-year-old females in the general population is 24 g, then, using a significance level of .01, test the hypothesis that the intake of saturated fat in this group is lower than that in the general population.\n\n\nLet \\(X_i\\) be the average daily intake of saturated fat of individual \\(i\\). We assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 24\\) versus \\(H_A: \\mu &lt; 24\\). We are told that \\(\\bar{x} = 11\\), \\(s = 11\\), and \\(n = 8\\). We first calculate the \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{11 - 24}{11 / \\sqrt{8}}\n\\] Numerically\n\n(11 - 24) / (11 / sqrt(8))\n\n[1] -3.343\n\n\nFor the critical value method, we compare this to \\(t_{n-1, \\alpha}\\) where \\(\\alpha = 0.01\\)\n\nqt(p = 0.01, df = 8 - 1)\n\n[1] -2.998\n\n\nRecall that since we are testing if the mean is lower than some value, evidence against the null is for very negative \\(t\\)-statistics. Since \\(t &lt; t_{n-1, \\alpha}\\) (-3.343 &lt; -2.998), we reject \\(H_0\\), and conclude that we have evidence that the average saturated fat intake is lower in this group.\n\n\n\n\n7.4Solution\n\n\nCompute a p-value for the hypothesis test in Problem 7.3.\n\n\nWe could also calculate the \\(p\\)-value, \\(P(T &lt; -3.343)\\) where \\(T \\sim t_{n-1}\\)\n\n\n\n\n\n\n\n\n\nNumerically\n\npt(q = -3.343, df = 7)\n\n[1] 0.006184\n\n\nSince the \\(p\\)-value is less than 0.01, we reject the null and conclude that the average saturated fat intake for this group is lower than the general population.\n\n\n\n\n7.5Solution\n\n\nWhat is the relationship between your answers to Problems 7.3 and 7.4?\n\n\n\nLet \\(t\\) be our observed \\(t\\)-statistic.\nLet \\(t_{n-1, \\alpha}\\) be the \\(\\alpha\\) quantile of a \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nLet \\(T\\) be a random variable that follows a \\(t\\)-distribution with \\(n-1\\) degrees of freedom. That is, \\(T \\sim t_{n-1}\\).\n\nThen \\(t\\) is less than \\(t_{n-1, \\alpha}\\) (critical value method) if and only if \\(P(T &lt; t) &lt; \\alpha\\) (\\(p\\)-value method).\n\n\n\nSuppose we are uncertain what effect a vegetarian diet will have on the level of linoleic-acid intake in Problem 7.1.\n\n7.6Solution\n\n\nWhat are the null and alternative hypotheses in this case?\n\n\nUsing the same notation as in 7.1, we are testing\n\n\\(H_0: \\mu = 15\\)\n\\(H_A: \\mu \\neq 15\\)\n\n\n\n\n\n7.7Solution\n\n\nCompare the mean level of linoleic acid in the vegetarian population with that of the general population under the hypotheses in Problem 7.6. Report a \\(p\\)-value.\n\n\nWe use the same \\(t\\)-statistic as before \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{13 - 15}{4 / \\sqrt{10}} = -1.581\n\\] But now “more extreme” means just further away from 0. For the critical value method, we compare our observed \\(t\\)-statistic to \\(t_{n-1, 1 - \\alpha/2}\\)\n\nqt(p = 1 - 0.05 / 2, df = 10 - 1)\n\n[1] 2.262\n\n\nSince \\(|t| &lt; t_{n-1,1 - \\alpha/2}\\) (1.581 &lt; 2.262), we again fail to reject \\(H_0\\) at significance level 0.05. More commonly, we would calculate a \\(p\\)-value as \\(P(|T| &gt; 1.581)\\)\n\n\n\n\n\n\n\n\n\nIn R\n\n2 * pt(q = -1.581, df = 10 - 1)\n\n[1] 0.1483\n\n\nSince the \\(p\\)-value is 0.1483, we fail to reject \\(H_0\\) at significance level 0.05 and conclude that we do not have evidence that mean linoleic acid is different in this group than in the general population.\n\n\n\n\n7.10Solution\n\n\nSuppose we are uncertain what effect a vegetarian diet will have on saturated fat intake. Compute a two-sided p-value for the hypothesis testing situation mentioned in Problem 7.3.\n\n\nWe use the same \\(t\\)-statistic as before \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{11 - 24}{11 / \\sqrt{8}} = -3.343\n\\] But now “more extreme” means just further away from 0. For the critical value method, we compare our observed \\(t\\)-statistic to \\(t_{n-1, 1 - \\alpha/2}\\)\n\nqt(p = 1 - 0.01 / 2, df = 8 - 1)\n\n[1] 3.499\n\n\nSince \\(|t| &lt; t_{n-1,1 - \\alpha/2}\\) (3.343 &lt; 3.499), this time we fail to reject \\(H_0\\) at significance level 0.01. More commonly, we would calculate a \\(p\\)-value as \\(P(|T| &gt; 3.343)\\)\n\n\n\n\n\n\n\n\n\nIn R\n\n2 * pt(q = -3.343, df = 8 - 1)\n\n[1] 0.01237\n\n\nSince the \\(p\\)-value is 0.01237 (larger than 0.01), we fail to reject \\(H_0\\) at significance level 0.01 and conclude that we do not have evidence that mean saturated fat is different in this group than in the general population.\n\n\n\n\n7.11Solution\n\n\nCompute a lower one-sided 95% CI for the true mean intake of linoleic acid in the vegetarian population depicted in Problem 7.1.\n\n\nWe need an upper bound of the form \\[\n\\bar{x} + t_{n-1, 1-\\alpha}\\frac{s}{\\sqrt{n}}\n\\] We were given all terms except \\(t_{n-1,1-\\alpha}\\), where \\(\\alpha = 0.05\\)\n\nqt(p = 1 - 0.05, df = 10 - 1)\n\n[1] 1.833\n\n\nPlugging in values, we get \\[\n13 + 1.833 \\times 4 / \\sqrt{10}\n\\] Numerically\n\n13 + 1.833 * 4 / sqrt(10)\n\n[1] 15.32\n\n\nSo the one-sided 95% CI is \\[\n\\mu \\leq 15.32\n\\]\n\n\n\n\n7.12Solution\n\n\nHow does your answer to Problem 7.11 relate to your answer to Problem 7.1?\n\n\nSince the 95% CI include 15 (from 7.11), we would fail to reject at level 0.05 for \\(H_0: \\mu = 15\\) versus \\(H_A: \\mu &lt; 15\\) (from 7.1)."
  },
  {
    "objectID": "review/pp_07.html#occupational-health",
    "href": "review/pp_07.html#occupational-health",
    "title": "Chapter 7 Practice Problems",
    "section": "Occupational Health",
    "text": "Occupational Health\nSuppose that 28 cancer deaths are noted among workers ex- posed to asbestos in a building-materials plant from 1981–1985. Only 20.5 cancer deaths are expected from statewide cancer-mortality rates.\n\n7.17Solution\n\n\nWhat is the estimated SMR for total cancer mortality?\n\n\n\n\n\n\n\n7.18Solution\n\n\nIs there a significant excess or deficit of total cancer deaths among these workers?\n\n\n\n\n\n\nIn the same group of workers, 7 deaths due to leukemia are noted. Only 4.5 are expected from statewide rates.\n\n7.19Solution\n\n\nWhat is the estimated SMR for leukemia?\n\n\n\n\n\n\n\n7.20Solution\n\n\nIs there a significant excess or deficit of leukemia deaths among these workers?"
  },
  {
    "objectID": "review/pp_07.html#infectious-disease",
    "href": "review/pp_07.html#infectious-disease",
    "title": "Chapter 7 Practice Problems",
    "section": "Infectious Disease",
    "text": "Infectious Disease\nSuppose the annual incidence of diarrhea (defined as 1+ episodes per year) in a Third-World country is 5% in children under the age of 2.\n\n7.21Solution\n\n\nIf 10 children out of 108 under the age of 2 in a poor rural community in the country have 1+ episodes of diarrhea in a year, then test if this represents a significant departure from the overall rate for the country using the critical-value method.\n\n\nLet \\(X\\) be the number with 1+ episodes in a year. Then \\(X \\sim \\mathrm{Binom}(108, p)\\). We are told that \\(\\hat{p} = 10 / 108 = 0.09259\\). We are testing \\(H_0: p = 0.05\\) versus \\(H_A: p \\neq 0.05\\). We can use the normal method since \\(np(1-p)\\geq 5\\)\n\n108 * 0.05 * (1 - 0.05)\n\n[1] 5.13\n\n\nWe calculate the \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.09259 - 0.05}{\\sqrt{0.05(1-0.05)/108}}\n\\] Numerically, the \\(z\\)-value is\n\n(0.09259 - 0.05) / sqrt(0.05 * (1 - 0.05) / 108)\n\n[1] 2.031\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|0.09259 - 0.05| - 1/216}{\\sqrt{0.05(1-0.05)/108}}\n\\] Numerically,\n\n(abs(0.09259 - 0.05) - 1/216) / sqrt(0.05 * (1 - 0.05) / 108)\n\n[1] 1.81\n\n\nFor the critical-value method, we compare this \\(z\\)-statistic to \\(z_{1 - \\alpha/2}\\). Let \\(\\alpha = 0.05\\)\n\nqnorm(1 - 0.05 / 2)\n\n[1] 1.96\n\n\nSince \\(|z| &lt; z_{1-\\alpha}\\) (1.81 &lt; 1.96), we fail to reject \\(H_0\\) and conclude that we do not have evidence that diarrhea rate differs from the country as a whole.\n\n\n\n\n7.22Solution\n\n\nReport a p-value corresponding to your answer to Problem 7.21\n\n\nWe could alternatively calculate \\(P(Z &gt; |z|)\\) given \\(Z \\sim N(0,1)\\).\n\n\n\n\n\n\n\n\n\nNumerically, the \\(p\\)-value is\n\n2 * pnorm(q = -1.81)\n\n[1] 0.0703\n\n\nSince this is greater than 0.05, we again fail to reject at significance level 0.05 and conclude that we do not have evidence that the diarrhea rate differs than the country as a whole.\nYou can also use prop.test() to do this\n\nprop.test(x = 10, n = 108, p = 0.05) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0703\n\n\nNote that the \\(p\\)-value is still somewhat small. So we might say we have weak evidence."
  },
  {
    "objectID": "review/pp_07.html#ophthalmology",
    "href": "review/pp_07.html#ophthalmology",
    "title": "Chapter 7 Practice Problems",
    "section": "Ophthalmology",
    "text": "Ophthalmology\nSuppose the distribution of systolic blood pressure in the general population is normal with a mean of 130 mm Hg and a standard deviation of 20 mm Hg. In a special subgroup of 85 people with glaucoma, we find that the mean systolic blood pressure is 135 mm Hg with a standard deviation of 22 mm Hg.\n\n7.23Solution\n\n\nAssuming that the standard deviation of the glaucoma patients is the same as that of the general population, test for an association between glaucoma and high blood pressure.\n\n\nLet \\(X_i\\) be the systolic blood pressure for glaucoma patient \\(i\\). Then we assume \\(X \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0:\\mu = 130\\) versus \\(H_A: \\mu \\neq 130\\). We are told that \\(\\bar{x} = 135\\) and \\(n = 85\\). We are further told to assume that we know \\(\\sigma = 20\\). We calculate a \\(z\\)-statistic \\[\nz = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{135 - 130}{20 / \\sqrt{85}}\n\\] Numerically\n\n(135 - 130) / (20 / sqrt(85))\n\n[1] 2.305\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.305)\n\n[1] 0.02117\n\n\nSo we have some evidence that the systolic blood pressure differs in the glaucoma group than the general population.\n\n\n\n\n7.24Solution\n\n\nAnswer Problem 7.23 without making the assumption concerning the standard deviation.\n\n\nWe are in the same set up as in 7.23, except we are told that \\(s = 22\\) and we cannot assume that we know \\(\\sigma\\). We calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{135 - 130}{22 / \\sqrt{85}}\n\\] Numerically\n\n(135 - 130) / (22 / sqrt(85))\n\n[1] 2.095\n\n\nWe compare this to a \\(t_{84}\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pt(q = -2.095, df = 84)\n\n[1] 0.03918\n\n\nAgain, we have evidence that the glaucoma group has a different SBP than the population as a whole."
  },
  {
    "objectID": "review/pp_07.html#occupational-health-1",
    "href": "review/pp_07.html#occupational-health-1",
    "title": "Chapter 7 Practice Problems",
    "section": "Occupational Health 1",
    "text": "Occupational Health 1\nSuppose that 28 cancer deaths are noted among workers exposed to asbestos in a building-materials plant from 1981–1985. Only 20.5 cancer deaths are expected from statewide cancer-mortality rates.\n\n7.17Solution\n\n\nWhat is the estimated SMR for total cancer mortality?\n\n\nSMR is \\[\n\\frac{\\text{observed}}{\\text{expected}}\\times 100\\% = \\frac{28}{20.5}\\times 100\\%\n\\]\n\n28 / 20.5 * 100\n\n[1] 136.6\n\n\nSo 136.6%.\n\n\n\n\n7.18Solution\n\n\nIs there a significant excess or deficit of total cancer deaths among these workers?\n\n\nWe use the poisson test. Let \\(X\\) be the number of cancer deaths. Then we assume \\(X \\sim \\mathrm{Pois}(\\mu)\\). We are testing \\(H_0: \\mu = 20.5\\) versus \\(H_A: \\mu &gt; 20.5\\). The \\(p\\)-value of the exact test is just \\(P(X \\geq 28)\\) assuming \\(X \\sim \\mathrm{Pois}(20.5)\\), since that would be the probability of seeing values as or more extreme than what we saw:\n\n\n\n\n\n\n\n\n\n\n1 - ppois(q = 27, lambda = 20.5)\n\n[1] 0.06632\n\n\nYou can also get this with poisson.test()\n\npoisson.test(x = 28, alternative = \"greater\", r = 20.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0663\n\n\nThe \\(p\\)-value is somewhat small. So we perhaps have some weak evidence of larger cancer rates. But the evidence is very weak.\n\n\n\nIn the same group of workers, 7 deaths due to leukemia are noted. Only 4.5 are expected from statewide rates.\n\n7.19Solution\n\n\nWhat is the estimated SMR for leukemia?\n\n\nSMR is \\[\n\\frac{\\text{observed}}{\\text{expected}}\\times 100\\% = \\frac{7}{4.5}\\times 100\\%\n\\]\n\n7 / 4.5 * 100\n\n[1] 155.6\n\n\nSo 155.6%.\n\n\n\n\n7.20Solution\n\n\nIs there a significant excess or deficit of leukemia deaths among these workers?\n\n\nLet \\(Y\\) be the number of leukemia deaths. Then we assume \\(Y \\sim \\mathrm{Pois}(\\mu)\\). We are testing \\(H_0: \\mu = 4.5\\) versus \\(H_A: \\mu \\neq 4.5\\). We do an exact Poisson test here, summing over all probabilities less (or as) likely than the observed value of 7.\n\n\n\n\n\n\n\n\n\nThis is done via poisson.test()\n\npoisson.test(x = 7, r = 4.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.230\n\n\nSince the \\(p\\)-value is pretty large, we do not have evidence that the leukemia rates differ in this group.\n\n\n\n\n\n\nTipNote\n\n\n\nNot for the exam, but for your own edification. The below shows that R’s default way is what we discussed:\n\nx &lt;- dpois(x = 0:20, lambda = 4.5)\nsum(x[x &lt;= x[[8]]])\n\n[1] 0.23\n\n\nRosner does 2 times the larger one-tailed \\(p\\)-value:\n\n2 * (1 - ppois(q = 6, lambda = 4.5))\n\n[1] 0.3379\n\n\nSo if you read his solution, the \\(p\\)-value will differ."
  },
  {
    "objectID": "review/pp_07.html#cancer",
    "href": "review/pp_07.html#cancer",
    "title": "Chapter 7 Practice Problems",
    "section": "Cancer",
    "text": "Cancer\n\n7.26Solution\n\n\nAn area of current interest in cancer epidemiology is the possible role of oral contraceptives (OC’s) in the development of breast cancer. Suppose that in a group of 1000 premenopausal women ages 40–49 who are current users of OC’s, 15 subsequently develop breast cancer over the next 5 years. If the expected 5-year incidence rate of breast cancer in this group is 1.2% based on national incidence rates, then test the hypothesis that there is an association between current OC use and the subsequent development of breast cancer."
  },
  {
    "objectID": "review/pp_07.html#cancer-1",
    "href": "review/pp_07.html#cancer-1",
    "title": "Chapter 7 Practice Problems",
    "section": "Cancer 1",
    "text": "Cancer 1\n\n7.26Solution\n\n\nAn area of current interest in cancer epidemiology is the possible role of oral contraceptives (OC’s) in the development of breast cancer. Suppose that in a group of 1000 premenopausal women ages 40–49 who are current users of OC’s, 15 subsequently develop breast cancer over the next 5 years. If the expected 5-year incidence rate of breast cancer in this group is 1.2% based on national incidence rates, then test the hypothesis that there is an association between current OC use and the subsequent development of breast cancer.\n\n\nLet \\(X\\) be the number with breast cancer after 5 years. Then \\(X \\sim \\mathrm{Binom}(1000, p)\\). We want to test \\(H_0: p = 0.012\\) versus \\(H_A: p \\neq 0.012\\). We are told that \\(x = 15\\) was observed, so \\(\\hat{p} = 15/1000 = 0.015\\). We can use a normal approximation since \\(np_0(1-p_0) &gt; 5\\)\n\n1000 * 0.012 * (1 - 0.012)\n\n[1] 11.86\n\n\nWe calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.015 - 0.012}{\\sqrt{0.012(1-0.012)/1000}}\n\\] Numerically\n\n(0.015 - 0.012) / sqrt(0.012 * (1 - 0.012) / 1000)\n\n[1] 0.8713\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|0.015 - 0.012| - 1/2000}{\\sqrt{0.012(1-0.012)/1000}}\n\\] Numerically,\n\n(abs(0.015 - 0.012) - 1/2000) / sqrt(0.012 * (1 - 0.012) / 1000)\n\n[1] 0.7261\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-0.7261)\n\n[1] 0.4678\n\n\nSince the \\(p\\)-value is so large, we have no evidence that these women have a different breast cancer rate than the general population.\nIn R, we would use prop.test()\n\nprop.test(x = 15, n = 1000, p = 0.012) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.468"
  },
  {
    "objectID": "review/pp_07.html#renal-disease",
    "href": "review/pp_07.html#renal-disease",
    "title": "Chapter 7 Practice Problems",
    "section": "Renal Disease",
    "text": "Renal Disease\nThe level of serum creatinine in the blood is considered a good indicator of the presence or absence of kidney disease. Normal people generally have low concentrations of serum creatinine, whereas diseased people have high concentrations. Suppose we want to look at the relation between analgesic abuse and kidney disorder. In particular, suppose we look at 15 people working in a factory who are known to be “analgesic abusers” (i.e., they take more than 10 pills per day) and we measure their creatinine levels. The creatinine levels are\n\ncret &lt;- tibble(\n  creatine = c(0.9, 1.1, 1.6, 2.0, 0.8,\n               0.7, 1.4, 1.2, 1.5, 0.8,\n               1.0, 1.1, 1.4, 2.2, 1.4)\n)\ncret\n\n# A tibble: 15 × 1\n   creatine\n      &lt;dbl&gt;\n 1      0.9\n 2      1.1\n 3      1.6\n 4      2  \n 5      0.8\n 6      0.7\n 7      1.4\n 8      1.2\n 9      1.5\n10      0.8\n11      1  \n12      1.1\n13      1.4\n14      2.2\n15      1.4\n\n\n\n7.29Solution\n\n\nIf we assume that creatinine levels for normal people are normally distributed with mean 1.0, then can we make any comment about the levels for analgesic abusers via some statistical test?\n\n\n\n\n\n\n\n7.30Solution\n\n\nSuppose the standard deviation of serum creatinine in the general population = 0.40. Can we compare the variance of serum creatinine among analgesic abusers versus the variance of serum creatinine in the general population?"
  },
  {
    "objectID": "review/pp_07.html#otolaryngology",
    "href": "review/pp_07.html#otolaryngology",
    "title": "Chapter 7 Practice Problems",
    "section": "Otolaryngology",
    "text": "Otolaryngology\nOtitis media is an extremely common disease of the middle ear in children under 2 years of age. It can cause prolonged hearing loss during this period and may result in subsequent defects in speech and language. Suppose we wish to design a study to test the latter hypothesis, and we set up a study group consisting of children with 3 or more episodes of otitis media in the first 2 years of life. We have no idea what the size of the effect will be. Thus, we set up a pilot study with 20 cases, and find that 5 of the cases have speech and language defects at age 3.\n\n7.31Solution\n\n\nIf we regard this experience as representative of what would occur in a large study and if we know that 15%of all normal children have speech and language defects by age 3, then how large of a study group is needed to have an 80% chance of detecting a significant difference using a one-sided test at the 5% level?\n\n\n\n\n\n\n\n7.32Solution\n\n\nSuppose only 50 cases can be recruited for the study group. How likely are we to find a significant difference if the true proportion of affected children with speech and language defects at age 3 is the same as that in the pilot study?"
  },
  {
    "objectID": "review/pp_07.html#epidemiology",
    "href": "review/pp_07.html#epidemiology",
    "title": "Chapter 7 Practice Problems",
    "section": "Epidemiology",
    "text": "Epidemiology\nHeight and weight are often used in epidemiological studies as possible predictors of disease outcomes. If the people in the study are assessed in a clinic, then heights and weights are usually measured directly. However, if the people are interviewed at home or by mail, then a person’s self-reported height and weight are often used instead. Suppose we conduct a study on 10 people to test the comparability of these two methods. The data for weight are given in the following data frame.\n\nwt &lt;- tibble(\n  Subject_number = 1:10,\n  Self_reported_weight = c(120, 120, 135, 118, 120, 190, 124, 175, 133, 125),\n  Measured_weight = c(125, 118, 139, 120, 125, 198, 128, 176, 131, 125),\n  Difference = c(-5, 2, -4, -2, -5, -8, -4, -1, 2, 0)\n)\nwt\n\n# A tibble: 10 × 4\n   Subject_number Self_reported_weight Measured_weight Difference\n            &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n 1              1                  120             125         -5\n 2              2                  120             118          2\n 3              3                  135             139         -4\n 4              4                  118             120         -2\n 5              5                  120             125         -5\n 6              6                  190             198         -8\n 7              7                  124             128         -4\n 8              8                  175             176         -1\n 9              9                  133             131          2\n10             10                  125             125          0\n\n\n\n7.33Solution\n\n\nShould a one-sided or two-sided test be used here?\n\n\nTwo-sided. We are just interested in if self-reported is comparable to measured.\n\n\n\n\n7.34Solution\n\n\nWhich test procedure should be used to test the preceding hypothesis?\n\n\nA one-sample two-sided \\(t\\)-test on the differences.\nLet \\(X_i\\) be the difference in self-reported and measured weight on subject \\(i\\). Then we assume \\(X_i \\sim N(\\mu,\\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu \\neq 0\\).\n\n\n\n\n7.35Solution\n\n\nConduct the test in Problem 7.34 using the critical-value method with α = .05.\n\n\nWe calculate summary statistics\n\nwt |&gt;\n  summarize(mean = mean(Difference), sd = sd(Difference), n = n())\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  -2.5  3.27    10\n\n\nWe calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{-2.5 - 0}{3.274 / \\sqrt{10}}\n\\] Numerically\n\n-2.5 / (3.274 / sqrt(10))\n\n[1] -2.415\n\n\nWe compare this to \\(t_{n-1,1 - \\alpha/2}\\) with \\(\\alpha = 0.05\\)\n\nqt(p = 1 - 0.05 / 2, df = 10 - 1)\n\n[1] 2.262\n\n\nSince \\(|t| &gt; t_{n-1,1-\\alpha/2}\\) (2.415 &gt; 2.262), reject the null at significance level 0.05 and conclude that we have evidence that the mean self-reported weights differ from the mean measured weights.\n\n\n\n\n7.36Solution\n\n\nCompute the p-value for the test in Problem 7.34.\n\n\nWe compare this \\(t\\)-statistic to a \\(t_{9}\\) distribution\n\nplt_t(ub = -2.415, two_sided = TRUE, df = 9) +\n  geom_vline(xintercept = -2.415, lty = 2)\n\n\n\n\n\n\n\n\n\n2 * pt(-2.415, df = 14)\n\n[1] 0.02999\n\n\nSince the \\(p\\)-value is less than 0.05, we reject at significance level 0.05 and conclude that we have evidence that the mean self-reported weights differ from the mean measured weights.\n\n\n\n\n7.37Solution\n\n\nIs there evidence of digit preference among the self-reported weights? Specifically, compare the observed proportion of self-reported weights whose last digit is 0 or 5 with the expected proportion based on chance and report a p-value.\n\n\nThe expected proportion based on chance is 2/10 = 0.2 (each digit is equally likely). Let \\(X\\) be the number of individuals with a last digit of either 2 or 0. Then \\(X \\sim \\mathrm{Binom}(10, p)\\). We want to test \\(H_0: p = 0.2\\) versus \\(H_0: p &gt; 0.2\\). I am using a one-sided test since we suspect 0’s and 5’s are more likely. We observe \\(x = 7\\) individuals with either a 0 or a 5 as a last digit. We cannot use the normal approximation because \\(np_0(1-p_0) &lt; 5\\)\n\n10 * 0.2 * (1 - 0.2)\n\n[1] 1.6\n\n\nThe exact \\(p\\)-value is the \\(P(X \\geq 7)\\) given \\(X \\sim \\mathrm{Binom}(10, 0.2)\\). You cannot even see the mass on the PMF below, so the \\(p\\)-value is going to be tiny:\n\nplt_binom(size = 10, prob = 0.2, lb = 7)\n\n\n\n\n\n\n\n\nWe can calculate this by\n\n1 - pbinom(q = 6, size = 10, prob = 0.2)\n\n[1] 0.0008644\n\n\nAlternative, we can use binom.test()\n\nbinom.test(x = 7, n = 10, p = 0.2, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n   p.value\n     &lt;dbl&gt;\n1 0.000864\n\n\nWe thus have very strong evidence that there are more 0’s and 5’s as last digits than would happen by chance."
  },
  {
    "objectID": "review/pp_07.html#hypertension",
    "href": "review/pp_07.html#hypertension",
    "title": "Chapter 7 Practice Problems",
    "section": "Hypertension",
    "text": "Hypertension\nSeveral studies have been performed relating urinary potassium excretion to blood-pressure level. These studies have tended to show an inverse relationship between these two variables, with the higher the level of potassium excretion, the lower the BP level. Therefore, a treatment trial is planned to look at the effect of potassium intake in the form of supplement capsules on changes in DBP level. Suppose that in a pilot study, 20 people are given potassium supplements for 1 month. The data are as follows:\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean change (1 month-baseline)\n-3.2\n\n\nsd change\n8.5\n\n\nn\n20.0\n\n\n\n\n\n\n\n\n7.38Solution\n\n\nWhat test should be used to assess if potassium supplements have any effect on DBP level?\n\n\nA one-sample \\(t\\)-test. Let \\(X_i\\) be the change in DBP level for individual \\(i\\). Then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu \\neq 0\\). Note that if \\(\\mu &gt; 0\\) then the DBP is higher after 1 month, and if \\(\\mu &lt; 0\\) then the DBP is lower after 1 month.\n\n\n\n\n7.39Solution\n\n\nPerform the test in Problem 7.38 using a two-sided test and report the p-value.\n\n\nWe are told that \\(\\bar{x} = -3.2\\), \\(s = 8.5\\), and \\(n = 20\\). We calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{-3.2 - 0}{8.5 / \\sqrt{20}}\n\\] Numerically\n\n-3.2 / (8.5 / sqrt(20))\n\n[1] -1.684\n\n\nWe compare this to a \\(t_{19}\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pt(q = -1.684, df = 19)\n\n[1] 0.1085\n\n\nThe \\(p\\)-value is pretty large, so we do not have evidence that the mean difference is different from 0 after one month.\n\n\n\n\n7.40Solution\n\n\nDerive a 95% CI for the true mean change based on the preceding data. What is the relationship of your results here and in Problem 7.39?\n\n\nThis is \\[\n\\bar{x} \\pm t_{n-1, 1-\\alpha/2}\\frac{s}{\\sqrt{n}}\n\\] Where \\(t_{n-1, 1-\\alpha/2}\\) with \\(\\alpha = 0.05\\) and \\(n = 20\\) is\n\nqt(1 - 0.05 / 2, df = 19)\n\n[1] 2.093\n\n\nPlugging in values \\[\n-3.2 \\pm 2.093 \\times \\frac{8.5}{\\sqrt{20}}\n\\] Numerically\n\n-3.2 - 2.093 * 8.5 / sqrt(20)\n\n[1] -7.178\n\n-3.2 + 2.093 * 8.5 / sqrt(20)\n\n[1] 0.7781\n\n\nIn 7.39, the \\(p\\)-value (against the null of \\(H_0: \\mu = 0\\)) was greater than 0.05. This shows up here because 0 is within the 95% confidence interval\n\n\n\n\n7.41Solution\n\n\nHow many subjects need to be studied to have an 80% chance of detecting a significant treatment effect using a two-sided test with an α level of .05 if the mean and sd of the pilot study are assumed to be the population mean and sd?\n\n\nWe need a sample size of 58 individuals.\n\npower.t.test(\n  delta = 3.2, \n  sd = 8.5, \n  sig.level = 0.05, \n  power = 0.8, \n  type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 57.33\n          delta = 3.2\n             sd = 8.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided"
  },
  {
    "objectID": "review/pp_07.html#hospital-epidemiology",
    "href": "review/pp_07.html#hospital-epidemiology",
    "title": "Chapter 7 Practice Problems",
    "section": "Hospital Epidemiology",
    "text": "Hospital Epidemiology\nA study was conducted to identify characteristics that would predict 1-year survival for patients admitted to the medical service at New York Hospital. One factor that was considered was the physician’s estimate of the patients’ severity of illness at the time of admission. Suppose that it is expected, based on previous studies in this hospital, that 2/3 of admitted patients will survive for at least 1 year.\n\n7.42Solution\n\n\nIf 47% of 136 patients deemed severely ill survive at least 1 year, then what test can be used to test the hypothesis that the severity-of-illness rating is predictive of 1-year mortality?\n\n\nLet \\(X\\) be the number of severely ill patients that die. Then \\(X \\sim \\mathrm{Binom}(n, p)\\). We want to test \\(H_0: p = 2/3\\) versus \\(H_A: p \\neq 2/3\\). This is a one-samnple binomial test.\n\n\n\n\n7.43Solution\n\n\nPerform the test in Problem 7.42 using the critical-value method based on an α level of .05.\n\n\nWe can use a normal approximation since \\(np_0(1-p_0) \\geq 5\\)\n\n136 * 2/3 * (1 - 2/3)\n\n[1] 30.22\n\n\nWith \\(\\hat{p} = 0.47\\), we calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.47 - \\frac{2}{3}}{\\sqrt{\\frac{2}{3}(1-\\frac{2}{3})/136}}\n\\] Numerically\n\n(0.47 - 2/3) / sqrt(2/3 * (1 - 2/3) / 136)\n\n[1] -4.865\n\n\nWe compare this to a \\(N(0,1)\\) distribution. The critical value method would compare this \\(z\\)-statistic to \\(z_{1-\\alpha/2}\\)\n\nqnorm(1 - 0.05 / 2)\n\n[1] 1.96\n\n\nSince \\(|z|&gt;z_{1-\\alpha/2}\\) (4.865 &gt; 1.96), we have evidence at the 0.05 significance level that patients deemed “severely ill” have a different survival probability than the general hospital population.\n\n\n\n\n7.44Solution\n\n\nProvide a 95% CI for the 1-year survival rate among severely ill patients.\n\n\nWe calculate \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We can get \\(z_{1-\\alpha/2}\\) where \\(\\alpha = 0.05\\) via\n\nqnorm(1 - 0.05/2)\n\n[1] 1.96\n\n\nPlugging in, we get \\[\n0.47 \\pm 1.96 \\cdot \\sqrt{ \\frac{ 0.47(1 - 0.47) }{136} }\n\\] Numerically, this is\n\n0.47 - 1.96 * sqrt(0.47 * (1 - 0.47) / 136)\n\n[1] 0.3861\n\n0.47 + 1.96 * sqrt(0.47 * (1 - 0.47) / 136)\n\n[1] 0.5539\n\n\n\n\n\nSuppose that it is expected, based on previous studies, that among all patients who survive hospitalization, 75% will survive for 1 year. Furthermore, of the 136 severely ill patients, 33 die during hospitalization, and an additional 39 die during the 1st year, but after hospitalization.\n\n7.45Solution\n\n\nTest the hypothesis that severity of illness is predictive of 1-year mortality among patients who are discharged from the hospital. Report a p-value.\n\n\nLet \\(X\\) be the number of severely ill patients that survived hospitalization that also survived during the first year. Then \\(X \\sim \\mathrm{Binom}(136 - 33, p) = \\mathrm{Binom}(103, p)\\). We want to test \\(H_0: p = 0.75\\) versus \\(H_A: p \\neq 0.75\\). We are told that \\(x = 103 - 39 = 64\\), so \\(\\hat{p} = 64 / 103 = 0.6214\\). We can use a normal approximation since \\(np_0(1-p_0) &gt; 5\\)\n\n103 * 0.75 * (1 - 0.75)\n\n[1] 19.31\n\n\nWe calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.6214 - 0.75}{\\sqrt{0.75(1-0.75)/103}}\n\\] Numerically\n\n(0.6214 - 0.75) / sqrt(0.75 * (1 - 0.75) / 103)\n\n[1] -3.014\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|0.6214 - 0.75| - 1/206}{\\sqrt{0.75(1-0.75)/103}}\n\\] Numerically\n\n(abs(0.6214 - 0.75) - 1/206) / sqrt(0.75 * (1 - 0.75) / 103)\n\n[1] 2.9\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.9)\n\n[1] 0.003732\n\n\nSince the \\(p\\)-value is so small, we have strong evidence that severly ill patients have a different 1-year survival rate post-hospital.\nIn R, we would use prop.test()\n\nprop.test(x = 64, n = 103, p = 0.75) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1 0.00372"
  },
  {
    "objectID": "review/pp_07.html#cancer-2",
    "href": "review/pp_07.html#cancer-2",
    "title": "Chapter 7 Practice Problems",
    "section": "Cancer 2",
    "text": "Cancer 2\nA group of investigators wishes to explore the relationship between the use of hair dyes and the development of breast cancer in females. A group of 1000 beauticians 30–39 years of age is identified and followed for 5 years. After 5 years, 20 new cases of breast cancer have occurred. Assume that breast-cancer incidence over this time period for an average woman in this age group is 7/1000. We wish to test the hypothesis that using hair dyes increases the risk of breast cancer.\n\n7.27Solution\n\n\nIs a one-sided or two-sided test appropriate here?\n\n\nOne-sided. We are interested in one direction (hair dies increase the risk).\nLet \\(X\\) be the number of breast cancer cases. Then \\(X \\sim \\mathrm{Binom}(1000, p)\\). We are testing \\(H_0: p = 7/1000\\) versus \\(H_A: p &gt; 7/1000\\).\n\n\n\n\n7.28Solution\n\n\nTest the hypothesis.\n\n\nSince \\(np_0(1-p_0) \\geq 5\\), we can use the normal approximation.\n\n1000 * 0.007 * (1 - 0.007)\n\n[1] 6.951\n\n\nWith \\(\\hat{p} = 20 / 1000\\), we calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.02 - 0.007}{\\sqrt{0.007(1-0.007)/1000}}\n\\] Numerically\n\n(0.02 - 0.007) / sqrt(0.007 * (1 - 0.007) / 1000)\n\n[1] 4.931\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{\\hat{p} - p_0 - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.02 - 0.007 - 1/2000}{\\sqrt{0.007(1-0.007)/1000}}\n\\] Numerically\n\n(0.02 - 0.007 - 1/2000) / sqrt(0.007 * (1 - 0.007) / 1000)\n\n[1] 4.741\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n1 - pnorm(4.741)\n\n[1] 1.063e-06\n\n\nSince the \\(p\\)-value is so small, we have strong evidence that these women have a higher breast cancer rate than the general population.\nIn R, we would use prop.test()\n\nprop.test(x = 20, n = 1000, p = 0.007, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n     p.value\n       &lt;dbl&gt;\n1 0.00000106"
  },
  {
    "objectID": "review/pp_07.html#renal-disease-1",
    "href": "review/pp_07.html#renal-disease-1",
    "title": "Chapter 7 Practice Problems",
    "section": "Renal Disease 1",
    "text": "Renal Disease 1\nThe level of serum creatinine in the blood is considered a good indicator of the presence or absence of kidney disease. Normal people generally have low concentrations of serum creatinine, whereas diseased people have high concentrations. Suppose we want to look at the relation between analgesic abuse and kidney disorder. In particular, suppose we look at 15 people working in a factory who are known to be “analgesic abusers” (i.e., they take more than 10 pills per day) and we measure their creatinine levels. The creatinine levels are\n\ncret &lt;- tibble(\n  creatine = c(0.9, 1.1, 1.6, 2.0, 0.8,\n               0.7, 1.4, 1.2, 1.5, 0.8,\n               1.0, 1.1, 1.4, 2.2, 1.4)\n)\ncret\n\n# A tibble: 15 × 1\n   creatine\n      &lt;dbl&gt;\n 1      0.9\n 2      1.1\n 3      1.6\n 4      2  \n 5      0.8\n 6      0.7\n 7      1.4\n 8      1.2\n 9      1.5\n10      0.8\n11      1  \n12      1.1\n13      1.4\n14      2.2\n15      1.4\n\n\n\n7.29Solution\n\n\nIf we assume that creatinine levels for normal people are normally distributed with mean 1.0, then can we make any comment about the levels for analgesic abusers via some statistical test?\n\n\nLet \\(X_i\\) be the creatine level for analgesic abuser \\(i\\). Then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 1\\) versus \\(H_A: \\mu \\neq 1\\). We calculate summary statistics\n\ncret |&gt;\n  summarize(\n    mean = mean(creatine),\n    sd = sd(creatine),\n    n = n())\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  1.27 0.435    15\n\n\nWe calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{1.273 - 1}{0.435 / \\sqrt{15}}\n\\] Numerically\n\n(1.273 - 1) / (0.435 / sqrt(15))\n\n[1] 2.431\n\n\nWe compare this to a \\(t_{14}\\) distribution\n\nplt_t(lb = 2.431, two_sided = TRUE, df = 14) +\n  geom_vline(xintercept = 2.431, lty = 2)\n\n\n\n\n\n\n\n\n\n2 * pt(-2.431, df = 14)\n\n[1] 0.02909\n\n\nSince the \\(p\\)-value is so small, we have evidence that the mean creatine levels of the analgesic abusers is different than normal folks.\nThe real-way in R uses t.test()\n\nt.test(creatine ~ 1, data = cret, mu = 1) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0289\n\n\nThe difference is caused by rounding error when calculating the \\(t\\)-statistic."
  },
  {
    "objectID": "review/pp_07.html#cardiovascular-disease",
    "href": "review/pp_07.html#cardiovascular-disease",
    "title": "Chapter 7 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nThe relationship between serum cholesterol and coronary-heart-disease mortality has been a subject of much debate over the past 30 years. Some data relevant to this question were presented in the Whitehall study. It was shown there that among 3615 men 40–64 years of age who were in the top quintile of serum cholesterol at baseline, 194 died from coronary heart disease over the next 10 years. Suppose the incidence rate of coronary-heart-disease mortality over 10 years among men in this age group in Great Britain is 4%.\n\n7.53Solution\n\n\nWhat test can be performed to compare the incidence rate of coronary-heart-disease mortality among men in the top cholesterol quintile vs general-population incidence rates?\n\n\nLet \\(X\\) be the the number that died. Then \\(X \\sim \\mathrm{Binom}(3615, p)\\). We want to test \\(H_0: p = 0.04\\) versus \\(H_A: p \\neq 0.04\\). We are told that \\(x = 194\\), so \\(\\hat{p} = 194 / 3615\\).\n\n194 / 3615\n\n[1] 0.05367\n\n\nWe would run a one-sample binomial test. We can use a normal approximation since \\(np_0(1-p_0) \\geq 5\\)\n\n3615 * 0.04 * (1 - 0.04)\n\n[1] 138.8\n\n\n\n\n\n\n7.54Solution\n\n\nImplement the test in Problem 7.53 and report a p-value.\n\n\nWe calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.05367 - 0.04}{\\sqrt{0.04(1-0.04)/3615}}\n\\] Numerically\n\n(0.05367 - 0.04) / sqrt(0.04 * (1 - 0.04) / 3615)\n\n[1] 4.194\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|0.05367 - 0.04| - 1 / 7230}{\\sqrt{0.04(1-0.04)/3615}}\n\\] Numerically,\n\n(abs(0.05367 - 0.04) - 1/7230) / sqrt(0.04 * (1 - 0.04) / 3615)\n\n[1] 4.152\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-4.152)\n\n[1] 3.296e-05\n\n\nSince the \\(p\\)-value is so small, we have strong evidence that coronary heart disease mortality rate for this group differs than that of the general population.\nIn R, we would use prop.test()\n\nprop.test(x = 194, n = 3615, p = 0.04) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n    p.value\n      &lt;dbl&gt;\n1 0.0000332\n\n\n\n\n\n\n7.55Solution\n\n\nConstruct a 95% CI for the true incidence rate in the group of 40–64-year-old men in the top quintile of serum cholesterol.\n\n\nWe calculate \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We can get \\(z_{1-\\alpha/2}\\) where \\(\\alpha = 0.05\\) via\n\nqnorm(1 - 0.05/2)\n\n[1] 1.96\n\n\nPlugging in, we get \\[\n0.05367 \\pm 1.96 \\cdot \\sqrt{ \\frac{0.05367(1 - 0.05367)}{3615} }\n\\] Numerically, this is\n\n0.05367 - 1.96 * sqrt(0.05367 * (1 - 0.05367) / 3615)\n\n[1] 0.04632\n\n0.05367 + 1.96 * sqrt(0.05367 * (1 - 0.05367) / 3615)\n\n[1] 0.06102\n\n\nIn R, we would use prop.test()\n\nprop.test(x = 194, n = 3615, p = 0.04) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.0467    0.0616\n\n\nAgain, the CI’s differs because R uses a different (better) method."
  },
  {
    "objectID": "review/pp_07.html#cancer-3",
    "href": "review/pp_07.html#cancer-3",
    "title": "Chapter 7 Practice Problems",
    "section": "Cancer 3",
    "text": "Cancer 3\nRadiotherapy is a common treatment for breast cancer, with about 25% of women receiving this form of treatment. Assume that the figure 25% is based on a very large sample and is known without error. One hypothesis is that radiotherapy applied to the contralateral breast may be a risk factor for development of breast cancer in the opposite breast 5 or more years after the initial cancer. Suppose that 655 women are identified who developed breast cancer in the opposite breast 5 or more years after the initial cancer.\n\n7.46Solution\n\n\nIf 206 of the women received radiotherapy after their initial diagnosis, then test the hypothesis that radiotherapy is associated with the development of breast cancer in the opposite breast. Please report a p-value.\n\n\nLet \\(X\\) be the number who received radiotherapy after their initial diagnosis among the 655 women. Then \\(X \\sim \\mathrm{Binom}(655, p)\\). Since 25% of breast cancer patients get radio therapy, we would expect about a quarter of these 655 women to have done so. If we see more than a quarter, this might indicate that radiotherapy women are overrepresented in cases. We want to test if \\(H_0: p = 0.25\\) versus \\(H_A: p \\neq 0.25\\). We are told that \\(x = 206\\), or \\(\\hat{p} = 206 / 655 = 0.3145\\). We can use a normal approximation since \\(np_0(1-p_0) &gt; 5\\)\n\n655 * 0.25 * (1 - 0.25)\n\n[1] 122.8\n\n\nWe calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.3145 - 0.25}{\\sqrt{0.25(1-0.25)/655}}\n\\] Numerically\n\n(0.3145 - 0.25) / sqrt(0.25 * (1 - 0.25) / 655)\n\n[1] 3.812\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|0.3145 - 0.25| - 1/1310}{\\sqrt{0.25(1-0.25)/655}}\n\\] Numerically,\n\n(abs(0.3145 - 0.25) - 1/1310) / sqrt(0.25 * (1 - 0.25) / 655)\n\n[1] 3.767\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-3.767)\n\n[1] 0.0001652\n\n\nSince the \\(p\\)-value is so small, we have strong evidence that radiotherapy is associated with breast cancer in the opposite breast.\nIn R, we would use prop.test()\n\nprop.test(x = 206, n = 655, p = 0.25) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n   p.value\n     &lt;dbl&gt;\n1 0.000165\n\n\n\n\n\n\n7.47Solution\n\n\nProvide a 95% CI for the true proportion of women with contralateral breast cancer who received radiotherapy treatment.\n\n\nWe calculate \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We can get \\(z_{1-\\alpha/2}\\) where \\(\\alpha = 0.05\\) via\n\nqnorm(1 - 0.05/2)\n\n[1] 1.96\n\n\nPlugging in, we get \\[\n0.3145 \\pm 1.96 \\cdot \\sqrt{ \\frac{ 0.3145(1 - 0.3145) }{655} }\n\\] Numerically, this is\n\n0.3145 - 1.96 * sqrt(0.3145 * (1 - 0.3145) / 655)\n\n[1] 0.2789\n\n0.3145 + 1.96 * sqrt(0.3145 * (1 - 0.3145) / 655)\n\n[1] 0.3501\n\n\nIn R, we would use prop.test()\n\nprop.test(x = 206, n = 655, p = 0.25) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.279     0.352\n\n\nThe CI’s differs because R uses a different (better) method.\n\n\n\n\n7.48Solution\n\n\nSuppose that our p-value in Problem 7.46 = .03 (this is not necessarily the actual p-value in Problem 7.46). If we conduct a test using the critical-value method with α = .05, then would we accept or reject \\(H_0\\) and why? (Do not actually conduct the test.)\n\n\nReject. The \\(p\\)-value is less than \\(\\alpha\\).\n\n\n\n\n7.49Solution\n\n\nSuppose we wanted a 99% CI instead of a 95% CI in Problem 7.47. Would the length of the 99% CI be narrower, wider, or the same as the CI in Problem 7.47? (Do not actually construct the interval.)\n\n\nWider. Higher confidence requires larger intervals."
  },
  {
    "objectID": "review/pp_07.html#nutrition-2",
    "href": "review/pp_07.html#nutrition-2",
    "title": "Chapter 7 Practice Problems",
    "section": "Nutrition 2",
    "text": "Nutrition 2\nA food-frequency questionnaire was mailed to 20 subjects to assess the intake of various food groups. The sample standard deviation of vitamin-C intake over the 20 subjects was 15 (exclusive of vitamin-C supplements). Suppose we know from using an in-person diet-interview method in a large previous study that the standard deviation is 20.\n\n7.13Solution\n\n\nWhat hypotheses can be used to test if there are any differences between the standard deviations of the two methods?\n\n\n\n\n\n\n\n7.14Solution\n\n\nPerform the test described in Problem 7.13 and report a p-value.\n\n\n\n\n\n\nThe sample standard deviation for log (vitamin-A intake) exclusive of supplements based on the 20 subjects using the food-frequency questionnaire was 0.016. Suppose the standard deviation from the diet-interview method is known to be 0.020 based on a large previous study.\n\n7.15Solution\n\n\nTest the hypothesis that the variances using the two methods are the same. Use the critical-value method with a \\(\\alpha\\) = .05\n\n\n\n\n\n\n\n7.16Solution\n\n\nReport a p-value corresponding to the test in Problem 7.15."
  },
  {
    "objectID": "review/pp_07.html#occupational-health-2",
    "href": "review/pp_07.html#occupational-health-2",
    "title": "Chapter 7 Practice Problems",
    "section": "Occupational Health 2",
    "text": "Occupational Health 2\n\n7.25Solution\n\n\nSuppose it is known that the average life expectancy of a 50-year-old man in 1945 was 18.5 years. Twenty men aged 50 who have been working for at least 20 years in a potentially hazardous industry were ascertained in 1945. On follow-up in 1985 all the men have died, with an average lifetime of 16.2 years and a standard deviation of 7.3 years since 1945. Assuming that life expectancy of 50-year-old men is approximately normally distributed, test if the underlying life expectancy for workers in this industry is shorter than for comparably aged men in the general population.\n\n\nLet \\(X_i\\) be the lifetime of worker \\(i\\). Then we assume that \\(X_i \\sim N(\\mu, \\sigma^2)\\). We are testing \\(H_0: \\mu = 18.5\\) versus \\(H_A: \\mu &lt; 18.5\\). We are told that \\(\\bar{x} = 16.2\\), \\(s = 7.3\\), and \\(n = 20\\). We calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{16.2 - 18.5}{7.3 / \\sqrt{20}}\n\\] Numerically\n\n(16.2 - 18.5) / (7.3 / sqrt(20))\n\n[1] -1.409\n\n\nWe compare this to a \\(t_{19}\\) distribution\n\n\n\n\n\n\n\n\n\n\npt(q = -1.409, df = 19)\n\n[1] 0.0875\n\n\nThe \\(p\\)-value is pretty large, so we have at best weak evidence that the average lifetime is less than the general population."
  },
  {
    "objectID": "review/pp_07.html#cancer-4",
    "href": "review/pp_07.html#cancer-4",
    "title": "Chapter 7 Practice Problems",
    "section": "Cancer 4",
    "text": "Cancer 4\nIt is well established that exposure to ionizing radiation at or after puberty increases a woman’s risk of breast cancer. However, it is uncertain whether such exposure early in life is also carcinogenic. A study was performed in a cohort of 1201 women who received x-ray treatment in infancy for an enlarged thymus gland and were followed prospectively for 36 years. It was found that 22 breast cancers occurred over a 36-year period among the 1201 women.\n\n7.56Solution\n\n\nIf the expected incidence rate of breast cancer in this age group over this time period is 1 event per 200 women based on New York State cancer-incidence rates, then test the hypothesis that irradiation has an effect on breast-cancer incidence. Please report a p-value and construct a 95% CI for the incidence rate among the exposed group.\n\n\nLet \\(X\\) be the number of breast cancer cases. Then \\(X \\sim \\mathrm{Binom}(1201, p)\\). We want to test \\(H_0: p = 1/200\\) versus \\(H_A: p \\neq 1/200\\). We are told that \\(x = 22\\), so \\(\\hat{p} = 22/1201\\)\n\n22/1201\n\n[1] 0.01832\n\n\nWe can use a normal approximation since \\(np_0(1-p_0) \\geq 5\\)\n\n1201 * 1/200 * (1 - 1/200)\n\n[1] 5.975\n\n\nWe calculate a \\(z\\)-statistic \\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.01832 - 0.005}{\\sqrt{0.005(1-0.005)/1201}}\n\\] Numerically\n\n(0.01832 - 0.005) / sqrt(0.005 * (1 - 0.005) / 1201)\n\n[1] 6.545\n\n\nThough, it is better to use a continuity correction \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|0.01832 - 0.005| - 1/2402}{\\sqrt{0.005(1-0.005)/1201}}\n\\] Numerically,\n\n(abs(0.01832 - 0.005) - 1/2402) / sqrt(0.005 * (1 - 0.005) / 1201)\n\n[1] 6.34\n\n\nWe compare this to a \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-6.34)\n\n[1] 2.298e-10\n\n\nSince the \\(p\\)-value is so small, we have very strong evidence the there is excess risk among the irradiated group.\nFor a 95% confidence interval, we calculate \\[\n\\hat{p} \\pm z_{1 - \\alpha/2} \\cdot \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\] We can get \\(z_{1-\\alpha/2}\\) where \\(\\alpha = 0.05\\) via\n\nqnorm(1 - 0.05/2)\n\n[1] 1.96\n\n\nPlugging in, we get \\[\n0.01832 \\pm 1.96 \\cdot \\sqrt{ \\frac{0.01832(1 - 0.01832)}{1201} }\n\\] Numerically, this is\n\n0.01832 - 1.96 * sqrt(0.01832 * (1 - 0.01832) / 1201)\n\n[1] 0.01074\n\n0.01832 + 1.96 * sqrt(0.01832 * (1 - 0.01832) / 1201)\n\n[1] 0.0259\n\n\nIn R, we would use prop.test() for all of this\n\nprop.test(x = 22, n = 1201, p = 0.005) |&gt;\n  tidy() |&gt;\n  select(p.value, conf.low, conf.high)\n\n# A tibble: 1 × 3\n   p.value conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 2.31e-10   0.0118    0.0281\n\n\nA final summary might be something like this:\n\nWe have strong evidence that women who receive x-ray treatment in infancy for an enlarged thymus gland have a different breast-cancer rate later in life (\\(p &lt; 0.001\\)). We estimate that this rate is about 1.8% (95% CI of 1.2% to 2.8%), which is higher than the national average (0.5%).\n\n\n\n\nIt was also found that 6 breast cancers occurred over a 36-year period among a subgroup of 138 women who were exposed to a high radioactive dose (0.50–1.99 gray, where a gray is a unit of radiation absorbed by the breast tissue).\n\n7.57Solution\n\n\nTest the hypothesis that the high-risk subgroup is at excess risk for breast cancer, and report a p-value.\n\n\nLet \\(X\\) be the number with breast cancer, then \\(X \\sim \\mathrm{Binom}(138, p)\\). We want to test \\(H_0: p = 0.005\\) versus \\(H_A: p \\neq 0.005\\). We are told that \\(x = 6\\), so \\(\\hat{p} = 6 / 138\\)\n\n6 / 138\n\n[1] 0.04348\n\n\nWe cannot use the normal approximation since \\(np_0(1-p_0) &lt; 5\\)\n\n138 * 0.005 * (1 - 0.005)\n\n[1] 0.6866\n\n\nThis sums the probabilities less than or equal to what we saw if \\(X \\sim \\mathrm{Binom}(138, 0.005)\\), zooming in to the picture:\n\nplt_binom(size = 138, prob = 0.005, rng = c(5, 10), valmax = 6)\n\n\n\n\n\n\n\n\nIn R, the\n\nbinom.test(x = 6, n = 138, p = 0.005) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n    p.value\n      &lt;dbl&gt;\n1 0.0000764\n\n\nSo we again have very strong evidence that this group has a different breast cancer rate than the general population."
  },
  {
    "objectID": "review/pp_07.html#renal-disease-2",
    "href": "review/pp_07.html#renal-disease-2",
    "title": "Chapter 7 Practice Problems",
    "section": "Renal Disease 2",
    "text": "Renal Disease 2\nTo compare two methods for the determination of uric acid, 23 blood samples from 23 individuals were divided and analyzed both by the colorimetric method and the uricase method. Suppose the sample means of the colorimetric and uricase assessments were 6.26 and 6.20 mg/dL, respectively, the sample standard deviation of the paired difference between repeated assessments is 0.50 mg/dL, and it is reasonable to assume that this paired difference has a normal distribution.\n\n7.50Solution\n\n\nWhat is the best estimate for the mean difference between approaches?\n\n\nThis is the difference in means, or 6.26 - 6.20\n\n6.26 - 6.20\n\n[1] 0.06\n\n\nSo, 0.06 mg/dL\n\n\n\n\n7.51Solution\n\n\nConstruct a 95% CI for the mean difference.\n\n\nLet \\(X_i\\) be the difference in uric acid measures, then \\(X_i \\sim N(\\mu, \\sigma^2)\\). We want to obtain a 95% CI for \\(\\mu\\). We are told that \\(\\bar(x) = 0.06\\), \\(s = 0.50\\), and \\(n = 23\\). The interval is of the form \\[\n\\bar{x} \\pm t_{n-1, 1 - \\alpha/2}\\frac{s}{\\sqrt{n}}\n\\] The \\(t\\)-quantile can be calculated, for \\(\\alpha = 0.05\\), as\n\nqt(p = 1 - 0.05/2, df = 23 - 1)\n\n[1] 2.074\n\n\nPlugging in, we get \\[\n0.06 \\pm 2.074 \\times \\frac{0.5}{\\sqrt{23}}\n\\] Numerically,\n\n0.06 - 2.074 * 0.5 / sqrt(23)\n\n[1] -0.1562\n\n0.06 + 2.074 * 0.5 / sqrt(23)\n\n[1] 0.2762\n\n\nSo we are 95% confident the mean difference lies between -0.1562 mg/dL and 0.2762 mg/dL.\n\n\n\n\n7.52Solution\n\n\nWhat can be concluded about the difference between assessments by the two methods?\n\n\nWe do not have evidence that the two measures differ on average since the 95% CI includes 0."
  },
  {
    "objectID": "review/pp_08.html",
    "href": "review/pp_08.html",
    "title": "Chapter 8 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_08.html#ophthalmology",
    "href": "review/pp_08.html#ophthalmology",
    "title": "Chapter 8 Practice Problems",
    "section": "Ophthalmology",
    "text": "Ophthalmology\nIn a study of the natural history of retinitis pigmentosa (RP), 94 RP patients were followed for 3 years. Among 90 patients with complete follow-up, the mean ±1 se of log (visual-field loss) over 1, 2, and 3 years was \\(0.02 \\pm 0.04\\), \\(0.08 \\pm 0.05\\), and \\(0.14 \\pm 0.07\\), respectively.\n\n8.1Solution\n\n\nWhat test procedure can be used to test for changes in log (visual field) over any given time period?\n\n\n\n\n\n\n\n8.2Solution\n\n\nmplement the procedure in Problem 8.1 to test for significant changes in visual field over 1 year. Report a p-value.\n\n\n\n\n\n\n\n8.3Solution\n\n\nAnswer Problem 8.2 for changes over 2 years.\n\n\n\n\n\n\n\n8.4Solution\n\n\nAnswer Problem 8.2 for changes over 3 years.\n\n\n\n\n\n\n\n8.5Solution\n\n\nFind the upper 5th percentile of an F distribution with 24 and 30 df.\n\n\n\n\n\n\n\n8.6Solution\n\n\nSuppose we have two normally distributed samples of sizes 9 and 15 with sample standard deviations of 13.7 and 7.2, respectively. Test if the variances are significantly different in the two samples."
  },
  {
    "objectID": "review/pp_08.html#pathology",
    "href": "review/pp_08.html#pathology",
    "title": "Chapter 8 Practice Problems",
    "section": "Pathology",
    "text": "Pathology\nThe data in the following data frame are measurements from a group of 10 normal males and 11 males with left-heart disease taken at autopsy at a particular hospital. Measurements were made on several variables at that time, and the table presents the measurements on total heart weight (THW) and total body weight (BW). Assume that the diagnosis of left-heart disease is made independently of these variables.\n\nheart &lt;- tibble(\n  disease_status = c(rep(\"disease\", 11), rep(\"normal\", 10)),\n  observation_number = c(1:11, 1:10),\n  thw = c(450, 760, 325, 495, 285, 450, 460, 375, 310, 615, 425,\n          245, 350, 340, 300, 310, 270, 300, 360, 405, 290),\n  bw = c(54.6, 73.5, 50.3, 44.6, 58.1, 61.3, 75.3, 41.1, 51.5, 41.7, 59.7,\n         40.8, 67.4, 53.3, 62.2, 65.5, 47.5, 51.2, 74.9, 59.0, 40.5)\n)\nglimpse(heart)\n\nRows: 21\nColumns: 4\n$ disease_status     &lt;chr&gt; \"disease\", \"disease\", \"disease\", \"disease\", \"diseas…\n$ observation_number &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6…\n$ thw                &lt;dbl&gt; 450, 760, 325, 495, 285, 450, 460, 375, 310, 615, 4…\n$ bw                 &lt;dbl&gt; 54.6, 73.5, 50.3, 44.6, 58.1, 61.3, 75.3, 41.1, 51.…\n\n\n\n8.7Solution\n\n\nTest for a significant difference in mean total heart weight between the diseased and normal groups. Assume unequal variances. Note that the estimated degrees of freedom turns out to be \\(\\nu = 12.45\\).\n\n\nLet \\(X_i\\) be the total heart weight for normal male \\(i\\). Let \\(Y_j\\) be the total heart weight for diseased male \\(j\\). Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma_1^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma_2^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We have the following summary statistics:\n\nheart |&gt;\n  group_by(disease_status) |&gt;\n  summarize(\n    xbar = mean(thw),\n    s_squared = var(thw),\n    n = n()\n  )\n\n# A tibble: 2 × 4\n  disease_status  xbar s_squared     n\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 disease          450    19415     11\n2 normal           317     2218.    10\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{1}{n_1}s_1^2 + \\frac{1}{n_2}s_2^2}} = \\frac{450 - 317}{\\sqrt{\\frac{1}{11}19415 + \\frac{1}{10}2218}}\n\\] Numerically,\n\n(450 - 317) / sqrt(19415 / 11 + 2218 / 10)\n\n[1] 2.984\n\n\nFrom the question, we are told to compare this to a \\(t\\) distribution with 12.45 degrees of freedom.\n\n\n\n\n\n\n\n\n\nNumerically,\n\n2 * pt(-2.984, df = 12.45)\n\n[1] 0.011\n\n\nThe \\(p\\)-value is pretty small, so we have evidence that mean total heart weight differs between diseased and normal individuals.\nIn R, the real-way would be:\n\nt.test(thw ~ disease_status, data = heart) |&gt;\n  tidy() |&gt;\n  select(p.value, estimate, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  p.value estimate conf.low conf.high\n    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.0110      133     36.3      230.\n\n\nWe might say something like:\n\nWe have strong evidence that mean total heart weight differs between diseased and normal males (p = 0.011). The diseased group is estimated to have a heart size that is 133 larger (95% CI 36.27 to 229.7 larger).\n\n\n\n\n\n8.8Solution\n\n\nTest for a significant difference in mean body weight between the diseased and normal groups. Assume equal variances.\n\n\nLet \\(X_i\\) be the body weight for normal male \\(i\\). Let \\(Y_j\\) be the body weight for diseased male \\(j\\). Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We have the following summary statistics:\n\nheart |&gt;\n  group_by(disease_status) |&gt;\n  summarize(\n    xbar = mean(bw),\n    s_squared = var(bw),\n    n = n()\n  )\n\n# A tibble: 2 × 4\n  disease_status  xbar s_squared     n\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 disease         55.6      133.    11\n2 normal          56.2      133.    10\n\n\nWe need the pooled sample variance \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(11 - 1)133.4 + (10 - 1)133.1}{11 + 10 - 2}\n\\] Numerically\n\n((11 - 1) * 133.4 + (10 - 1) * 133.1) / (11 + 10 - 2)\n\n[1] 133.3\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{55.61 - 56.23}{\\sqrt{133.3}\\sqrt{\\frac{1}{11} + \\frac{1}{10}}}\n\\] Numerically,\n\n(55.61 - 56.23) / (sqrt(133.3) * sqrt(1 / 11 + 1 / 10))\n\n[1] -0.1229\n\n\nWe compare this to a \\(t\\) distribution with \\(n_1 + n_2 - 2 = 11 + 10 - 2 = 19\\) degrees of freedom:\n\n\n\n\n\n\n\n\n\nNumerically,\n\n2 * pt(-0.1229, df = 19)\n\n[1] 0.9035\n\n\nThe \\(p\\)-value is very large. So we have no evidence in a mean difference in body weight.\nIn R, the real-way would be:\n\nt.test(bw ~ disease_status, data = heart, var.equal = TRUE) |&gt;\n  tidy() |&gt;\n  select(p.value, estimate, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  p.value estimate conf.low conf.high\n    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   0.903   -0.621    -11.2      9.94\n\n\nWe might say something like:\n\nWe have no evidence in a difference in mean body weight between the diseased and normal groups (p = 0.9). We estimate that the normal group weigs 0.12 more (95% CI 11.19 more to 9.94 less)."
  },
  {
    "objectID": "review/pp_08.html#psychiatry-renal-disease",
    "href": "review/pp_08.html#psychiatry-renal-disease",
    "title": "Chapter 8 Practice Problems",
    "section": "Psychiatry, Renal Disease",
    "text": "Psychiatry, Renal Disease\nSevere anxiety often occurs in patients who must undergo chronic hemodialysis. A set of progressive relaxation exercises was shown on videotape to a group of 38 experimental subjects, while a set of neutral videotapes was shown to a control group of 23 patients who were also on chronic hemodialysis. The results of a psychiatric questionnaire (the State-Trait Anxiety Inventory) are presented in Table 8.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.1\n\n\nPretest and Posttest State-Trait Anxiety means and standard deviations for the experimental and control groups of hemodialysis patients\n\n\nGroup\n\nPretest\n\n\nPost-test\n\n\n\nMean\nsd\nn\nMean\nsd\nn\n\n\n\n\nExperimental\n37.51\n10.66\n38\n33.42\n10.18\n38\n\n\nControl\n36.42\n8.59\n23\n39.71\n9.16\n23\n\n\n\n\n\n\n\n\n8.9Solution\n\n\nPerform a statistical test to compare the experimental and control groups’ pretest scores. Assume equal variances.\n\n\nLet \\(X_i\\) be the pretest questionnaire value for experimental individual \\(i\\). Let \\(Y_j\\) be the pretest questionnaire value for control individual \\(j\\). Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We need the pooled sample variance \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(38 - 1)10.66^2 + (23 - 1)8.59^2}{38 + 23 - 2}\n\\] Numerically\n\n((38 - 1) * 10.66^2 + (23 - 1) * 8.59^2) / (38 + 23 - 2)\n\n[1] 98.78\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{37.51 - 36.42}{\\sqrt{98.78}\\sqrt{\\frac{1}{38} + \\frac{1}{23}}}\n\\] Numerically,\n\n(37.51 - 36.42) / (sqrt(98.78) * sqrt(1 / 38 + 1 / 23))\n\n[1] 0.4151\n\n\nWe compare this to a \\(t\\) distribution with \\(n_1 + n_2 - 2 = 38 + 23 - 2 = 59\\) degrees of freedom:\n\n\n\n\n\n\n\n\n\nNumerically,\n\n2 * pt(-0.4151, df = 59)\n\n[1] 0.6796\n\n\nThe \\(p\\)-value is very large. So we have no evidence in a mean difference in pre-test values.\n\n\n\n\n8.10Solution\n\n\nPerform a statistical test to compare the experimental and control groups’ posttest scores. Assume equal variances.\n\n\nLet \\(X_i\\) be the posttest questionnaire value for experimental individual \\(i\\). Let \\(Y_j\\) be the posttest questionnaire value for control individual \\(j\\). Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We need the pooled sample variance \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(38 - 1)10.18^2 + (23 - 1)9.16^2}{38 + 23 - 2}\n\\] Numerically\n\n((38 - 1) * 10.18^2 + (23 - 1) * 9.16^2) / (38 + 23 - 2)\n\n[1] 96.28\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{33.42 - 39.71}{\\sqrt{96.28}\\sqrt{\\frac{1}{38} + \\frac{1}{23}}}\n\\] Numerically,\n\n(33.42 - 39.71) / (sqrt(96.28) * sqrt(1 / 38 + 1 / 23))\n\n[1] -2.426\n\n\nWe compare this to a \\(t\\) distribution with \\(n_1 + n_2 - 2 = 38 + 23 - 2 = 59\\) degrees of freedom:\n\n\n\n\n\n\n\n\n\nNumerically,\n\n2 * pt(-2.426, df = 59)\n\n[1] 0.01834\n\n\nThe \\(p\\)-value is pretty small. So we have some evidence in a mean difference in posttest values."
  },
  {
    "objectID": "review/pp_08.html#hypertension",
    "href": "review/pp_08.html#hypertension",
    "title": "Chapter 8 Practice Problems",
    "section": "Hypertension",
    "text": "Hypertension\nBlood-pressure measurements taken on the left and right arms of a person are assumed to be comparable. To test this assump- tion, 10 volunteers are obtained and systolic blood-pressure readings are taken simultaneously on both arms by two different observers, Ms. Jones for the left arm and Mr. Smith for the right arm. The data are given in the following data frame.\n\narmbp &lt;- tibble(\n  Patient = 1:10,\n  Left_arm = c(130, 120, 135, 100, 98, 110, 123, 136, 140, 155),\n  Right_arm = c(126, 124, 127, 95, 102, 109, 124, 132, 137, 156)\n)\narmbp\n\n# A tibble: 10 × 3\n   Patient Left_arm Right_arm\n     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1       1      130       126\n 2       2      120       124\n 3       3      135       127\n 4       4      100        95\n 5       5       98       102\n 6       6      110       109\n 7       7      123       124\n 8       8      136       132\n 9       9      140       137\n10      10      155       156\n\n\n\n8.11Solution\n\n\nAssuming that the two observers are comparable, test whether or not the two arms give comparable readings.\n\n\n\n\n\n\n\n8.12Solution\n\n\nSuppose we do not assume that the two observers are comparable. Can the experiment, as it is defined, detect differences between the two arms? If not, can you suggest an alternative experimental design so as to achieve this aim?"
  },
  {
    "objectID": "review/pp_08.html#ophthalmology-1",
    "href": "review/pp_08.html#ophthalmology-1",
    "title": "Chapter 8 Practice Problems",
    "section": "Ophthalmology 1",
    "text": "Ophthalmology 1\nIn a study of the natural history of retinitis pigmentosa (RP), 94 RP patients were followed for 3 years. Among 90 patients with complete follow-up, the mean ±1 se of log (visual-field loss) over 1, 2, and 3 years was \\(0.02 \\pm 0.04\\), \\(0.08 \\pm 0.05\\), and \\(0.14 \\pm 0.07\\), respectively.\n\n8.1Solution\n\n\nWhat test procedure can be used to test for changes in log (visual field) over any given time period?\n\n\nA one-sample \\(t\\)-test on the visual-field loss. This is a also known as a paired \\(t\\)-test.\n\n\n\n\n8.2Solution\n\n\nImplement the procedure in Problem 8.1 to test for significant changes in visual field over 1 year. Report a p-value.\n\n\nLet \\(X_i\\) be the the log loss over year 1 for individual \\(i\\). Then we assume \\(X_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu \\neq 0\\). We are told that \\(\\bar{x} = 0.2\\), \\(s/\\sqrt{n} = 0.04\\). Note that we are told “se”, not “sd”. We calculate a \\(t\\)-statistic \\[\n\\frac{\\bar{x} - \\mu_0}{s\\sqrt{n}} = \\frac{0.02}{0.04}\n\\]\n\n0.02 / 0.04\n\n[1] 0.5\n\n\nWe compare this to a \\(t_{n-1} = t_{93}\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pt(q = -0.5, df = 93)\n\n[1] 0.6183\n\n\nSince the \\(p\\)-value is so large, we do not have evidence that there is on average visual loss after one year.\n\n\n\n\n8.3Solution\n\n\nAnswer Problem 8.2 for changes over 2 years.\n\n\nThe same task as Problem 8.1, but with \\[\nt = 0.08 / 0.05 = 1.6\n\\] We compare this to a \\(t_{n-1} = t_{93}\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pt(q = -1.6, df = 93)\n\n[1] 0.113\n\n\nSince the \\(p\\)-value is large, we do not have evidence that there is on average visual loss after one year.\n\n\n\n\n8.4Solution\n\n\nAnswer Problem 8.2 for changes over 3 years.\n\n\nThe same task as Problem 8.1, but with \\[\nt = 0.14 / 0.07 = 2\n\\] We compare this to a \\(t_{n-1} = t_{93}\\) distribution\n\n\n\n\n\n\n\n\n\n\n2 * pt(q = -2, df = 93)\n\n[1] 0.04842\n\n\nThe \\(p\\)-value is somewhat small. So we have weak evidence of a mean visual loss that is different from 0."
  },
  {
    "objectID": "review/pp_08.html#cardiovascular-disease",
    "href": "review/pp_08.html#cardiovascular-disease",
    "title": "Chapter 8 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nA study was performed in 1976 to relate the use of oral contraceptives to the levels of various lipid fractions in a group of 163 nonpregnant, premenopausal women ages 21–39. The mean serum cholesterol among 66 current users of oral contraceptives was 201 ± 37 (mg/dL) (mean ± sd), whereas for 97 nonusers it was 193 ± 37 (mg/dL).\n\n8.18Solution\n\n\nTest for significant differences in mean cholesterol levels between the two groups. Assume equal variances.\n\n\nLet \\(X_i\\) be the serum cholesterol for oral contraceptive user \\(i\\). Let \\(Y_j\\) be the serum cholesterol for nonuser \\(j\\). Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We have the following summary statistics. We are told we have the following statistics:\n\n\\(\\bar{x} = 201\\), \\(s_1 = 37\\), \\(n_1 = 66\\)\n\\(\\bar{y} = 193\\), \\(s_2 = 37\\), \\(n_2 = 97\\)\n\nWe need the pooled sample variance. Since the sample variance is \\(37^2\\) in both groups, it should be \\(37^2\\). But we can verify: \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(66 - 1)37^2 + (97 - 1)37^2}{66 + 97 - 2}\n\\] Numerically\n\n((66 - 1) * 37^2 + (97 - 1) * 37^2) / (66 + 97 - 2)\n\n[1] 1369\n\n37^2\n\n[1] 1369\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{201 - 193}{37\\sqrt{\\frac{1}{66} + \\frac{1}{97}}}\n\\] Numerically,\n\n(201 - 193) / (37 * sqrt(1 / 66 + 1 / 97))\n\n[1] 1.355\n\n\nWe compare this to a \\(t\\) distribution with \\(n_1 + n_2 - 2 = 66 + 97 - 2 = 161\\) degrees of freedom. At \\(\\alpha = 0.05\\), the critical value would be\n\nqt(p = 1 - 0.05 / 2, df = 161)\n\n[1] 1.975\n\n\nSince 1.355&lt; 1.975 (\\(|t| &lt; t_{n-1, 1-\\alpha/2}\\)), we would fail to reject the null at significance level 0.05.\n\n\n\n\n8.19Solution\n\n\nReport a p-value based on your hypothesis test in Problem 8.18.\n\n\nWe can also calculate a \\(p\\)-value:\n\nplt_t(lb = 1.355, two_sided = TRUE, df = 161) +\n  geom_vline(xintercept = 1.355, lty = 2)\n\n\n\n\n\n\n\n\nNumerically, we have\n\n2 * pt(-1.355, df = 161)\n\n[1] 0.1773\n\n\nThe \\(p\\)-value is large, so we do not have evidence that there is a difference in mean serum cholesterol between OC and non-OC users.\n\n\n\n\n8.20Solution\n\n\nDerive a 95% CI for the true mean difference in cholesterol levels between the groups.\n\n\nThe equation is of the form \\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\times s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\] The appropriate quantile is\n\nqt(p = 1 - 0.05/2, df = 161)\n\n[1] 1.975\n\n\nPlutting in, we get \\[\n(201 - 193) \\pm 1.975 \\times 37 \\sqrt{\\frac{1}{66} + \\frac{1}{97}}\n\\] Numerically\n\n(201 - 193) - 1.975 * 37 * sqrt(1/66 + 1/97)\n\n[1] -3.66\n\n(201 - 193) + 1.975 * 37 * sqrt(1/66 + 1/97)\n\n[1] 19.66\n\n\n\n\n\n\n8.21Solution\n\n\nSuppose the two-tailed p-value in Problem 8.19 = .03 and the two-sided 95% CI in Problem 8.20 = (−0.6, 7.3). Do these two results contradict each other? Why or why not? (Note: These values are not necessarily the actual results in Problems 8.19 and 8.20.)\n\n\nYes. The \\(p\\)-value is less than 0.03, which is less than 0.05, so the 95% CI should include 0. But we are told it does not."
  },
  {
    "objectID": "review/pp_08.html#hypertension-1",
    "href": "review/pp_08.html#hypertension-1",
    "title": "Chapter 8 Practice Problems",
    "section": "Hypertension 1",
    "text": "Hypertension 1\nBlood-pressure measurements taken on the left and right arms of a person are assumed to be comparable. To test this assumption, 10 volunteers are obtained and systolic blood-pressure readings are taken simultaneously on both arms by two different observers, Ms. Jones for the left arm and Mr. Smith for the right arm. The data are given in the following data frame.\n\narmbp &lt;- tibble(\n  Patient = 1:10,\n  Left_arm = c(130, 120, 135, 100, 98, 110, 123, 136, 140, 155),\n  Right_arm = c(126, 124, 127, 95, 102, 109, 124, 132, 137, 156)\n)\narmbp\n\n# A tibble: 10 × 3\n   Patient Left_arm Right_arm\n     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1       1      130       126\n 2       2      120       124\n 3       3      135       127\n 4       4      100        95\n 5       5       98       102\n 6       6      110       109\n 7       7      123       124\n 8       8      136       132\n 9       9      140       137\n10      10      155       156\n\n\n\n8.11Solution\n\n\nAssuming that the two observers are comparable, test whether or not the two arms give comparable readings.\n\n\nWe will do a paired \\(t\\)-test. Let \\(D_i\\) be the left arm’s blood pressure minus the right arm’s blood pressure for patient \\(i\\). We assume \\(D_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu \\neq 0\\). We calculate summary statistics\n\narmbp |&gt;\n  mutate(diff = Left_arm - Right_arm) |&gt;\n  summarize(\n    mean = mean(diff),\n    sd = sd(diff),\n    n = n())\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1   1.5  3.98    10\n\n\nWe calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{1.5 - 0}{3.979 / \\sqrt{10}}\n\\] Numerically\n\n(1.5 - 0) / (3.979 / sqrt(10))\n\n[1] 1.192\n\n\nWe compare this to a \\(t_{9}\\) distribution\n\nplt_t(lb = 1.192, two_sided = TRUE, df = 9) +\n  geom_vline(xintercept = 1.192, lty = 2)\n\n\n\n\n\n\n\n\n\n2 * pt(-1.192, df = 9)\n\n[1] 0.2637\n\n\nSince the \\(p\\)-value is so large, we do not have evidence that the mean difference in blood pressures is different from 0.\nThe real-way in R uses t.test()\n\narmbp &lt;- mutate(armbp, diff = Left_arm - Right_arm)\nt.test(diff ~ 1, data = armbp, mu = 0) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.264\n\n\n\n\n\n\n8.12Solution\n\n\nSuppose we do not assume that the two observers are comparable. Can the experiment, as it is defined, detect differences between the two arms? If not, can you suggest an alternative experimental design so as to achieve this aim?\n\n\nNo. Observer is completely confounded with arm (are any observed differences because of the arm used or because of the observer used?)\nWe could have Ms. Jones measure each arm on each patient, and have Mr. Smith also measure each arm on each patient."
  },
  {
    "objectID": "review/pp_08.html#pulmonary-disease",
    "href": "review/pp_08.html#pulmonary-disease",
    "title": "Chapter 8 Practice Problems",
    "section": "Pulmonary Disease",
    "text": "Pulmonary Disease\nForced expiratory volume (FEV) is a standard measure of pulmonary function representing the volume of air expelled in 1 second. Suppose we enroll 10 nonsmoking males age 35–39, heights 68–72 inches in a longitudinal study and measure their FEV (L) initially (year 0) and 2 years later (year 2). The data in in the following data frame are obtained.\n\npulm &lt;- tibble(\n  person = 1:10,\n  fev0 = c(3.22, 4.06, 3.85, 3.50, 2.80, 3.25, 4.20, 3.05, 2.86, 3.50),\n  fev2 = c(2.95, 3.75, 4.00, 3.42, 2.77, 3.20, 3.90, 2.76, 2.75, 3.32)\n)\npulm\n\n# A tibble: 10 × 3\n   person  fev0  fev2\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1  3.22  2.95\n 2      2  4.06  3.75\n 3      3  3.85  4   \n 4      4  3.5   3.42\n 5      5  2.8   2.77\n 6      6  3.25  3.2 \n 7      7  4.2   3.9 \n 8      8  3.05  2.76\n 9      9  2.86  2.75\n10     10  3.5   3.32\n\n\nWith these summary statistics\n\n\n\n\n\n\n\n\nvariable\nmean\nsd\n\n\n\n\nfev0\n3.429\n0.4852\n\n\nfev2\n3.282\n0.4798\n\n\n\n\n\n\n\n\n8.30Solution\n\n\nWhat are the appropriate null and alternative hypotheses in this case to test if mean pulmonary function has decreased over 2 years?\n\n\nLet \\(\\mu\\) be the mean decrease in pulminory function (fev0 - fev2). Then \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu &gt; 0\\).\n\n\n\n\n8.31Solution\n\n\nIn words, what is the meaning of a type I and a type II error here?\n\n\n\nType I Error: We say mean FEV has declined but it hasn’t.\nType I Error: We say we don’t have evidence that mean FEV has declined when it has.\n\n\n\n\n\n8.32Solution\n\n\nCarry out the test in Problem 8.30. What are your conclusions?\n\n\nWe will do a paired \\(t\\)-test. Let \\(D_i\\) be the FEV at year 0 minus the FEV at year 2 in individual \\(i\\). We assume \\(D_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu &gt; 0\\). We calculate summary statistics\n\npulm |&gt;\n  mutate(diff = fev0 - fev2) |&gt;\n  summarize(\n    mean = mean(diff),\n    sd = sd(diff),\n    n = n())\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 0.147 0.150    10\n\n\nWe calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{0.147 - 0}{0.1505 / \\sqrt{10}}\n\\] Numerically\n\n(0.147 - 0) / (0.1505 / sqrt(10))\n\n[1] 3.089\n\n\nWe compare this to a \\(t_{9}\\) distribution\n\nplt_t(lb = 3.089, two_sided = FALSE, df = 9, rng = c(-4, 4)) +\n  geom_vline(xintercept = 3.089, lty = 2)\n\n\n\n\n\n\n\n\n\n1 - pt(3.089, df = 9)\n\n[1] 0.006475\n\n\nThe \\(p\\)-value is 0.006475, so we have evidence that FEV is lower after two years on average.\nThe real-way in R uses t.test()\n\npulm &lt;- mutate(pulm, diff = fev0 - fev2)\nt.test(diff ~ 1, data = pulm, alternative = \"greater\") |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1 0.00647\n\n\n\n\n\nAnother aspect of the preceding study involves looking at the effect of smoking on baseline pulmonary function and on change in pulmonary function over time. We must be careful, since FEV depends on many factors, particularly age and height. Suppose we have a comparable group of 15 men in the same age and height group as in Table 8.5 who are smokers, and we measure their FEV at year 0 and year 2. The data are given in the following data frame.\n\npulm2 &lt;- tibble(\n  person = 1:15,\n  fev0 = c(2.85, 3.32, 3.01, 2.95, 2.78, 2.86, 2.78, 2.90,\n           2.76, 3.00, 3.26, 2.84, 2.50, 3.59, 3.30),\n  fev2 = c(2.88, 3.40, 3.02, 2.84, 2.75, 3.20, 2.96, 2.74,\n           3.02, 3.08, 3.00, 3.40, 2.59, 3.29, 3.32)\n)\npulm2\n\n# A tibble: 15 × 3\n   person  fev0  fev2\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1  2.85  2.88\n 2      2  3.32  3.4 \n 3      3  3.01  3.02\n 4      4  2.95  2.84\n 5      5  2.78  2.75\n 6      6  2.86  3.2 \n 7      7  2.78  2.96\n 8      8  2.9   2.74\n 9      9  2.76  3.02\n10     10  3     3.08\n11     11  3.26  3   \n12     12  2.84  3.4 \n13     13  2.5   2.59\n14     14  3.59  3.29\n15     15  3.3   3.32\n\n\nWith these summary statistics:\n\n\n\n\n\n\n\n\nvariable\nmean\nsd\n\n\n\n\nfev0\n2.980\n0.2786\n\n\nfev2\n3.033\n0.2504\n\n\n\n\n\n\n\n\n8.33Solution\n\n\nWhat are the appropriate null and alternative hypotheses to compare the FEV of smokers and nonsmokers at baseline?\n\n\nLet \\(\\mu_1\\) be the mean FEV at baseline in the smoking group. Let \\(\\mu_2\\) be the mean FEV at baseline in the non-smoking group. Then we are testing \\(H_0: \\mu_1  = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\).\n\n\n\n\n8.34Solution\n\n\nCarry out the procedure(s) necessary to conduct the test in Problem 8.33. Assume equal variances.\n\n\nLet \\(X_i\\) be the FEV at baseline for individual \\(i\\) in the smoking. Let \\(Y_j\\) be the FEV at baseline for individual \\(j\\) in the non-smoking group. Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We are told we have the following statistics:\n\n\\(\\bar{x} = 2.980\\), \\(s_1 = 0.2786\\), \\(n_1 = 15\\)\n\\(\\bar{y} = 3.429\\), \\(s_2 = 0.4852\\), \\(n_2 = 10\\)\n\nWe need the pooled sample variance. \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(15 - 1)0.2786^2 + (10 - 1)0.4852^2}{15 + 10 - 2}\n\\] Numerically\n\n((15 - 1) * 0.2786^2 + (10 - 1) * 0.4852^2) / (15 + 10 - 2)\n\n[1] 0.1394\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{2.980 - 3.429}{\\sqrt{0.1394}\\sqrt{\\frac{1}{15} + \\frac{1}{10}}}\n\\] Numerically,\n\n(2.980 - 3.429) / (sqrt(0.1394) * sqrt(1 / 15 + 1 / 10))\n\n[1] -2.946\n\n\nWe compare this to a \\(t\\) distribution with \\(n_1 + n_2 - 2 = 15 + 10 - 2 = 23\\) degrees of freedom. At \\(\\alpha = 0.05\\), the critical value would be\n\nqt(p = 1 - 0.05 / 2, df = 23)\n\n[1] 2.069\n\n\nSince 2.946 &gt; 2.069 (\\(|t| &gt; t_{n-1, 1-\\alpha/2}\\)), we would reject the null at significance level 0.05.\nWe can also calculate a \\(p\\)-value:\n\nplt_t(ub = -2.946, two_sided = TRUE, df = 23, rng = c(-4, 4)) +\n  geom_vline(xintercept = -2.946, lty = 2)\n\n\n\n\n\n\n\n\nNumerically, we have\n\n2 * pt(-2.946, df = 795)\n\n[1] 0.003313\n\n\nThe \\(p\\)-value is pretty small, so we evidence that the FEV’s between the two groups differ at baseline on average.\n\n\n\n\n8.35Solution\n\n\nSuggest a procedure for testing whether or not the change in pulmonary function over 2 years is the same in the two groups.\n\n\nDo a two-sample \\(t\\)-test on the differences in the two groups."
  },
  {
    "objectID": "review/pp_08.html#hypertension-2",
    "href": "review/pp_08.html#hypertension-2",
    "title": "Chapter 8 Practice Problems",
    "section": "Hypertension 2",
    "text": "Hypertension 2\nA 1982 study by the Lipid Research CIinics looked at the relationship between alcohol consumption and level of systolic blood pressure in women not using oral contraceptives. Alcohol consumption was categorized as follows: no alcohol use; ≤ 10 oz/week alcohol consumption; &gt; 10 oz/week alcohol consumption. The results for women 30–39 years of age are given in Table 8.4.\nSuppose we wish to compare the levels of systolic blood pressure of groups “no alcohol use” and “≤ 10 oz/week alcohol consumption” and we have no prior information regarding which group has higher blood pressure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.4\n\n\nRelationship between systolic blood pressure and alcohol consumption in 30–39-year-old women not using oral contraceptives\n\n\nAlcohol Consumption\n\nSystolic blood pressure (mm Hg)\n\n\n\nMean\nSD\nN\n\n\n\n\nNo alcohol use\n110.5\n13.3\n357\n\n\n≤10 oz per week\n109.1\n13.4\n440\n\n\n&gt;10 oz per week\n114.5\n14.9\n23\n\n\n\n\n\n\n\n\n8.22Solution\n\n\nShould a one-sample or two-sample test be used here?\n\n\nTwo-sample. We have two different groups.\n\n\n\n\n8.23Solution\n\n\nShould a one-sided or two-sided test be used here?\n\n\nTwo-sided. We are not a priori interested in one direction or the other.\n\n\n\n\n8.24Solution\n\n\nWhich test procedure(s) should be used to test the preceding hypotheses?\n\n\nA two-sample \\(t\\)-test.\n\n\n\n\n8.25Solution\n\n\nCarry out the test in Problem 8.24 and report a p-value. Assume equal variances.\n\n\nLet \\(X_i\\) be the systolic blood pressure individual \\(i\\) in the no alcohol use group. Let \\(Y_j\\) be the systolc blood pressure for individual \\(j\\) in the low alcohol use group. Then we assume \\[\\begin{align*}\nX_i &\\sim N(\\mu_1, \\sigma^2)\\\\\nY_j &\\sim N(\\mu_2, \\sigma^2)\n\\end{align*}\\] We want to test \\(H_0: \\mu_1 = \\mu_2\\) versus \\(H_A: \\mu_1 \\neq \\mu_2\\). We are told we have the following statistics:\n\n\\(\\bar{x} = 110.5\\), \\(s_1 = 13.3\\), \\(n_1 = 357\\)\n\\(\\bar{y} = 109.1\\), \\(s_2 = 13.4\\), \\(n_2 = 440\\)\n\nWe need the pooled sample variance. \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(357 - 1)13.3^2 + (440 - 1)13.4^2}{357 + 440 - 2}\n\\] Numerically\n\n((357 - 1) * 13.3^2 + (440 - 1) * 13.4^2) / (357 + 440 - 2)\n\n[1] 178.4\n\n\nWe calculate the following \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\bar{y}}{s\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{110.5 - 109.1}{\\sqrt{178.4}\\sqrt{\\frac{1}{357} + \\frac{1}{440}}}\n\\] Numerically,\n\n(110.5 - 109.1) / (sqrt(178.4) * sqrt(1 / 357 + 1 / 440))\n\n[1] 1.472\n\n\nWe compare this to a \\(t\\) distribution with \\(n_1 + n_2 - 2 = 357 + 440 - 2 = 795\\) degrees of freedom. At \\(\\alpha = 0.05\\), the critical value would be\n\nqt(p = 1 - 0.05 / 2, df = 795)\n\n[1] 1.963\n\n\nSince 1.472 &lt; 1.963 (\\(|t| &lt; t_{n-1, 1-\\alpha/2}\\)), we would fail to reject the null at significance level 0.05.\nWe can also calculate a \\(p\\)-value:\n\nplt_t(lb = 1.472, two_sided = TRUE, df = 795) +\n  geom_vline(xintercept = 1.472, lty = 2)\n\n\n\n\n\n\n\n\nNumerically, we have\n\n2 * pt(-1.472, df = 795)\n\n[1] 0.1414\n\n\nThe \\(p\\)-value is large, so we do not have evidence that there is a difference in mean systolic blood pressure between the two groups.\n\n\n\n\n8.26Solution\n\n\nCompute a 95% CI for the mean difference in blood pressure between the two groups.\n\n\nThe equation is of the form \\[\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 - 2,\\ 1 - \\alpha/2} \\times s \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\] The appropriate quantile is\n\nqt(p = 1 - 0.05/2, df = 795)\n\n[1] 1.963\n\n\nPlutting in, we get \\[\n(110.5 - 109.1) \\pm 1.963 \\times \\sqrt{178.4} \\sqrt{\\frac{1}{357} + \\frac{1}{440}}\n\\] Numerically\n\n(110.5 - 109.1) - 1.963 * sqrt(178.4) * sqrt(1/357 + 1/440)\n\n[1] -0.4676\n\n(110.5 - 109.1) + 1.963 * sqrt(178.4) * sqrt(1/357 + 1/440)\n\n[1] 3.268"
  },
  {
    "objectID": "review/pp_08.html#hypertension-3",
    "href": "review/pp_08.html#hypertension-3",
    "title": "Chapter 8 Practice Problems",
    "section": "Hypertension 3",
    "text": "Hypertension 3\nA study of the relationship between salt intake and blood pressure of infants is in the planning stages. A pilot study is done, comparing five 1-year-old infants on a high-salt diet with five 1-year-old infants on a low-salt diet. The results are given in Table 8.7.\n\n\n\n\n\n\n\n\nTable 8.7\n\n\nRelationship between salt intake and level of systolic blood pressure (SBP)\n\n\n\nMean SBP\nsd SBP\nn\n\n\n\n\nHigh-salt diet\n90.8\n10.3\n5\n\n\nLow-salt diet\n87.2\n9.2\n5\n\n\n\n\n\n\n\n\n8.36Solution\n\n\nIf the means and standard deviations in Table 8.7 are considered to be true population parameters, then, using a one-sided test with significance level = .05 , how many infants are needed in each group to have an 80% chance of detecting a significant difference?\n\n\nLet’s calculate the pooled estimate of the variance: \\[\ns^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} = \\frac{(5 - 1)10.3^2 + (5 - 1)9.2^2}{5 + 5 - 2}\n\\]\n\n((5 - 1) * 10.3^2 + (5 - 1) * 9.2^2) / (5 + 5 - 2)\n\n[1] 95.37\n\n\n\npower.t.test(\n  delta = 90.8 - 87.2, \n  sd = sqrt(95.37), \n  sig.level = 0.05, \n  power = 0.8, \n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 91.68\n          delta = 3.6\n             sd = 9.766\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nSo we would need about 92 individuals in each group.\n\n\n\n\n8.38Solution\n\n\nSuppose the budget will only allow for recruiting 50 high-salt-diet and 50 low-salt-diet infants into the study. How much power would such a study have of detecting a significant difference using a one-sided test with significance level = .05 if the true difference between the groups is 5 mm Hg?\n\n\n\npower.t.test(\n  n = 50,\n  delta = 5,\n  sd = sqrt(95.37),\n  sig.level = 0.05, \n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 5\n             sd = 9.766\n      sig.level = 0.05\n          power = 0.8152\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nThe power is 0.8152.\n\n\n\n\n8.39Solution\n\n\nAnswer Problem 8.38 for a true difference of 2 mm Hg.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 2,\n  sd = sqrt(95.37),\n  sig.level = 0.05, \n  type = \"two.sample\",\n  alternative = \"one.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 2\n             sd = 9.766\n      sig.level = 0.05\n          power = 0.265\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nThe power is 0.265.\n\n\n\n\n8.40Solution\n\n\nAnswer Problem 8.38 if a two-sided test is used and the true difference is 5 mm Hg.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 5,\n  sd = sqrt(95.37),\n  sig.level = 0.05, \n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 5\n             sd = 9.766\n      sig.level = 0.05\n          power = 0.7173\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power is 0.7173.\n\n\n\n\n8.41Solution\n\n\nAnswer Problem 8.40 if the true difference is 2 mm Hg.\n\n\n\npower.t.test(\n  n = 50,\n  delta = 2,\n  sd = sqrt(95.37),\n  sig.level = 0.05, \n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 2\n             sd = 9.766\n      sig.level = 0.05\n          power = 0.1721\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power is 0.1721."
  },
  {
    "objectID": "review/pp_08.html#environmental-health",
    "href": "review/pp_08.html#environmental-health",
    "title": "Chapter 8 Practice Problems",
    "section": "Environmental Health",
    "text": "Environmental Health\nA study was conducted relating lead level in umbilical cord blood and cognitive development. Three groups were identified at birth (high/medium/low) cord blood lead. One issue is the consistency of the differences in blood-lead levels over time between these three groups. The data in Table 8.9 were presented.\n\n\n\n\n\n\n\n\nTable 8.9\n\n\nChange in blood-lead level in the first two years of life by cord blood-lead group\n\n\nCord blood-lead group\n\nBirth\n6 months\n12 months\n18 months\n24 months\n\n\n\n\nLow\nMean ± sd\n1.8 ± 0.6\n4.6 ± 3.9\n5.8 ± 5.1\n6.7 ± 5.5\n5.4 ± 4.8\n\n\n\nn\n85\n70\n69\n65\n61\n\n\nMedium\nMean ± sd\n6.5 ± 0.3\n7.0 ± 7.8\n8.5 ± 7.6\n8.3 ± 5.8\n7.2 ± 5.0\n\n\n\nn\n88\n70\n70\n65\n63\n\n\nHigh\nMean ± sd\n14.76 ± 3.0\n7.0 ± 8.7\n8.8 ± 6.4\n7.6 ± 5.8\n7.7 ± 8.5\n\n\n\nn\n76\n61\n60\n57\n58\n\n\n\n\n\n\n\n\n8.49Solution\n\n\nWhat test procedure can be used to test if there are differences between the observed blood-lead levels at 24 months between the low and high groups (as defined at baseline)?\n\n\nTwo-sample t-test.\n\n\n\n\n8.50Solution\n\n\nImplement the test procedure in Problem 8.49 and report a p-value. Do not assume equal variances. The degrees of freedom ended up being 89.\n\n\n\n\nT-statistic: -1.805 \nCompare to t-distribution with 89 degrees of freedom\nCritical Value: 1.987 \nAt significance level 0.05 , Fail to reject since |t| &lt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.07443 \n95 % CI: ( -4.832 , 0.2317 )\n\n\n\n\n\n\n8.51Solution\n\n\nProvide a 95% CI for the difference in mean blood lead levels between the low and high groups at 24 months.\n\n\nThis was done in 8.50."
  },
  {
    "objectID": "review/pp_08.html#ophthalmology-2",
    "href": "review/pp_08.html#ophthalmology-2",
    "title": "Chapter 8 Practice Problems",
    "section": "Ophthalmology 2",
    "text": "Ophthalmology 2\nA topic of current interest in ophthalmology is whether or not spherical refraction is different between the left and right eyes. For this purpose refraction is measured in both eyes of 17 people. The data are given in the following data frame.\n\nrefract &lt;- tibble(\n  id = 1:17,\n  od = c(1.75, -4, -1.25, 1, -1, -0.75, -2.25, 0.25, 0, \n         -1, 0.5, -8.5, 0.5, -5.25, -2.25, -6.5, 1.75),\n  os = c(2, -4, -1, 1, -1, 0.25, -2.25, 0.25, 0.5, -1.25, \n         -1.75, -5, 0.5, -4.75, -2.5, -6.25, 1.75)\n) |&gt;\n  mutate(diff = od - os)\nrefract\n\n# A tibble: 17 × 4\n      id    od    os  diff\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  1.75  2    -0.25\n 2     2 -4    -4     0   \n 3     3 -1.25 -1    -0.25\n 4     4  1     1     0   \n 5     5 -1    -1     0   \n 6     6 -0.75  0.25 -1   \n 7     7 -2.25 -2.25  0   \n 8     8  0.25  0.25  0   \n 9     9  0     0.5  -0.5 \n10    10 -1    -1.25  0.25\n11    11  0.5  -1.75  2.25\n12    12 -8.5  -5    -3.5 \n13    13  0.5   0.5   0   \n14    14 -5.25 -4.75 -0.5 \n15    15 -2.25 -2.5   0.25\n16    16 -6.5  -6.25 -0.25\n17    17  1.75  1.75  0   \n\n\n\n8.14Solution\n\n\nIs a one-sided or two-sided test needed here?\n\n\nA two-sided test, since we are interested in just if there is a difference.\n\n\n\n\n8.15Solution\n\n\nWhich of the following test procedures is appropriate to use on these data? (More than one may be necessary.)\n\nPaired t test\nTwo-sample t test for independent samples with equal variances\nTwo-sample t test for independent samples with unequal variances\nOne-sample t test\n\n\n\nJust a paired \\(t\\)-test, since each observational unit is an individual.\n\n\n\n\n8.16Solution\n\n\nCarry out the hypothesis test(s) in Problem 8.15 and report a p-value.\n\n\nLet \\(D_i\\) be the left eye’s refraction minus the right eye’s refraction for individual \\(i\\). We assume \\(D_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu \\neq 0\\). We calculate summary statistics\n\nrefract |&gt;\n  summarize(\n    mean = mean(diff),\n    sd = sd(diff),\n    n = n())\n\n# A tibble: 1 × 3\n    mean    sd     n\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 -0.206  1.07    17\n\n\nWe calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{-0.2059 - 0}{1.073 / \\sqrt{17}}\n\\] Numerically\n\n(-0.2059 - 0) / (1.073 / sqrt(17))\n\n[1] -0.7912\n\n\nWe compare this to a \\(t_{16}\\) distribution. If we use \\(\\alpha = 0.05\\), the critical value is\n\nqt(1 - 0.05 / 2, df = 16)\n\n[1] 2.12\n\n\nSince 0.7912 &lt; 2.12 (\\(|t| &lt; t_{n-1, 1-\\alpha/2}\\)), we would fail to reject the null at significance level 0.05.\nWe can also calculate a \\(p\\)-value:\n\nplt_t(ub = -0.7912, two_sided = TRUE, df = 16) +\n  geom_vline(xintercept = -0.7912, lty = 2)\n\n\n\n\n\n\n\n\n\n2 * pt(-0.7912, df = 9)\n\n[1] 0.4492\n\n\nSince the \\(p\\)-value is so large, we do not have evidence that the mean difference in rafraction is different from 0.\nThe real-way in R uses t.test()\n\nt.test(diff ~ 1, data = refract, mu = 0) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.440\n\n\n\n\n\n\n8.17Solution\n\n\nEstimate a 90% CI for the mean difference in spherical refraction between the two eyes.\n\n\nThis is \\[\n\\bar{d} \\pm t_{n-1, 1-\\alpha/2}\\frac{s}{\\sqrt{n}}\n\\] The appropriate quantile is\n\nqt(p = 1 - 0.1 / 2, df = 16)\n\n[1] 1.746\n\n\nPlugging in numbers, we get \\[\n-0.2059 \\pm 1.746 \\times \\frac{1.073}{\\sqrt{17}}\n\\] Numerically,\n\n-0.2059 - 1.746 * 1.073 / sqrt(17)\n\n[1] -0.6603\n\n-0.2059 + 1.746 * 1.073 / sqrt(17)\n\n[1] 0.2485\n\n\nThe real way in R is\n\nt.test(diff ~ 1, data = refract, conf.level = 0.9) |&gt;\n  tidy() |&gt;\n  select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.660     0.248"
  },
  {
    "objectID": "review/pp_08.html#neurology",
    "href": "review/pp_08.html#neurology",
    "title": "Chapter 8 Practice Problems",
    "section": "Neurology",
    "text": "Neurology\nAn article published in 1986 describes physical, social, and psychological problems in patients with multiple sclerosis. Patients were classified as to mild, moderate, and severe disease and were graded on physical health using the McMaster physical health index, and mental health using the Rand mental health index scale. The data are shown in Table 8.11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.11\n\n\nPhysical and mental health indices for patients with multiple sclerosis\n\n\n\n\nMild\n\n\nSevere\n\n\n\nMean\nsd\nn\nMean\nsd\nn\n\n\n\n\nMcMaster physical health index\n0.76\n0.24\n65\n0.23\n0.12\n82\n\n\nRand mental health index\n157.30\n26.20\n65\n149.10\n34.40\n82\n\n\n\n\n\n\n\n\n8.55Solution\n\n\nWhat test procedure should be used to test if there are differences in physical health between the two groups?\n\n\nTwo-sample \\(t\\)-test with two-sided alternative.\n\n\n\n\n8.56Solution\n\n\nPerform the test in Problem 8.55 with the critical-value method using a two-sided test with an a level of .05. Assume equal variances.\n\n\n\n\nPooled Standard Deviation: 0.1829 \nT-statistic: 17.44 \nCompare to t-distribution with 145 degrees of freedom\nCritical Value: 1.976 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 1.96e-37 \n95 % CI: ( 0.47 , 0.59 )\n\n\n\n\n\n\n8.57Solution\n\n\nWhat test procedure should be used to test if there are differences in mental health between the two groups?\n\n\nAnother two-sample \\(t\\)-test with two-sided alternative.\n\n\n\n\n8.58Solution\n\n\nPerform the test in Problem 8.57 with the critical-value method using an α level of .05.\n\n\n\n\nPooled Standard Deviation: 31.05 \nT-statistic: 1.59 \nCompare to t-distribution with 145 degrees of freedom\nCritical Value: 1.976 \nAt significance level 0.05 , Fail to reject since |t| &lt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.1139 \n95 % CI: ( -1.991 , 18.39 )"
  },
  {
    "objectID": "review/pp_08.html#hypertension-4",
    "href": "review/pp_08.html#hypertension-4",
    "title": "Chapter 8 Practice Problems",
    "section": "Hypertension 4",
    "text": "Hypertension 4\nThe effect of sodium restriction on blood pressure remains a controversial subject. To test this hypothesis a group of 83 individuals participated in a study of restricted sodium intake ( ≤ 75 mEq/24 hrs) for a period of 12 weeks. The effect on diastolic blood pressure (DBP) is reported in Table 8.8.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.8\n\n\nEffect of sodium restriction on diastolic blood pressure\n\n\n\n\nBaseline period\n\n\nDiet period\n\n\nChange from control (diet-baseline)\n\n\n\nMean\nsd\nn\nMean\nsd\nn\nMean\nsd\nn\n\n\n\n\nAge &lt; 40\n69.7\n8.5\n61\n69.1\n8.5\n61\n-0.7\n6.2\n61\n\n\nAge ≥ 40\n77\n8\n22\n71.9\n7.5\n22\n-5.0\n4.7\n22\n\n\nTotal\n\n\n\n\n\n\n-1.8\n6.1\n83\n\n\n\n\n\n\n\n\n8.42Solution\n\n\nWhat is the appropriate procedure to test for whether sodium restriction has had an impact on mean DBP?\n\n\nPaired \\(t\\)-test.\n\n\n\n\n8.43Solution\n\n\nImplement the procedure in Problem 8.42 for people age &lt; 40 using the critical-value method.\n\n\n\n\nT-statistic: -0.8818 \nCompare to t-distribution with 60 degrees of freedom\nCritical Value: 2 \nAt significance level 0.05 , Fail to reject since |t| &lt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.3814 \n95 % CI: ( -2.288 , 0.8879 )\n\n\n\n\n\nOne of the interesting findings is the difference in response to dietary therapy between people in the two age groups.\n\n8.44Solution\n\n\nTest the hypothesis that the response to sodium restriction is different in the two groups and report a p-value. Assume equal variances\n\n\n\n\nPooled Standard Deviation: 5.848 \nT-statistic: 2.957 \nCompare to t-distribution with 81 degrees of freedom\nCritical Value: 1.99 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.004075 \n95 % CI: ( 1.406 , 7.194 )\n\n\n\n\n\n\n8.45Solution\n\n\nObtain a 95% CI for the response to dietary therapy in each age group separately.\n\n\n\n-0.7 - qt(1 - 0.05 / 2, df = 61 - 1) * 6.2 / sqrt(61)\n\n[1] -2.288\n\n-0.7 + qt(1 - 0.05 / 2, df = 61 - 1) * 6.2 / sqrt(61)\n\n[1] 0.8879\n\n\n\n-5 - qt(1 - 0.05 / 2, df = 22 - 1) * 4.7 / sqrt(22)\n\n[1] -7.084\n\n-5 + qt(1 - 0.05 / 2, df = 22 - 1) * 4.7 / sqrt(22)\n\n[1] -2.916\n\n\n\n\n\n\n8.46Solution\n\n\nObtain a 95% CI for the difference in response between the 2 age groups.\n\n\nI did this in 8.44.\n\n\n\nSuppose the results of this study are to be used to plan a larger study on the effects of sodium restriction on DBP.\n\n8.47Solution\n\n\nHow many subjects need to be enrolled in the larger study to test if sodium restriction results in lower DBP if the mean and sd of decline in DBP over the total group of 83 subjects are used for planning purposes and a 90% chance of detecting a significant difference using a two-sided test with a 5% level of significance is desired?\n\n\n\npower.t.test(\n  delta = 1.8,\n  sd = 6.1,\n  sig.level = 0.05,\n  power = 0.9,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 122.6\n          delta = 1.8\n             sd = 6.1\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\n123 individuals.\n\n\n\n\n8.48Solution\n\n\nSuppose 200 patients are enrolled in the larger study. How much power would the larger study have if a two-sided test with significance level = .05 is used?\n\n\n\npower.t.test(\n  n = 200,\n  delta = 1.8,\n  sd = 6.1,\n  sig.level = 0.05,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n          delta = 1.8\n             sd = 6.1\n      sig.level = 0.05\n          power = 0.9858\n    alternative = two.sided\n\n\nA power of 0.9858."
  },
  {
    "objectID": "review/pp_08.html#cardiology",
    "href": "review/pp_08.html#cardiology",
    "title": "Chapter 8 Practice Problems",
    "section": "Cardiology",
    "text": "Cardiology\nA study is planned of the effect of drug therapy with cholesterol-lowering drugs on the coronary arteries of patients with severe angina. A pilot study was performed whereby each patient received the placebo treatment and had an angiogram at baseline and again three years later at follow-up. The average diameter of three coronary arteries was measured at baseline and follow-up. The results are given in Table 8.14.\n\n\n\n\n\n\n\n\nTable 8.14\n\n\nChange in diameter of coronary arteries in patients with severe angina\n\n\nMean change*\nsd\nn\n\n\n\n\n-0.049\n0.237\n8\n\n\n\n*Follow-up - baseline\n\n\n\n\n\n\n\n\nSuppose we wish to test the hypothesis that there has been significant change over 3 years in the placebo group.\n\n8.63Solution\n\n\nWhat test should be used to test this hypothesis?\n\n\nThis is a paired \\(t\\)-test. Two-sided alternative.\n\n\n\n\n8.64Solution\n\n\nImplement the test in Problem 8.63 and report a p-value.\n\n\n\n\nT-statistic: -0.5848 \nCompare to t-distribution with 7 degrees of freedom\nCritical Value: 2.365 \nAt significance level 0.05 , Fail to reject since |t| &lt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.577 \n95 % CI: ( -0.2471 , 0.1491 )\n\n\n\n\n\n\n8.65Solution\n\n\nSuppose it is assumed that the mean change over 3 years would be 0 for an active drug group and –0.049 for a placebo group. We intend to randomize subjects to placebo and active drug and compare the mean change in the two groups, How much power would the proposed study have if 500 subjects in each of the drug and placebo groups are available to participate in the study, a two-sided test is used with α = .05 and the variances in each group are assumed to be the same?\n\n\n\npower.t.test(\n  n = 500, \n  delta = 0.049, \n  sd = 0.237,\n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 500\n          delta = 0.049\n             sd = 0.237\n      sig.level = 0.05\n          power = 0.9042\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe power would be 0.9042."
  },
  {
    "objectID": "review/pp_08.html#cardiovascular-disease-pediatrics",
    "href": "review/pp_08.html#cardiovascular-disease-pediatrics",
    "title": "Chapter 8 Practice Problems",
    "section": "Cardiovascular Disease, Pediatrics",
    "text": "Cardiovascular Disease, Pediatrics\nA family-based behavior-change program was used to modify cardiovascular risk factors among teenage children of patients with ischemic heart disease. The mean baseline level and change in HDL-cholesterol level after a 6-month period in the program is given in Table 8.15.\n\n\n\n\n\n\n\n\nTable 8.15\n\n\nMean baseline level and change in HDL-cholesterol level (mmol/L) among teenage children over a 6-month period\n\n\n\nMean\nsd\nn\n\n\n\n\nBaseline\n1.20\n0.32\n44\n\n\n6-months\n1.12\n0.35\n44\n\n\nDifference (6-months - Baseline)\n-0.08\n0.23\n44\n\n\n\n\n\n\n\n\n8.66Solution\n\n\nWhat test procedure can be used to test if there has been a change in mean HDL-cholesterol levels among teenage children who undergo such a program?\n\n\nA paired \\(t\\)-test with two-sided alternative.\n\n\n\n\n8.67Solution\n\n\nImplement the test procedure mentioned in Problem 8.66 and report a p-value (two-sided) as precisely as possible using the appropriate tables.\n\n\n\n\nT-statistic: -2.307 \nCompare to t-distribution with 43 degrees of freedom\nCritical Value: 2.017 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.02592 \n95 % CI: ( -0.1499 , -0.01007 )\n\n\n\n\n\n\n8.68Solution\n\n\nProvide a 95% CI for the true mean change over 6 months among teenage children exposed to the program.\n\n\nThis was done in 8.67.\n\n\n\n\n8.69Solution\n\n\nSuppose the results in Problem 8.67 are statistically significant (they may or may not be). Does this necessarily mean that the education program per se is the reason why the HDL-cholesterol levels have changed? If not, is there some way to change the design to allow us to be more confident about the specific effects of the education program?\n\n\nNo. We need a control group to compare against."
  },
  {
    "objectID": "review/pp_08.html#endocrinology",
    "href": "review/pp_08.html#endocrinology",
    "title": "Chapter 8 Practice Problems",
    "section": "Endocrinology",
    "text": "Endocrinology\nA study was performed comparing the rate of bone formation between black and white adults. The data in Table 8.16 were presented.\n\n\n\n\n\n\n\n\nTable 8.16\n\n\nComparison of bone-formation rate between black versus white adults\n\n\n\nBlacks (n = 12) Mean ± se\nWhites (n = 13) Mean ± se\n\n\n\n\nBone-formation rate\n0.033 ± 0.007\n0.095 ± 0.012\n\n\n\n\n\n\n\n\n8.70Solution\n\n\nWhat method can be used to compare mean bone-formation rate between blacks and whites?\n\n\nA two-sample \\(t\\)-test with a two-sided alternative.\n\n\n\n\n8.71Solution\n\n\nImplement the method in Problem 8.70 and report a p-value. Assume equal variances.\n\n\nNote that they gave use standard errors, not standard deviations.\n\n\nPooled Standard Deviation: 0.03547 \nT-statistic: -4.367 \nCompare to t-distribution with 23 degrees of freedom\nCritical Value: 2.069 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.0002257 \n95 % CI: ( -0.09137 , -0.03263 )\n\n\n\n\n\n\n8.72Solution\n\n\nObtain a 95% CI for the mean difference in bone-formation rate between the two groups.\n\n\nThis was donein 8.71."
  },
  {
    "objectID": "review/pp_08.html#health-promotion",
    "href": "review/pp_08.html#health-promotion",
    "title": "Chapter 8 Practice Problems",
    "section": "Health Promotion",
    "text": "Health Promotion\nOne goal of an exercise training program is to improve the physical fitness of individuals as measured by their resting heart rate. Suppose it is known that the mean resting heart rate of a group of sedentary individuals is 85 beats/min with a sd of 10 beats/min. A clinical trial is proposed where half the subjects will be randomized to a low intensity exercise program (group A), while the other half will get no exercise training (group B). It is expected that after 6 months, the mean resting heart rate of group A will decline by 3 beats/min while group B will show no mean change in resting heart rate. Assume that the correlation coefficient between resting heart rate measured 6 months apart in the same individual is .8.\n\n8.73Solution\n\n\nHow many subjects are needed in each group to achieve an 80% power if a two-sided test is used with significance level of .05?\n\n\n\n\n\n\n\n8.74Solution\n\n\nSuppose that 50 subjects are randomized to each group. How much power will the study have if a 2-sided test is used with a significance level of .05?"
  },
  {
    "objectID": "review/pp_08.html#ophthalmology-3",
    "href": "review/pp_08.html#ophthalmology-3",
    "title": "Chapter 8 Practice Problems",
    "section": "Ophthalmology 3",
    "text": "Ophthalmology 3\nRetinitis pigmentosa (RP) is the name given to a family of inherited retinal degenerative diseases that may be transmitted through various modes of inheritance. The most common features include a history of night blindness, loss of visual field, and pigment clumping in the retina. Several reports of lipid abnormalities have been reported in RP patients. However, a consistent trend as regards either excesses or deficiencies in lipid levels has not been apparent. In one study, fatty-acid levels were measured in a group of RP patients and normal controls. The data in Table 8.10 were reported on one particular fatty acid (docosahexaenoic acid) (labeled 22:6w3) in individuals with dominant disease and normal controls.\n\n\n\n\n\n\n\n\nTable 8.10\n\n\nMean levels of plasma 22:6w3 (adjusted for age) (Units are nmol/mL plasma)\n\n\n\nMean\nsd\nn\n\n\n\n\nDominant affected individuals\n34.8\n20.8\n36\n\n\nNormal controls\n47.8\n30.3\n68\n\n\n\n\n\n\n\n\n8.52Solution\n\n\nWhat is an appropriate procedure to test if the mean level of 22:6w3 differs between dominant affected individuals and normal controls? State the hypotheses being tested. Is a one-sided or a two-sided test appropriate here?\n\n\nTwo-sample \\(t\\)-test with two-sided alternative.\n\n\n\n\n8.53Solution\n\n\nPerform the hypothesis test in Problem 8.52 and report a p-value. Do not assume equal variances. The degrees of freedom is estimated to be 95.1.\n\n\n\n\nT-statistic: -2.573 \nCompare to t-distribution with 95.1 degrees of freedom\nCritical Value: 1.985 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 0.01162 \n95 % CI: ( -23.03 , -2.971 )\n\n\n\n\n\n\n8.54Solution\n\n\nProvide a 95% CI for the difference in means between the two groups. What does it mean in words?\n\n\nThis was done in 8.53."
  },
  {
    "objectID": "review/pp_08.html#hypertension-5",
    "href": "review/pp_08.html#hypertension-5",
    "title": "Chapter 8 Practice Problems",
    "section": "Hypertension 5",
    "text": "Hypertension 5\nAs one component of a study of blood pressure (BP) response to nonpharmacologic interventions, subjects with high normal BP (DBP of 80-89) who were moderately overweight were randomized to either an active weight-loss intervention or a control group. Blood-pressure measurements were taken both at baseline and after 18 months of intervention and follow-up. Using change in blood pressure over this period as the outcome, the data in Table 8.12 were obtained for diastolic blood pressure (DBP).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.12\n\n\nComparison of change in DBP between the weight loss versus the control group\n\n\nTreatment Group\n\nChange in DBP (mm Hg)\n\n\n\nn\nMean\nsd\n\n\n\n\nWeight loss\n308\n-6.16\n5.88\n\n\nControl\n246\n-3.91\n6.12\n\n\n\n\n\n\n\n\n8.60Solution\n\n\nAssuming that the variances are equal in the two groups, test whether there was a significant difference in mean DBP change between the weight-loss versus the control group.\n\n\n\n\nPooled Standard Deviation: 5.988 \nT-statistic: -4.395 \nCompare to t-distribution with 552 degrees of freedom\nCritical Value: 1.964 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 1.332e-05 \n95 % CI: ( -3.256 , -1.244 )\n\n\n\n\n\nResearchers were also interested in the amount of weight loss attributable to the intervention. Mean weight change for each group is given in Table 8.13.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.13\n\n\nComparison of change in weight between the weight-loss group versus the control group\n\n\nTreatment Group\n\nChange in weight* (kg)\n\n\n\nn\nMean\nsd\n\n\n\n\nWeight loss\n293\n-3.83\n6.12\n\n\nControl\n235\n0.07\n4.01\n\n\n\n*18 month weight-baseline weight\n\n\n\n\n\n\n\n\n\n8.62Solution\n\n\nTest whether the intervention was effective in reducing weight in the weight-loss versus the control group. Assume equal variances.\n\n\n\n\nPooled Standard Deviation: 5.286 \nT-statistic: -8.425 \nCompare to t-distribution with 526 degrees of freedom\nCritical Value: 1.964 \nAt significance level 0.05 , Reject since |t| &gt; critical value \n\n\n\n\n\n\n\n\n\nP-value: 3.469e-16 \n95 % CI: ( -4.809 , -2.991 )"
  },
  {
    "objectID": "review/pp_09.html",
    "href": "review/pp_09.html",
    "title": "Chapter 9 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_09.html#ophthalmology",
    "href": "review/pp_09.html#ophthalmology",
    "title": "Chapter 9 Practice Problems",
    "section": "Ophthalmology",
    "text": "Ophthalmology\nSuppose an ophthalmologist reviews fundus photographs of 30 patients with macular degeneration both before and 3 months after receiving a laser treatment. To assess the efficacy of treatment, each patient is rated as improved, remained the same, or declined.\n\n9.1Solutions\n\n\nIf 20 patients improved, 7 declined, and 3 remained the same, then assess whether or not patients undergoing this treatment are showing significant change from baseline to 3 months afterward. Report a p-value.\n\n\nThis is a sign test. Let \\(X\\) be the number of patients that improved, and let \\(n\\) be the number of patients that showed a change. Then \\(X \\sim \\mathrm{Binom}(n, p)\\). We want to test \\(H_0: p = 1/2\\) versus \\(H_A: p \\neq 1/2\\). We are told that \\(x = 20\\) and \\(n = 27\\). We can use the normal approximation to the binomial since \\(np_0(1-p_0) \\geq 5\\)\n\n27 * 0.5 * (1 - 0.5)\n\n[1] 6.75\n\n\nWe calculate the \\(z\\) statistic \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|20/27 - 0.5| - 1/54}{\\sqrt{0.5(1-0.5)/27}}\n\\]\n\n(abs(20/27 - 0.5) - 1/54) / sqrt(0.5 * (1 - 0.5) / 27)\n\n[1] 2.309\n\n\nWe compare this to standard normal distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.309)\n\n[1] 0.02094\n\n\nSo we have evidence that there is a different from baseline.\nThe real way in R is\n\nprop.test(x = 20, n = 27, p = 0.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0209\n\n\nThe exact binomial approach, which is always valid, calculates the sum of the probabilities that are at least as probabable as \\(x = 20\\) given \\(X \\sim \\mathrm{Binom}(27, 0.5)\\)\n\n\n\n\n\n\n\n\n\n\nbinom.test(x = 20, n = 27, p = 0.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0192\n\n\n\n\n\nSuppose that the patients are divided into two groups according to initial visual acuity (VA). Of the 14 patients with VA 20-40 or better, 8 improved, 5 declined, and 1 stayed the same. Of the 16 patients with VA worse than 20–40, 12 improved, 2 declined, and 2 stayed the same.\n\n9.2Solutions\n\n\nAssess the results in the subgroup of patients with VA of 20–40 or better.\n\n\nThis is also a sign test with observed \\(x = 8\\) and \\(n = 13\\). We need to use an exact test since \\(np_0(1-p_0) &lt; 5\\)\n\n13 * 0.5 * (1 - 0.5)\n\n[1] 3.25\n\n\n\n\n\n\n\n\n\n\n\n\nbinom.test(x = 8, n = 13, p = 0.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.581\n\n\nSo we do not have evidence of a difference.\n\n\n\n\n9.3Solutions\n\n\nAssess the results in the subgroup of patients with VA worse than 20–40.\n\n\nThis is also a sign test with observed \\(x = 12\\) and \\(n = 14\\). We need to use an exact test since \\(np_0(1-p_0) &lt; 5\\)\n\n14 * 0.5 * (1 - 0.5)\n\n[1] 3.5\n\n\n\n\n\n\n\n\n\n\n\n\nbinom.test(x = 12, n = 14, p = 0.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0129\n\n\nSo we do have evidence of a difference."
  },
  {
    "objectID": "review/pp_09.html#diabetes",
    "href": "review/pp_09.html#diabetes",
    "title": "Chapter 9 Practice Problems",
    "section": "Diabetes",
    "text": "Diabetes\nAn experiment was conducted to study responses to different methods of taking insulin in patients with type I diabetes. The percentages of glycosolated hemoglobin initially and 3 months after taking insulin by nasal spray are given in the following data frame.\n\nhemo &lt;- tibble(\n  id = 1:8,\n  before = c(11.0, 7.7, 5.9, 9.5, 8.7, 8.6, 11.0, 6.9),\n  after3 = c(10.2, 7.9, 6.5, 10.4, 8.8, 9.0, 9.5, 7.6)\n)\nhemo\n\n# A tibble: 8 × 3\n     id before after3\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1   11     10.2\n2     2    7.7    7.9\n3     3    5.9    6.5\n4     4    9.5   10.4\n5     5    8.7    8.8\n6     6    8.6    9  \n7     7   11      9.5\n8     8    6.9    7.6\n\n\n\n9.4Solutions\n\n\nPerform a t test to compare the percentages of glycosolated hemoglobin before and 3 months after treatment.\n\n\nWe will do a paired \\(t\\)-test. Let \\(D_i\\) be the difference in percentages of glycosolated hemoglobin (before - after3) for individual \\(i\\). We assume \\(D_i \\sim N(\\mu, \\sigma^2)\\). We want to test \\(H_0: \\mu = 0\\) versus \\(H_A: \\mu \\neq 0\\). We calculate summary statistics\n\nhemo |&gt;\n  mutate(diff = before - after3) |&gt;\n  summarize(\n    mean = mean(diff),\n    sd = sd(diff),\n    n = n())\n\n# A tibble: 1 × 3\n     mean    sd     n\n    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 -0.0750 0.821     8\n\n\nWe calculate a \\(t\\)-statistic \\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{-0.075 - 0}{0.8207 / \\sqrt{8}}\n\\] Numerically\n\n(-0.075 - 0) / (0.8207 / sqrt(8))\n\n[1] -0.2585\n\n\nWe compare this to a \\(t_{7}\\) distribution\n\nplt_t(ub = -0.2585, two_sided = TRUE, df = 7) +\n  geom_vline(xintercept = -0.2585, lty = 2)\n\n\n\n\n\n\n\n\n\n2 * pt(-0.2585, df = 9)\n\n[1] 0.8018\n\n\nSince the \\(p\\)-value is so large, we do not have evidence that the mean difference in percentage of glycosolated hemoglobin is different from 0.\nThe real-way in R uses t.test()\n\nhemo &lt;- mutate(hemo, diff = before - after3)\nt.test(diff ~ 1, data = hemo, mu = 0) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.803\n\n\n\n\n\n\n9.5aSolutions\n\n\nSuppose normality is not assumed. What nonparameteric test would you use?\n\n\nThe Wicoxin Signed-rank test.\n\n\n\n\n9.5bSolutions\n\n\nCalculate the test-statistic for the tetst in 9.5a.\n\n\nWe first rank by the absolute values\n\n\n\n\n\n\n\n\nDifference\n|Difference|\nRank\n\n\n\n\n-0.1\n0.1\n1\n\n\n-0.2\n0.2\n2\n\n\n-0.4\n0.4\n3\n\n\n-0.6\n0.6\n4\n\n\n-0.7\n0.7\n5\n\n\n0.8\n0.8\n6\n\n\n-0.9\n0.9\n7\n\n\n1.5\n1.5\n8\n\n\n\n\n\n\n\nWe then sum the ranks of either all of the negative numbers, or all of the positive numbers (doesn’t matter which one).\nIf you sum the ranks of the positives, you get\n\n6 + 8\n\n[1] 14\n\n\nIf you sum the ranks of the negatives, you get\n\n1 + 2 + 3 + 4 + 5 + 7\n\n[1] 22\n\n\n\n\n\n\n9.5cSolutions\n\n\nUse R to run the test from 9.5 aa and report the p-value.\n\n\n\nwilcox.test(diff ~ 1, data = hemo) |&gt;\n  tidy() |&gt;\n  select(statistic, p.value)\n\n# A tibble: 1 × 2\n  statistic p.value\n      &lt;dbl&gt;   &lt;dbl&gt;\n1        14   0.641\n\n\nWe would again fail to reject \\(H_0\\).\n\n\n\n\n9.6Solutions\n\n\nCompare your results in Problems 9.4 and 9.5.\n\n\nBoth indicate no evidence of a difference."
  },
  {
    "objectID": "review/pp_09.html#pathology",
    "href": "review/pp_09.html#pathology",
    "title": "Chapter 9 Practice Problems",
    "section": "Pathology",
    "text": "Pathology\nThe data in the following data frame are measurements from a group of 10 normal males and 11 males with left-heart disease taken at autopsy at a particular hospital. Measurements were made on several variables at that time, and the table presents the measurements on total heart weight (THW) and total body weight (BW). Assume that the diagnosis of left-heart disease is made independently of these variables.\n\nheart &lt;- tibble(\n  status = c(rep(\"disease\", 11), rep(\"normal\", 10)),\n  thw = c(450, 760, 325, 495, 285, 450, 460, 375, 310, 615, 425,\n          245, 350, 340, 300, 310, 270, 300, 360, 405, 290),\n  bw = c(54.6, 73.5, 50.3, 44.6, 58.1, 61.3, 75.3, 41.1, 51.5, 41.7, 59.7,\n         40.8, 67.4, 53.3, 62.2, 65.5, 47.5, 51.2, 74.9, 59.0, 40.5)\n)\n\n\n9.7Solutions\n\n\nSuppose normality is not assumed. What nonparametric test can be used to compare total heart weight of males with left-heart disease with that of normal males?\n\n\nA Wilcoxin rank-sum test. This is a two-sample problem.\n\n\n\n\n9.8aSolutions\n\n\nCalculate a test-statistic for the test in 9.7.\n\n\nWe first rank all of the thw values\n\n\n\n\n\n\n\n\nstatus\nthw\nrank\n\n\n\n\nnormal\n245\n1.0\n\n\nnormal\n270\n2.0\n\n\ndisease\n285\n3.0\n\n\nnormal\n290\n4.0\n\n\nnormal\n300\n5.5\n\n\nnormal\n300\n5.5\n\n\ndisease\n310\n7.5\n\n\nnormal\n310\n7.5\n\n\ndisease\n325\n9.0\n\n\nnormal\n340\n10.0\n\n\nnormal\n350\n11.0\n\n\nnormal\n360\n12.0\n\n\ndisease\n375\n13.0\n\n\nnormal\n405\n14.0\n\n\ndisease\n425\n15.0\n\n\ndisease\n450\n16.5\n\n\ndisease\n450\n16.5\n\n\ndisease\n460\n18.0\n\n\ndisease\n495\n19.0\n\n\ndisease\n615\n20.0\n\n\ndisease\n760\n21.0\n\n\n\n\n\n\n\nNotice that there are two ties. We handle this by giving them the average rank (e.g., the average of ranks 16 and 17, resulting in both having rank 16.5).\nWe now sum either all of the ranks of the disease group, or all of the ranks of the normal group (doesn’t matter which).\nFor the normal group, we get\n\n1 + 2 + 4 + 5.5 + 5.5 + 7.5 + 10 + 11 + 12 + 14\n\n[1] 72.5\n\n\nFor the disease group, we get\n\n3 + 7.5 + 9 + 13 + 15 + 16.5 + 16.5 + 18 + 19 + 20 + 21\n\n[1] 158.5\n\n\n\n\n\n\n9.8aSolutions\n\n\nRun the test from 9.7 in R and report a p-value.\n\n\n\nwilcox.test(thw ~ status, data = heart, exact = FALSE) |&gt;\n  tidy() |&gt;\n  select(statistic, p.value)\n\n# A tibble: 1 × 2\n  statistic p.value\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      92.5 0.00911\n\n\nNote (but not important for you) that R’s statistic subtract \\(m(m+1)/2\\) from the statistic, where \\(m\\) is the sample size of the group. So\n\n158.5 - 11 * (11 + 1) / 2\n\n[1] 92.5"
  },
  {
    "objectID": "review/pp_09.html#cardiology",
    "href": "review/pp_09.html#cardiology",
    "title": "Chapter 9 Practice Problems",
    "section": "Cardiology",
    "text": "Cardiology\nPropranolol is a standard drug given to ease the pain of patients with episodes of unstable angina. A new drug for the treatment of this disease is tested on 30 pairs of patients who are matched on a one-to-one basis according to age, sex, and clinical condition and are assessed as to the severity of their pain. Suppose that in 15 pairs of patients, the patient with the new drug has less pain; in 10 pairs of patients, the patient with propranolol has less pain; and in 5 pairs of patients, the pain is about the same with the two drugs.\n\n9.9Solutions\n\n\nWhat is the appropriate test to use here?\n\n\nUse the sign test. Let \\(X\\) be the number of patients with less pain. Let \\(n\\) be the number of patients reporting a difference. Then \\(X \\sim \\mathrm{Binom}(n, p)\\). We want to test \\(H_0: p = 1/2\\) versus \\(H_A: p \\neq 1/2\\).\n\n\n\n\n9.10Solutions\n\n\nPerform the test in Problem 9.9 and report a p-value.\n\n\nWe are told that we observe \\(x = 10\\) and \\(n = 25\\). We can use the normal approximation to the binomial since \\(np_0(1-p_0) \\geq 5\\)\n\n25 * 0.5 * (1 - 0.5)\n\n[1] 6.25\n\n\nWe calculate the \\(z\\) statistic \\[\nz = \\frac{|\\hat{p} - p_0| - 1/(2n)}{\\sqrt{p_0(1-p_0)/n}} = \\frac{|10/25 - 0.5| - 1/50}{\\sqrt{0.5(1-0.5)/25}}\n\\]\n\n(abs(10/25 - 0.5) - 1/50) / sqrt(0.5 * (1 - 0.5) / 25)\n\n[1] 0.8\n\n\nWe compare this to standard normal distribution\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-0.8)\n\n[1] 0.4237\n\n\nSo we do not have evidence of a difference in pain level. The real way in R is\n\nprop.test(x = 10, n = 25, p = 0.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.424\n\n\nThe exact binomial approach, which is always valid, calculates the sum of the probabilities that are at least as probabable as \\(x = 10\\) given \\(X \\sim \\mathrm{Binom}(25, 0.5)\\)\n\n\n\n\n\n\n\n\n\n\nbinom.test(x = 10, n = 25, p = 0.5) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.424"
  },
  {
    "objectID": "review/pp_09.html#ophthalmology-1",
    "href": "review/pp_09.html#ophthalmology-1",
    "title": "Chapter 9 Practice Problems",
    "section": "Ophthalmology",
    "text": "Ophthalmology\nA camera has been developed to detect the presence of cataract more accurately. Using this camera, the gray level of each point (or pixel) in the lens of a human eye can be characterized into 256 gradations, where a gray level of 1 represents black and a gray level of 256 represents white. To test the camera, photographs were taken of 6 randomly selected normal eyes and 6 randomly selected cataractous eyes (the two groups consist of different people). The median gray level of each eye was computed over the 10,000+ pixels in the lens. The data are given in the data frame below.\n\ncateract &lt;- tibble(\n  status = c(rep(\"cateractous\", 6), rep(\"normal\", 6)),\n  gray_level = c(161, 140, 136, 171, 106, 149, 158, 182, 185, 145, 167, 177)\n)\n\n\n9.11Solutions\n\n\nWhat nonparametric test could be used to compare the median gray levels of normal and cataractous eyes?\n\n\nA Wilcoxin rank-sum test.\n\n\n\n\n9.12aSolutions\n\n\nCalculate the test-statistic from the test in 9.11.\n\n\nWe first rank all of the gray_level values\n\n\n\n\n\n\n\n\nstatus\ngray_level\nrank\n\n\n\n\ncateractous\n106\n1\n\n\ncateractous\n136\n2\n\n\ncateractous\n140\n3\n\n\nnormal\n145\n4\n\n\ncateractous\n149\n5\n\n\nnormal\n158\n6\n\n\ncateractous\n161\n7\n\n\nnormal\n167\n8\n\n\ncateractous\n171\n9\n\n\nnormal\n177\n10\n\n\nnormal\n182\n11\n\n\nnormal\n185\n12\n\n\n\n\n\n\n\nWe then sum either all of the normal ranks or all of the cateractous ranks (doesn’t matter which one).\nFor the normal ranks:\n\n4 + 6 + 8 + 10 + 11 + 12\n\n[1] 51\n\n\nFor the cateractous ranks\n\n1 + 2 + 3 + 5 + 7 + 9\n\n[1] 27\n\n\n\n\n\n\n9.12bSolutions\n\n\nUse R to carry out the test from 9.11.\n\n\n\nwilcox.test(gray_level ~ status, data = cateract) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0649\n\n\nSo we have some weak evidence of a difference in gray pixels between the two groups."
  },
  {
    "objectID": "review/pp_09.html#psychiatry",
    "href": "review/pp_09.html#psychiatry",
    "title": "Chapter 9 Practice Problems",
    "section": "Psychiatry",
    "text": "Psychiatry\nSuppose we are conducting a study of the effectiveness of lithium therapy for manic-depressive patients. The study is carried out at two different centers, and we want to determine if the patient populations are comparable at baseline. A self-rating questionnaire about their general psychological well-being is administered to the prospective patients at the two different centers. The outcome measure on the questionnaire is a four-category scale: (1) = feel good; (2) = usually feel good, once in a while feel nervous; (3) = feel nervous half the time; (4) = usually feel nervous. Suppose the data at the two different centers are as follows:\n\nquest &lt;- tibble(\n  feeling = c(3, 4, 1, 1, 3, 2, 3, 4, 4, 3, 2, 4, 4, 4,\n              1, 2, 1, 3, 2, 4, 1, 2, 1, 3, 1, 2, 2, 2, 1, 3),\n  center = c(rep(\"1\", 14), rep(\"2\", 16))\n)\n\n\n9.13Solutions\n\n\nWhat type of data does this type of scale represent?\n\n\nOrdinal.\n\n\n\n\n9.14Solutions\n\n\nWhy might a parametric test not be useful with this type of data?\n\n\nThe parametric test you know is the \\(t\\)-test, and this assumes cardinal values. E.g., it assumes the mean and standard deviations are meaningful, but they are not appropriate here since we have ordinal values.\n\n\n\n\n9.15Solutions\n\n\nAssess if there is any significant difference in the responses of the two patient populations using a non-parametric test. Just do this in R. No need to calculate the test statistic.\n\n\nWe should use a Wilcox rank-sum test.\n\nwilcox.test(feeling ~ center, data = quest, exact = FALSE) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0120\n\n\nWe have some evidence that there is a difference in centers."
  },
  {
    "objectID": "review/pp_09.html#psychiatry-1",
    "href": "review/pp_09.html#psychiatry-1",
    "title": "Chapter 9 Practice Problems",
    "section": "Psychiatry",
    "text": "Psychiatry\nMuch attention has been given in recent years to the role of transcendental meditation in improving health, particularly in lowering blood pressure. One hypothesis that emerges from this work is that transcendental meditation might also be useful in treating psychiatric patients with symptoms of anxiety. Suppose that a protocol of meditational therapy is administered once a day to 20 patients with anxiety. The patients are given a psychiatric exam at baseline and at a follow-up exam 2 months later. The degree of improvement is rated on a 10-point scale, with 1 indicating the most improvement and 10 the least improvement. Similarly, 26 comparably affected patients with anxiety are given standard psychotherapy and are asked to come back 2 months later for a follow-up exam. The results are given in Table 9.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.2\n\n\nDegree of improvement in patients with anxiety who are treated with transcendental meditation or psychotherapy\n\n\n\nMeditation\n\n\nPsychotherapy\n\n\n\nDegree of Improvement\nFrequency\nDecree of Improvement\nFrequency\n\n\n\n\n1\n3\n1\n0\n\n\n2\n4\n2\n2\n\n\n3\n7\n3\n5\n\n\n4\n3\n4\n3\n\n\n5\n2\n5\n8\n\n\n6\n1\n6\n4\n\n\n7\n0\n7\n2\n\n\n8\n0\n8\n1\n\n\n9\n0\n9\n1\n\n\n10\n0\n10\n0\n\n\n\n\n\n\n\n\n9.16Solutions\n\n\nWhy might a parametric test not be useful here?\n\n\nThe parametric test you know is the \\(t\\)-test, and this assumes cardinal values. E.g., it assumes the mean and standard deviations are meaningful, but they are not appropriate here since we have ordinal values.\n\n\n\n\n9.17Solutions\n\n\nWhat nonparametric test should be used to analyze these data?\n\n\nA Wilcox rank-sum test."
  },
  {
    "objectID": "08_reg/11_notes.html",
    "href": "08_reg/11_notes.html",
    "title": "Chapter 11: Regression",
    "section": "",
    "text": "Simple/Multiple Linear Regression\nChapter 11.1–11.6, 11.9, 11.10 of Rosner\nSkip every other section (the ones on correlation)"
  },
  {
    "objectID": "08_reg/11_notes.html#assumptions-and-violations",
    "href": "08_reg/11_notes.html#assumptions-and-violations",
    "title": "Chapter 11: Regression",
    "section": "Assumptions and Violations",
    "text": "Assumptions and Violations\n\nThe linear model has many assumptions.\nYou should always check these assumptions.\nAssumptions in decreasing order of importance\n\nLinearity - The relationship looks like a straight line.\nIndependence - The knowledge of the value of one observation does not give you any information on the value of another.\nEqual Variance - The spread is the same for every value of \\(x\\)\nNormality - The distribution of the errors isn’t too skewed and there aren’t any too extreme points. (Only an issue if you have outliers and a small number of observations because of the central limit theorem).\n\nProblems when violated\n\nLinearity violated - Linear regression line does not pick up actual relationship. Results aren’t meaningful.\nIndependence violated - Linear regression line is unbiased, but standard errors are off. Your \\(p\\)-values are too small.\nEqual Variance violated - Linear regression line is unbiased, but standard errors are off. Your \\(p\\)-values may be too small, or too large.\nNormality violated - Unstable results if outliers are present and sample size is small. Not usually a big deal.\n\n\n\nExerciseSolution\n\n\nWhat assumptions are made about the distribution of the explanatory variable (the \\(x_i\\)’s)?\n\n\nNone. Inference is conditional on the \\(x_i\\)’s."
  },
  {
    "objectID": "08_reg/11_notes.html#evaluating-independence",
    "href": "08_reg/11_notes.html#evaluating-independence",
    "title": "Chapter 11: Regression",
    "section": "Evaluating Independence",
    "text": "Evaluating Independence\n\nThink about the problem.\n\nWere different responses measured on the same observational/experimental unit?\nWere data collected in groups?\n\nExample of non-independence: The temperature today and the temperature tomorrow. If it is warm today, it is probably warm tomorrow.\nExample of non-independence: You are collecting a survey. To obtain individuals, you select a house at random and then ask all participants in this house to answer the survey. The participants’ responses inside each house are probably not independent because they probably share similar beliefs/backgrounds/situations/genetics.\nExample of independence: You are collecting a survey. To obtain individuals, you randomly dial phone numbers until an individual picks up the phone."
  },
  {
    "objectID": "08_reg/11_notes.html#evaluating-other-assumptions",
    "href": "08_reg/11_notes.html#evaluating-other-assumptions",
    "title": "Chapter 11: Regression",
    "section": "Evaluating other assumptions",
    "text": "Evaluating other assumptions\n\nEvaluate issues by plotting the residuals.\nThe residuals are the observed values minus the predicted values. \\[\nr_i = y_i - \\hat{y}_i\n\\]\nIn the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\).\nObtain the residuals by using augment() from broom. They will be the .resid variable.\n\naout &lt;- augment(lmout)\nglimpse(aout)\n\nRows: 31\nColumns: 8\n$ birthweight &lt;dbl&gt; 25, 25, 25, 27, 27, 27, 24, 30, 30, 31, 30, 31, 30, 28, 32…\n$ estriol     &lt;dbl&gt; 7, 9, 9, 12, 14, 16, 16, 14, 16, 16, 17, 19, 21, 24, 15, 1…\n$ .fitted     &lt;dbl&gt; 25.78, 27.00, 27.00, 28.82, 30.04, 31.25, 31.25, 30.04, 31…\n$ .resid      &lt;dbl&gt; -0.7808, -1.9971, -1.9971, -1.8217, -3.0381, -4.2545, -7.2…\n$ .hat        &lt;dbl&gt; 0.18662, 0.13214, 0.13214, 0.07257, 0.04762, 0.03448, 0.03…\n$ .sigma      &lt;dbl&gt; 3.885, 3.868, 3.868, 3.872, 3.844, 3.802, 3.630, 3.889, 3.…\n$ .cooksd     &lt;dbl&gt; 5.888e-03, 2.396e-02, 2.396e-02, 9.589e-03, 1.659e-02, 2.2…\n$ .std.resid  &lt;dbl&gt; -0.22656, -0.56104, -0.56104, -0.49505, -0.81472, -1.13312…\n\n\nYou should always make the following scatterplots. The residuals always go on the \\(y\\)-axis.\n\nFits \\(\\hat{y}_i\\) vs residuals \\(r_i\\).\nResponse \\(y_i\\) vs residuals \\(r_i\\).\nExplanatory variable \\(x_i\\) vs residuals \\(r_i\\).\n\nIn the simple linear model, you can probably evaluate these issues by plotting the data (\\(x_i\\) vs \\(y_i\\)). But residual plots generalize to much more complicated models, whereas just plotting the data does not.\n\n\nExample 1: A perfect residual plot\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeans are straight lines\nResiduals seem to be centered at 0 for all \\(x\\)\nVariance looks equal for all \\(x\\)\nEverything looks perfect\n\n\n\nExample 2: Curved Monotone Relationship, Equal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rexp(100)\nx &lt;- x - min(x) + 0.5\ny &lt;- log(x) * 20 + rnorm(100, sd = 4)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved (but always increasing) relationship between \\(x\\) and \\(y\\).\nVariance looks equal for all \\(x\\)\nResidual plot has a parabolic shape.\nSolution: These indicate a \\(\\log\\) transformation of \\(x\\) could help.\n\ndf_fake %&gt;%\n  mutate(logx = log(x)) -&gt;\n  df_fake\nlm_fake &lt;- lm(y ~ logx, data = df_fake)\n\n\n\n\nExample 3: Curved Non-monotone Relationship, Equal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- -x^2 + rnorm(100)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved relationship between \\(x\\) and \\(y\\)\nSometimes the relationship is increasing, sometimes it is decreasing.\nVariance looks equal for all \\(x\\)\nResidual plot has a parabolic form.\nSolution: Include a squared term in the model (or hire a statistician).\n\nlmout &lt;- lm(y ~ x + I(x^2), data = df_fake)\ntidy(lmout)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   0.0567    0.118      0.482 6.31e- 1\n2 x             0.0172    0.108      0.159 8.74e- 1\n3 I(x^2)       -1.12      0.0848   -13.2   2.20e-23\n\n\n\n\n\nExample 4: Curved Relationship, Variance Increases with \\(Y\\)\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- exp(x + rnorm(100, sd = 1/2))\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved relationship between \\(x\\) and \\(y\\)\nVariance looks like it increases as \\(y\\) increases\nResidual plot has a parabolic form.\nResidual plot variance looks larger to the right and smaller to the left.\nSolution: Take a log-transformation of \\(y\\).\n\ndf_fake %&gt;%\n  mutate(logy = log(y)) -&gt;\n  df_fake\nlm_fake &lt;- lm(logy ~ x, data = df_fake)\n\n\n\n\nExample 5: Linear Relationship, Equal Variances, Skewed Distribution\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraight line relationship between \\(x\\) and \\(y\\).\nVariances about equal for all \\(x\\)\nSkew for all \\(x\\)\nResidual plots show skew.\nSolution: Do nothing, but report skew (usually OK to do)\n\n\n\nExample 6: Linear Relationship, Unequal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- runif(100) * 10\ny &lt;- 0.85 * x + rnorm(100, sd = (x - 5) ^ 2)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear relationship between \\(x\\) and \\(y\\).\nVariance is different for different values of \\(x\\).\nResidual plots really good at showing this.\nSolution: The modern solution is to use sandwich estimates of the standard errors (hire a statistician).\n\nlibrary(lmtest)\nlibrary(sandwich)\nlm_fake &lt;- lm(y ~ x, data = df_fake)\nrobust_se &lt;- vcovHC(lm_fake, type = \"HC1\")\ncoeftest(lm_fake, vcov = robust_se) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -2.86     2.81      -1.02 0.311     -8.44       2.72\n2 x               1.37     0.513      2.67 0.00892    0.351      2.39"
  },
  {
    "objectID": "08_reg/11_notes.html#intervals",
    "href": "08_reg/11_notes.html#intervals",
    "title": "Chapter 11: Regression",
    "section": "Intervals",
    "text": "Intervals\nIn Progress\nConfidence Intervals\nPrediction Intervals\n\nThe confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) are easy to obtain from the output of tidy() if you set conf.int = TRUE.\n\nlmtide &lt;- tidy(lmout, conf.int = TRUE)\nselect(lmtide, conf.low, conf.high)\n\n# A tibble: 2 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   16.2      26.9  \n2    0.308     0.908"
  },
  {
    "objectID": "08_reg/11_notes_1.html",
    "href": "08_reg/11_notes_1.html",
    "title": "Chapter 11: Regression",
    "section": "",
    "text": "Simple Linear Regression\nChapter 11.1–11.6, 11.9, 11.10 of Rosner"
  },
  {
    "objectID": "08_reg/11_notes_1.html#assumptions-and-violations",
    "href": "08_reg/11_notes_1.html#assumptions-and-violations",
    "title": "Chapter 11: Regression",
    "section": "Assumptions and Violations",
    "text": "Assumptions and Violations\n\nThe linear model has many assumptions.\nYou should always check these assumptions.\nAssumptions in decreasing order of importance\n\nLinearity - The relationship looks like a straight line.\nIndependence - The knowledge of the value of one observation does not give you any information on the value of another.\nEqual Variance - The spread is the same for every value of \\(x\\)\nNormality - The distribution of the errors isn’t too skewed and there aren’t any too extreme points. (Only an issue if you have outliers and a small number of observations because of the central limit theorem).\n\nProblems when violated\n\nLinearity violated - Linear regression line does not pick up actual relationship. Results aren’t meaningful.\nIndependence violated - Linear regression line is unbiased, but standard errors are off. Your \\(p\\)-values are too small.\nEqual Variance violated - Linear regression line is unbiased, but standard errors are off. Your \\(p\\)-values may be too small, or too large.\nNormality violated - Unstable results if outliers are present and sample size is small. Not usually a big deal.\n\n\n\nExerciseSolution\n\n\nWhat assumptions are made about the distribution of the explanatory variable (the \\(x_i\\)’s)?\n\n\nNone. Inference is conditional on the \\(x_i\\)’s."
  },
  {
    "objectID": "08_reg/11_notes_1.html#evaluating-independence",
    "href": "08_reg/11_notes_1.html#evaluating-independence",
    "title": "Chapter 11: Regression",
    "section": "Evaluating Independence",
    "text": "Evaluating Independence\n\nThink about the problem.\n\nWere different responses measured on the same observational/experimental unit?\nWere data collected in groups?\n\nExample of non-independence: The temperature today and the temperature tomorrow. If it is warm today, it is probably warm tomorrow.\nExample of non-independence: You are collecting a survey. To obtain individuals, you select a house at random and then ask all participants in this house to answer the survey. The participants’ responses inside each house are probably not independent because they probably share similar beliefs/backgrounds/situations/genetics.\nExample of independence: You are collecting a survey. To obtain individuals, you randomly dial phone numbers until an individual picks up the phone."
  },
  {
    "objectID": "08_reg/11_notes_1.html#evaluating-other-assumptions",
    "href": "08_reg/11_notes_1.html#evaluating-other-assumptions",
    "title": "Chapter 11: Regression",
    "section": "Evaluating other assumptions",
    "text": "Evaluating other assumptions\n\nEvaluate issues by plotting the residuals.\nThe residuals are the observed values minus the predicted values. \\[\nr_i = y_i - \\hat{y}_i\n\\]\nIn the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\).\nObtain the residuals by using augment() from broom. They will be the .resid variable.\n\naout &lt;- augment(lmout)\nglimpse(aout)\n\nRows: 31\nColumns: 8\n$ birthweight &lt;dbl&gt; 25, 25, 25, 27, 27, 27, 24, 30, 30, 31, 30, 31, 30, 28, 32…\n$ estriol     &lt;dbl&gt; 7, 9, 9, 12, 14, 16, 16, 14, 16, 16, 17, 19, 21, 24, 15, 1…\n$ .fitted     &lt;dbl&gt; 25.78, 27.00, 27.00, 28.82, 30.04, 31.25, 31.25, 30.04, 31…\n$ .resid      &lt;dbl&gt; -0.7808, -1.9971, -1.9971, -1.8217, -3.0381, -4.2545, -7.2…\n$ .hat        &lt;dbl&gt; 0.18662, 0.13214, 0.13214, 0.07257, 0.04762, 0.03448, 0.03…\n$ .sigma      &lt;dbl&gt; 3.885, 3.868, 3.868, 3.872, 3.844, 3.802, 3.630, 3.889, 3.…\n$ .cooksd     &lt;dbl&gt; 5.888e-03, 2.396e-02, 2.396e-02, 9.589e-03, 1.659e-02, 2.2…\n$ .std.resid  &lt;dbl&gt; -0.22656, -0.56104, -0.56104, -0.49505, -0.81472, -1.13312…\n\n\nYou should always make the following scatterplots. The residuals always go on the \\(y\\)-axis.\n\nFits \\(\\hat{y}_i\\) vs residuals \\(r_i\\).\nResponse \\(y_i\\) vs residuals \\(r_i\\).\nExplanatory variable \\(x_i\\) vs residuals \\(r_i\\).\n\nIn the simple linear model, you can probably evaluate these issues by plotting the data (\\(x_i\\) vs \\(y_i\\)). But residual plots generalize to much more complicated models, whereas just plotting the data does not.\n\n\nExample 1: A perfect residual plot\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeans are straight lines\nResiduals seem to be centered at 0 for all \\(x\\)\nVariance looks equal for all \\(x\\)\nEverything looks perfect\n\n\n\nExample 2: Curved Monotone Relationship, Equal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rexp(100)\nx &lt;- x - min(x) + 0.5\ny &lt;- log(x) * 20 + rnorm(100, sd = 4)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved (but always increasing) relationship between \\(x\\) and \\(y\\).\nVariance looks equal for all \\(x\\)\nResidual plot has a parabolic shape.\nSolution: These indicate a \\(\\log\\) transformation of \\(x\\) could help.\n\ndf_fake %&gt;%\n  mutate(logx = log(x)) -&gt;\n  df_fake\nlm_fake &lt;- lm(y ~ logx, data = df_fake)\n\n\n\n\nExample 3: Curved Non-monotone Relationship, Equal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- -x^2 + rnorm(100)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved relationship between \\(x\\) and \\(y\\)\nSometimes the relationship is increasing, sometimes it is decreasing.\nVariance looks equal for all \\(x\\)\nResidual plot has a parabolic form.\nSolution: Include a squared term in the model (or hire a statistician).\n\nlmout &lt;- lm(y ~ x^2, data = df_fake)\n\n\n\n\nExample 4: Curved Relationship, Variance Increases with \\(Y\\)\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- exp(x + rnorm(100, sd = 1/2))\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved relationship between \\(x\\) and \\(y\\)\nVariance looks like it increases as \\(y\\) increases\nResidual plot has a parabolic form.\nResidual plot variance looks larger to the right and smaller to the left.\nSolution: Take a log-transformation of \\(y\\).\n\ndf_fake %&gt;%\n  mutate(logy = log(y)) -&gt;\n  df_fake\nlm_fake &lt;- lm(logy ~ x, data = df_fake)\n\n\n\n\nExample 5: Linear Relationship, Equal Variances, Skewed Distribution\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraight line relationship between \\(x\\) and \\(y\\).\nVariances about equal for all \\(x\\)\nSkew for all \\(x\\)\nResidual plots show skew.\nSolution: Do nothing, but report skew (usually OK to do)\n\n\n\nExample 6: Linear Relationship, Unequal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- runif(100) * 10\ny &lt;- 0.85 * x + rnorm(100, sd = (x - 5) ^ 2)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear relationship between \\(x\\) and \\(y\\).\nVariance is different for different values of \\(x\\).\nResidual plots really good at showing this.\nSolution: The modern solution is to use sandwich estimates of the standard errors (hire a statistician).\n\nlibrary(sandwich)\nlm_fake &lt;- lm(y ~ x, data = df_fake)\nsemat &lt;- sandwich(lm_fake)\ntidy(lm_fake) %&gt;%\n  mutate(sandwich_se = sqrt(diag(semat)),\n         sandwich_t  = estimate / sandwich_se,\n         sandwich_p  = 2 * pt(-abs(sandwich_t), df = df.residual(lm_fake)))\n\n# A tibble: 2 × 8\n  term    estimate std.error statistic p.value sandwich_se sandwich_t sandwich_p\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 (Inter…    -2.86     2.01      -1.43 1.57e-1       2.78       -1.03    0.307  \n2 x           1.37     0.345      3.97 1.37e-4       0.508       2.70    0.00827"
  },
  {
    "objectID": "08_reg/11_notes_2.html",
    "href": "08_reg/11_notes_2.html",
    "title": "Chapter 11: Correlation",
    "section": "",
    "text": "Learning Objectives\n\nPearson Correlation\nRank Correlation\nRosner 11.7, 11.8, 11.11, 11.12, 11.13\n\n\n\nReferences"
  },
  {
    "objectID": "08_reg/11_notes_1.html#some-exercises-on-assumptions",
    "href": "08_reg/11_notes_1.html#some-exercises-on-assumptions",
    "title": "Chapter 11: Regression",
    "section": "Some Exercises on Assumptions",
    "text": "Some Exercises on Assumptions\n\nExerciseSolution\n\n\nEvaluate the assumptions of the linear fit of birthweight on estriol.\n\n\nLooks good. No curvilinear trends, equal variance looks fine.\n\nggplot(estriol, aes(x = estriol, y = birthweight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\n\n\nlm(birthweight ~ estriol, data = estriol) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolutions\n\n\nEvaluate the assumptions of the linear fit of blood alcohol level on beers.\n\n\nLooks good. No curvilinear trends, equal variance looks fine.\n\nggplot(bac, aes(x = beers, y = bac)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\n\n\nlm(bac ~ beers, data = bac) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nEvaluate the assumptions of the linear fit of fev level on height.\n\n\nIt looks very curved, doesn’t it? But the equal variance assumption looks fine.\n\nggplot(fev, aes(x = height, y = fev)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\n\n\nlm(fev ~ height, data = fev) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nLogging height doesn’t do anything, so we could add a quadratic trend\n\nggplot(fev, aes(x = height, y = fev)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE)\n\n\n\n\n\n\n\nlm(fev ~ height + I(height^2), data = fev) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nThis looks way better. The squared-height term is significant:\n\nlm(fev ~ height + I(height^2), data = fev) |&gt;\n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  7.77      2.93          2.65 0.0265 \n2 height      -0.118     0.0378       -3.12 0.0124 \n3 I(height^2)  0.000542  0.000121      4.48 0.00154\n\n\n\n\n\n\nExerciseSolution\n\n\nThe following data frame, from Wilson (1999), contains the numbers of reptile and amphibian species and the island areas for seven islands in the West Indies.\n\nspecies &lt;- tribble(\n  ~Area, ~Species,\n  44218, 100,\n  29371, 108,\n  4244, 45,\n  3435, 53,\n  32, 16,\n  5, 11,\n  1, 7)\n\nFind an appropriate linear regression model for relating the effect of island area on species number. Find the regression estimates. Interpret them.\n\n\nPlotting the data, it looks like we need to take logs:\nFit the linear model:\n\nspecies %&gt;%\n  mutate(log_area = log2(Area),\n         log_species = log2(Species)) -&gt;\n  species\n\nlm_sp &lt;- lm(log_species ~ log_area, data = species)\ntidy(lm_sp)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    2.79     0.127       22.0 0.00000362\n2 log_area       0.250    0.0121      20.6 0.00000496\n\n\nIslands with twice the area have 2^0.25 times as many species on average."
  },
  {
    "objectID": "08_reg/11_notes.html#some-exercises-on-assumptions",
    "href": "08_reg/11_notes.html#some-exercises-on-assumptions",
    "title": "Chapter 11: Regression",
    "section": "Some Exercises on Assumptions",
    "text": "Some Exercises on Assumptions\n\nExerciseSolution\n\n\nEvaluate the assumptions of the linear fit of birthweight on estriol.\n\n\nLooks good. No curvilinear trends, equal variance looks fine.\n\nggplot(estriol, aes(x = estriol, y = birthweight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\n\n\nlm(birthweight ~ estriol, data = estriol) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolutions\n\n\nEvaluate the assumptions of the linear fit of blood alcohol level on beers.\n\n\nLooks good. No curvilinear trends, equal variance looks fine.\n\nggplot(bac, aes(x = beers, y = bac)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\n\n\nlm(bac ~ beers, data = bac) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nEvaluate the assumptions of the linear fit of fev level on height.\n\n\nIt looks very curved, doesn’t it? But the equal variance assumption looks fine.\n\nggplot(fev, aes(x = height, y = fev)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n\n\n\n\n\n\n\nlm(fev ~ height, data = fev) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nLogging height doesn’t do anything, so we could add a quadratic trend\n\nggplot(fev, aes(x = height, y = fev)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE)\n\n\n\n\n\n\n\nlm(fev ~ height + I(height^2), data = fev) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n\n\n\n\n\n\n\nThis looks way better. The squared-height term is significant:\n\nlm(fev ~ height + I(height^2), data = fev) |&gt;\n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  7.77      2.93          2.65 0.0265 \n2 height      -0.118     0.0378       -3.12 0.0124 \n3 I(height^2)  0.000542  0.000121      4.48 0.00154\n\n\n\n\n\n\nExerciseSolution\n\n\nThe following data frame, from Wilson (1999), contains the numbers of reptile and amphibian species and the island areas for seven islands in the West Indies.\n\nspecies &lt;- tribble(\n  ~Area, ~Species,\n  44218, 100,\n  29371, 108,\n  4244, 45,\n  3435, 53,\n  32, 16,\n  5, 11,\n  1, 7)\n\nFind an appropriate linear regression model for relating the effect of island area on species number. Find the regression estimates. Interpret them.\n\n\nPlotting the data, it looks like we need to take logs:\nFit the linear model:\n\nspecies %&gt;%\n  mutate(log_area = log2(Area),\n         log_species = log2(Species)) -&gt;\n  species\n\nlm_sp &lt;- lm(log_species ~ log_area, data = species)\ntidy(lm_sp)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)    2.79     0.127       22.0 0.00000362\n2 log_area       0.250    0.0121      20.6 0.00000496"
  },
  {
    "objectID": "08_reg/12_notes.html",
    "href": "08_reg/12_notes.html",
    "title": "Chapter 12: ANOVA",
    "section": "",
    "text": "Learning Objectives\n\nANOVA\nChapter 12 of Rosner\n\n\n\nInterpreting Models\n\nANOVA is just multiple linear regression with categorical explanatory variable(s).\nANCOVA is just multiple linear regression with a categorical explanatory variable of interest and a few other explanatory variables, at least one of which is continuous.\nThere are really only two differences:\n\nThe notation.\nMore emphasis on reporting sums of squares.\n\n\n\n\nInterpreting One-way ANOVA Models\n\nThe Swiss Analgesic Study aimed to evaluate the impact of phenacetin-containing analgesics on kidney function and health.\n\nIt involved 624 women from Basel, Switzerland, who had high phenacetin intake and 626 women with low or no phenacetin intake (control group). The study used urine N-acetyl-P-aminophenyl (NAPAP) levels to measure recent phenacetin use, dividing the study group into high-NAPAP and low-NAPAP subgroups. Both subgroups had higher NAPAP levels than the control group. The women were examined in 1967-1968 and again in 1969, 1970, 1971, 1972, 1975, and 1978, with kidney function assessed through various laboratory tests, including serum-creatinine levels. You can read about the data here, and load it in via:\n\nswiss &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/swiss.csv\")\n\n\n\nSums of Squares for One-way ANOVA Models\n\n\nInterpreting Two-way ANOVA Models\n\n\nSums of Squares for Two-way ANOVA models\n\n\nInterpreting ANCOVA Models\n\n\nSums of Squares for ANCOVA Models\n\n\nReferences"
  },
  {
    "objectID": "08_reg/11_notes.html#model-interpretation-and-estimation",
    "href": "08_reg/11_notes.html#model-interpretation-and-estimation",
    "title": "Chapter 11: Regression",
    "section": "Model, Interpretation, and Estimation",
    "text": "Model, Interpretation, and Estimation\n\nSetting:\n\nOne response variable \\(Y\\)\nMultiple explanatory variables \\(X_1, X_2, \\ldots, X_k\\)\n\nExample: Systolic blood pressure (SBP) is typically measured in newborns. This is used for diagnostic purposes and for studies relating infant SBP to adult SBP. However, SBP is known to vary by birthweight and by the timing of when SBP was measured (in number of days after birth). It is thus important to be able to specifically characterize the relationship between age (days), birthweight (oz), and SBP (mm Hg). Data from a study including 16 newborns are below:\n\nbp &lt;- tribble(\n  ~age, ~birthweight, ~sbp,\n  3, 135, 89,\n  4, 120, 90,\n  3, 100, 83,\n  2, 105, 77,\n  4, 130, 92,\n  5, 125, 98,\n  2, 125, 82,\n  3, 105, 85,\n  5, 120, 96,\n  4, 90, 95,\n  2, 120, 80,\n  3, 95, 79,\n  3, 120, 86,\n  4, 150, 97,\n  3, 160, 92,\n  3, 125, 88,\n)\n\n\n\n\n\n\n\n\nTipMultiple Linear Regression Model (two explanatory variables)\n\n\n\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\n\\]\n\n\\(Y_i\\): Response variable for individual \\(i\\).\n\\(X_{i1}\\): First explanatory variable for individual \\(i\\)\n\\(X_{i2}\\): Second explanatory variable for individual \\(i\\).\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) are called the (partial) regression coefficients.\n\n\n\n\nThe model says that, for fixed values of \\(X_{i1}\\) and \\(X_{i2}\\), the expected value of \\(Y_i\\) is \\[\nE[Y_i|X_{i1}, X_{i2}] = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}\n\\]\nThe actual value of \\(Y_i\\) varies about this point according to a normal distribution.\n\n\n\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) are parameters. We estimate them by ordinary least squares. That is, find the values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) that minimize \\[\n\\sum_{i=1}^n[y_i - (\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2})]^2\n\\]\nCall the OLS estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), and \\(\\hat{\\beta}_2\\).\nThe fits are again the estimated values of \\(y\\) \\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_{i1} + \\hat{\\beta}_2X_{i2}\n\\]\nThe residuals are again the deviations from the fits \\[\ne_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1X_{i1} + \\hat{\\beta}_2X_{i2})\n\\]\nThe estimate of \\(\\sigma^2\\) is again the variance of the residuals. \\[\n\\hat{\\sigma}^2 = \\frac{1}{n - 3}\\sum_{i=1}^ne_i^2\n\\]\nThe \\(n-3\\) is because we have three parameters (\\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\))\n\nRecall: when we just have \\(Y_i \\sim N(\\mu, \\sigma^2)\\), we divide by \\(n-1\\) because of the one parameter \\(\\mu\\).\nRecall: when we use simple linear regression \\(Y_i \\sim N(\\beta_0+\\beta_1X_i, \\sigma^2)\\), we divide by \\(n-2\\) because we have two parameters, \\(\\beta_0\\) and \\(\\beta_1\\)\nNow, we have \\(Y_i \\sim N(\\beta_0+\\beta_1X_{i1} + \\beta_2X_{i2}, \\sigma^2)\\) and three parameters.\n\nYou use lm() to get the OLS estimates in R.\n\nPut the response variable to the left of the tilde ~\nPut the explanatory variables to the right of the tilde ~, separated by +\n\nOrder does not matter.\n\n\nLet’s fit the multiple linear regression model for SBP on age and birthweight.\n\nlm_bp &lt;- lm(sbp ~ age + birthweight, data = bp)\ntidy(lm_bp)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   53.5      4.53       11.8  0.0000000257\n2 age            5.89     0.680       8.66 0.000000934 \n3 birthweight    0.126    0.0343      3.66 0.00290     \n\n\nThe estimated regression line is \\[\ny = 53.45 + 5.89x_1 + 0.13x_2\n\\]\n\n\n\n\n\n\n\nTipInterpret \\(\\beta_j\\)\n\n\n\n\n\\(\\beta_0\\): If \\((X_1, X_2) = (0, 0)\\) is in the range of the data, then \\(\\beta_0\\) is the expected value of \\(Y\\) at \\(X_{1} = 0\\) and \\(X_2 = 0\\). Otherwise, it is just the \\(y\\)-intercept of the regression surface.\n\\(\\beta_1\\): Individuals with one unit larger \\(X_1\\) but the same value of \\(X_2\\) have \\(\\beta_1\\) larger \\(Y\\) values, on average.\n\\(\\beta_2\\): Individuals with one unit larger \\(X_2\\) but the same value of \\(X_1\\) have \\(\\beta_2\\) larger \\(Y\\) values, on average.\n\n\n\n\nIn the SBP example, we would interpret these as:\n\nBabies that are measured one day later but have the same birthweight have a blood pressure 5.89 mm Hg higher on average.\nBabies that weigh one ounce more but are measured at the same age have a blood pressure 0.13 mm Hg higher on average.\n\nWhere does that interpretation come from:\n\\[\ny_{old} = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2}\n\\] \\[\\begin{align*}\ny_{new} &= \\beta_0 + \\beta_1(x_{i1} + 1) + \\beta_2x_{i2}\\\\\n&= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_1\\\\\n&= y_{old} + \\beta_1\n\\end{align*}\\]\nThe interpretation of the relationship between SBP and weight isn’t too useful because 1 ounce isn’t a big difference in bith weight. But we can also say what the expected difference in SBP would be if we had babies that differed by, say, 10 oz.\nBabies that weigh 10 ounces more but are measured at the same age have a blood pressure 1.4 mm Hg higher on average.\n\n\nExerciseSolution\n\n\nProve the interpretation of \\(\\beta_2\\) in the baby weight example that we just stated.\n\n\n\\[\ny_{old} = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2}\n\\] \\[\\begin{align*}\ny_{new} &= \\beta_0 + \\beta_1(x_{i1} + 10) + \\beta_2x_{i2}\\\\\n&= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + 10\\beta_1\\\\\n&= y_{old} + 10\\beta_1\n\\end{align*}\\]\n\n\n\n\nExerciseSolutions\n\n\nData were collected from 20 healthy females, 25–34 years old, to study the relationship between body fat and three predictors. Variables include\n\ntriceps: Triceps skinfold thickness (mm).\nthigh: Thigh circumference (cm).\nmidarm: Midarm circumference (cm)\nfat: Body fat (percent).\n\nThe data can be downloaded from https://dcgerard.github.io/stat_320/data/body.csv\nDownload the data and fit a regression model of fat on triceps and midarm. State the model you fit, defining any paramters and variables. Interpret the regression coefficients.\n\n\n\nLet \\(Y_i\\) be the body fat of female \\(i\\)\nLet \\(X_{i1}\\) be the triceps skinfold thickness of woman \\(i\\).\nLet \\(X_{i2}\\) be the midarm circumference of woman \\(i\\).\n\nOur model is \\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\epsilon_i\n\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\nWe fit this model by least squares:\n\nbody &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/body.csv\")\nlm(fat ~ triceps + midarm, data = body) |&gt;\n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)    6.79      4.49       1.51 0.149      \n2 triceps        1.00      0.128      7.80 0.000000512\n3 midarm        -0.431     0.177     -2.44 0.0258     \n\n\nOur estimated regression line is \\[\ny = 6.79 + \\beta_1 - 0.43 \\beta_2\n\\]\nWomen with 1 mm thicker triceps skinfold thickness, but the same midarm circumference, have 1 percentage points higher body fat on average.\nWomen with 1 cm larger midarm circumference, but the same triceps skinfold thickness, have 0.43 percentage points lower body fat on average.\n\n\n\n\nWhen you were intrepreting the regression coefficients in the above exercise, you should say “percentage points”, not “percent.\n\nPercentage points are additive differences. E.g. 20% is 10 percentage points higher than 10%\nPercent is for multiplicative differences. E.g. 20% is 100% higher than 10%.\n\nDid the results from the body fat example surprise?\n\n\n\n\n\n\n\nImportant\n\n\n\nSimple linear regression coefficients might be very very different than multiple linear regression coefficients.\n\n\n\nlm(fat ~ midarm, data = body) |&gt;\n  tidy() |&gt;\n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)   14.7  \n2 midarm         0.199\n\nlm(fat ~ triceps + midarm, data = body) |&gt;\n  tidy() |&gt;\n  select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    6.79 \n2 triceps        1.00 \n3 midarm        -0.431\n\n\n\nThe different signs for the coefficient of midarm occur because we control for tricepts.\n\nThe SLR coefficient is the relationship between fat and midarm\nThe MLR coefficient is the relationship between fat and midarm for fixed values of triceps (controlling for triceps).\n\nHere is the relationship between fat and midarm\n\n\n\n\n\n\n\n\n\nHere is the relationship between fat and midarm controlling for triceps"
  },
  {
    "objectID": "08_reg/11_notes.html#testing",
    "href": "08_reg/11_notes.html#testing",
    "title": "Chapter 11: Regression",
    "section": "Testing",
    "text": "Testing\n\nWe can test if a regression coefficent is 0, adjusting for other variables via a \\(t\\)-method.\n\n\n\n\n\n\n\nTipTest For Individual Regression Coefficients\n\n\n\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\\(H_A\\): \\(\\beta_j \\neq 0\\)\nIf the null were true, then \\[\nt = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)} \\sim t_{n - p}\n\\]\nWe compare our observed \\(t\\) statistic to this null distribution to get a \\(p\\)-value\n\n\n\n\n\n\n\n\n\nConfidence intervals are of the form \\[\n\\hat{\\beta}_j \\pm t_{n-p,1-\\alpha/2}\\text{SE}(\\hat{\\beta}_j)\n\\]\n\n\n\n\nYou can get the \\(p\\)-values of this test, and confidence intervals, via the output of lm().\n\nlm(fat ~ triceps + thigh + midarm, data = body) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  select(term, p.value, conf.low, conf.high)\n\n# A tibble: 4 × 4\n  term        p.value conf.low conf.high\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.258   -94.4     329.  \n2 triceps       0.170    -2.06     10.7 \n3 thigh         0.285    -8.33      2.62\n4 midarm        0.190    -5.57      1.20\n\n\nThis is only a test for \\(\\beta_j = 0\\) given all other variables are in the model. The results of this test will differ depending on what other variables are in the model, sometimes drastically.\n\nlm(fat ~ triceps + midarm, data = body) |&gt;\n  tidy() |&gt;\n  select(term, p.value)\n\n# A tibble: 3 × 2\n  term            p.value\n  &lt;chr&gt;             &lt;dbl&gt;\n1 (Intercept) 0.149      \n2 triceps     0.000000512\n3 midarm      0.0258     \n\n\nIn this case, the \\(p\\)-values are so different because of collinearity.\n\n\n\n\n\n\n\nTipCollinearity\n\n\n\nThe correlation between explanatory variables.\n\n\n\nIntuitively, if two variables are highly collinear, then they are picking up the same information. So once you control for one of those variables, there is no longer an association between the response and the remaining variable.\n\n\nExerciseSolution\n\n\nIn the body fat example, we saw that the \\(p\\)-values were all very large in the linear model that included all three of triceps skinfold thickness, thigh circumference, and midarm circumference. Does this mean that none of the variables are associated with body fat?\n\n\nNo! This means that we don’t have evidence that each variable is associated with body fat controlling for the other two. If we don’t control for the other two, we might see an association.\n\n\n\n\nWhen you have variables that are highly collinear, you should remove some of them, since they are picking up the same information.\nFrom this, it is far better to consider these tests as comparing two models, one of which is a submodel of the other.\nE.g., in the two explanatory variable case, \\(H_0: \\beta_2 = 0\\) versus \\(H_A: \\beta_j \\neq 0\\) is equivalent to:\n\n\\(H_0: Y_i = \\beta_0 + \\beta_1X_{i1} + \\epsilon_i\\)\n\\(H_0: Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\epsilon_i\\)\n\nThis intuitively leads us to the ANOVA approach to hypothesis testing.\n\n\n\n\n\n\n\nTipANOVA Approach to Hypothesis Testing\n\n\n\n\n\\(H_0\\): (Reduced Model) \\(Y_i = \\beta_0 + \\beta_1X_{i1} + \\cdots + \\beta_{q-1}X_{q-1} + \\epsilon_i\\)\n\\(H_0\\): (Full Model) \\(Y_i = \\beta_0 + \\beta_1X_{i1} + \\cdots + \\beta_{q-1}X_{p-1} + \\epsilon_i\\), for \\(p &gt; q\\)\nI.e., we are testing whether we \\(X_q, X_{q+1},\\ldots,X_{p-1}\\) are associated with \\(Y\\) controlling for \\(X_1,X_2,\\ldots,X_{q-1}\\).\nCalculate the SSE(R) and SSE(F), \\(\\text{df}_F = n-p\\), \\(\\text{df}_R = n-q\\)\nCalculate the \\(F\\) statistic \\[\nF^* = \\frac{[\\text{SSE(R)} - SSE(F)] / (\\text{df}_{R} - \\text{df}_F)}{\\text{SSE(F)} / \\text{df}_F}\n\\]\nIf the null were true, then \\(F\\) follows an \\(F\\) distribution with numerator degrees of freedom \\(p-q\\) and numerator degrees of freedom \\(n-p\\).\nCompare our observed \\(F\\) statistic to this distribution to get a \\(p\\)-value\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo run this test in R, separately fit reduced and full models, then use anova().\n\nreduced &lt;- lm(fat ~ triceps, data = body)\nfull &lt;- lm(fat ~ triceps + thigh + midarm, data = body)\nanova(reduced, full) |&gt;\n  tidy()\n\n# A tibble: 2 × 7\n  term                           df.residual   rss    df sumsq statistic p.value\n  &lt;chr&gt;                                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 fat ~ triceps                           18 143.     NA  NA       NA    NA     \n2 fat ~ triceps + thigh + midarm          16  98.4     2  44.7      3.64  0.0500\n\n\nThis ANOVA table is of the form\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\n\\(p\\)-value\n\n\n\n\nReduced\n\\(\\text{df}_R\\)\nSSE(R)\n\n\n\n\n\n\nFull\n\\(\\text{df}_F\\)\nSSE(F)\n\\(\\text{df}_R - \\text{df}_F\\)\nSSE(R) - SSE(F)\n\\(F^*\\)\n\\(p\\)-value\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf we reject the \\(F\\)-test, then we have evidence that at least one of \\(X_{q}, X_{q+1},\\ldots,X_{p}\\) is associated with \\(Y\\), controlling for \\(X_1, X_2, \\ldots, X_{q-1}\\). NOT that all of them are associated. The \\(F\\) test only tells you that at least one variable is associated.\n\n\n\n\n\n\n\n\nTipOverall \\(F\\)-test\n\n\n\nIt is so common to run the following hypothesis test\n\n\\(H_0\\): (Reduced) \\(Y_i = \\beta_0 + \\epsilon_i\\)\n\\(H_0\\): (Reduced) \\(Y_i = \\beta_0 + \\sum_{j=1}^{p-1}\\beta_jX_{ij} + \\epsilon_i\\)\nThis is called the overall \\(F\\)-test\nThis is output by glance() from {broom}.\n\n\n\n\nHere is the result of the overall \\(F\\)-test for SBP on age and birthweight\n\nlm(sbp ~ age + birthweight, data = bp) |&gt;\n  glance() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n      p.value\n        &lt;dbl&gt;\n1 0.000000984\n\n\nSo we have very strong evidence that either age or birthweight are associated with SBP.\n\n\nExerciseSolution\n\n\nFrom the below ANOVA output, calculate the \\(p\\)-value from the \\(F\\)-statistic directly.\n\nreduced &lt;- lm(fat ~ triceps, data = body)\nfull &lt;- lm(fat ~ triceps + thigh + midarm, data = body)\nanova(reduced, full) |&gt;\n  tidy()\n\n# A tibble: 2 × 7\n  term                           df.residual   rss    df sumsq statistic p.value\n  &lt;chr&gt;                                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 fat ~ triceps                           18 143.     NA  NA       NA    NA     \n2 fat ~ triceps + thigh + midarm          16  98.4     2  44.7      3.64  0.0500\n\n\n\n\n\n1 - pf(3.635, df1 = 2, df2 = 16)\n\n[1] 0.04996\n\n\n\n\n\n\nExerciseSolution\n\n\nFrom the below output, calculate a 95% confidence interval for the coefficient estimates.\n\nlm(fat ~ triceps + thigh, data = body) |&gt;\n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -19.2       8.36     -2.29   0.0348\n2 triceps        0.222     0.303     0.733  0.474 \n3 thigh          0.659     0.291     2.26   0.0369\n\nnrow(body)\n\n[1] 20\n\n\n\n\nThe appropriate \\(t\\)-quantile is\n\nqt(p = 1 - 0.05 / 2, df = 20 - 3)\n\n[1] 2.11\n\n\n\n\\(\\beta_0\\):\n\n-19.1742 - 2.11 * 8.3606    \n\n[1] -36.82\n\n-19.1742 + 2.11 * 8.3606    \n\n[1] -1.533\n\n\n\\(\\beta_1\\):\n\n0.2224 - 2.11 * 0.3034\n\n[1] -0.4178\n\n0.2224 + 2.11 * 0.3034\n\n[1] 0.8626\n\n\n\\(\\beta_2\\)\n\n0.6594 - 2.11 * 0.2912\n\n[1] 0.04497\n\n0.6594 + 2.11 * 0.2912\n\n[1] 1.274\n\n\n\nLet’s verify:\n\nlm(fat ~ triceps + thigh, data = body) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  select(term, conf.low, conf.high)\n\n# A tibble: 3 × 3\n  term        conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -36.8       -1.53 \n2 triceps      -0.418      0.863\n3 thigh         0.0451     1.27 \n\n\n\n\n\n\nExerciseSolution\n\n\nFrom the below output, calculate the \\(p\\)-values from the \\(t\\)-statistics\n\nlm(fat ~ triceps + thigh, data = body) |&gt;\n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -19.2       8.36     -2.29   0.0348\n2 triceps        0.222     0.303     0.733  0.474 \n3 thigh          0.659     0.291     2.26   0.0369\n\nnrow(body)\n\n[1] 20\n\n\n\n\n\n\\(\\beta_0\\)\n\n2 * pt(-2.2934, df = 20 - 3)\n\n[1] 0.03484\n\n\n\\(\\beta_1\\)\n\n2 * pt(-0.7328, df = 20 - 3)\n\n[1] 0.4737\n\n\n\\(\\beta_2\\)\n\n2 * pt(-2.2646, df = 20 - 3)\n\n[1] 0.0369"
  },
  {
    "objectID": "08_reg/11_notes.html#more-than-two-explanatory-variables",
    "href": "08_reg/11_notes.html#more-than-two-explanatory-variables",
    "title": "Chapter 11: Regression",
    "section": "More than Two Explanatory Variables",
    "text": "More than Two Explanatory Variables\n\n\n\n\n\n\nTipMultiple linear regression model\n\n\n\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^{p-1}\\beta_jX_{ij} + \\epsilon_i\n\\]\n\n\\(Y_i\\): Response variable for individual \\(i\\).\n\\(X_{ij}\\): The \\(j\\)th explanatory variable for individual \\(i\\)\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\n\n\n\nWe say there are \\(p-1\\) predictors in the model so that we have \\(p\\) variables, which make the math equations nicer.\nWe estimate the \\(\\beta_j\\)’s again by ordinary least squares, finding the values that minimize \\[\n\\sum_{i=1}^n\\left[y_i - \\left(\\beta_0 + \\sum_{j=1}^{p-1}\\beta_jX_{ij}\\right)\\right]^2\n\\]\nThe OLS estimates are \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_{p-1}\\)\nYou get the OLS estimates by lm().\nSuppose we have a model for fat on triceps, thigh, and midarm. Let\n\n\\(Y_i\\) be the body fat for woman \\(i\\).\n\\(X_{i1}\\) be the tricep skinfold thickness for woman \\(i\\).\n\\(X_{i2}\\) be the thigh circumference for woman \\(i\\).\n\\(X_{i3}\\) be the midarm circumference for woman \\(i\\).\n\nThen we fit the model \\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\n\\] where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\).\nWe fit this model by lm()\n\nlm(fat ~ triceps + thigh + midarm, data = body) |&gt;\n  tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   117.       99.8       1.17   0.258\n2 triceps         4.33      3.02      1.44   0.170\n3 thigh          -2.86      2.58     -1.11   0.285\n4 midarm         -2.19      1.60     -1.37   0.190\n\n\nOur estimated regression equation is \\[\ny = 117.09 + 4.33 x_1 - 2.86x_2 - 2.19x_3\n\\]\nWe interpret the regression coefficients similarly\n\n\\(\\beta_j\\) is the average difference in \\(y\\) values when \\(x_j\\) differs by 1, but all other variables are the same.\n\nE.g., Woman with 1mm larger skinfold thickness, but the same thigh and midarm circumferences, have 4.334 percentage points larger body fat on average.\nThis is a huge mouthful, so in practice we say\n\n\\(\\beta_j\\) is the average difference in \\(y\\) values when \\(x_j\\) differs by 1, controlling for all other variables.\n\nThat is, controlling is shorthand to say what the statistical relationship is given those variables are fixed."
  },
  {
    "objectID": "08_reg/11_notes.html#assessing-assumptions",
    "href": "08_reg/11_notes.html#assessing-assumptions",
    "title": "Chapter 11: Regression",
    "section": "Assessing Assumptions",
    "text": "Assessing Assumptions\n\nPlot residuals vs fits. If there are any trends or unequal variances, then you have issues.\nLet’s do this for the regression of sbp on age and birthweight.\n\nlm(sbp ~ age + birthweight, data = bp) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2, col = 2)\n\n\n\n\n\n\n\n\nIt looks like there might be one outlier/influential point. In STAT-415, you learn about statistics to evaluate if points are outliers or influential.\nThe variance of the residuals varies based on their \\(x\\) values (closer to the mean has lower variance). So folks often prefer plotting the standardized residuals that adjustes fo this. They are standardized to all have variance 1.\n\nlm(sbp ~ age + birthweight, data = bp) |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .std.resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2, col = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIn R, “standardized residuals” are Rosner’s “internally studentized residuals”\n\nNo leave-one-out calculation of standard error.\naugment() returns these.\n\nIn R, “studentized residuals” are Rosner’s “externally studentized residuals”\n\nLeave-one-out calculation of standard error.\nMore robust to outliers.\n\n\n\n\n\nAnother common plot is a partial-residual plot. It plots the association between \\(X_j\\) and \\(Y\\) after removing the effects of all other variables.\n\n\n\n\n\n\n\nTipPartial Residual Plot AKA Added Variable Plot AKA Adjusted Variable Plot\n\n\n\n\nRegress \\(Y\\) (as the response) on every predictor except \\(X_j\\). Obtain the residuals, and call these \\(e_i(Y)\\)\nRegress \\(X_j\\) (as the response) on every other predictor. Obtain the residuals, and call these \\(e_i(X_k)\\)\nMake a scatterplot of \\(e_i(X_k)\\) versus \\(e_i(Y)\\).\n\n\n\n\nIntuition: \\(e_i(Y)\\) and \\(e_i(X_k)\\) reflect the part of each variable that is not linearly associated with the other predictors. See if they are linear or not.\nDon’t try to make one manually, just use car::avPlot().\n\nlm_bp &lt;- lm(sbp ~ age + birthweight, data = bp)\ncar::avPlot(model = lm_bp, variable = \"age\")\n\n\n\n\n\n\n\ncar::avPlot(model = lm_bp, variable = \"birthweight\")\n\n\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\nEvaluate the regression of body fat on tricep skinfold thickness, thigh circumference, and midarm circumference.\n\n\nIt looks really good!\n\nlm_body &lt;- lm(fat ~ triceps + thigh + midarm, data = body)\nlm_body |&gt;\n  augment() |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2, col = 2)\n\n\n\n\n\n\n\n\nAdded variable plots look good too!\n\ncar::avPlots(lm_body)\n\n\n\n\n\n\n\n\nThough, note that there is huge multicollinearity in these data. In STAT-415, you’ll learn about variance inflation factors, which evaluate multicollinearity. In this case, the VIF’s are all huge (rule of thumb is they should be less than 10)\n\nlm_fat &lt;- lm(fat ~ triceps + thigh + midarm, data = body)\ncar::vif(lm_fat)\n\ntriceps   thigh  midarm \n  708.8   564.3   104.6"
  },
  {
    "objectID": "08_reg/11_notes.html#including-categorical-variables",
    "href": "08_reg/11_notes.html#including-categorical-variables",
    "title": "Chapter 11: Regression",
    "section": "Including Categorical Variables",
    "text": "Including Categorical Variables\n\nConsider the lead data that you can download from https://dcgerard.github.io/stat_320/data/lead.csv.\nWe will be interested in these variables:\n\nmaxfwt: Finger-wrist tapping test in dominant hand (number of taps in one 10 second trial)\nGroup: Either control (low lead) or exposed (high lead)\nageyrs: Age in years\nsex: Sex. Either male or female\n\n\nlead &lt;- read_csv(\"https://dcgerard.github.io/stat_320/data/lead.csv\") |&gt;\n  select(maxfwt, Group, ageyrs, sex)\nglimpse(lead)\n\nRows: 124\nColumns: 4\n$ maxfwt &lt;dbl&gt; 72, 61, 49, 48, 51, 49, 50, 58, 50, 51, 59, 65, 57, 53, 74, 50,…\n$ Group  &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"control…\n$ ageyrs &lt;dbl&gt; 11.08, 9.42, 11.08, 6.92, 11.25, 6.50, 6.92, 15.00, 7.17, 7.25,…\n$ sex    &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"female…\n\n\nThe goal is to assess the association between maxfwt and Group, adjusting for ageyrs and sex.\nBut group is a categorical variable. How do we include it?\n\n\n\n\n\n\n\nTipDummy Variables AKA Indicator Variables AKA One-hot Transformation\n\n\n\nFor a explanatory variable \\(X\\) with \\(k\\) levels, create \\(k-1\\) new variables, where variable \\(j\\) is \\[\nX_{ij} =\n\\begin{cases}\n1 & \\text{ if } X \\text{ is level } j\\\\\n0 & \\text{ otherwise}\n\\end{cases}\n\\]\n\n\n\nJust include the \\(k-1\\) dummy variables in the model.\nE.g., set \\(X_{i1} = 1\\) if in the exposed group and \\(0\\) otherwise.\nIf our regression model is \\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2 X_{i2} + \\beta_3X_{i3} + \\epsilon_i\n\\] Then our model for the control group is \\[\nY_i = \\beta_0 + \\beta_2 X_{i2} + \\beta_3X_{i3} + \\epsilon_i\n\\] And out model for the exposed group is \\[\nY_i = \\beta_0 + \\beta_1 + \\beta_2 X_{i2} + \\beta_3X_{i3} + \\epsilon_i\n\\] Thus, \\(\\beta_1\\) is the mean difference in maxfwt between exposed and control groups, adjusting for age and sex.\nR will automatically do this dummy variable encoding for you if you pass it a character or factor variable.\n\nlm(maxfwt ~ Group + ageyrs + sex, data = lead) |&gt;\n  tidy()\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     25.4      4.01      6.34  7.82e- 9\n2 Groupexposed    -4.89     2.04     -2.40  1.84e- 2\n3 ageyrs           2.71     0.331     8.20  1.14e-12\n4 sexmale          1.64     2.01      0.817 4.16e- 1\n\n\nSince the result is Groupexposed, this says that the coefficient corresponding to the exposed level in the Group variable is estimated to be -4.889.\n\n\nExerciseSolution\n\n\nWrite out the entire model that was fit above. Note carefully that sex is also categorical. What is the model for males? What is the model for females?\n\n\n\nLet \\(Y_i\\) be the maxfwt for child \\(i\\)\nLet \\(X_{i1}\\) be 1 if child \\(i\\) is exposed and 0 if control.\nLet \\(X_{i2}\\) be the age of child \\(i\\).\nLet \\(X_{i3}\\) be 1 if the child is male and 0 if female.\n\nThen our model is \\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\beta_3X_{i3} + \\epsilon_i\n\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\nThe model for males is \\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\beta_3 + \\epsilon_i\n\\] And out model for females is \\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\epsilon_i\n\\]\nThus, we estimate that Males have 1.641 more taps than females on average, adjusting for age and exposure status.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you have nothing but categorical explanatory variables in a model, then folks call this an ANOVA model.\nIf you have a mix of categorical and continuous explanatory variables (like here), then folks call this an ANCOVA model (analysis of covariance)\nSome statisticians would be upset that I am being so simplistic. Don’t tell them. This is just between you and me.\n\n\n\nLet’s evaluate the fit of the model. There are some large values, but it doesn’t look too bad:\n\nlm_lead &lt;- lm(maxfwt ~ Group + ageyrs + sex, data = lead)\naout &lt;- augment(lm_lead)\nggplot(aout, aes(x = .fitted, y = .std.resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, lty = 2, col = 2)"
  },
  {
    "objectID": "review/pp_10.html",
    "href": "review/pp_10.html",
    "title": "Chapter 10 Practice Problems",
    "section": "",
    "text": "These practice problems mostly come from Rosner’s publicly available study sheet at the books companion website. The solutions are my own, since we differ slightly on what we are looking for in the solutions."
  },
  {
    "objectID": "review/pp_10.html#cardiovascular-disease",
    "href": "review/pp_10.html#cardiovascular-disease",
    "title": "Chapter 10 Practice Problems",
    "section": "Cardiovascular Disease",
    "text": "Cardiovascular Disease\nIn a 1985 study of the effectiveness of streptokinase in the treatment of patients who have been hospitalized after myocardial infarction, 9 of 199 males receiving streptokinase and 13 of 97 males in the control group died within 12 months.\n\n10.1Solution\n\n\nUse the normal-theory method to test for significant differences in 12-month mortality between the two groups.\n\n\nLet \\(X_1\\) be the number of deaths out of the \\(n_1 = 199\\) males receiving strptokinase. Let \\(X_2\\) be the number of deaths out of the \\(n_2 = 97\\) control males. Then \\(X_1 \\sim \\mathrm{Binom}(199, p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(97, p_2)\\). We are testing \\(H_0: p_1 = p_2\\) versus \\(H_A: p_1 \\neq p_2\\). We calculate statistics\n\n\\(\\hat{p}_1 = 9/199 = 0.04523\\)\n\\(\\hat{p}_2 = 13/97 = 0.134\\)\n\\(\\hat{p} = (9 + 13) / (97 + 199) = 0.07432\\)\n\nUsing the normal theory, we calculate the test statistic \\[\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)\\hat{p}(1-\\hat{p})}} = \\frac{0.04535 - 0.134}{\\left(\\frac{1}{199} + \\frac{1}{97}\\right)\\times0.07432\\times(1-0.07432)}\n\\] Numerically,\n\n(0.04535 - 0.134) / sqrt((1/199 + 1/97) * 0.07432 * (1 - 0.07432))\n\n[1] -2.729\n\n\nWe compare this to the null distribution of \\(N(0,1)\\)\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.729)\n\n[1] 0.006353\n\n\nSo there is a significant difference of death rates between the two groups.\nYou can use prop.test() to do this. It will give you slightly different (better) results because it will do a continutity correction.\n\nprop.test(x = c(9, 13), n = c(199, 97)) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0125\n\n\n\n\n\n\n10.2Solution\n\n\nConstruct the observed and expected contingency tables for these data.\n\n\nObserved\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstreptokinase\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nYes\n9\n190\n199\n\n\nNo\n13\n84\n97\n\n\nTotal\n22\n274\n296\n\n\n\n\n\n\n\nExpected:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstreptokinase\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nYes\n14.791\n184.21\n199\n\n\nNo\n7.209\n89.79\n97\n\n\nTotal\n22.000\n274.00\n296\n\n\n\n\n\n\n\n\n\n\n\n10.3Solution\n\n\nPerform the test in Problem 10.1 using the contingency-table method.\n\n\nWe calculate \\(\\sum_{\\text{categories}} \\frac{(o-e)^2}{e}\\)$ + + + $$ Numerically\n\n(9 - 14.791)^2 / 14.791 + (190 - 184.21)^2 / 184.21 + (13 - 7.209)^2 / 7.209 + (84 - 89.79)^2 / 89.79\n\n[1] 7.475\n\n\nUnder the null, this follows a chi-squared distribution with 1 degree of freedom (\\(\\chi^2_1\\)). We get the \\(p\\)-value by the area under the curve above 7.475\n\n\n\n\n\n\n\n\n\n\n1 - pchisq(q = 7.475, df = 1)\n\n[1] 0.006256\n\n\nSo we have strong evidence of an association between streptokinase use and death.\nThe real-way in R is to use chisq.test()\n\nmatrix(c(9, 190, 13, 84), nrow = 2, ncol = 2, byrow = TRUE) |&gt;\n  chisq.test() |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1      6.24  0.0125         1 Pearson's Chi-squared test with Yates' continuity…\n\n\nThe \\(p\\)-value again differs because before we did not do a continuity correction.\n\n\n\n\n10.4Solution\n\n\nCompare your results in Problems 10.1 and 10.3.\n\n\nThe \\(p\\)-values are the exact same. Theoretically, this should be the case."
  },
  {
    "objectID": "review/pp_10.html#cardiovascular-disease-1",
    "href": "review/pp_10.html#cardiovascular-disease-1",
    "title": "Chapter 10 Practice Problems",
    "section": "Cardiovascular Disease 1",
    "text": "Cardiovascular Disease 1\nIn a 1985 study of the effectiveness of streptokinase in the treatment of patients who have been hospitalized after myocardial infarction, 9 of 199 males receiving streptokinase and 13 of 97 males in the control group died within 12 months.\n\n10.1Solution\n\n\nUse the normal-theory method to test for significant differences in 12-month mortality between the two groups.\n\n\nLet \\(X_1\\) be the number of deaths out of the \\(n_1 = 199\\) males receiving strptokinase. Let \\(X_2\\) be the number of deaths out of the \\(n_2 = 97\\) control males. Then \\(X_1 \\sim \\mathrm{Binom}(199, p_1)\\) and \\(X_2 \\sim \\mathrm{Binom}(97, p_2)\\). We are testing \\(H_0: p_1 = p_2\\) versus \\(H_A: p_1 \\neq p_2\\). We calculate statistics\n\n\\(\\hat{p}_1 = 9/199 = 0.04523\\)\n\\(\\hat{p}_2 = 13/97 = 0.134\\)\n\\(\\hat{p} = (9 + 13) / (97 + 199) = 0.07432\\)\n\nUsing the normal theory, we calculate the test statistic \\[\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)\\hat{p}(1-\\hat{p})}} = \\frac{0.04535 - 0.134}{\\left(\\frac{1}{199} + \\frac{1}{97}\\right)\\times0.07432\\times(1-0.07432)}\n\\] Numerically,\n\n(0.04535 - 0.134) / sqrt((1/199 + 1/97) * 0.07432 * (1 - 0.07432))\n\n[1] -2.729\n\n\nWe compare this to the null distribution of \\(N(0,1)\\)\n\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.729)\n\n[1] 0.006353\n\n\nSo there is a significant difference of death rates between the two groups.\nYou can use prop.test() to do this. It will give you slightly different (better) results because it will do a continutity correction.\n\nprop.test(x = c(9, 13), n = c(199, 97)) |&gt;\n  tidy() |&gt;\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0125\n\n\n\n\n\n\n10.2Solution\n\n\nConstruct the observed and expected contingency tables for these data.\n\n\nObserved\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstreptokinase\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nYes\n9\n190\n199\n\n\nNo\n13\n84\n97\n\n\nTotal\n22\n274\n296\n\n\n\n\n\n\n\nExpected:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstreptokinase\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nYes\n14.791\n184.21\n199\n\n\nNo\n7.209\n89.79\n97\n\n\nTotal\n22.000\n274.00\n296\n\n\n\n\n\n\n\n\n\n\n\n10.3Solution\n\n\nPerform the test in Problem 10.1 using the contingency-table method.\n\n\nWe calculate \\(\\sum_{\\text{categories}} \\frac{(o-e)^2}{e}\\)$ + + + $$ Numerically\n\n(9 - 14.791)^2 / 14.791 + (190 - 184.21)^2 / 184.21 + (13 - 7.209)^2 / 7.209 + (84 - 89.79)^2 / 89.79\n\n[1] 7.475\n\n\nUnder the null, this follows a chi-squared distribution with 1 degree of freedom (\\(\\chi^2_1\\)). We get the \\(p\\)-value by the area under the curve above 7.475\n\n\n\n\n\n\n\n\n\n\n1 - pchisq(q = 7.475, df = 1)\n\n[1] 0.006256\n\n\nSo we have strong evidence of an association between streptokinase use and death.\nThe real-way in R is to use chisq.test()\n\nmatrix(c(9, 190, 13, 84), nrow = 2, ncol = 2, byrow = TRUE) |&gt;\n  chisq.test() |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1      6.24  0.0125         1 Pearson's Chi-squared test with Yates' continuity…\n\n\nThe \\(p\\)-value again differs because before we did not do a continuity correction.\n\n\n\n\n10.4Solution\n\n\nCompare your results in Problems 10.1 and 10.3.\n\n\nThe \\(p\\)-values are the exact same. Theoretically, this should be the case."
  },
  {
    "objectID": "review/pp_10.html#cardiovascular-disease-2",
    "href": "review/pp_10.html#cardiovascular-disease-2",
    "title": "Chapter 10 Practice Problems",
    "section": "Cardiovascular Disease 2",
    "text": "Cardiovascular Disease 2\nIn the streptokinase study in Problem 10.1, 2 of 15 females receiving streptokinase and 4 of 19 females in the control group died within 12 months.\n\n10.5Solution\n\n\nWhy is Fisher’s exact test the appropriate procedure to test for differences in 12-month mortality rates between these two groups?\n\n\nThe sample size is too low for asymptotic approximation to work well. Our rule of thumb is that we need an expected count of at least five in each cell. Our observed table is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstreptokinase\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nYes\n2\n13\n15\n\n\nNo\n4\n15\n19\n\n\nTotal\n6\n28\n34\n\n\n\n\n\n\n\nThese are small, but we base our decision to do an exact test based on the expected counts, which are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstreptokinase\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nYes\n2.647\n12.35\n15\n\n\nNo\n3.353\n15.65\n19\n\n\nTotal\n6.000\n28.00\n34\n\n\n\n\n\n\n\nThere are two cells here with expected counts under 5, so we cannot use an asymptotic approach.\n\n\n\n\n10.6Solution\n\n\nWrite down all possible tables with the same row and column margins as given in the observed data.\n\n\n\\[\n\\begin{pmatrix}\n0 & 15\\\\\n6 & 13\n\\end{pmatrix},\n\\begin{pmatrix}\n1 & 14\\\\\n5 & 14\n\\end{pmatrix},\n\\begin{pmatrix}\n2 & 13\\\\\n4 & 15\n\\end{pmatrix},\n\\begin{pmatrix}\n3 & 12\\\\\n3 & 16\n\\end{pmatrix},\n\\begin{pmatrix}\n4 & 11\\\\\n2 & 17\n\\end{pmatrix},\n\\begin{pmatrix}\n5 & 10\\\\\n1 & 18\n\\end{pmatrix},\n\\begin{pmatrix}\n6 & 9\\\\\n0 & 19\n\\end{pmatrix}\n\\]\n\n\n\n\n10.8Solution\n\n\nThe conditional null probabilities for tables in question 10.6 are:\n\n\n[1] 0.020174 0.129690 0.302609 0.327826 0.173555 0.042425 0.003721\n\n\nOur observed table is the third one (with probability 0.302609\nEvaluate whether or not there is a significant difference between the mortality rates for streptokinase and control-group females using a two-sided test.\n\n\nThe \\(p\\)-value is\n\n0.020174 + 0.129690 + 0.302609 + 0.173555 + 0.042425 + 0.003721\n\n[1] 0.6722\n\n\nSo we do not have a significance difference between the mortaility rates for the two groups.\nWe can verify our calculation in R\n\nmatrix(c(2, 13, 4, 15), nrow = 2, ncol = 2, byrow = TRUE) |&gt;\n  fisher.test() |&gt;\n  tidy()\n\n# A tibble: 1 × 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1    0.586   0.672   0.0459      4.89 Fisher's Exact Test for Count… two.sided"
  },
  {
    "objectID": "syllabus_draft.html",
    "href": "syllabus_draft.html",
    "title": "STAT 320 - Biostatistics",
    "section": "",
    "text": "Instructor: Dr. David Gerard\nEmail: dgerard@american.edu\nOffice: DMTI 106E\n\n\nQ1 Learning Outcomes:\n \n\nStudents will solve quantitative problems including approaches that go beyond memorized procedures.\nStudents will demonstrate an understanding of mathematical relationships from multiple perspectives, such as functions from graphical, verbal, numerical, and analytic points of view.\n\n\n\nQ2 Learning Outcomes:\n \n\nTranslate real-world questions or intellectual inquiries into quantitative frameworks.\nSelect and apply appropriate quantitative methods or reasoning.\nDraw appropriate insights from the application of a quantitative framework.\nExplain quantitative reasoning and insights using appropriate forms of representation so that others could replicate the findings.\n\n\n\nCourse Description\nSTAT-320 is an introduction to the statistical methodology commonly used in public health, medical, and biological studies. This course emphasizes working with data and communicating statistical ideas. A breadth of topics will be covered including: study design, tests of significance, confidence intervals, t-procedures, chi-square and Fisher’s exact test, linear regression, logistic regression, analysis of variance, nonparametric methods, and more advanced topics as time permits. The R computer program will be used to conduct analyses.\nThe major focus for this course is the ideas behind, and the methods for, drawing conclusions about a population from a sample. At the end of this course you will be expected to identify the major concepts related to statistical reasoning and to statistical inferences for drawing such conclusions, recognize how these concepts are used in disciplines related to health and medicine, and implement the methods yourself in statistical analyses using the methods covered. In particular, you are expected to be able to (1) identify the appropriate statistical model or models for a given analysis, (2) write the model in the correct notation, (3) implement the model in the R software package on a given set of data, (4) interpret the output in the context of the study, (5) diagnose model deficiences, (6) suggest improvements to the model if necessary, and (7) summarize the results of the analysis. Work will be a balance between understanding the concepts underlying a method, implementation of the method, and interpretation of the results.\n\n\nRequired Text\n\nRosner, B. (2016) Fundamentals of Biostatistics, Eighth Edition. Brooks/Cole, Boston, MA, USA.\n\n\nThere will be occasional readings from other sources, such as journal articles, for class discussion or for homework assignments. These will be posted in Canvas or links will be given to find these online.\n\n\n\nGrading\n\n\n\n\n\nAssignment\nPercent\n\n\n\n\nHomeworks\n10%\n\n\nParticipation\n10%\n\n\nExams 1, 2, and 3\n80%\n\n\n\n\n\n\n\n\n\nParticipation:\n\nShow up to class. Stay off your phones. Engage with the in-class exercises. You don’t need to complete or turn in the exercises—just make a genuine effort to try them.\nParticipation points will only be deducted under the following circumstances:\n\nYou miss \\(\\geq\\) 20% of the classes without explanation. Occasional absences are fine. I will take attendance most days.\nYou’re not making a good-faith effort on in-class exercises. For example, if you’re clearly working on something else, I’ll make a note and begin sending you warnings before deducting points. Again, you just need to try the exercises—you don’t need to complete them perfectly.\nYou engage in behavior that is clearly disrespectful to me or your classmates. This includes things like repeated interruptions or dismissive comments. Our classroom is a space for learning, where everyone is respected and discourse remains civil and scholarly.\n\n\nExams\n\nExams are not officially cumulative, but since statistical concepts build on one another, they are effectively cumulative.\nYou may bring one handwritten reference sheet (8.5’’ × 11’’, both sides). Typed sheets are not allowed.\nNo other resources are permitted.\n\nIf you touch your calculator, phone, computer, smartwatch, smart glasses, or any similar device during the exam, it will result in an automatic fail for the course.\n\nYou may not leave the room during the exam unless you are turning it in.\n\nIf you are unable to remain in the room for the full exam time, please contact ASAC to request an official accommodation.\n\nMake-up exams will only be provided under extraordinary circumstances with written documentation to explain any absence.\n\nHomeworks\n\nHomework assignments are designed to reinforce your understanding of course concepts and to help you prepare for the exams.\nYou are permitted to use generative AI tools (e.g., ChatGPT) to assist with homework. But I think you will fail the class if you do this. Remember, homework is only worth a small portion of the grade, and homeworks are how you study for the exams.\nI strongly recommend using such tools only after you have made a sincere effort to solve the problems on your own, such as checking your work or seeking clarification.\nEducational research (e.g. https://doi.org/10.1177/1529100612453266) consistently shows that actively working through practice problems is among the most effective ways to learn quantitative material. In contrast, passively reading solutions—whether written by others or generated by AI—offers minimal learning benefit. If you rely primarily on AI-generated solutions, you may not be adequately prepared for the exams.\nTo allow some flexibility, your lowest homework score will be dropped.\nSee the AI Policy section below for information on Homework Audits.\n\n\nUsual grade cutoffs will be used:\n\n\n\n\n\nGrade\nLower\nUpper\n\n\n\n\nA\n93\n100\n\n\nA-\n90\n92\n\n\nB+\n88\n89\n\n\nB\n83\n87\n\n\nB-\n80\n82\n\n\nC+\n78\n79\n\n\nC\n73\n77\n\n\nC-\n70\n72\n\n\nD\n60\n69\n\n\nF\n0\n59\n\n\n\n\n\nIndividual assignments will not be curved. However, at the discretion of the instructor, the overall course grade at the end of the semester may be curved.\n\n\nStudy Recommendations\nIf you are interested, the study techniques in https://doi.org/10.1177/1529100612453266 are listed as\n\nHigh Utility:\n\nPractice testing\n\nSelf-testing or taking practice tests over to-be-learned material\n\nDistributed practice\n\nImplementing a schedule of practice that spreads out study activities over time\n\n\nModerate Utility:\n\nElaborative interrogation:\n\nGenerating an explanation for why an explicitly stated fact or concept is true\n\nSelf-explanation:\n\nExplaining how new information is related to known information, or explaining steps taken during problem solving\n\nInterleaved practice\n\nImplementing a schedule of practice that mixes different kinds of problems, or a schedule of study that mixes different kinds of material, within a single study session\n\n\nLow Utility:\n\nRereading\n\nRestudying text material again after an initial reading\n\nSummarization\n\nWriting summaries (of various lengths) of to-be-learned texts\n\nHighlighting\n\nMarking potentially important portions of to-be-learned materials while reading\n\nThe keyword mnemonic\n\nUsing keywords and mental imagery to associate verbal materials\n\nImagery use for text learning\n\nAttempting to form mental images of text materials while reading or listening\n\n\n\n\n\nLate Work Policy\n\nAll assignments must be submitted on the day they are due.\nEach student will have two three-day extensions, where you can turn in the assignment on Thursday by end-of-day.\nPlease just let me know ahead of time that you will be using one of your two extensions.\nPlease do not tell me why you need the extension. Any reason is a fine reason.\nAny homeworks not submitted by the due date will receive a grade of 0.\n\n\n\nAI Policy\n\nYou can use generative AI (e.g. ChatGPT, CoPilot, etc) on the homeworks if you want.\nHomeworks are your only study exercises for the exams. So I wouldn’t use AI to do them for me except to check my work after I am done.\nHomework Audits: I reserve the right to ask you to explain any of your homework solutions or concepts covered by the homeworks. I might do this by calling you into my office and having you do some board work, asking you probing questions about concepts, and perhaps giving you new related questions. Based on your explanation (or lack thereof), I may modify your homework grade. Your homework grade is not necessarily based on your written solutions, but rather on your demonstrated understanding of the concepts.\n\n\n\nImportant Dates\n\n02/18 (Tentative): Exam 1 (Chapters 2, 3, 4, and 5)\n03/11: Spring Break (No Class)\n03/20: Last day to Withdraw\n04/08 (Tentative): Exam 2 (Chapters 6, 7, and 8)\n04/29: Spring Study Day (No Class)\n05/06 (11:20 AM – 01:50 PM): Exam 3 (Chapters 9, 10, and 11)\n\nDates for Exams 1 and 2 are very tentative. Those exams will take place one or two weeks after we finish the material they cover.\n\n\nComputing and Software\nWe will use the R computing language to complete some assignment questions. R is free and may be downloaded from the R website (http://cran.r-project.org/). In addition, I highly recommend you interface with R through the free RStudio IDE (https://www.rstudio.com/). R and RStudio are also available on computers in the Anderson Computing Complex in addition to various labs across campus. R Studio may also be run from your web browser using American University’s Virtual Applications System. Please see me during office hours if you have questions regarding R.\n\n\nData\nData sets for homeworks assignments and examples from the textbook are available on the Data page. Almost all of these are cleaned versions of the data from the book’s companion website.\n\n\nAcademic Integrity\n\nStandards of academic conduct are set forth in the university’s Academic Integrity Code. By registering for this course, students have acknowledged their awareness of the Academic Integrity Code and they are obliged to become familiar with their rights and responsibilities as defined by the Code. Violations of the Academic Integrity Code will not be treated lightly and disciplinary action will be taken should violations occur. This includes cheating, fabrication, and plagiarism.\nNo resources are allowed for the exam except the 1 page (8.5’’ by 11’’) handwritten cheat sheet (and a pen or pencil, of course). If you touch your phone/computer/smart watch/smart glasses/etc during the exam then that is an automatic fail for the course.\nAll solutions that I provide are under my copyright. These solutions are for personal use only and may not be distributed to anyone else. Giving these solutions to others, including other students or posting them on the internet, is a violation of my copyright and a violation of the student code of conduct.\n\n\n\nSharing Course Content:\nStudents are not permitted to make visual or audio recordings (including livestreams) of lectures or any class-related content or use any type of recording device unless prior permission from the instructor is obtained and there are no objections from any student in the class. If permission is granted, only students registered in the course may use or share recordings and any electronic copies of course materials (e.g., PowerPoints, formulas, lecture notes, and any discussions – online or otherwise). Use is limited to educational purposes even after the end of the course. Exceptions will be made for students who present a signed Letter of Accommodation from the Academic Support and Access Center. Further details are available from the ASAC website.\n\n\nUse of Student Work\nThe professor will use academic work that you complete for educational purposes in this course during this semester. Your registration and continued enrollment constitute your consent.\n\n\nSyllabus Change Policy\nThis syllabus is a guide for the course and is subject to change with advanced notice. These changes may come via email or Canvas. Make sure to check Canvas and your university-supplied email regularly. You are accountable for all such communications."
  },
  {
    "objectID": "review/pp_10.html#obstetrics",
    "href": "review/pp_10.html#obstetrics",
    "title": "Chapter 10 Practice Problems",
    "section": "Obstetrics",
    "text": "Obstetrics\nSuppose there are 500 pairs of pregnant women who participate in a prematurity study and are paired in such a way that the body weights of the 2 women in a pair are within 5 lb of each other. One of the 2 women is given a placebo and the other drug A to see if drug A has an effect in preventing prematurity. Suppose that in 30 pairs of women, both women in a pair have a premature child; in 420 pairs of women, both women have a normal child; in 35 pairs of women, the woman taking drug A has a normal child and the woman taking the placebo has a premature child; in 15 pairs of women, the woman taking drug A has a premature child and the woman taking the placebo has a normal child.\n\n10.13Solution\n\n\nAssess the statistical significance of these results.\n\n\nWe wish to test the null hypothesis that the probability of a premature child is the same in both the drug and the control groups, against the alternative that they differ. This is a matched pairs design, so we should be thinking about McNemar’s test. Let’s build a paired contingency table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrol\n\ndrug\n\nTotal\n\n\nNormal\nPremature\n\n\n\n\nNormal\n420\n15\n435\n\n\nPremature\n35\n30\n65\n\n\nTotal\n455\n45\n500\n\n\n\n\n\n\n\nLet \\(X\\) be the number of pairs where the drug woman has a premature baby and the control woman has a normal baby. Then \\(X \\sim \\mathrm{Binom}(50, p)\\). We are testing the null \\(H_0: p = 1/2\\) versus \\(H_A: p \\neq 1/2\\). Since \\(n \\geq 20\\), we can use the normal-based approach. We could also use an exact approach. Let’s calculate the test statistic \\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{15/50 - 1/2}{\\sqrt{1/2(1 - 1/2)/50}}\n\\] Numerically (also doing a continuity correction)\n\n(15/50 - 0.5 + 1/(2 * 50)) / sqrt(0.5 * (1 - 0.5) / 50)\n\n[1] -2.687\n\n\nWe compare this to a standard normal distribution\n\nplt_t(ub = -2.687, two_sided = TRUE, rng = c(-4, 4)) +\n  geom_vline(xintercept = -2.687, lty = 2, col = 2)\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.687)\n\n[1] 0.00721\n\n\nThis fits the “real-way” using prop.test()\n\nprop.test(x = 15, n = 50, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  p.value conf.low conf.high\n    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.00721    0.183     0.448\n\n\nEither way, we have strong evidence that the premature rate differs between drug and control groups. Indeed, our confidence interval indicates that the drug reduces the premature rate."
  },
  {
    "objectID": "review/pp_10.html#cardiovascular-disease-3",
    "href": "review/pp_10.html#cardiovascular-disease-3",
    "title": "Chapter 10 Practice Problems",
    "section": "Cardiovascular Disease 3",
    "text": "Cardiovascular Disease 3\nA 1979 study investigated the relationship between cigarette smoking and subsequent mortality in men with a prior history of coronary disease. It was found that 264 out of 1731 non-smokers and 208 out of 1058 smokers had died in the 5-year period after the study began.\n\n10.12Solution\n\n\nAssuming that the age distributions of the two groups are comparable, compare the mortality rates in the two groups.\n\n\nWe have this observed table of counts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nNo\n264\n1467\n1731\n\n\nYes\n208\n850\n1058\n\n\nTotal\n472\n2317\n2789\n\n\n\n\n\n\n\nExpected counts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsmoker\n\ndied\n\nTotal\n\n\nYes\nNo\n\n\n\n\nNo\n292.9\n1438.1\n1731\n\n\nYes\n179.1\n878.9\n1058\n\n\nTotal\n472.0\n2317.0\n2789\n\n\n\n\n\n\n\nWe calculate the test-statistic \\[\n\\frac{(264 - 292.9)^2}{292.9} + \\frac{(1467 - 1438.1)^2}{1438.1} + \\frac{(208 - 179.1)^2}{179.1} + \\frac{(850 - 878.9)^2}{878.9}\n\\] Numerically\n\n(264 - 292.9)^2 / 292.9 + (1467 - 1438.1)^2 / 1438.1 + (208 - 179.1)^2 / 179.1 + (850 - 878.9)^2 / 878.9\n\n[1] 9.046\n\n\nWe compare this to a chi-squared distribution with one degree of freedom\n\n\n\n\n\n\n\n\n\n\n1 - pchisq(q = 9.046, df = 1)\n\n[1] 0.002633\n\n\nSo we have strong evidence that there is an association between smoking status and death.\nThe real-way in R is\n\nmatrix(c(264, 1467, 208, 850), nrow = 2, ncol = 2, byrow = TRUE) |&gt;\n  chisq.test() |&gt;\n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                             \n1      8.77 0.00307         1 Pearson's Chi-squared test with Yates' continuity…\n\n\nThe p-values differ because of the continuity correction."
  },
  {
    "objectID": "review/pp_10.html#obstetrics-1",
    "href": "review/pp_10.html#obstetrics-1",
    "title": "Chapter 10 Practice Problems",
    "section": "Obstetrics 1",
    "text": "Obstetrics 1\nSuppose there are 500 pairs of pregnant women who participate in a prematurity study and are paired in such a way that the body weights of the 2 women in a pair are within 5 lb of each other. One of the 2 women is given a placebo and the other drug A to see if drug A has an effect in preventing prematurity. Suppose that in 30 pairs of women, both women in a pair have a premature child; in 420 pairs of women, both women have a normal child; in 35 pairs of women, the woman taking drug A has a normal child and the woman taking the placebo has a premature child; in 15 pairs of women, the woman taking drug A has a premature child and the woman taking the placebo has a normal child.\n\n10.13Solution\n\n\nAssess the statistical significance of these results.\n\n\nWe wish to test the null hypothesis that the probability of a premature child is the same in both the drug and the control groups, against the alternative that they differ. This is a matched pairs design, so we should be thinking about McNemar’s test. Let’s build a paired contingency table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrol\n\ndrug\n\nTotal\n\n\nNormal\nPremature\n\n\n\n\nNormal\n420\n15\n435\n\n\nPremature\n35\n30\n65\n\n\nTotal\n455\n45\n500\n\n\n\n\n\n\n\nLet \\(X\\) be the number of pairs where the drug woman has a premature baby and the control woman has a normal baby. Then \\(X \\sim \\mathrm{Binom}(50, p)\\). We are testing the null \\(H_0: p = 1/2\\) versus \\(H_A: p \\neq 1/2\\). Since \\(n \\geq 20\\), we can use the normal-based approach. We could also use an exact approach. Let’s calculate the test statistic \\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{15/50 - 1/2}{\\sqrt{1/2(1 - 1/2)/50}}\n\\] Numerically (also doing a continuity correction)\n\n(15/50 - 0.5 + 1/(2 * 50)) / sqrt(0.5 * (1 - 0.5) / 50)\n\n[1] -2.687\n\n\nWe compare this to a standard normal distribution\n\nplt_t(ub = -2.687, two_sided = TRUE, rng = c(-4, 4)) +\n  geom_vline(xintercept = -2.687, lty = 2, col = 2)\n\n\n\n\n\n\n\n\n\n2 * pnorm(-2.687)\n\n[1] 0.00721\n\n\nThis fits the “real-way” using prop.test()\n\nprop.test(x = 15, n = 50, p = 1/2) |&gt;\n  tidy() |&gt;\n  select(p.value, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  p.value conf.low conf.high\n    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 0.00721    0.183     0.448\n\n\nEither way, we have strong evidence that the premature rate differs between drug and control groups. Indeed, our confidence interval indicates that the drug reduces the premature rate."
  },
  {
    "objectID": "review/pp_10.html#obstetrics-2",
    "href": "review/pp_10.html#obstetrics-2",
    "title": "Chapter 10 Practice Problems",
    "section": "Obstetrics 2",
    "text": "Obstetrics 2\nAn issue of current interest is the effect of delayed childbearing on pregnancy outcome. In a recent paper a population of first deliveries was assessed for low-birthweight deliveries (&lt;2500 g) according to the woman’s age and prior pregnancy history. The data in Table 10.3 were presented.\n\n\n\n\n\n\n\n\nTable 10.3\n\n\nRelationship of age and pregnancy history to low-birthweight deliveries\n\n\nAge\nHistory1\nn\nPercentage Low Birthweight\n\n\n\n\n≥30\nNo\n225\n3.56\n\n\n≥30\nYes\n88\n6.82\n\n\n&lt;30\nNo\n906\n3.31\n\n\n&lt;30\nYes\n503\n1.31\n\n\n\n1 History = Yes if a woman had a prior history of spontaneous abortion or infertility, = no otherwise\n\n\n\n\n\n\n\n\n\n10.35Solution\n\n\nWhat test can be used to assess the effect of age on low-birthweight deliveries among women with a negative history?\n\n\nLet’s convert this into a 2x2 table. There are 225 women age ≥30 with a negative histroy. Among those, 225 * 0.0356 = 8 have a low birthweight. There are 906 women &lt;30 with a negative history, among those 906 * 0.0331 = 30 have a low birthweight.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\nLow Birthweight\n\nTotal\n\n\nYes\nNo\n\n\n\n\n≥30\n8\n217\n225\n\n\n&lt;30\n30\n876\n906\n\n\nTotal\n38\n1093\n1131\n\n\n\n\n\n\n\nWe can do a chi-squared test to test the null that age is independent of low birthweight status, against the alternative that they are dependent.\n\n\n\n\n10.36Solution\n\n\nPerform the test in Problem 10.35 and report a p-value.\n\n\nYou should be able to do this by hand now. Calculate the expected counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\nLow Birthweight\n\nTotal\n\n\nYes\nNo\n\n\n\n\n≥30\n5.57\n217.4\n223\n\n\n&lt;30\n30.44\n875.6\n906\n\n\nTotal\n36.01\n1093.0\n1129\n\n\n\n\n\n\n\nThen calculate the chi-squared statistic, which ends up being about 0.033. The \\(p\\)-value then is calculated from a \\(\\chi^2_1\\) distribution\n\n\n\n\n\n\n\n\n\n\n1 - pchisq(q = 0.033, df = 1)\n\n[1] 0.8559\n\n\nSo we have no evidence of an association between age of first birth and low birthweight status among women without a history."
  }
]