---
title: "Chapter 8: Two-sample Inference"
author: "David Gerard"
date: today
---

```{r}
#| message: false
#| echo: false
library(tidyverse)
library(broom)
knitr::opts_chunk$set(echo = TRUE, 
            fig.width = 4, 
            fig.height = 3, 
            fig.align = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
plt_t <- function(mu = 0, sig = 1, lb = -Inf, ub = Inf, df = Inf, rng = c(-3, 3), two_sided = FALSE, col = "#E69F00", lwd = 1) {
  tibble(x = seq(mu + rng[[1]] * sig, mu + rng[[2]] * sig, length.out = 500)) |>
    mutate(y = dt(x = (x - mu) / sig, df = df)) ->
    df1
  df1 |>
    filter(x > lb, x < ub) ->
    df2
  ggplot() +
    geom_line(data = df1, mapping = aes(x = x, y = y), linewidth = lwd) +
    geom_area(data = df2, mapping = aes(x = x, y = y), fill = col) +
    geom_line(data = df2, mapping = aes(x = x, y = y), color = col, linewidth = lwd) +
    theme_classic() +
    theme(axis.title = element_blank()) ->
    pl
  
  if (two_sided) {
    df1 |>
      filter(x > -ub, x < -lb) ->
      df3
    pl <- pl + 
      geom_area(data = df3, mapping = aes(x = x, y = y), fill = col) +
      geom_line(data = df3, mapping = aes(x = x, y = y), color = col, linewidth = lwd)
  }
  pl
}
```

# Paired t-test

- Compare 2 populations where parameters are not known.

- A **paired sample** is where observations in each population are matched.

  ![](./05_figs/paired_pop.png){alt="diagram showing paired samples"}\ 

- Example: Twin study where one twin smokes more than the other.
  - Population 1: lighter smoking twins
  - Population 2: heavier smoking twins 
  - Matched pair: each pair of twins

- Example: We measure blood pressure on the same individual at 2 time points
  - Population 1: pre oral contraceptive (OC)
  - Population 2: post OC
  - Matched pair: the same individual  

- This second example is one of a **longitudinal study**, where we follow the same people over time

- Let  
  - $X_i \sim N(\mu_i, \sigma^2)$ 
    - For example, pre-OC  
  - $Y_i \sim N(\mu_i + \Delta, \sigma^2)$ 
    - For example, post-OC

- Hypotheses:  
  $H_0$: $\Delta = 0$  
  $H_1$: $\Delta \ne 0$

- This tests if there is a difference between populations while allowing each pair to have their own baseline mean $\mu_i$.

- Define differences:  
  $D_i = Y_i - X_i \sim N(\Delta, \sigma_D^2)$

- Variance of differences:  
  $\sigma_D^2 = \text{var}(X) + \text{var}(Y) - 2\ \text{cov}(X, Y)$  
  - But this is a nuisance parameter, so just call it $\sigma_D^2$

- So, just use a one-sample t-test on $D_i$

::: {.callout-important}
A paired t-test is just a one-sample t-test on differences.
:::

::: { .callout-tip}
## Paired t-test
Input: $X_1,X_2,\ldots,X_n$ and $Y_1,Y_2,\ldots,Y_n$ where $X_i$ and $Y_i$ are matched pairs.

- We assume $X_i \sim N(\mu_i, \sigma_x^2)$ and $Y_i \sim N(\mu_i + \Delta, \sigma_y^2)$.
- We test:
  - $H_0: \Delta = d_0$
  - $H_A: \Delta \neq d_0$ or $\Delta > d_0$ or $\Delta < d_0$.
  - where $d_0$ = null mean difference between populatoins (e.g. 0)
- We calculate:
  - $D_i = X_i - Y_i$
  - $s_D$ = standard deviation of $D_i$'s  
  - $\bar{D}$ = mean of $D_i$'s
  
-   The $t$ statistic follows a $t_{n-1}$ distribution if $H_0$ is true:
    $$
    t = \frac{\bar{D} - d_0}{s_D / \sqrt{n}}
    $$

-   $H_1: \Delta \neq d_0$:
    $$
    \text{p-value} = 2\texttt{pt}(-|t|, n-1)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-t", "t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \Delta > d_0$:
    $$
    \text{p-value} = 1 - \texttt{pt}(t, n-1)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \Delta < d_0$:
    $$
    \text{p-value} = \texttt{pt}(t, n-1)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

- A $(1 - \alpha) \cdot 100\%$ confidence interval for $\Delta$ is:
  $$
  \bar{D} \pm t_{n-1, 1 - \alpha/2} \cdot \frac{s_D}{\sqrt{n}}
  $$
:::

---

[Paired t-tests in R](./05_two_sample_t.qmd#paired-t-tests)

---

# Two-sample t-tests with equal variances

- More commonly, studies have 2 independent samples.

- Example:  
  - Collect one group of OC users  
  - Collect a separate group of non-OC users

- **Cross-sectional study**: data collected at one point in time (units under different conditions)

- Assume:
  $$
  X_1, X_2, \ldots, X_{n_1} \overset{\text{iid}}{\sim} N(\mu_1, \sigma_1^2)
  $$
  $$
  Y_1, Y_2, \ldots, Y_{n_2} \overset{\text{iid}}{\sim} N(\mu_2, \sigma_2^2)
  $$

- Note: different sample sizes are possible, and the observations are not paired.

- Hypotheses:
  - $H_0$: $\mu_1 = \mu_2$
  - $H_1$: $\mu_1 \ne \mu_2$, or $\mu_1 < \mu_2$, or $\mu_1 > \mu_2$

- For now, assume $\sigma_1^2 = \sigma_2^2 = \sigma^2$ 
  - Assumes the two populations have the same variability, which is often not valid.
  - We will relax this later
  
- We observe:
  - $\bar{X} =$ mean of $X_i$'s  
  - $\bar{Y} =$ mean of $Y_i$'s  
  - $s_1^2 =$ sample variance of $X_i$'s  
  - $s_2^2 =$ sample variance of $Y_i$'s  
  - $n_1 =$ sample size 1  
  - $n_2 =$ sample size 2

- Consider $\bar{X} - \bar{Y}$
  
  $$
  E[\bar{X} - \bar{Y}] = E[\bar{X}] - E[\bar{Y}] = \mu_1 - \mu_2
  $$

  - Equals 0 under $H_0$, not 0 under $H_1$

- Variance of difference:

  \begin{align*}
  \text{Var}(\bar{X} - \bar{Y}) &= \text{Var}(\bar{X}) + \text{Var}(\bar{Y}) - 2\,\text{Cov}(\bar{X}, \bar{Y})\\
  &= \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2} \\
  &= \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_2} \right)
  \end{align*}
  (Covariance term is 0 due to independence)
  
- Therefore, by properties of the normal distribution:
  $$
  \bar{X} - \bar{Y} \sim N\left(\mu_1 - \mu_2, \sigma^2\left(\frac{1}{n_1} + \frac{1}{n_2} \right)\right)
  $$

- If $\sigma^2$ were known, then could base our test on the distribution of the mean divided by the standard deviation:

  $$
  \frac{\bar{X} - \bar{Y}}{\sigma\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim N(0, 1)
  $$

- We could then compare this statistic to a $N(0,1)$ distribution to get p-value.

- However, $\sigma^2$ is never known in practice, so we need to estimate it.

::: {.callout-tip}
## Pooled Sample Variance
- Assuming $\sigma_1^2 = \sigma_2^2 = \sigma^2$, then we estimate $\sigma^2$ with the **pooled sample variance**:

$$
s^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
$$
:::

- The pooled sample variance can equivalently be written as:

  $$
  s^2 = \frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2
  $$

  - This should show you that higher weight goes to the sample with larger $n$

- Our test statistic becomes:

  $$
  \frac{\bar{X} - \bar{Y}}{s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
  $$
  
- This follows a $t_{n_1 + n_2 - 2}$ distribution only if $H_0$ is true.
  - It follows something else if $H_1$ is true.

::: {.callout-important}
## Two-sample t-test with Equal Variances
Input: $X_1, X_2, \ldots, X_{n_1}$ and $Y_1, Y_2, \ldots, Y_{n_2}$ (sample sizes might be different)

- We assume $X_i \sim N(\mu_1, \sigma^2)$ and $Y_i \sim N(\mu_2, \sigma^2)$.
  - Equal variances, possibly different means
- We test:
  - $H_0: \mu_1 = \mu_2$
  - $H_A: \mu_1 \neq \mu_2$ or $\mu_1 > \mu_2$ or $\mu_1 < \mu_2$.
- We calculate:
  - $\bar{X}$, $\bar{Y}$, and the pooled sample variance
    $$
    s^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
    $$
  
-   The $t$ statistic follows a $t_{n_1 + n_2 - 2}$ distribution if $H_0$ is true:
    $$
    t = \frac{\bar{X} - \bar{Y}}{s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1 + n_2 - 2}
    $$

-   $H_1: \mu_1 \neq \mu_2$:
    $$
    \text{p-value} = 2\texttt{pt}(-|t|, n_1 + n_2 - 2)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-t", "t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \mu_1 > \mu_2$:
    $$
    \text{p-value} = 1 - \texttt{pt}(t, n_1 + n_2 - 2)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```
    
-   $H_1: \mu_1 < \mu_2$:
    $$
    \text{p-value} = \texttt{pt}(t, n_1 + n_2 - 2)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

- A $(1 - \alpha)100\%$ confidence interval is

  $$
  (\bar{X} - \bar{Y}) \pm t_{n_1 + n_2 - 2,\ 1 - \alpha/2} \cdot s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
  $$
:::


---

[Two-sample t-test in R, equal variance](./05_two_sample_t.qmd#unpaired-equal-variance)

---

# Test for Equal Variances

- Because of the equal variance assumption above, folks have developed statistical tests for whether the variances are indeed equal.

- Let:
  $$
  X_i \sim N(\mu_1, \sigma_1^2)
  $$  
  $$
  Y_i \sim N(\mu_2, \sigma_2^2)
  $$

- Hypotheses:
  - $H_0$: $\sigma_1^2 = \sigma_2^2$  
  - $H_1$: $\sigma_1^2 \ne \sigma_2^2$

- The test is based on $s_1^2$ and $s_2^2$. 
  - If they are very different, this provides evidence that $\sigma_1^2 \neq \sigma_2^2$.

- Nobody does this in real life because:
  1. Very sensitive to non-normality. In contrast, the $t$-test is not sensitive because of the CLT
  2. The equal variance $t$-test is robust to violations in equal variance assumption.
  3. Nobody assumes equal variances anyway because they all use Welch's <img src="./05_figs/grapes_small.png" width="3%"> 2-sample $t$-test (§8.7)

- If your boss asks you to test for equal variances, use `var.test()`

---

[Test for equal variance in R](./05_two_sample_t.qmd#test-for-equal-variance)

---

# Two-sample t-test with unequal variances

- We'll now relax the equal variance assumption
  - This is a mathy way to say that we won't assume equal variances.
  
- Our approach is called Welch's <img src="./05_figs/grapes_small.png" width="3%"> t-test 

- Always use this unless you know for sure that the variances are equal.

- Let:
  - $X_i \sim N(\mu_1, \sigma_1^2)$ with sample size $n_1$  
  - $Y_i \sim N(\mu_2, \sigma_2^2)$ with sample size $n_2$

- Then:

$$
\bar{X} - \bar{Y} \sim N\left(\mu_1 - \mu_2,\ \frac{1}{n_1} \sigma_1^2 + \frac{1}{n_2} \sigma_2^2\right)
$$

- The test statistic is the mean divided by the estimated standard error

  $$
  t = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}}
  $$

- This is approximately $t_\nu$ if $H_0$ is true

- The degrees of freedom $\nu$ for the null distribution is a weird thing called the **Satterthwaite approximation**:

  $$
  \nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1 - 1} + \frac{(s_2^2/n_2)^2}{n_2 - 1}}
  $$

  - You don't need to remember this.
  
  - This $\nu$ is just to make the $t_\nu$ distribution as close as possible to the actual null distribution of test statistic.
  
::: {.callout-important}
## Two-sample t-test with Equal Variances
Input: $X_1, X_2, \ldots, X_{n_1}$ and $Y_1, Y_2, \ldots, Y_{n_2}$ (sample sizes might be different)

- We assume $X_i \sim N(\mu_1, \sigma_1^2)$ and $Y_i \sim N(\mu_2, \sigma_2^2)$.
  - Possibly unequal variances, possibly different means
- We test:
  - $H_0: \mu_1 = \mu_2$
  - $H_A: \mu_1 \neq \mu_2$ or $\mu_1 > \mu_2$ or $\mu_1 < \mu_2$.
- We calculate:
  - $\bar{X}$, $\bar{Y}$, $s_1^2$, and $s_2^2$
  
-   The $t$ statistic follows a $t_{\nu}$ distribution if $H_0$ is true:
    $$
    t = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}}
    $$

-   $\nu$ is derived from Satterthwaite's Equation.

-   $H_1: \mu_1 \neq \mu_2$:
    $$
    \text{p-value} = 2\texttt{pt}(-|t|, \nu)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-t", "t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \mu_1 > \mu_2$:
    $$
    \text{p-value} = 1 - \texttt{pt}(t, \nu)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```
    
-   $H_1: \mu_1 < \mu_2$:
    $$
    \text{p-value} = \texttt{pt}(t, \nu)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

- A $(1 - \alpha)100\%$ confidence interval is

  $$
  (\bar{X} - \bar{Y}) \pm t_{\nu,\ 1 - \alpha/2} \cdot \sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}
  $$
:::

---

[Two-sample t-test in R, equal variance](./05_two_sample_t.qmd#unpaired-unequal-variance)

---

# Sample Size and Power for 2-Sample t-tests

- Idea: Given
  - $\mu_1 - \mu_2$ (effect size)  
  - $\sigma_1^2$ (variance of sample 1)  
  - $\sigma_2^2$ (variance of sample 2)  
  - $\alpha$ (significance level)  
  - $n_1$ (sample size 1)  
  - $n_2$ (sample size 2)  

  Then we can calculate power $1 - \beta$ using similar methods as before.

- To get $n_1$, $n_2$, assume $n_2 = k n_1$ for known $k$  
  - E.g., we know we have equal sample sizes, or we know group 1 will have twice as many folks, etc.
  - Total sample size: $n_1 + k n_1 = n_1 \left(1 + k\right)$
  - You then have an equation of the form $g(n_1) = 1-\beta$, where $g(\cdot)$ is a function that gets the power given a known sample size.
  - To get the sample size given power, you can solve for $n_1$.

---

[Power in R](./05_two_sample_t.qmd#power-and-sample-size-calculations-in-two-sample-t-tests)

---

**Skip Section 8.10**

# Assumptions of t-methods

There are three assumptions, in decreasing order of importance (first is most important):

1. **Independence**  
   - Check by thinking about sampling design  
     - Did you measure units in clusters (e.g. all from the same family)
     - Did you measure the same units over time
   - If violated, use more complicated methods
     - Anova, multiple linear regression, longitudinal approaches

2. **Equal variance**  
   - Not required for Welch's <img src="./05_figs/grapes_small.png" width="3%"> t-test  
   - Just use Welch’s <img src="./05_figs/grapes_small.png" width="3%"> t-test

3. **Normality**
   - Violated if there are outliers or skew
   - Check using:
     - Histograms
     - Boxplots
     - QQ-plots (more on this later)
   - Only a big deal if $n$ is small (e.g., $< 50$) **and** there are **lots** of skew/outliers

-   Note: Need **normality within each group**, not for the pooled or marginal distribution.

    ```{r}
    #| echo: false
    set.seed(1)
    bind_rows(
      tibble(x = rnorm(n = 100, mean = -2, sd = 1), group = "A"),
      tibble(x = rnorm(n = 100, mean = 2, sd = 1), group = "B")
    ) |>
      ggplot(aes(x = x, fill = group)) +
      geom_histogram(bins = 30)
    ```

    - Marginal distribution is not normal → that's okay

    - - Check by histograms and Q–Q plots in each group.

- If normality is violated:

  1. If all values are $> 0$, try logging the $X_i$'s
  
  2. If all values are $\geq 0$, try taking the square root of the $X_i$'s
  
  3. Remove outliers and report both results
  
  4. Use a nonparametric method (see Chapter 9)

# Exercises 8.3--8.11

The mean ±1 sd of ln [calcium intake (mg)] among 25 females, 12 to 14 years of age, below the poverty level is 6.56 ± 0.64. Similarly, the mean ± 1 sd of ln [calcium intake (mg)] among 40 females, 12 to 14 years of age, above the poverty level is 6.80 ± 0.76.

::: {.panel-tabset}
## Exercise 8.3
What is the appropriate procedure to test for a significant difference in means between the two groups?

## Solution
A two-sample t-test. Equal variance would probably be OK, but why not just use Welch <img src="./05_figs/grapes_small.png" width="3%">?

I think Rosner has you run a test for equal variance to determine if we assume equal variances or not. But don't do this. In real life, just run Welch <img src="./05_figs/grapes_small.png" width="3%">.

But it's easier for me to test the equal variance approach. So in subsequent problems, assume the variances are equal.
:::

::: {.panel-tabset}
## Exercise 8.4
Implement the procedure in Problem 8.3 using the critical-value method at significance level 0.1.

## Hint

- $\bar{X} = 6.56$
- $\bar{Y} = 6.80$
- $s_1^2 = 0.64^2 = 0.4096$
- $s_2^2 = 0.76^2 - 0.5776$
- $n_1 = 25$
- $n_2 = 40$

First calculate the pooled sample variance.

## Solution

We assume

- $X_1,\ldots,X_{25} \sim N(\mu_1,\sigma^2)$
- $Y_1,\ldots,Y_{40} \sim N(\mu_2,\sigma^2)$

We are testing

- $H_0: \mu_1 = \mu_2$
- $H_1: \mu_1 \neq \mu_2$

We need the the pooled sample variance:
```{r}
((25 - 1) * 0.64^2 + (40 - 1) * 0.76^2) / (25 + 40 - 2)
```

The $t$-statistic is then
```{r}
(6.56 - 6.8) / (sqrt(.5136) * sqrt(1 / 25 + 1/40))
```

We compare this to a $t$ distribution with 25 + 40 - 2 = 63 degrees of freedom. We calculate the $1-\alpha/2 = 1 - 0.1 / 2 = 0.95$ quantile of the $t$ distribution (it's $\alpha/2$ because it's a two-sided test) to get
```{r}
qt(p = 0.95, df = 63)
```

Since $|-1.314| = 1.314 < 1.669$, we fail to reject the null hypothesis at significance level 0.1 and conclude that we do not have evidence to say that females above and below the poverty level have different mean calcium intakes.
:::

::: {.panel-tabset}
## Exercise 8.5
What is the p-value corresponding to your answer to Problem 8.4?

## Hint

```{r}
#| echo: false
plt_t(ub = -1.314, two_sided = TRUE) +
  theme_void()
```


## Solution
We compare the t-statistic of -1.314 for a $t$ distribution with 25 + 40 - 2 = 63 degrees of freedom. Since this is a two-sided test, we need the area in both tails.
```{r}
2 * pt(-1.314, df = 63)
```

```{r}
#| echo: false
#| fig-height: 2
#| fig-width: 3
plt_t(ub = -1.314, two_sided = TRUE) +
  scale_x_continuous(breaks = c(-1.314, 1.314), labels = c("-1.314", 1.314)) +
  geom_vline(xintercept = -1.314, linetype = "dashed") +
  theme(axis.text.x = element_text(size = 15),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```
:::

::: {.panel-tabset}
## Exercise 8.6
Compute a 95% CI for the difference in means between the two groups.

## Hint

Estimate $\pm$ Multiplier $\times$ Standard Error

We already calculate the pooled sample variance. You need the $1 - \alpha/2$ quantile of the $t$ distribution with $n_1 + n_2 - 2$ degrees of freedom, which you can get with `qt()`.


## Solution
We calculate, for $\alpha = 0.05$:
$$
(\bar{X} - \bar{Y}) \pm t_{n_1 + n_2 - 2,\ 1 - \alpha/2} \cdot s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
$$

We already calculated the pooled sample variance as $s^2 = 0.5136$. Thus We also have that the 1 - 0.05/2 = 0.975 quantile of the t distribution with 25 + 40 -2 = 63 degrees of freedom is
```{r}
qt(0.975, df = 63)
```

Thus, our interval is:
```{r}
6.56 - 6.8 - 1.998 * sqrt(0.5136) * sqrt(1/25 + 1/40)
6.56 - 6.8 + 1.998 * sqrt(0.5136) * sqrt(1/25 + 1/40)
```
:::

::: {.panel-tabset}
## Exercise 8.7
Suppose an equal number of 12- to 14-year-old girls below and above the poverty level are recruited to study differences in calcium intake. How many girls should be recruited to have an 80% chance of detecting a significant difference using a two-sided test with α = .05?

## Hint

Use `power.t.test()`. Use the estimates above for your effect size and standard deviation.

## Solution
We'll use the estimates above as our "wild guess" for the effect size
$$
\bar{X} - \bar{Y} = 6.56 - 6.8 = -0.24
$$
and the estimated standard deviation comes from the pooled sample variance
$$
\sqrt{0.5136} = 0.7167
$$

```{r}
power.t.test(
  delta = 0.24,
  sd = 0.7167, 
  sig.level = 0.05,
  power = 0.8,
  type = "two.sample",
  alternative = "two.sided")
```

That means we need at least 141 individual per group.
:::

::: {.panel-tabset}
## Exercise 8.8
Answer Problem 8.7 if a one-sided rather than a two-sided test is used.

## Hint

It's just one change in `power.t.test()` from 8.7.

## Solution
```{r}
power.t.test(
  delta = 0.24,
  sd = 0.7167, 
  sig.level = 0.05,
  power = 0.8,
  type = "two.sample",
  alternative = "one.sided")
```
So we need at least 111 individuals.
:::

::: {.panel-tabset}
## Exercise 8.10
Suppose 50 girls above the poverty level and 50 girls below the poverty level are recruited for the study. How much power will the study have of finding a significant difference using a two-sided test with α = .05, assuming that the population parameters are the same as the sample estimates in Problem 8.2?

## Hint

Just give `power.t.test()` `n` instead of `power`.

## Solution
```{r}
power.t.test(
  n = 50,
  delta = 0.24,
  sd = 0.7167, 
  sig.level = 0.05,
  type = "two.sample",
  alternative = "two.sided")
```
The power will be 0.3813.

:::

::: {.panel-tabset}
## Exercise 8.11
Answer Problem 8.10 assuming a one-sided rather than a two-sided test is used.

## Hint

Just change one thing from `power.t.test()` in problem 8.10.

## Solution
```{r}
power.t.test(
  n = 50,
  delta = 0.24,
  sd = 0.7167, 
  sig.level = 0.05,
  type = "two.sample",
  alternative = "one.sided")
```
The power will be 0.5071.
:::
