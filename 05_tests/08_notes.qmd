---
title: "Chapter 8: Two-sample Inference"
author: "David Gerard"
date: today
---

# Paired t-test

- Compare 2 populations where parameters are not known.

- A **paired sample** is where observations in each population are matched.

**XYZ IMAGE HERE**

- Example: Twin study where one twin smokes more than the other.
  - Pop 1: lighter smoking twin  
  - Pop 2: heavier smoking twin  
  - Matched: each pair of twins

- Example: Same individual at 2 time points
  - Pop 1: pre oral contraceptive (OC)  
  - Pop 2: post OC  
  - Matched: the same individual  
  - Measure: blood pressure pre and post OC

- **Longitudinal study**: follow same people over time

- Let  
  $X_i \sim N(\mu_i, \sigma^2)$ for example, pre-OC  
  $Y_i \sim N(\mu_i + \Delta, \sigma^2)$ for example, post-OC

- Hypotheses:  
  $H_0$: $\Delta = 0$  
  $H_1$: $\Delta \ne 0$

- This tests if there is a difference between populations while allowing each pair to have their own baseline mean $\mu_i$.

- Define differences:  
  $D_i = Y_i - X_i \sim N(\Delta, \sigma_D^2)$

- Variance of differences:  
  $\sigma_D^2 = \text{var}(X) + \text{var}(Y) - 2\ \text{cov}(X, Y)$  
  - But nuisance parameters, so just call it $\sigma_D^2$

- So, just use a one-sample t-test on $D_i$

- A paired t-test is just a one-sample t-test on differences.

$$
t = \frac{\bar{D} - d_0}{s_D / \sqrt{n}} \sim t_{n-1}
$$

**XYZ IMAGE HERE**

- $d_0$ = null value  
- $s_D$ = SD of $D_i$'s  
- $\bar{D}$ = mean of $D_i$'s

- Get $(1 - \alpha) \cdot 100\%$ confidence interval by

$$
\bar{D} \pm t_{n-1, 1 - \alpha/2} \cdot \frac{s_D}{\sqrt{n}}
$$

where  
$t_{n-1, 1 - \alpha/2}$ is the $t$-quantile with df = $n - 1$  
(typically $\alpha = 0.05$)


# Two-sample t-tests with equal variances

- More common studies have 2 independent samples.

- Example:  
  - Collect one group of OC users  
  - Collect a separate group of non-OC users

- **Cross-sectional study**: data collected at one point in time (units under different conditions)

$$
X_1, X_2, \ldots, X_{n_1} \overset{\text{iid}}{\sim} N(\mu_1, \sigma_1^2)
$$

$$
Y_1, Y_2, \ldots, Y_{n_2} \overset{\text{iid}}{\sim} N(\mu_2, \sigma_2^2)
$$

- Different sample sizes possible, not paired.

- Hypotheses:
  - $H_0$: $\mu_1 = \mu_2$
  - $H_1$: $\mu_1 \ne \mu_2$, or $\mu_1 < \mu_2$, or $\mu_1 > \mu_2$

- For now, assume $\sigma_1^2 = \sigma_2^2 = \sigma^2$ (i.e., equal/pooled variance)

- We observe:
  - $\bar{X} =$ mean of $X_i$'s  
  - $\bar{Y} =$ mean of $Y_i$'s  
  - $s_1^2 =$ sample variance of $X_i$'s  
  - $s_2^2 =$ sample variance of $Y_i$'s  
  - $n_1 =$ sample size 1  
  - $n_2 =$ sample size 2

- Consider $\bar{X} - \bar{Y}$

$$
\mathbb{E}[\bar{X} - \bar{Y}] = \mathbb{E}[\bar{X}] - \mathbb{E}[\bar{Y}] = \mu_1 - \mu_2
$$

- Equals 0 under $H_0$, not 0 under $H_1$

- Variance of difference:

$$
\text{Var}(\bar{X} - \bar{Y}) = \text{Var}(\bar{X}) + \text{Var}(\bar{Y}) - 2\,\text{Cov}(\bar{X}, \bar{Y})
$$

- Covariance term is 0 due to independence

$$
= \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2} = \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_2} \right)
$$

- Therefore,

$$
\bar{X} - \bar{Y} \sim N\left(\mu_1 - \mu_2, \sigma^2\left(\frac{1}{n_1} + \frac{1}{n_2} \right)\right)
$$

- If $\sigma^2$ were known, then could test using

$$
\frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim N(0, 1)
$$

- If $H_0$ true, compare stat to $N(0,1)$ to get p-value

- Assuming $\sigma_1^2 = \sigma_2^2 = \sigma^2$, estimate $\sigma^2$ with the **pooled sample variance**:

$$
s^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
$$

- Equivalent to

$$
s^2 = \frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2
$$

- Higher weight goes to sample with larger $n$

- Test statistic becomes:

$$
\frac{\bar{X} - \bar{Y}}{s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1 + n_2 - 2}
$$

- If $H_0$ true

- Let:

$$
X_1, X_2, \ldots, X_{n_1} \sim N(\mu_1, \sigma^2)
$$

$$
Y_1, Y_2, \ldots, Y_{n_2} \sim N(\mu_2, \sigma^2)
$$

- Pooled variance:

$$
s^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
$$

- Test statistic:

$$
t = \frac{\bar{X} - \bar{Y}}{s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1 + n_2 - 2}
$$

**XYZ IMAGE HERE**

- Calculate area in only one tail if alternative is one-sided.

- $(1 - \alpha) \cdot 100\%$ confidence interval:

$$
(\bar{X} - \bar{Y}) \pm t_{n_1 + n_2 - 2,\ 1 - \alpha/2} \cdot s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
$$

where  
$t_{n_1 + n_2 - 2,\ 1 - \alpha/2}$ is the $t$-quantile with df = $n_1 + n_2 - 2$

---

[t-test in R](./05_two_sample_t.qmd)

---

# Test for Equal Variances

- $X_i \sim N(\mu_1, \sigma_1^2)$  
  $Y_i \sim N(\mu_2, \sigma_2^2)$

- Hypotheses:
  - $H_0$: $\sigma_1^2 = \sigma_2^2$  
  - $H_1$: $\sigma_1^2 \ne \sigma_2^2$

- Run a test based on $s_1^2$, $s_2^2$

- Nobody does this in real life because:
  1. Very sensitive to non-normality — $t$-test is not sensitive because of the CLT
  2. Equal variance $t$-test is robust to violations in equal variance assumption
  3. Nobody assumes equal variances anyway because they all use Welch’s 2-sample $t$-test (§8.7)

- If your boss asks you to test for equal variances, use `var.test()`

---

F-stat in R

---

# Two-sample t-test with unequal variances

- Always use this unless you know for sure that the variances are equal.

- Let:
  - $X_i \sim N(\mu_1, \sigma_1^2)$ with sample size $n_1$  
  - $Y_i \sim N(\mu_2, \sigma_2^2)$ with sample size $n_2$

- Then:

$$
\bar{X} - \bar{Y} \sim N\left(\mu_1 - \mu_2,\ \frac{1}{n_1} \sigma_1^2 + \frac{1}{n_2} \sigma_2^2\right)
$$

- Test statistic:

$$
t = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}} = \frac{\text{effect}}{\text{SE}}
$$

- Approximately $t_\nu$ if $H_0$ is true

- Degrees of freedom $\nu$ is a weird thing called the **Satterthwaite approximation**:

$$
\nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1 - 1} + \frac{(s_2^2/n_2)^2}{n_2 - 1}}
$$

- You don’t need to remember this.

**XYZ IMAGE HERE**

- Use this $\nu$ to make the $t$ as close to the actual distribution of $t$ as possible.

- Confidence interval:

$$
(\bar{X} - \bar{Y}) \pm t_{\nu,\ 1 - \alpha/2} \cdot \sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}
$$

---

t-tests in R

---

# Sample Size and Power for 2-Sample t-tests

- Idea: Given
  - $\mu_1 - \mu_2$ (effect)  
  - $\sigma_1^2$ (variance of sample 1)  
  - $\sigma_2^2$ (variance of sample 2)  
  - $\alpha$ (significance level)  
  - $n_1$ (sample size 1)  
  - $n_2$ (sample size 2)  

- Can calculate power $1 - \beta$ using similar methods as before.

- To get $n_1$, $n_2$, assume $n_2 = k n_1$ for known $k$  
  (i.e., known equal sample sizes or group 1 will have twice as many folks)

- Then solve for $n_1$ given a fixed power.

- Total sample size:

$$
\text{Sample Size} = n_1 + k n_1 = n_1 \left(1 + k\right)
$$

---

Power in R

---

**Skip Section 8.10**

# Assumptions of t-methods

1. **Independence**  
   - Check by thinking about sampling design  
   - If violated, use more complicated methods

2. **Equal variance**  
   - Not required for Welch's t-test  
   - Just use Welch’s t-test

3. **Normality**
   - Check using:
     - Histograms
     - Boxplots
     - Shapiro test
   - Only a big deal if $n$ is small (e.g., $< 50$) **and** there are lots of skew/outliers

::: {.callout-important}
Note: Need **normality within each group**, not for the pooled or marginal distribution.
:::

**XYZ IMAGE HERE**  (Boxplots and density plots for group 1 and group 2)

Marginal distribution is not normal → that's okay

- Check by histograms and Q–Q plots (more later) in each group.

- If violated:

1. If all $> 0$, try logging $X_i$'s

2. Remove outliers and report both results

3. Use a nonparametric method (see Chapter 9)

# Exercises 8.3--8.8

The mean ±1 sd of ln [calcium intake (mg)] among 25 females, 12 to 14 years of age, below the poverty level is 6.56 ± 0.64. Similarly, the mean ± 1 sd of ln [calcium intake (mg)] among 40 females, 12 to 14 years of age, above the poverty level is 6.80 ± 0.76.

::: {.panel-tabset}
## Exercise 8.3
What is the appropriate procedure to test for a significant difference in means between the two groups?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.4
Implement the procedure in Problem 8.3 using the critical-value method.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.5
What is the p-value corresponding to your answer to Problem 8.4?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.6
Compute a 95% CI for the difference in means between the two groups.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.7
Suppose an equal number of 12- to 14-year-old girls below and above the poverty level are recruited to study differences in calcium intake. How many girls should be recruited to have an 80% chance of detecting a significant difference using a two-sided test with α = .05?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.8
Answer Problem 8.7 if a one-sided rather than a two-sided test is used.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.9
Using a two-sided test with α = .05, answer Problem 8.7, anticipating that two girls above the poverty level will be recruited for every one girl below the poverty level who is recruited.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.10
Suppose 50 girls above the poverty level and 50 girls below the poverty level are recruited for the study. How much power will the study have of finding a significant difference using a two-sided test with α = .05, assuming that the population parameters are the same as the sample estimates in Problem 8.2?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.11
Answer Problem 8.10 assuming a one-sided rather than a two-sided test is used.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.12
Suppose 50 girls above the poverty level and 25 girls below the poverty level are recruited for the study. How much power will the study have if a two-sided test is used with α = .05?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.13
Answer Problem 8.12 assuming a one-sided test is used with α = .05.

## Hint

## Solution
:::
