---
title: "Chapter 8: Two-sample Inference"
author: "David Gerard"
date: today
---

```{r}
#| message: false
#| echo: false
library(tidyverse)
library(broom)
knitr::opts_chunk$set(echo = TRUE, 
            fig.width = 4, 
            fig.height = 3, 
            fig.align = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
plt_t <- function(mu = 0, sig = 1, lb = -Inf, ub = Inf, df = Inf, rng = c(-3, 3), two_sided = FALSE, col = "#E69F00", lwd = 1) {
  tibble(x = seq(mu + rng[[1]] * sig, mu + rng[[2]] * sig, length.out = 500)) |>
    mutate(y = dt(x = (x - mu) / sig, df = df)) ->
    df1
  df1 |>
    filter(x > lb, x < ub) ->
    df2
  ggplot() +
    geom_line(data = df1, mapping = aes(x = x, y = y), linewidth = lwd) +
    geom_area(data = df2, mapping = aes(x = x, y = y), fill = col) +
    geom_line(data = df2, mapping = aes(x = x, y = y), color = col, linewidth = lwd) +
    theme_classic() +
    theme(axis.title = element_blank()) ->
    pl
  
  if (two_sided) {
    df1 |>
      filter(x > -ub, x < -lb) ->
      df3
    pl <- pl + 
      geom_area(data = df3, mapping = aes(x = x, y = y), fill = col) +
      geom_line(data = df3, mapping = aes(x = x, y = y), color = col, linewidth = lwd)
  }
  pl
}
```

# Paired t-test

- Compare 2 populations where parameters are not known.

- A **paired sample** is where observations in each population are matched.

  ![](./05_figs/paired_pop.png){alt="diagram showing paired samples"}\ 

- Example: Twin study where one twin smokes more than the other.
  - Population 1: lighter smoking twins
  - Population 2: heavier smoking twins 
  - Matched pair: each pair of twins

- Example: We measure blood pressure on the same individual at 2 time points
  - Population 1: pre oral contraceptive (OC)
  - Population 2: post OC
  - Matched pair: the same individual  

- This second example is one of a **longitudinal study**, where we follow the same people over time

- Let  
  - $X_i \sim N(\mu_i, \sigma^2)$ 
    - For example, pre-OC  
  - $Y_i \sim N(\mu_i + \Delta, \sigma^2)$ 
    - For example, post-OC

- Hypotheses:  
  $H_0$: $\Delta = 0$  
  $H_1$: $\Delta \ne 0$

- This tests if there is a difference between populations while allowing each pair to have their own baseline mean $\mu_i$.

- Define differences:  
  $D_i = Y_i - X_i \sim N(\Delta, \sigma_D^2)$

- Variance of differences:  
  $\sigma_D^2 = \text{var}(X) + \text{var}(Y) - 2\ \text{cov}(X, Y)$  
  - But this is a nuisance parameter, so just call it $\sigma_D^2$

- So, just use a one-sample t-test on $D_i$

::: {.callout-important}
A paired t-test is just a one-sample t-test on differences.
:::

::: { .callout-tip}
## Paired t-test
Input: $X_1,X_2,\ldots,X_n$ and $Y_1,Y_2,\ldots,Y_n$ where $X_i$ and $Y_i$ are matched pairs.

- We assume $X_i \sim N(\mu_i, \sigma_x^2)$ and $Y_i \sim N(\mu_i + \Delta, \sigma_y^2)$.
- We test:
  - $H_0: \Delta = d_0$
  - $H_A: \Delta \neq d_0$ or $\Delta > d_0$ or $\Delta < d_0$.
  - where $d_0$ = null mean difference between populatoins (e.g. 0)
- We calculate:
  - $D_i = X_i - Y_i$
  - $s_D$ = standard deviation of $D_i$'s  
  - $\bar{D}$ = mean of $D_i$'s
  
-   The $t$ statistic follows a $t_{n-1}$ distribution if $H_0$ is true:
    $$
    t = \frac{\bar{D} - d_0}{s_D / \sqrt{n}}
    $$

-   $H_1: \Delta \neq d_0$:
    $$
    \text{p-value} = 2\texttt{pt}(-|t|, n-1)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-t", "t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \Delta > d_0$:
    $$
    \text{p-value} = 1 - \texttt{pt}(t, n-1)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \Delta < d_0$:
    $$
    \text{p-value} = \texttt{pt}(t, n-1)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

- A $(1 - \alpha) \cdot 100\%$ confidence interval for $\Delta$ is:
  $$
  \bar{D} \pm t_{n-1, 1 - \alpha/2} \cdot \frac{s_D}{\sqrt{n}}
  $$
:::

---

[Paired t-tests in R](./05_two_sample_t.qmd#paired-t-tests)

---

# Two-sample t-tests with equal variances

- More commonly, studies have 2 independent samples.

- Example:  
  - Collect one group of OC users  
  - Collect a separate group of non-OC users

- **Cross-sectional study**: data collected at one point in time (units under different conditions)

- Assume:
  $$
  X_1, X_2, \ldots, X_{n_1} \overset{\text{iid}}{\sim} N(\mu_1, \sigma_1^2)
  $$
  $$
  Y_1, Y_2, \ldots, Y_{n_2} \overset{\text{iid}}{\sim} N(\mu_2, \sigma_2^2)
  $$

- Note: different sample sizes are possible, and the observations are not paired.

- Hypotheses:
  - $H_0$: $\mu_1 = \mu_2$
  - $H_1$: $\mu_1 \ne \mu_2$, or $\mu_1 < \mu_2$, or $\mu_1 > \mu_2$

- For now, assume $\sigma_1^2 = \sigma_2^2 = \sigma^2$ 
  - Assumes the two populations have the same variability, which is often not valid.
  - We will relax this later
  
- We observe:
  - $\bar{X} =$ mean of $X_i$'s  
  - $\bar{Y} =$ mean of $Y_i$'s  
  - $s_1^2 =$ sample variance of $X_i$'s  
  - $s_2^2 =$ sample variance of $Y_i$'s  
  - $n_1 =$ sample size 1  
  - $n_2 =$ sample size 2

- Consider $\bar{X} - \bar{Y}$
  
  $$
  E[\bar{X} - \bar{Y}] = E[\bar{X}] - E[\bar{Y}] = \mu_1 - \mu_2
  $$

  - Equals 0 under $H_0$, not 0 under $H_1$

- Variance of difference:

  \begin{align*}
  \text{Var}(\bar{X} - \bar{Y}) &= \text{Var}(\bar{X}) + \text{Var}(\bar{Y}) - 2\,\text{Cov}(\bar{X}, \bar{Y})\\
  &= \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2} \\
  &= \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_2} \right)
  \end{align*}
  (Covariance term is 0 due to independence)
  
- Therefore, by properties of the normal distribution:
  $$
  \bar{X} - \bar{Y} \sim N\left(\mu_1 - \mu_2, \sigma^2\left(\frac{1}{n_1} + \frac{1}{n_2} \right)\right)
  $$

- If $\sigma^2$ were known, then could base our test on the distribution of the mean divided by the standard deviation:

  $$
  \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim N(0, 1)
  $$

- We could then compare this statistic to a $N(0,1)$ distribution to get p-value.

- However, $\sigma^2$ is never known in practice, so we need to estimate it.

::: {.callout-tip}
## Pooled Sample Variance
- Assuming $\sigma_1^2 = \sigma_2^2 = \sigma^2$, then we estimate $\sigma^2$ with the **pooled sample variance**:

$$
s^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
$$
:::

- The pooled sample variance can equivalently be written as:

  $$
  s^2 = \frac{n_1 - 1}{n_1 + n_2 - 2} s_1^2 + \frac{n_2 - 1}{n_1 + n_2 - 2} s_2^2
  $$

  - This should show you that higher weight goes to the sample with larger $n$

- Our test statistic becomes:

  $$
  \frac{\bar{X} - \bar{Y}}{s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
  $$
  
- This follows a $t_{n_1 + n_2 - 2}$ distribution only if $H_0$ is true.
  - It follows something else if $H_1$ is true.

::: {.callout-important}
## Two-sample t-test with Equal Variances
Input: $X_1, X_2, \ldots, X_{n_1}$ and $Y_1, Y_2, \ldots, Y_{n_2}$ (sample sizes might be different)

- We assume $X_i \sim N(\mu_1, \sigma^2)$ and $Y_i \sim N(\mu_2, \sigma^2)$.
  - Equal variances, possibly different means
- We test:
  - $H_0: \mu_1 = \mu_2$
  - $H_A: \mu_1 \neq \mu_2$ or $\mu_1 > \mu_2$ or $\mu_1 < \mu_2$.
- We calculate:
  - $\bar{X}$, $\bar{Y}$, and the pooled sample variance
    $$
    s^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
    $$
  
-   The $t$ statistic follows a $t_{n_1 + n_2 - 2}$ distribution if $H_0$ is true:
    $$
    t = \frac{\bar{X} - \bar{Y}}{s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1 + n_2 - 2}
    $$

-   $H_1: \mu_1 \neq \mu_2$:
    $$
    \text{p-value} = 2\texttt{pt}(-|t|, n_1 + n_2 - 2)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-t", "t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \mu_1 > \mu_2$:
    $$
    \text{p-value} = 1 - \texttt{pt}(t, n_1 + n_2 - 2)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```
    
-   $H_1: \mu_1 < \mu_2$:
    $$
    \text{p-value} = \texttt{pt}(t, n_1 + n_2 - 2)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

- A $(1 - \alpha)100\%$ confidence interval is

  $$
  (\bar{X} - \bar{Y}) \pm t_{n_1 + n_2 - 2,\ 1 - \alpha/2} \cdot s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
  $$
:::


---

[Two-sample t-test in R, equal variance](./05_two_sample_t.qmd#unpaired-equal-variance)

---

# Test for Equal Variances

- Because of the equal variance assumption above, folks have developed statistical tests for whether the variances are indeed equal.

- Let:
  $$
  X_i \sim N(\mu_1, \sigma_1^2)
  $$  
  $$
  Y_i \sim N(\mu_2, \sigma_2^2)
  $$

- Hypotheses:
  - $H_0$: $\sigma_1^2 = \sigma_2^2$  
  - $H_1$: $\sigma_1^2 \ne \sigma_2^2$

- The test is based on $s_1^2$ and $s_2^2$. 
  - If they are very different, this provides evidence that $\sigma_1^2 \neq \sigma_2^2$.

- Nobody does this in real life because:
  1. Very sensitive to non-normality. In contrast, the $t$-test is not sensitive because of the CLT
  2. The equal variance $t$-test is robust to violations in equal variance assumption.
  3. Nobody assumes equal variances anyway because they all use Welch's <img src="./05_figs/grapes_small.png" width="3%"> 2-sample $t$-test (§8.7)

- If your boss asks you to test for equal variances, use `var.test()`

---

[Test for equal variance in R](./05_two_sample_t.qmd#test-for-equal-variance)

---

# Two-sample t-test with unequal variances

- We'll now relax the equal variance assumption
  - This is a mathy way to say that we won't assume equal variances.
  
- Our approach is called Welch's <img src="./05_figs/grapes_small.png" width="3%"> t-test 

- Always use this unless you know for sure that the variances are equal.

- Let:
  - $X_i \sim N(\mu_1, \sigma_1^2)$ with sample size $n_1$  
  - $Y_i \sim N(\mu_2, \sigma_2^2)$ with sample size $n_2$

- Then:

$$
\bar{X} - \bar{Y} \sim N\left(\mu_1 - \mu_2,\ \frac{1}{n_1} \sigma_1^2 + \frac{1}{n_2} \sigma_2^2\right)
$$

- The test statistic is the mean divided by the estimated standard error

  $$
  t = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}}
  $$

- This is approximately $t_\nu$ if $H_0$ is true

- The degrees of freedom $\nu$ for the null distribution is a weird thing called the **Satterthwaite approximation**:

  $$
  \nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{(s_1^2/n_1)^2}{n_1 - 1} + \frac{(s_2^2/n_2)^2}{n_2 - 1}}
  $$

  - You don't need to remember this.
  
  - This $\nu$ is just to make the $t_\nu$ distribution as close as possible to the actual null distribution of test statistic.
  
::: {.callout-important}
## Two-sample t-test with Equal Variances
Input: $X_1, X_2, \ldots, X_{n_1}$ and $Y_1, Y_2, \ldots, Y_{n_2}$ (sample sizes might be different)

- We assume $X_i \sim N(\mu_1, \sigma_1^2)$ and $Y_i \sim N(\mu_2, \sigma_2^2)$.
  - Possibly unequal variances, possibly different means
- We test:
  - $H_0: \mu_1 = \mu_2$
  - $H_A: \mu_1 \neq \mu_2$ or $\mu_1 > \mu_2$ or $\mu_1 < \mu_2$.
- We calculate:
  - $\bar{X}$, $\bar{Y}$, $s_1^2$, and $s_2^2$
  
-   The $t$ statistic follows a $t_{\nu}$ distribution if $H_0$ is true:
    $$
    t = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}}
    $$

-   $\nu$ is derived from Satterthwaite's Equation.

-   $H_1: \mu_1 \neq \mu_2$:
    $$
    \text{p-value} = 2\texttt{pt}(-|t|, \nu)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5, two_sided = TRUE) +
      scale_x_continuous(breaks = c(-1.5, 1.5), labels = c("-t", "t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

-   $H_1: \mu_1 > \mu_2$:
    $$
    \text{p-value} = 1 - \texttt{pt}(t, \nu)
    $$
    
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(lb = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```
    
-   $H_1: \mu_1 < \mu_2$:
    $$
    \text{p-value} = \texttt{pt}(t, \nu)
    $$
    ```{r}
    #| echo: false
    #| fig-height: 2
    #| fig-width: 3
    plt_t(ub = 1.5) +
      scale_x_continuous(breaks = c(1.5), labels = c("t")) +
      geom_vline(xintercept = 1.5, linetype = "dashed") +
      theme(axis.text.x = element_text(size = 15),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
    ```

- A $(1 - \alpha)100\%$ confidence interval is

  $$
  (\bar{X} - \bar{Y}) \pm t_{\nu,\ 1 - \alpha/2} \cdot \sqrt{\frac{1}{n_1} s_1^2 + \frac{1}{n_2} s_2^2}
  $$
:::

---

[Two-sample t-test in R, equal variance](./05_two_sample_t.qmd#unpaired-unequal-variance)

---

# Sample Size and Power for 2-Sample t-tests

- Idea: Given
  - $\mu_1 - \mu_2$ (effect size)  
  - $\sigma_1^2$ (variance of sample 1)  
  - $\sigma_2^2$ (variance of sample 2)  
  - $\alpha$ (significance level)  
  - $n_1$ (sample size 1)  
  - $n_2$ (sample size 2)  

  Then we can calculate power $1 - \beta$ using similar methods as before.

- To get $n_1$, $n_2$, assume $n_2 = k n_1$ for known $k$  
  - E.g., we know we have equal sample sizes, or we know group 1 will have twice as many folks, etc.
  - Total sample size: $n_1 + k n_1 = n_1 \left(1 + k\right)$
  - You then have an equation of the form $g(n_1) = 1-\beta$, where $g(\cdot)$ is a function that gets the power given a known sample size.
  - To get the sample size given power, you can solve for $n_1$.

---

[Power in R](./05_two_sample_t.qmd#power-and-sample-size-calculations-in-two-sample-t-tests)

---

**Skip Section 8.10**

# Assumptions of t-methods

There are three assumptions, in decreasing order of importance (first is most important):

1. **Independence**  
   - Check by thinking about sampling design  
     - Did you measure units in clusters (e.g. all from the same family)
     - Did you measure the same units over time
   - If violated, use more complicated methods
     - Anova, multiple linear regression, longitudinal approaches

2. **Equal variance**  
   - Not required for Welch's <img src="./05_figs/grapes_small.png" width="3%"> t-test  
   - Just use Welch’s <img src="./05_figs/grapes_small.png" width="3%"> t-test

3. **Normality**
   - Violated if there are outliers or skew
   - Check using:
     - Histograms
     - Boxplots
     - QQ-plots (more on this later)
   - Only a big deal if $n$ is small (e.g., $< 50$) **and** there are **lots** of skew/outliers

-   Note: Need **normality within each group**, not for the pooled or marginal distribution.

    ```{r}
    #| echo: false
    set.seed(1)
    bind_rows(
      tibble(x = rnorm(n = 100, mean = -2, sd = 1), group = "A"),
      tibble(x = rnorm(n = 100, mean = 2, sd = 1), group = "B")
    ) |>
      ggplot(aes(x = x, fill = group)) +
      geom_histogram(bins = 30)
    ```

    - Marginal distribution is not normal → that's okay

    - - Check by histograms and Q–Q plots in each group.

- If normality is violated:

  1. If all values are $> 0$, try logging the $X_i$'s
  
  2. If all values are $\geq 0$, try taking the square root of the $X_i$'s
  
  3. Remove outliers and report both results
  
  4. Use a nonparametric method (see Chapter 9)

# Exercises 8.3--8.8

The mean ±1 sd of ln [calcium intake (mg)] among 25 females, 12 to 14 years of age, below the poverty level is 6.56 ± 0.64. Similarly, the mean ± 1 sd of ln [calcium intake (mg)] among 40 females, 12 to 14 years of age, above the poverty level is 6.80 ± 0.76.

::: {.panel-tabset}
## Exercise 8.3
What is the appropriate procedure to test for a significant difference in means between the two groups?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.4
Implement the procedure in Problem 8.3 using the critical-value method.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.5
What is the p-value corresponding to your answer to Problem 8.4?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.6
Compute a 95% CI for the difference in means between the two groups.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.7
Suppose an equal number of 12- to 14-year-old girls below and above the poverty level are recruited to study differences in calcium intake. How many girls should be recruited to have an 80% chance of detecting a significant difference using a two-sided test with α = .05?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.8
Answer Problem 8.7 if a one-sided rather than a two-sided test is used.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.9
Using a two-sided test with α = .05, answer Problem 8.7, anticipating that two girls above the poverty level will be recruited for every one girl below the poverty level who is recruited.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.10
Suppose 50 girls above the poverty level and 50 girls below the poverty level are recruited for the study. How much power will the study have of finding a significant difference using a two-sided test with α = .05, assuming that the population parameters are the same as the sample estimates in Problem 8.2?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.11
Answer Problem 8.10 assuming a one-sided rather than a two-sided test is used.

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.12
Suppose 50 girls above the poverty level and 25 girls below the poverty level are recruited for the study. How much power will the study have if a two-sided test is used with α = .05?

## Hint

## Solution
:::

::: {.panel-tabset}
## Exercise 8.13
Answer Problem 8.12 assuming a one-sided test is used with α = .05.

## Hint

## Solution
:::
