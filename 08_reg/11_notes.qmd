---
title: "Chapter 11: Regression"
author: "David Gerard"
date: today
bibliography: "../bib.bib"
---

```{r}
#| message: false
#| echo: false
library(tidyverse)
library(broom)
library(patchwork)
knitr::opts_chunk$set(echo = TRUE, 
            fig.width = 4, 
            fig.height = 3, 
            fig.align = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
plt_t <- function(
    mu = 0, 
    sig = 1, 
    lb = -Inf,
    ub = Inf,
    df = Inf, 
    rng = c(-3, 3), 
    two_sided = FALSE,
    col = "#E69F00",
    lwd = 1) {
  tibble(x = seq(mu + rng[[1]] * sig, mu + rng[[2]] * sig, length.out = 500)) |>
    mutate(y = dt(x = (x - mu) / sig, df = df)) ->
    df1
  df1 |>
    filter(x > lb, x < ub) ->
    df2
  ggplot() +
    geom_line(data = df1, mapping = aes(x = x, y = y), linewidth = lwd) +
    geom_area(data = df2, mapping = aes(x = x, y = y), fill = col) +
    geom_line(data = df2, mapping = aes(x = x, y = y), color = col, linewidth = lwd) +
    theme_classic() +
    theme(axis.title = element_blank()) ->
    pl
  
  if (two_sided) {
    df1 |>
      filter(x > -ub, x < -lb) ->
      df3
    pl <- pl + 
      geom_area(data = df3, mapping = aes(x = x, y = y), fill = col) +
      geom_line(data = df3, mapping = aes(x = x, y = y), color = col, linewidth = lwd)
  }
  pl
}
```

# Learning Objectives

- Simple/Multiple Linear Regression
- Chapter 11.1--11.6, 11.9, 11.10 of Rosner
- Skip every other section (the ones on correlation)

# Motivation
- The simple linear model is not sexy.
- But the most commonly used methods in Statistics are either
    [specific applications of it](https://lindeloev.github.io/tests-as-linear/)
    or are generalizations of it.
- Understanding it well will help you better understand methods
  taught in other classes.
- We teach Linear Regression in STAT 415/615 (the most important course we teach). 
  So these notes are just meant to give you a couple tools that you can 
  build on in that course.

# Broom

```{r}
library(broom)
```

- For the most popular model output (t-tests, linear models, generalized
  linear models), the `broom` package provides three
  functions that aid in data analysis.
  1. `tidy()`: Provides summary information of the model (such as parameter
     estimates and $p$-values) in a tidy format. We used this last class.
  2. `augment()`: Creates data derived from the model and adds it to the
     original data, such as residuals
     and fits. It formats this augmented data in a tidy format.
  3. `glance()`: Prints a single row summary of the model fit.
    
- These three functions are *very* useful and incorporate well with the 
  tidyverse.
  
- You've seen me use `tidy()` many times in this class. Below, we will see examples of using `augment()` and `glance()`.

# Estradiol vs Birthweight

-   For this lesson, we will use the study from @greene1963urinary exploring the association between the estradiol level in pregnant women and birthweight.

    ```{r}
    #| message: false
    library(tidyverse)
    estriol <- read_csv("../data/estriol.csv")
    glimpse(estriol)
    ```
- `estriol` is measured in milligrams per 24 hours. `birthweight` is in units of 100 grams.
  
-   To begin, we'll look at the association between these variables
  
    ```{r}
    ggplot(estriol, aes(x = estriol, birthweight)) +
      geom_point() +
      xlab("Estriol (mg/24hr)") +
      ylab("Birthweight (g/100)")
    ```
    
- It seems that estriol is positively associated with birthweight.    
    
- It seems that the data approximately fall on a line.

# Line Review
- Every line may be represented by a formula of the form
  $$
  Y = \beta_0 + \beta_1 X
  $$
- $Y$ = response variable on $y$-axis
- $X$ = explanatory variable on the $x$-axis
- $\beta_1$ = slope (rise over run)
  - How much larger is $Y$ when $X$ is 1 unit larger.
  - If $\beta_1 < 0$ then the line slopes downward. 
  - If $\beta_1 > 0$ then the line slopes upward. 
  - If $\beta_1 = 0$ then the line is horizontal.
- $\beta_0$ = $y$-intercept (the value of the line at $X = 0$)

- You can represent any line in terms of its slope and its $y$-intercept:

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 2.5
beta0 <- 1
beta1 <- 2
x1 <- -1
x2 <- 2
y1 <- beta0 + x1 * beta1
y2 <- beta0 + x2 * beta1

ggplot() +
  geom_abline(slope = beta1, intercept = beta0, linewidth = 2) +
  annotate(geom = "segment", x = -Inf, xend = 0, y = beta0, yend = beta0, lty = 2, color = "grey") +
  annotate(geom = "segment", x = -Inf, xend = 1, y = beta0 + beta1, yend = beta0 + beta1, lty = 2, color = "grey") +
  annotate(geom = "segment", x = 0, xend = 0, y = -Inf, yend = beta0 + beta1, lty = 2, color = "grey") +
  annotate(geom = "segment", x = 1, xend = 1, y = -Inf, yend = beta0 + beta1, lty = 2, color = "grey") +
  scale_x_continuous(breaks = c(0, 1), limits = c(x1, x2)) +
  scale_y_continuous(limits = c(y1, y2), breaks = c(beta0, beta0 + beta1), labels = c(expression(beta[0]), expression(beta[0]+beta[1]))) +
  theme_classic() +
  theme(axis.text = element_text(size = 15)) +
  ggtitle(expression(beta[1]>0)) ->
  pl1


beta1 <- -2
y1 <- beta0 + x1 * beta1
y2 <- beta0 + x2 * beta1
ggplot() +
  geom_abline(slope = beta1, intercept = beta0, linewidth = 2) +
  annotate(geom = "segment", x = -Inf, xend = 0, y = beta0, yend = beta0, lty = 2, color = "grey") +
  annotate(geom = "segment", x = -Inf, xend = 1, y = beta0 + beta1, yend = beta0 + beta1, lty = 2, color = "grey") +
  annotate(geom = "segment", x = 0, xend = 0, y = -Inf, yend = beta0, lty = 2, color = "grey") +
  annotate(geom = "segment", x = 1, xend = 1, y = -Inf, yend = beta0 + beta1, lty = 2, color = "grey") +
  scale_x_continuous(breaks = c(0, 1), limits = c(x1, x2)) +
  scale_y_continuous(limits = c(y1, y2), breaks = c(beta0, beta0 + beta1), labels = c(expression(beta[0]), expression(beta[0]+beta[1]))) +
  theme_classic() +
  theme(axis.text = element_text(size = 15)) +
  ggtitle(expression(beta[1]<0)) ->
  pl2

beta1 <- 0
y1 <- beta0 + x1 * beta1
y2 <- beta0 + x2 * beta1
ggplot() +
  geom_abline(slope = beta1, intercept = beta0, linewidth = 2) +
  annotate(geom = "segment", x = 0, xend = 0, y = -Inf, yend = beta0, lty = 2, color = "grey") +
  annotate(geom = "segment", x = 1, xend = 1, y = -Inf, yend = beta0 + beta1, lty = 2, color = "grey") +
  scale_x_continuous(breaks = c(0, 1), limits = c(x1, x2)) +
  scale_y_continuous(limits = c(y1, y2), breaks = c(beta0), labels = c(expression(beta[0]==beta[0]+beta[1]))) +
  theme_classic() +
  theme(axis.text = element_text(size = 15)) +
  ggtitle(expression(beta[1]==0)) ->
  pl3

pl1 + pl3 + pl2
```



::: {.panel-tabset}
## Exercise
Suppose we consider the line defined by the following equation:
$$
Y = 2 + 4X
$$

1. What is the value of $Y$ at $X = 3$?
2. What is the value of $Y$ at $X = 4$?
3. What is the difference in $Y$ values at $X = 3$ versus $X = 4$?
4. What is the value of $Y$ at $X = 0$?

## Solution
1. 2 + 4 * 3 = 14
2. 2 + 4 * 4 = 18
3. 4. You don't need to compare (1) and (2). Just look at the slope.
4. 2. This is the y-intercept.
:::

# Simple Linear Regression Model

- A line does not *exactly* fit the estriol dataset. But a line does
  *approximate* the estriol data.
  
- Model: Response variable = line + noise.
  $$
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
  $$

- We typically assume that the noise ($\epsilon_i$'s) for each individual has 
  mean 0 and some variance $\sigma^2$. We estimate $\sigma^2$.

::: {.callout-important}
## Linear Model in a Nutshell
*Given* $X_i$, mean of $Y_i$ is $\beta_0 + \beta_1 X_i$. Points vary about this mean.
:::
 
 
```{r}
#| echo: false
#| eval: true
set.seed(1)
beta0 <- 1
beta1 <- 2
x1 <- -1
x2 <- 2
y1 <- beta0 + x1 * beta1
y2 <- beta0 + x2 * beta1
sigma <- 1
xmax <- 4

tibble(x = rep(1:xmax, each = 10)) |>
  mutate(y = beta0 + beta1 * x + rnorm(n())) ->
  df

xlim <- range(df$x) + c(-0.5, 0.5)
ylim <- range(df$y)

dfnorm <- tibble(y = seq(-3 * sigma, 3 * sigma, length.out = 100), z = stats::dnorm(y, mean = 0, sd = sigma), x1 = 1, x2 = 2, x3 = 3, x4 = 4) |>
  mutate(y1 = y + beta0 + 1 * beta1,
         y2 = y + beta0 + 2 * beta1,
         y3 = y + beta0 + 3 * beta1,
         y4 = y + beta0 + 4 * beta1)

xlim <- c(0.5, 4.5)
ylim <- c(min(c(df$y, dfnorm$y1)), max(c(df$y, dfnorm$y4)))

ggplot() +
  geom_abline(intercept = beta0, slope = beta1) +
  xlim(xlim) +
  ylim(ylim) +
  xlab("X") +
  ylab("Y") +
  scale_color_viridis_c(name = "Density\nof Y|X") +
  theme_bw() ->
  pl0


pl0 +
  geom_line(data = dfnorm, mapping = aes(x = x1, y = y1, color = z), lwd = 10) +
  geom_line(data = dfnorm, mapping = aes(x = x2, y = y2, color = z), lwd = 10) +
  geom_line(data = dfnorm, mapping = aes(x = x3, y = y3, color = z), lwd = 10) +
  geom_line(data = dfnorm, mapping = aes(x = x4, y = y4, color = z), lwd = 10) ->
  pl1

pl1 +
  geom_point(data = df, mapping = aes(x = x, y = y)) ->
  pl2

ggplot() +
  xlim(xlim) +
  ylim(ylim) +
  xlab("X") +
  ylab("Y") +
  geom_point(data = df, mapping = aes(x = x, y = y)) +
  theme_bw() ->
  pl3
```

-   There exists a regression line describing the relationship between $X$ and $E[Y|X]$:

    ```{r}
    #| echo: false
    pl0
    ```
    
-   The distribution of $Y$ is conditional on the value of $X$
    ```{r}
    #| echo: false
    pl1
    ```

-   Our $Y$ values are sampled from this distribution
    ```{r}
    #| echo: false
    pl2
    ```
    
-   But in real-life, we only see the points
    ```{r}
    #| echo: false
    pl3
    ```

    
- Some intuition:
  - The distribution of $Y$ is *conditional* on the value of $X$.
  - The distribution of $Y$ is assumed to have the **same variance**, 
    $\sigma^2$ for **all possible values of $X$**.
  - This last one is a considerable assumption.
  
- Interpretation:
  - Randomized Experiment: A 1 unit increase in $x$ results in a $\beta_1$ unit
    increase in $y$.
  - Observational Study: Individuals that differ only in 1 unit of $x$ are
    expected to differ by $\beta_1$ units of $y$.

::: {.panel-tabset}
## Exercise
What is the interpretation of $\beta_0$? 

## Solution
If 0 is in the range of the data, then it can be interpreted as the expected value of $Y$ at $X = 0$. But, typically, it has no interpretation except the $y$-intercept of the regression line.
:::
  
# Estimating Coefficients
- How do we estimate $\beta_0$ and $\beta_1$?
  - $\beta_0$ and $\beta_1$ are **parameters**
  - We want to estimate them from our **sample**\pause
  - Idea: Draw a line through the cloud of points and calculate the slope 
    and intercept of that line?
  - Problem: Subjective\pause
  - Another idea: Minimize residuals (sum of squared residuals).

- Ordinary Least Squares
  - Residuals: $\hat{\epsilon}_i = Y_{i} - (\hat{\beta}_0 + \hat{\beta}_1X_i)$
  - Sum of squared residuals: $\hat{\epsilon}_1^2 + \hat{\epsilon}_2^2 + \cdots + \hat{\epsilon}_n^2$
  - Find $\hat{\beta}_0$ and $\hat{\beta}_1$ that have small sum of squared residuals.
  - The obtained estimates, $\hat{\beta}_0$ and $\hat{\beta}_1$, are called
    the **ordinary least squares** (OLS) estimates.
    
-   Large sum of squares of three points

    ```{r}
    #| echo: false
    df <- tribble(
      ~x, ~y,
      0, 0,
      3, 3,
      1, 2)
    beta0 <- 0.7
    beta1 <- 0.5
    df |>
      mutate(
        id = 1:n(),
        fit = beta0 + x * beta1,
        res = y - fit,
        y1 = y,
        x1 = x,
        x2 = x - res,
        y2 = y,
        x3 = x - res,
        y3 = fit,
        x4 = x,
        y4 = fit) |>
      select(-fit, -res, -y, -x) |>
      pivot_longer(cols = -id, names_pattern = "([xy])(\\d+)", names_to = c(".value", "pair")) |>
      mutate(id = factor(id)) ->
      dfpoly
    
    ggplot() +
      geom_point(data = df, mapping = aes(x = x, y = y))  +
      geom_abline(intercept = beta0, slope = beta1) +
      geom_polygon(data = dfpoly, mapping = aes(x = x, y = y, fill = id), alpha = 1/2) +
      theme_bw() +
      xlim(0, 4) +
      ylim(0, 4) +
      scale_fill_discrete(palette = "Okabe-Ito")
    ```


-   Small sum of squares of three points

    ```{r}
    #| echo: false
    beta0 <- 0.429
    beta1 <- 0.929
    df |>
      mutate(
        id = 1:n(),
        fit = beta0 + x * beta1,
        res = y - fit,
        y1 = y,
        x1 = x,
        x2 = x - res,
        y2 = y,
        x3 = x - res,
        y3 = fit,
        x4 = x,
        y4 = fit) |>
      select(-fit, -res, -y, -x) |>
      pivot_longer(cols = -id, names_pattern = "([xy])(\\d+)", names_to = c(".value", "pair")) |>
      mutate(id = factor(id)) ->
      dfpoly
    
    ggplot() +
      geom_point(data = df, mapping = aes(x = x, y = y))  +
      geom_abline(intercept = beta0, slope = beta1) +
      geom_polygon(data = dfpoly, mapping = aes(x = x, y = y, fill = id), alpha = 1/2) +
      theme_bw() +
      xlim(0, 4) +
      ylim(0, 4) +
      scale_fill_discrete(palette = "Okabe-Ito")
    ```

-   Bad Fit:
    ```{r, echo=FALSE}
    beta0 <- 20
    beta1 <- 1
    estriol$fitted <- beta0 + estriol$estriol * beta1
    ss <- sum((estriol$fitted - estriol$birthweight)^2)
    estriol %>%
      ggplot(mapping = aes(x = estriol, y = birthweight)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Estriol (mg/24hr)") +
      ylab("Birthweight (g/100)") +
      geom_segment(mapping = aes(x = estriol, xend = estriol, y = birthweight, yend = fitted), alpha = 1/2) +
      ggtitle(paste0("Sum of Squares: ", round(ss, digits = 2)))
    ```
    
-   Better Fit:
    ```{r, echo=FALSE}
    beta0 <- 21
    beta1 <- 0.8
    estriol$fitted <- beta0 + estriol$estriol * beta1
    ss <- sum((estriol$fitted - estriol$birthweight)^2)
    estriol %>%
      ggplot(mapping = aes(x = estriol, y = birthweight)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Estriol (mg/24hr)") +
      ylab("Birthweight (g/100)") +
      geom_segment(mapping = aes(x = estriol, xend = estriol, y = birthweight, yend = fitted), alpha = 1/2) +
      ggtitle(paste0("Sum of Squares: ", round(ss, digits = 2)))
    ```
    
-   Best Fit (OLS Fit):
    ```{r, echo=FALSE}
    lmout <- lm(birthweight ~ estriol, data = estriol)
    beta0 <- coef(lmout)[1]
    beta1 <- coef(lmout)[2]
    estriol$fitted <- beta0 + estriol$estriol * beta1
    ss <- sum((estriol$fitted - estriol$birthweight)^2)
    estriol %>%
      ggplot(mapping = aes(x = estriol, y = birthweight)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Estriol (mg/24hr)") +
      ylab("Birthweight (g/100)") +
      geom_segment(mapping = aes(x = estriol, xend = estriol, y = birthweight, yend = fitted), alpha = 1/2) +
      ggtitle(paste0("Sum of Squares: ", round(ss, digits = 2)))
    ```

- How to find OLS fits in R:
  1. Make sure you have the explanatory variables in the format you want (e.g., after variable transformation)
      
  2. Use `lm()`
  
     ```{r}
     lmout <- lm(birthweight ~ estriol, data = estriol)
     lmtide <- tidy(lmout)
     select(lmtide, term, estimate)
     ```

-   The first argument in `lm()` is a **formula**, where the response variable
  is to the left of the tilde and the explanatory variable is to the right of
  the tilde. 
  
    ``` r
    response ~ explanatory
    ```
    
    This formula says that `lm()` should find the OLS estimates of the 
    following model: 
    
    response = $\beta_0$ + $\beta_1$explanatory + noise
  
- The `data` argument tells `lm()` where to find the response and 
  explanatory variables.

- We often put a "hat" over the coefficient names to denote that they are estimates:
  - $\hat{\beta}_0$ = `r round(lmtide$estimate[1], digits = 1)`.
  - $\hat{\beta}_1$ = `r round(lmtide$estimate[2], digits = 1)`.
  
- Thus, the estimated line is:
  - $E[Y_i]$ = `r round(lmtide$estimate[1], digits = 1)` + 
    `r round(lmtide$estimate[2], digits = 1)`$X_i$.
    
:::{.panel-tabset}
## Exercise
Forced expiratory volume (FEV) is commonly used to assess lung function. To determine whether an individual’s pulmonary function is abnormal, reference values for FEV in healthy individuals must first be established. A challenge in this process is that FEV varies with both age and height. For this analysis, we focus on boys aged 10 to 15 and propose a regression model:
FEV = α + β (height) + e. Data were collected on FEV and height for 655 boys in this age range living in Tecumseh, Michigan. The data frame below gives the average FEV (in liters) for twelve height categories, each spanning 4 cm. Obtain estimates of this linear model. What is the fitted regression line? Interpret the coefficients.

```{r}
fev <- tribble(
  ~height, ~fev,
  134, 1.7,
  138, 1.9,
  142, 2.0,
  146, 2.1,
  150, 2.2,
  154, 2.5,
  158, 2.7,
  162, 3.0,
  166, 3.1,
  170, 3.4,
  174, 3.8,
  178, 3.9
)
```
## Solutions
```{r}
lmout_fev <- lm(fev ~ height, data = fev)
tidy(lmout_fev)
```
We have $\hat{\beta}_0 = -5.31288$ and $\hat{\beta}_1 = 0.05131$. The fitted regression line is
$$
y = -5.31288 + 0.05131x
$$
We only interpret $\hat{\beta}_1$ since 0 is not in the range of the data. Folks who are 1 cm taller have an FEV 0.05 liters larger on average. You would get no points on an exam if you used the words "increase" or "change".
:::

# Estimating Variance

- We assume that the variance of $Y_i$ is the same for each $X_i$. 
- Call this variance $\sigma^2$.
-   We estimate it by the variability in the residuals.
    
    ```{r}
    #| echo: false
    estriol |>
      mutate(resid = birthweight - fitted,
             resid2 = resid^2) ->
      estriol
    estriol %>%
      ggplot(mapping = aes(x = estriol, y = birthweight)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Estriol (mg/24hr)") +
      ylab("Birthweight (g/100)") +
      geom_segment(mapping = aes(x = estriol, xend = estriol, y = birthweight, yend = fitted), alpha = 1/2)
    
    estriol |>
      ggplot(mapping = aes(x = estriol, y = resid)) +
      geom_point() +
      xlab("Estriol (mg/24hr)") +
      ylab("Residual") +
      geom_segment(mapping = aes(x = estriol, xend = estriol, y = 0, yend = resid), alpha = 1/2) +
      geom_hline(yintercept = 0, col = "blue", alpha = 1/2)
    
    estriol |>
      summarize(meansquared = mean(resid2),
                meanestriol = mean(estriol)) ->
      sumestriol
    
    estriol %>%
      ggplot(aes(x = resid)) +
      geom_histogram(bins = 7, fill = "white", color = "black") +
      xlab("Residuals")
    ```
- The variance of residuals is the estimated variance in the data. This is
  a general technique.
  - Technical note: people adjust for the number of parameters in a model when calculating the variance, so they no-longer divide by "n - 1".

-   In R, use the broom function `glance()` function to get the estimated standard deviation. It's the value in the `sigma` column.
    ```{r}
    glance(lmout) %>%
      select(sigma)
    ```
    
-   Estimating the variance/standard deviation is important because it is a component in the 
  standard error of $\hat{\beta}_1$ and $\hat{\beta}_0$. These standard
  errors are output by `tidy()`.
    ```{r}
    tidy(lmout) %>%
      select(term, std.error)
    ```
    
- The variance is also used when calculating prediction intervals.

# Hypothesis Testing

- The sign of $\beta_1$ denotes different types of relationships between
  the two quantitative variables:
  - $\beta_1 = 0$: The two quantitative variables are not linearly associated.
  - $\beta_1 > 0$: The two quantitative variables are positively associated.
  - $\beta_1 < 0$: The two quantitative variables are negatively associated.
    
- Hypothesis Testing:
  - We are often interested in testing if a relationship exists:
  - Two possibilities: 
    1. Alternative Hypothesis: $\beta_1 \neq 0$.
    2. Null Hypothesis: $\beta_1 = 0$.
  - Strategy: We calculate the probability of the data assuming possibility 2 
    (called a $p$-value). If this probability is low, we conclude possibility 1. 
    If the this probability is high, we don’t conclude anything.

- Graphic: 
  ![](cartoons/linear_p.png) \ 
  
```{r}
#| echo: false
#| eval: false
# set.seed(1)
# ybar <- mean(estriol$birthweight)
# ysd  <- sd(estriol$birthweight)
# miny <- min(estriol$birthweight) - 3.7
# maxy <- max(estriol$birthweight) + 0.7
# 
# ggplot(estriol, aes(x = estriol, birthweight)) +
#   geom_point(size = 0.5) +
#   theme_minimal() +
#   theme(axis.text = element_blank(),
#         axis.title = element_blank()) +
#   geom_smooth(method = "lm", se = FALSE) +
#   geom_rug(sides = "b") +
#   ylim(miny, maxy) ->
#   pl
# 
# ggsave(filename = "./cartoons/true_dat.pdf", plot = pl, height = 0.7, width = 1.1)
# 
# for (index in 1:5) {
#     estriol$sim <- rnorm(n = nrow(estriol), mean = ybar, sd = ysd)
#     lmtemp <- lm(sim ~ estriol, data = estriol)
#     cat(coef(lmtemp)[2], "\n")
#     ggplot(estriol, aes(x = estriol, sim)) +
#       geom_point(size = 0.5) +
#       theme_minimal() +
#       theme(axis.text = element_blank(),
#             axis.title = element_blank()) +
#       geom_smooth(method = "lm", se = FALSE) +
#       geom_rug(sides = "b") +
#       ylim(miny, maxy) ->
#       pl
#   ggsave(filename = paste0("./cartoons/simdat_dat", index, ".pdf"), plot = pl, height = 0.7, width = 1.1)
# }
```

- The sampling distribution of $\hat{\beta}_1$ comes from statistical theory. The  $t$-statistic is $\hat{\beta}_1 / SE(\hat{\beta}_1)$. It has a $t$-distribution with $n-2$ degrees of freedom.

  - $SE(\hat{\beta}_1)$: Estimated standard deviation of the sampling distribution of $\hat{\beta}_1$.

::: {.panel-tabset}    
## Exercise (8.18 in OpenIntro 4th Edition)

Which is higher? Determine if (i) or (ii) is higher or if they are equal. Explain your reasoning. For a regression line, the uncertainty associated with the slope estimate, $\hat{\beta}_1$ , is higher when

i. there is a lot of scatter around the regression line or
ii. there is very little scatter around the regression line

## Solution

(i). If there is a lot of scatter, then it is hard to pin-point the mean relationship. This can be formalized by looking at the contribution of the sample size in the standard error formula for $\hat{\beta}_1$.
:::

::: {.panel-tabset}
## Exercise
The `bac` dataset from @graham2003electronic examines sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer. They then measured their blood alchol content. You can load these data into R via

```{r}
bac <- tribble(
  ~beers, ~bac,
  5, 0.1,
  2, 0.03, 
  9, 0.19, 
  8, 0.12, 
  3, 0.04, 
  7, 0.095,
  3, 0.07, 
  5, 0.06, 
  3, 0.02,
  5, 0.05, 
  4, 0.07, 
  6, 0.1,  
  5, 0.085,
  7, 0.09, 
  1, 0.01, 
  4, 0.05) 
```

1.  Create an appropriate plot to visualize the association between the number of beers and the BAC.

2.  Does the relationship appear positive or negative?

3.  Write out equation of the OLS line.

4.  Do we have evidence that the number of beers is associated with BAC? Formally justify.

5.  Interpret the coefficient estimates.

6.  What happens to the standard errors of the estimates when we force the intercept to be 0? Why? You can force the intercept to be 0 by subtracting `1` on the right-hand-side of the formula in `lm()`.

## Solutions

1.  This is how you make the plot:
    ```{r}
    ggplot(bac, aes(x = beers, y = bac)) +
     geom_point()
    ```

2.  Positive. It points up.

3.  You fit the linear model like this:

    ```{r}
    lmout_bac <- lm(bac ~ beers, data = bac)
    tdat <- tidy(lmout_bac, conf.int = TRUE)
    beta0hat <- tdat$estimate[1]
    beta1hat <- tdat$estimate[2]
    ```
    The estimated regression line is this:
    $$
    y = -0.0127 + 0.01796 x
    $$

4.  Yes, the $p$-value is 2.969e-06, which provides strong evidence against the null that the two variables are not linearly associated.
    
5.  Every beer a person drinks is expected to increase their blood alcohol content by 0.018. I use causal language here because this was a randomized experiment.

6.  We fit the linear model by forcing the intercept to be 0 via this code:
    ```{r}
    lmout_bac2 <- lm(bac ~ beers - 1, data = bac)
    tdat2 <- tidy(lmout_bac2, conf.int = TRUE)
    tdat$std.error[2]
    tdat2$std.error
    ```
   
   The standard error shrinks. That's because we are having to estimate one less parameter and so we have degrees of freedom.
:::

# ANOVA approach to Testing

- There is an alternative way to conduct hypothesis testing in the linear model, through **ANOVA** (analysis of variances).

- In simple linear regression, this ANOVA approach will be equivalent to the t-based approach above. But it generalizes to more complicated scenarios for multiple linear regression.

::: {.callout-tip}
Analysis of variance has nothing to do with sample variances. It is about testing means. The "variance" should be thought of as variation attributible to different models.
:::

We need some definitions:

::: {.callout-tip}
## Fits
The estimated value of $y_i$ at a given $x_i$, denoted $\hat{y}_i$. In the simple linear regression model, this is
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i
$$
:::

::: {.callout-tip}
## Residuals
The deviation of the observed value of $y_i$ from the fitted value $\hat{y}_i$.
$$
e_i = y_i - \hat{y}_i
$$
:::

- ANOVA is about partitioning the variation of $y_i$ from $\bar{y}$ into components that are explained by the predictors and components that are not explained by the predictors.

::: {.callout-tip}
## Total Sum of Squares
The total variation of the response variable $y$. This is the amount of variation not explained by $\bar{y}$
$$
\text{SSE(R)} = \sum_{i=1}^n (y_i - \bar{y})^2
$$
This looks like the variance of $Y$ without the $\frac{1}{n-1}$ in front.
```{r}
#| echo: false
estriol |>
  mutate(ybar = mean(birthweight),
         resid_tot = birthweight - ybar) ->
  estriol
estriol |>
  ggplot() +
  geom_point(aes(x = estriol, y = birthweight)) +
  geom_segment(aes(x = estriol, xend = estriol, y = birthweight, yend = ybar)) +
  geom_hline(yintercept = mean(estriol$birthweight), col = "blue", linewidth = 1)
```
:::

::: {.callout-tip}
## Residual Sum of Squares
The variation about the regression line. This is the amount of variation not explained by the regression line
$$
\text{SSE(F)} = \sum_{i=1}^n(y_i - \hat{y}_i)^2 = \sum_{i=1}^ne_i^2
$$
```{r}
#| echo: false
estriol |>
  ggplot() +
  geom_point(aes(x = estriol, y = birthweight)) +
  geom_segment(aes(x = estriol, xend = estriol, y = birthweight, yend = fitted)) +
  geom_smooth(aes(x = estriol, y = birthweight), formula = y ~ x, method = "lm", se = FALSE)
```
:::

::: {.callout-tip}
## Regression Sum of Squares
The variation between the regression line and $\bar{y}$. This is the amount of variation explained by the regression line that is not explained by $\bar{y}$.
$$
\text{SSR} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2
$$
```{r}
#| echo: false
estriol |>
  ggplot() +
  geom_point(aes(x = estriol, y = birthweight)) +
  geom_segment(aes(x = estriol, xend = estriol, y = ybar, yend = fitted)) +
  geom_smooth(aes(x = estriol, y = birthweight), formula = y ~ x, method = "lm", se = FALSE) +
  geom_hline(yintercept = mean(estriol$birthweight), col = "blue", linewidth = 1)
```
:::

- The total sum of squares can be decomposed into the regression and residual sums of squares.

::: {.callout-tip}
## ANOVA Decomposition
- SSE(R) = SSR + SSE(F)
- Total SS = Regression SS + Residual SS

$$
\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n(y_i - \hat{y}_i)^2
$$
:::

- If the regression sum of squares is much larger than the residual sum of squares, then a large proportion of the variation not explained by $\bar{y}$ **can** be explained by the regression line. 
  - This would provide evidence that there is a linear relationship between $x$ and $y$.
  - The is the basis of the $F$-test  

- Each sum of squares is associated with a **degrees of freedom**

::: {.callout-tip}
## Degrees of Freedom
Intuitively (not formally), how many independent pieces of information do we have?

- For SSE(F)
  - $\text{df}_F = n - 2$ = sample size - #parameters ($\beta_0$ and $\beta_1$)

- For SSE(R)
  - $\text{df}_R = n - 1$ = sample size - #parameters ($\beta_0$)
  
- For SSR
  - $\text{df}_{\text{reg}} = \text{df}_R - \text{df}_F = 1$ = difference in number of parameters between the two models
:::
  
::: {.callout-tip}
## $F$-test for Simple Linear Regression
- $H_0$: Reduced Model: $Y_i = \beta_0 + \epsilon_i$ where $\epsilon_i \sim N(0, \sigma^2)$
- $H_1$: Full Model: $Y_i = \beta_0 + \beta_1X_i + \epsilon_i$ where $\epsilon_i \sim N(0, \sigma^2)$

- Calculate the test-statistic:
  $$
  F^* = \frac{\text{SSR} / \text{df}_{\text{reg}}}{\text{SSE(F)} / \text{df}_F} = \frac{[\text{SSE(R)} - SSE(F)] / (\text{df}_{R} - \text{df}_F)}{\text{SSE(F)} / \text{df}_F}
  $$

- Larger values of $F^*$ mean that the regression line picks up more variation and provides evidence against $H_0$.

- If $H_0$ is true, then $F^* \sim F_{\text{df}_R - \text{df}_F, \text{df}_F}$.
  - For SLR $\text{df}_R - \text{df}_F = 1$ and $\text{df}_F = n-2$

-   The $p$-value is the probability that $F > F^*$ given $F \sim F_{\text{df}_R - \text{df}_F, \text{df}_F}$.
    ```r
    1 - pf(q = fstar, df1 = 1, df2 = n - 2)
    ```
```{r}
#| echo: false
tibble(x = seq(0.01, 3, length.out = 500)) |>
  mutate(y = df(x = x, df1 = 1, df2 = 10)) ->
  df1
df1 |>
  filter(x > 0.8) ->
  df2
ggplot() +
  geom_line(data = df1, mapping = aes(x = x, y= y)) +
  geom_area(data = df2, mapping = aes(x = x, y = y), fill = "#E69F00", color = "#E69F00") +
  xlab("F") +
  ylab("PMF") +
  ggtitle(expression(F[list(df[R]-df[F],df[F])])) +
  geom_vline(xintercept = 0.8, lty = 2, col = 2)
```
:::


-   In R, to run the F-test, take the output of `lm()` and feed it into `anova()`.
    
    ```{r}
    lm(birthweight ~ estriol, data = estriol) |>
      anova() |>
      tidy()
    ```

- What is returned above is called an **ANOVA table** with the following elements

|            | df                          | ss              | ms                                  | statistic | $p$-value |
|------------|-----------------------------|-----------------|-------------------------------------|-----------|-----------|
| Regression | $\text{df}_R - \text{df}_F$ | SSE(R) - SSE(F) | [SSE(R) - SSE(F)]/(df$_R$ - df$_F$) | $F^*$     | $p$-value |
| Error      | $\text{df}_F$               | SSE(F)          | SSE(F) / df$_F$                     |           |           |
| Total      | $\text{df}_R$               | SSE(R)          | SSE(R) / df$_R$                     |           |           |


- Because of the decomposition of sums of squares, folks often report the strength of the linear association via the coefficient of determination.

::: {.callout-tip}
## Coefficient of Determination
$$
R^2 = 1 - \frac{\text{SSE(R)}}{\text{SSE(F)}} = \frac{\text{Regression SS}}{\text{Total SS}}
$$
:::

- $R^2$ is the proportion of variance in $Y$ that is explained by the regression, or 1 minus the proportion of variance in $Y$ not explained by the regression line.
  - $R^2 = 1$: All points lie exactly on the regression line.
  - $R^2 = 0$: Regression line is equal to $y = \bar{y}$ (a horizontal line).

::: {.callout-important}
$R^2$ tells you **NOTHING** about the appropriateness of the model. Given the model is true, it tells you how much variation in a response a predictor explains.
:::
  
- E.g., the model might be wrong (because of a curvilinear relationship) but the $R^2$ can still be high.

- E.g., the model might be correct (because a linear relationship is fine) but the $R^2$ can be 0 because the linear relationship is not strong.
  
::: {.panel-tabset}
## Exercise
From the FEV data, create an ANOVA table.

## Solution
```{r}
lm(fev ~ height, data = fev) |>
  anova() |>
  tidy()
```
:::

::: {.panel-tabset}
## Exercise
Using just the FEV ANOVA table, what is regression estimate of $\sigma^2$?

## Solution
This is the mean square of residuals Or, 0.01452. We can verify this by directly using the output of `lm()`.
```{r}
lm(fev ~ height, data =fev) |>
  glance() |>
  select(sigma) |>
  mutate(sigma2 = sigma^2)
```
:::

::: {.panel-tabset}
## Exercise
Using just the data from the FEV ANOVA table, what is the sample variance of FEV?

## Solution
This is the SSE(R)/df$_R$. We can get SSE(R) by summing the sums of squares in the ANOVA table
```{r}
6.0239 + 0.1452
```
and df$_R$ by summing the degrees of freedom in the ANOVA table
```{r}
10 + 1
```
So, the sampel variance is
```{r}
6.169 / 11
```
Let's verify by direct calculation
```{r}
fev |>
  summarize(var = var(fev))
```
:::

::: {.panel-tabset}
## Exercise
Calculate the $p$-value from the $F$-statistic and the degrees of freedom in the FEV ANOVA table.

## Solution
```{r}
1 - pf(q = 414.8, df1 = 1, df2 = 10)
```
:::

# Confidence Intervals

-   The confidence intervals for $\beta_0$ and $\beta_1$ are of the form
    \begin{align*}
    \hat{\beta}_0 &\pm t_{n-2, 1-\alpha/2}\text{SE}(\hat{\beta}_0)\\
    \hat{\beta}_1 &\pm t_{n-2, 1-\alpha/2}\text{SE}(\hat{\beta}_1)
    \end{align*}

-   The confidence intervals for $\beta_0$ and $\beta_1$ are easy to obtain from the output of `tidy()` if you set `conf.int = TRUE`.
  
    ```{r}
    lmtide <- tidy(lmout, conf.int = TRUE)
    select(lmtide, conf.low, conf.high)
    ```
    
::: {.panel-tabset}
## Exercise
Below is some output of `lm()` using the blood-alcohol data
```{r}
#| echo: false
lm(bac ~ beers, data = bac) |>
  tidy() |>
  select(term, estimate, std.error)
```
Recall that the sample size is $n = 16$. From this output, construct a 95% CI for $\beta_0$ and $\beta_1$

## Solution
The appropriate $t$-quantile is
```{r}
qt(p = 1 - 0.05 / 2, df = 16 - 2)
```
For $\beta_0$, the CI is:
```{r}
-0.01270 - 2.145 * 0.012638
-0.01270 + 2.145 * 0.012638
```
For $\beta_1$, the CI is
```{r}
0.01796 - 2.145 * 0.002402
0.01796 + 2.145 * 0.002402
```

Let's confirm:
```{r}
lm(bac ~ beers, data = bac) |>
  tidy(conf.int = TRUE) |>
  select(term, conf.low, conf.high)
```
:::


# Prediction (Interpolation)

- Definitions
  - **Interpolation**: Making estimates/predictions within the range of the data.
  - **Extrapolation**: Making estimates/predictions outside the range of the data.
  - Interpolation is good. Extrapolation is bad.

-   Interpolation
    ```{r}
    #| echo: false
    ggplot(data = estriol, mapping = aes(x = estriol, y = birthweight)) +
      geom_point() +
      geom_abline(slope = coef(lmout)[2], intercept = coef(lmout)[1], lwd = 1, col = "blue", alpha = 1/2) +
      annotate(geom = "point", x = 20, y = coef(lmout)[1] + 20 * coef(lmout)[2], color = "red", size = 3) +
      xlab("Estriol (mg/24hr)") +
      ylab("Residual")
    ```

-   Extrapolation
    ```{r}
    #| echo: false
    ggplot(data = estriol, mapping = aes(x = estriol, y = birthweight)) +
      geom_point() +
      geom_abline(slope = coef(lmout)[2], intercept = coef(lmout)[1], lwd = 1, col = "blue", alpha = 1/2) +
      annotate(geom = "point", x = 35, y = coef(lmout)[1] + 35 * coef(lmout)[2], color = "red", size = 3) +
      xlab("Estriol (mg/24hr)") +
      ylab("Residual")
    ```

- Why is extrapolation bad?
  1. Not sure if the linear relationship is the same outside the range of
     the data (because we don't have data there to see the relationship).
  2. Not sure if the variability is the same outside the range of the 
     data (because we don't have data there to see the variability).

- To make a prediction:
  1. You need a data frame with the exact same
     variable name as the explanatory variable. 
     ```{r}
     newdf <- tribble(~estriol,
                      20, 
                      25)
     ```
  2. Then you use the `predict()` function to obtain predictions.
     ```{r}
     newdf %>%
       mutate(predictions = predict(object = lmout, newdata = newdf)) ->
       newdf
     newdf
     ```

::: {.panel-tabset}
## Exercise
Derive the predictions above by hand (not using `predict()`).

## Solution
```{r}
21.523 + 0.608 * 20
21.523 + 0.608 * 25
```
:::

::: {.panel-tabset}  
## Exercise
From the BAC dataset, suppose someone had 5 beers. Use `predict()` to predict their BAC.
  
## Solution
```{r}
lmbeer <- lm(bac ~ beers, data = bac)
newdata <- tribble(~beers,
                   5)
predict(object = lmbeer, newdata = newdata)
```
:::


  
# Assumptions

## Assumptions and Violations

- The linear model has many assumptions.

- **You should always check these assumptions**.

- Assumptions in *decreasing* order of importance
  1. **Linearity** - The relationship looks like a straight line.
  2. **Independence** - The knowledge of the value of one observation does not 
     give you any information on the value of another.
  3. **Equal Variance** - The spread is the same for every value of $x$
  4. **Normality** - The distribution of the errors isn't too skewed and there aren't 
     any *too* extreme points. (Only an issue if you have outliers and a 
     small number of observations because of the 
     [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)).
     
- Problems when violated
  1. **Linearity** violated - Linear regression line does not pick up actual 
     relationship. Results aren't meaningful.
  2. **Independence** violated - Linear regression line is unbiased, but standard 
     errors are off. Your $p$-values are too small.
  3. **Equal Variance** violated - Linear regression line is unbiased, but standard 
     errors are off. Your $p$-values may be too small, or too large.
  4. **Normality** violated - Unstable results if outliers are present and sample size 
     is small. Not usually a big deal.

::: {.panel-tabset}  
## Exercise 
What assumptions are made about the distribution of the explanatory variable (the $x_i$'s)?

## Solution
None. Inference is conditional on the $x_i$'s.
:::
  
## Evaluating Independence

- Think about the problem.
  - Were different responses measured on the same observational/experimental unit?
  - Were data collected in groups?

- Example of non-independence: The temperature today and the temperature tomorrow. If it is warm today, it is probably warm tomorrow.

- Example of non-independence: You are collecting a survey. To obtain individuals, you select a house at random and then ask all participants in this house to answer the survey. The participants' responses inside each house are probably not independent because they probably share similar beliefs/backgrounds/situations/genetics.
  
- Example of independence: You are collecting a survey. To obtain individuals, you randomly dial phone numbers until an individual picks up the phone.

## Evaluating other assumptions

- **Evaluate issues by plotting the residuals.**

- The **residuals** are the observed values minus the predicted values.
  $$
  r_i = y_i - \hat{y}_i
  $$

- In the linear model, $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$.

-   Obtain the residuals by using `augment()` from broom. They will be the `.resid` variable.
    ```{r}
    aout <- augment(lmout)
    glimpse(aout)
    ```

- You should always make the following scatterplots. The residuals always go on the $y$-axis.
  - Fits $\hat{y}_i$ vs residuals $r_i$.
  - Response $y_i$ vs residuals $r_i$.
  - Explanatory variable $x_i$ vs residuals $r_i$.
    
- In the simple linear model, you can probably evaluate these issues by plotting the data ($x_i$ vs $y_i$). But residual plots generalize to *much* more complicated models, whereas just plotting the data does not.
  
```{block, eval = FALSE, echo = FALSE}  
- Solutions:
  1. Linearity Violated: Try a transformation. If the relationship looks 
     curved and monotone (i.e. either always increasing or always decreasing) then 
     try a log transformation.
  2. Independence Violated: You'll need more sophisticated statistical 
     techniques. Hire a statistician.
  3. Equal Variance Violated: If the relationship is also curved and monotone, 
     try a log transformation on the response variable. Or figure out how 
     to perform sandwich estimation.
  4. Normality Violated: Do nothing. But don't use prediction intervals 
     (confidence intervals are fine). Just report the violations from 
     normality. 
```
     
### Example 1: A perfect residual plot

```{r, echo = FALSE, message = FALSE}
set.seed(1)
x <- rnorm(100, sd = 1); y <- x + rnorm(100);
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data") + geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Means are straight lines
- Residuals seem to be centered at 0 for all $x$
- Variance looks equal for all $x$
- Everything looks perfect

### Example 2: Curved Monotone Relationship, Equal Variances

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- rexp(100)
    x <- x - min(x) + 0.5
    y <- log(x) * 20 + rnorm(100, sd = 4)
    df_fake <- tibble(x, y)
    ```

```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Curved (but always increasing) relationship between $x$ and $y$.
- Variance looks equal for all $x$
- Residual plot has a parabolic shape.
-   **Solution**: These indicate a $\log$ transformation of $x$ could help.
    ```{r}
    df_fake %>%
      mutate(logx = log(x)) ->
      df_fake
    lm_fake <- lm(y ~ logx, data = df_fake)
    ```

### Example 3: Curved Non-monotone Relationship, Equal Variances

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- rnorm(100)
    y <- -x^2 + rnorm(100)
    df_fake <- tibble(x, y)
    ```

```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Curved relationship between $x$ and $y$
- Sometimes the relationship is increasing, sometimes it is decreasing.
- Variance looks equal for all $x$
- Residual plot has a parabolic form.
-   **Solution**: Include a squared term in the model (or hire a statistician).
    ```{r}
    lmout <- lm(y ~ x + I(x^2), data = df_fake)
    tidy(lmout)
    ```

### Example 4: Curved Relationship, Variance Increases with $Y$

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- rnorm(100)
    y <- exp(x + rnorm(100, sd = 1/2))
    df_fake <- tibble(x, y)
    ```

```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Curved relationship between $x$ and $y$
- Variance looks like it increases as $y$ increases
- Residual plot has a parabolic form.
- Residual plot variance looks larger to the right and smaller to the left.
-   **Solution**: Take a log-transformation of $y$.
    ```{r}
    df_fake %>%
      mutate(logy = log(y)) ->
      df_fake
    lm_fake <- lm(logy ~ x, data = df_fake)
    ```


### Example 5: Linear Relationship, Equal Variances, Skewed Distribution

```{r, echo = FALSE}
set.seed(1)
x <- runif(200)
y <- 15 * x + rexp(200, 0.2)
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Straight line relationship between $x$ and $y$.
- Variances about equal for all $x$
- Skew for all $x$
- Residual plots show skew.
- **Solution**: Do nothing, but report skew (usually OK to do)

### Example 6: Linear Relationship, Unequal Variances

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- runif(100) * 10
    y <- 0.85 * x + rnorm(100, sd = (x - 5) ^ 2)
    df_fake <- tibble(x, y)
    ```


```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Linear relationship between $x$ and $y$.
- Variance is different for different values of $x$.
- Residual plots really good at showing this.
-   **Solution**: The modern solution is to use sandwich estimates of the standard errors
  (hire a statistician).
    ```{r}
    #| message: false
    library(lmtest)
    library(sandwich)
    lm_fake <- lm(y ~ x, data = df_fake)
    robust_se <- vcovHC(lm_fake, type = "HC1")
    coeftest(lm_fake, vcov = robust_se) |>
      tidy(conf.int = TRUE)
    ```
  
  
## Some Exercises on Assumptions

::: {.panel-tabset}
## Exercise
Evaluate the assumptions of the linear fit of birthweight on estriol.

## Solution
Looks good. No curvilinear trends, equal variance looks fine.
```{r}
ggplot(estriol, aes(x = estriol, y = birthweight)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE)

lm(birthweight ~ estriol, data = estriol) |>
  augment() |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_smooth(formula = y ~ x, method = "loess")
```
:::

::: {.panel-tabset}
## Exercise
Evaluate the assumptions of the linear fit of blood alcohol level on beers.

## Solutions
Looks good. No curvilinear trends, equal variance looks fine.
```{r}
ggplot(bac, aes(x = beers, y = bac)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE)

lm(bac ~ beers, data = bac) |>
  augment() |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_smooth(formula = y ~ x, method = "loess")
```
:::

::: {.panel-tabset}
## Exercise
Evaluate the assumptions of the linear fit of fev level on height.

## Solution
It looks very curved, doesn't it? But the equal variance assumption looks fine.
```{r}
ggplot(fev, aes(x = height, y = fev)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE)

lm(fev ~ height, data = fev) |>
  augment() |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_smooth(formula = y ~ x, method = "loess")
```

Logging height doesn't do anything, so we could add a quadratic trend
```{r}
ggplot(fev, aes(x = height, y = fev)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE)

lm(fev ~ height + I(height^2), data = fev) |>
  augment() |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_smooth(formula = y ~ x, method = "loess")
```
This looks way better. The squared-height term is significant:
```{r}
lm(fev ~ height + I(height^2), data = fev) |>
  tidy()
```
:::

::: {.panel-tabset}
## Exercise
The following data frame, from @wilson1999diversity, contains the numbers of reptile and amphibian species and the island areas for seven islands in the West Indies.
```{r}
species <- tribble(
  ~Area, ~Species,
  44218, 100,
  29371, 108,
  4244, 45,
  3435, 53,
  32, 16,
  5, 11,
  1, 7)
```
Find an appropriate linear regression model for relating the effect of island area on species number. Find the regression estimates. Interpret them.

## Solution
Plotting the data, it looks like we need to take logs:
```{r, echo = FALSE, eval = FALSE}
ggplot(species, aes(x = Area, y = Species)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()
```
Fit the linear model:
```{r}
species %>%
  mutate(log_area = log2(Area),
         log_species = log2(Species)) ->
  species

lm_sp <- lm(log_species ~ log_area, data = species)
tidy(lm_sp)
```
:::

# Interpreting Coefficients when you use logs

- Generally, when you use logs, you interpret associations on a 
  *multiplicative* scale instead of an *additive* scale.

- No log:
  - Model: $E[y_i] = \beta_0 + \beta_1 x_i$
  - Observations that differ by 1 unit in $x$ tend to differ by $\beta_1$ units in $y$.

- Log $x$:
  - Model: $E[y_i] = \beta_0 + \beta_1 \log_2(x_i)$
  - Observations that are twice as large in $x$ tend to differ by $\beta_1$ units in $y$.

- Log $y$:
  - Model: $E[\log_2(y_i)] = \beta_0 + \beta_1 x_i$
  - Observations that differ by 1 unit in $x$ tend to be $2^{\beta_1}$ times larger in $y$. 

- Log both:
  - Model: $E[\log_2(y_i)] = \beta_0 + \beta_1 \log_2(x_i)$
  - Observations that are twice as large in $x$ tend to be $2^{\beta_1}$ times larger in $y$. 

::: {.panel-tabset}  
## Exercise
Re-interpret the regression coefficients estimates you calculated using the `species` dataset.

## Solution
2^0.25 = 1.19, so:
    
Islands that are twice as large have 19% more species on average.
:::

# Multiple Linear Regression

## Model, Interpretation, and Estimation

- Setting:
  - One response variable $Y$
  - Multiple explanatory variables $X_1, X_2, \ldots, X_k$

-   Example: Systolic blood pressure (SBP) is typically measured in newborns. This is used for diagnostic purposes and for studies relating infant SBP to adult SBP. However, SBP is known to vary by birthweight and by the timing of when SBP was measured (in number of days after birth). It is thus important to be able to specifically characterize the relationship between age (days), birthweight (oz), and SBP (mm Hg). Data from a study including 16 newborns are below:
    ```{r}
    bp <- tribble(
      ~age, ~birthweight, ~sbp,
      3, 135, 89,
      4, 120, 90,
      3, 100, 83,
      2, 105, 77,
      4, 130, 92,
      5, 125, 98,
      2, 125, 82,
      3, 105, 85,
      5, 120, 96,
      4, 90, 95,
      2, 120, 80,
      3, 95, 79,
      3, 120, 86,
      4, 150, 97,
      3, 160, 92,
      3, 125, 88,
    )
    ```

::: {.callout-tip}
## Multiple Linear Regression Model (two explanatory variables)
$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2 X_{i2} + \epsilon_i
$$

- $Y_i$: Response variable for individual $i$.
- $X_{i1}$: First explanatory variable for individual $i$
- $X_{i2}$: Second explanatory variable for individual $i$.
- $\epsilon_i \sim N(0, \sigma^2)$
- $\beta_0$, $\beta_1$, and $\beta_2$ are called the **(partial) regression coefficients**.
:::

-   The model says that, for fixed values of $X_{i1}$ and $X_{i2}$, the expected value of $Y_i$ is
    $$
    E[Y_i|X_{i1}, X_{i2}] = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}
    $$
    
- The actual value of $Y_i$ varies about this point according to a normal distribution.

  ![](./cartoons/plane_me.png)\
  
- $\beta_0$, $\beta_1$, $\beta_2$ are parameters. We estimate them by ordinary least squares. That is, find the values of $\beta_0$, $\beta_1$, $\beta_2$ that minimize
$$
\sum_{i=1}^n[y_i - (\beta_0 + \beta_1x_{i1} + \beta_2x_{i2})]^2
$$

- Call the OLS estimates $\hat{\beta}_0$, $\hat{\beta}_1$, and $\hat{\beta}_2$.

-   The fits are again the estimated values of $y$
    $$
    \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1X_{i1} + \hat{\beta}_2X_{i2}
    $$
    
-   The residuals are again the deviations from the fits
    $$
    e_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1X_{i1} + \hat{\beta}_2X_{i2})
    $$

-   The estimate of $\sigma^2$ is again the variance of the residuals.
    $$
    \hat{\sigma}^2 = \frac{1}{n - 3}\sum_{i=1}^ne_i^2
    $$
    
- The $n-3$ is because we have three parameters ($\beta_0$, $\beta_1$, and $\beta_2$)
  - Recall: when we just have $Y_i \sim N(\mu, \sigma^2)$, we divide by $n-1$ because of the one parameter $\mu$.
  - Recall: when we use simple linear regression $Y_i \sim N(\beta_0+\beta_1X_i, \sigma^2)$, we divide by $n-2$ because we have two parameters, $\beta_0$ and $\beta_1$
  - Now, we have $Y_i \sim N(\beta_0+\beta_1X_{i1} + \beta_2X_{i2}, \sigma^2)$ and three parameters.

- You use `lm()` to get the OLS estimates in R.
  - Put the response variable to the left of the tilde `~`
  - Put the explanatory variables to the right of the tilde `~`, separated by `+`
    - Order does not matter.
  
-   Let's fit the multiple linear regression model for SBP on age and birthweight.
    ```{r}
    lm_bp <- lm(sbp ~ age + birthweight, data = bp)
    tidy(lm_bp)
    ```

-   The estimated regression line is
    $$
    y = 53.45 + 5.89x_1 + 0.13x_2
    $$


::: {.callout-tip}
## Interpret $\beta_j$
- $\beta_0$: If $(X_1, X_2) = (0, 0)$ is in the range of the data, then $\beta_0$ is the expected value of $Y$ at $X_{1} = 0$ and $X_2 = 0$. Otherwise, it is just the $y$-intercept of the regression surface.
- $\beta_1$: Individuals with one unit larger $X_1$ but the same value of $X_2$ have $\beta_1$ larger $Y$ values, on average.
- $\beta_2$: Individuals with one unit larger $X_2$ but the same value of $X_1$ have $\beta_2$ larger $Y$ values, on average.
:::

- In the SBP example, we would interpret these as:
  - Babies that are measured one day later but have the same birthweight have a blood pressure 5.89 mm Hg higher on average.
  - Babies that weigh one ounce more but are measured at the same age have a blood pressure 0.13 mm Hg higher on average.
  
-   Where does that interpretation come from:

    $$
    y_{old} = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}
    $$
    \begin{align*}
    y_{new} &= \beta_0 + \beta_1(x_{i1} + 1) + \beta_2x_{i2}\\
    &= \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_1\\
    &= y_{old} + \beta_1
    \end{align*}
    
- The interpretation  of the relationship between SBP and weight isn't too useful because 1 ounce isn't a big difference in bith weight. But we can also say what the expected difference in SBP would be if we had babies that differed by, say, 10 oz.

- Babies that weigh 10 ounces more but are measured at the same age have a blood pressure 1.4 mm Hg higher on average.

::: {.panel-tabset}
## Exercise
Prove the interpretation of $\beta_2$ in the baby weight example that we just stated.

## Solution
$$
y_{old} = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}
$$
\begin{align*}
y_{new} &= \beta_0 + \beta_1(x_{i1} + 10) + \beta_2x_{i2}\\
&= \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + 10\beta_1\\
&= y_{old} + 10\beta_1
\end{align*}
:::


::: {.panel-tabset}
## Exercise
Data were collected from 20 healthy females, 25–34 years old, to study the relationship between body fat and three predictors. Variables include

- `triceps`: Triceps skinfold thickness (mm).
- `thigh`: Thigh circumference (cm).
- `midarm`: Midarm circumference (cm)
- `fat`: Body fat (percent).

The data can be downloaded from <https://dcgerard.github.io/stat_320/data/body.csv>

Download the data and fit a regression model of `fat` on `triceps` and `midarm`. State the model you fit, defining any paramters and variables. Interpret the regression coefficients.

## Solutions
- Let $Y_i$ be the body fat of female $i$
- Let $X_{i1}$ be the triceps skinfold thickness of woman $i$.
- Let $X_{i2}$ be the midarm circumference of woman $i$.

Our model is
$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i
$$
where $\epsilon_i \sim N(0, \sigma^2)$

We fit this model by least squares:
```{r}
#| message: false
body <- read_csv("https://dcgerard.github.io/stat_320/data/body.csv")
lm(fat ~ triceps + midarm, data = body) |>
  tidy()
```
Our estimated regression line is
$$
y = 6.79 + \beta_1 - 0.43 \beta_2
$$

Women with 1 mm thicker triceps skinfold thickness, but the same midarm circumference, have 1 percentage points higher body fat on average.

Women with 1 cm larger midarm circumference, but the same triceps skinfold thickness, have 0.43 percentage points lower body fat on average.
:::

- When you were intrepreting the regression coefficients in the above exercise, you should say "percentage points", not "percent.
  - Percentage points are additive differences. E.g. 20% is 10 percentage points higher than 10%
  - Percent is for multiplicative differences. E.g. 20% is 100% higher than 10%.
  
- Did the results from the body fat example surprise? 

::: {.callout-important}
Simple linear regression coefficients might be **very very** different than multiple linear regression coefficients.
:::

```{r}
lm(fat ~ midarm, data = body) |>
  tidy() |>
  select(term, estimate)

lm(fat ~ triceps + midarm, data = body) |>
  tidy() |>
  select(term, estimate)
```


- The different signs for the coefficient of `midarm` occur because we **control** for tricepts.
  - The SLR coefficient is the relationship between `fat` and `midarm`
  - The MLR coefficient is the relationship between `fat` and `midarm` for fixed values of `triceps` (controlling for `triceps`).
  
-   Here is the relationship between `fat` and `midarm`
    ```{r}
    #| echo: false
    ggplot(body, aes(x = midarm, y = fat)) +
      geom_point() +
      geom_smooth(method = lm, formula = y ~ x, se = FALSE) +
      theme_bw() +
      ggtitle("Fat vs Midarm")
    ```

-   Here is the relationship between `fat` and `midarm` controlling for triceps
    ```{r}
    #| echo: false
    body |>
      mutate(tricat = cut(triceps, 2)) ->
      bodycut
    
    ggplot(bodycut, aes(x = midarm, y = fat, color = tricat)) +
      geom_point() +
      scale_color_discrete(palette = "Okabe-Ito", name = "Triceps Group") +
      geom_smooth(method = "lm", formula = y~x, se = FALSE) +
      theme_bw() +
      ggtitle("Fat vs Midarm | Triceps")
    ```

## More than Two Explanatory Variables

::: {.callout-tip}
## Multiple linear regression model
$$
Y_i = \beta_0 + \sum_{j=1}^{p-1}\beta_jX_{ij} + \epsilon_i
$$

- $Y_i$: Response variable for individual $i$.
- $X_{ij}$: The $j$th explanatory variable for individual $i$
- $\epsilon_i \sim N(0, \sigma^2)$
:::

- We say there are $p-1$ predictors in the model so that we have $p$ variables, which make the math equations nicer.

-   We estimate the $\beta_j$'s again by ordinary least squares, finding the values that minimize
    $$
    \sum_{i=1}^n\left[y_i - \left(\beta_0 + \sum_{j=1}^{p-1}\beta_jX_{ij}\right)\right]^2
    $$
    
- The OLS estimates are $\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_{p-1}$

- You get the OLS estimates by `lm()`.

-   Suppose we have a model for `fat` on `triceps`, `thigh`, and `midarm`. Let
    - $Y_i$ be the body fat for woman $i$.
    - $X_{i1}$ be the tricep skinfold thickness for woman $i$.
    - $X_{i2}$ be the thigh circumference for woman $i$.
    - $X_{i3}$ be the midarm circumference for woman $i$.
    
    Then we fit the model
    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i
    $$
    where $\epsilon_i \sim N(0,\sigma^2)$.
    
    We fit this model by `lm()`
    ```{r}
    lm(fat ~ triceps + thigh + midarm, data = body) |>
      tidy()
    ```
    
    Our estimated regression equation is
    $$
    y = 117.09 + 4.33 x_1 - 2.86x_2 - 2.19x_3
    $$
    
- We interpret the regression coefficients similarly
  - $\beta_j$ is the average difference in $y$ values when $x_j$ differs by 1, but all other variables are the same.
  
- E.g., Woman with 1mm larger skinfold thickness, but the same thigh and midarm circumferences, have 4.334 percentage points larger body fat on average.

- This is a huge mouthful, so in practice we say

  > $\beta_j$ is the average difference in $y$ values when $x_j$ differs by 1, controlling for all other variables.
  
- That is, **controlling** is shorthand to say what the statistical relationship is given those variables are fixed.

## Testing

- We can test if a regression coefficent is 0, **adjusting for other variables** via a $t$-method.

::: {.callout-tip}
## Test For Individual Regression Coefficients
- $H_0$: $\beta_j = 0$
- $H_A$: $\beta_j \neq 0$

-   If the null were **true**, then
    $$
    t = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \sim t_{n - p}
    $$
    
-   We compare our observed $t$ statistic to this null distribution to get a $p$-value

    ```{r}
    #| echo: false
    plt_t(lb = 2.2, df = 10, two_sided = TRUE, rng = c(-4, 4)) +
      geom_vline(xintercept = 2.2, lty = 2, col = 2)
    ```
    
-   Confidence intervals are of the form
    $$
    \hat{\beta}_j \pm t_{n-p,1-\alpha/2}\text{SE}(\hat{\beta}_j)
    $$
:::

-   You can get the $p$-values of this test, and confidence intervals, via the output of `lm()`.

    ```{r}
    lm(fat ~ triceps + thigh + midarm, data = body) |>
      tidy(conf.int = TRUE) |>
      select(term, p.value, conf.low, conf.high)
    ```


-   This is only a test for $\beta_j = 0$ given all other variables are in the model. The results of this test will differ depending on what other variables are in the model, sometimes drastically.

    ```{r}
    lm(fat ~ triceps + midarm, data = body) |>
      tidy() |>
      select(term, p.value)
    ```

- In this case, the $p$-values are so different because of **collinearity**.

::: {.callout-tip}
## Collinearity
The correlation between explanatory variables.
:::

- Intuitively, if two variables are highly collinear, then they are picking up the same information. So once you control for one of those variables, there is no longer an association between the response and the remaining variable.

::: {.panel-tabset}
## Exercise
In the body fat example, we saw that the $p$-values were all very large in the linear model that included all three of triceps skinfold thickness, thigh circumference, and midarm circumference. Does this mean that none of the variables are associated with body fat?

## Solution
No! This means that we don't have evidence that each variable is associated with body fat **controlling for the other two**. If we don't control for the other two, we might see an association.
:::

- When you have variables that are highly collinear, you should remove some of them, since they are picking up the same information.

- From this, it is far better to consider these tests as comparing two models, one of which is a submodel of the other.

- E.g., in the two explanatory variable case, $H_0: \beta_2 = 0$ versus $H_A: \beta_j \neq 0$ is equivalent to:
  - $H_0: Y_i = \beta_0 + \beta_1X_{i1} + \epsilon_i$
  - $H_0: Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i$
  
- This intuitively leads us to the ANOVA approach to hypothesis testing.

::: {.callout-tip}
## ANOVA Approach to Hypothesis Testing
- $H_0$: (Reduced Model) $Y_i = \beta_0 + \beta_1X_{i1} + \cdots + \beta_{q-1}X_{q-1} + \epsilon_i$
- $H_0$: (Full Model) $Y_i = \beta_0 + \beta_1X_{i1} + \cdots + \beta_{q-1}X_{p-1} + \epsilon_i$, for $p > q$

- I.e., we are testing whether we $X_q, X_{q+1},\ldots,X_{p-1}$ are associated with $Y$ controlling for $X_1,X_2,\ldots,X_{q-1}$.

- Calculate the SSE(R) and SSE(F), $\text{df}_F = n-p$, $\text{df}_R = n-q$

- Calculate the $F$ statistic
  $$
  F^* = \frac{[\text{SSE(R)} - SSE(F)] / (\text{df}_{R} - \text{df}_F)}{\text{SSE(F)} / \text{df}_F}
  $$

- If the null were **true**, then $F$ follows an $F$ distribution with numerator degrees of freedom $p-q$ and numerator degrees of freedom $n-p$. 

-   Compare our observed $F$ statistic to this distribution to get a $p$-value

    ```{r}
    #| echo: false
    tibble(x = seq(0, 7, length.out = 500)) |>
      mutate(y = df(x = x, df1 = 3, df2 = 10)) ->
      df1
    df1 |>
      filter(x > 2.1) ->
      df2
    ggplot() +
      geom_line(data = df1, mapping = aes(x = x, y= y)) +
      geom_area(data = df2, mapping = aes(x = x, y = y), fill = "#E69F00", color = "#E69F00") +
      xlab("F") +
      ylab("PMF") +
      ggtitle(expression(F[list(df[R]-df[F],df[F])])) +
      geom_vline(xintercept = 2.1, lty = 2, col = 2) +
      theme_bw()
    ```
:::

-   To run this test in R, separately fit reduced and full models, then use `anova()`.
    ```{r}
    reduced <- lm(fat ~ triceps, data = body)
    full <- lm(fat ~ triceps + thigh + midarm, data = body)
    anova(reduced, full) |>
      tidy()
    ```

- This ANOVA table is of the form

| term    | df.residual   | rss    | df                          | sumsq           | statistic | $p$-value |
|---------|---------------|--------|-----------------------------|-----------------|-----------|-----------|
| Reduced | $\text{df}_R$ | SSE(R) |                             |                 |           |           |
| Full    | $\text{df}_F$ | SSE(F) | $\text{df}_R - \text{df}_F$ | SSE(R) - SSE(F) | $F^*$     | $p$-value |

::: {.callout-important}
If we reject the $F$-test, then we have evidence that at least one of $X_{q}, X_{q+1},\ldots,X_{p}$ is associated with $Y$, controlling for $X_1, X_2, \ldots, X_{q-1}$. **NOT** that all of them are associated. The $F$ test only tells you that at least one variable is associated.
:::

::: {.callout-tip}
## Overall $F$-test
It is so common to run the following hypothesis test

- $H_0$: (Reduced) $Y_i = \beta_0 + \epsilon_i$
- $H_0$: (Reduced) $Y_i = \beta_0 + \sum_{j=1}^{p-1}\beta_jX_{ij} + \epsilon_i$

- This is called the **overall $F$-test**

- This is output by `glance()` from `{broom}`.
:::

-   Here is the result of the overall $F$-test for SBP on age and birthweight
    ```{r}
    lm(sbp ~ age + birthweight, data = bp) |>
      glance() |>
      select(p.value)
    ```
    So we have very strong evidence that either age or birthweight are associated with SBP.


::: {.panel-tabset}
## Exercise
From the below ANOVA output, calculate the $p$-value from the $F$-statistic directly.
```{r}
reduced <- lm(fat ~ triceps, data = body)
full <- lm(fat ~ triceps + thigh + midarm, data = body)
anova(reduced, full) |>
  tidy()
```

## Solution
```{r}
1 - pf(3.635, df1 = 2, df2 = 16)
```

:::

::: {.panel-tabset}
## Exercise
From the below output, calculate a 95% confidence interval for the coefficient estimates.
```{r}
lm(fat ~ triceps + thigh, data = body) |>
  tidy()
nrow(body)
```


## Solution
The appropriate $t$-quantile is
```{r}
qt(p = 1 - 0.05 / 2, df = 20 - 3)
```

-   $\beta_0$:
    ```{r}
    -19.1742 - 2.11 * 8.3606	
    -19.1742 + 2.11 * 8.3606	
    ```

-   $\beta_1$:
    ```{r}
    0.2224 - 2.11 * 0.3034
    0.2224 + 2.11 * 0.3034
    ```

-   $\beta_2$
    ```{r}
    0.6594 - 2.11 * 0.2912
    0.6594 + 2.11 * 0.2912
    ```
    
Let's verify:

```{r}
lm(fat ~ triceps + thigh, data = body) |>
  tidy(conf.int = TRUE) |>
  select(term, conf.low, conf.high)
```

:::

::: {.panel-tabset}
## Exercise
From the below output, calculate the $p$-values from the $t$-statistics
```{r}
lm(fat ~ triceps + thigh, data = body) |>
  tidy()
nrow(body)
```

## Solution
-   $\beta_0$
    ```{r}
    2 * pt(-2.2934, df = 20 - 3)
    ```

-   $\beta_1$
    ```{r}
    2 * pt(-0.7328, df = 20 - 3)
    ```
    
-   $\beta_2$
    ```{r}
    2 * pt(-2.2646, df = 20 - 3)
    ```
:::

## Assessing Assumptions

- Plot residuals vs fits. If there are any trends or unequal variances, then you have issues.

-   Let's do this for the regression of sbp on age and birthweight.
    ```{r}
    lm(sbp ~ age + birthweight, data = bp) |>
      augment() |>
      ggplot(aes(x = .fitted, y = .resid)) +
      geom_point() +
      geom_hline(yintercept = 0, lty = 2, col = 2)
    ```

- It looks like there might be one outlier/influential point. In STAT-415, you learn about statistics to evaluate if points are outliers or influential.

-   The variance of the residuals varies based on their $x$ values (closer to the mean has lower variance). So folks often prefer plotting the standardized residuals that adjustes fo this. They are standardized to all have variance 1.
    ```{r}
    lm(sbp ~ age + birthweight, data = bp) |>
      augment() |>
      ggplot(aes(x = .fitted, y = .std.resid)) +
      geom_point() +
      geom_hline(yintercept = 0, lty = 2, col = 2)
    ```
    
::: {.callout-important}
- In R, "standardized residuals" are Rosner's "internally studentized residuals"
  - No leave-one-out calculation of standard error.
  - `augment()` returns these.
- In R, "studentized residuals" are Rosner's "externally studentized residuals"
  - Leave-one-out calculation of standard error.
  - More robust to outliers.
  
:::

- Another common plot is a partial-residual plot. It plots the association between $X_j$ and $Y$ after removing the effects of all other variables.

::: {.callout-tip}
## Partial Residual Plot AKA Added Variable Plot AKA Adjusted Variable Plot
1. Regress $Y$ (as the response) on every predictor except $X_j$. Obtain the residuals, and call these $e_i(Y)$
2. Regress $X_j$ (as the response) on every other predictor. Obtain the residuals, and call these $e_i(X_k)$
3. Make a scatterplot of $e_i(X_k)$ versus $e_i(Y)$.
:::

- Intuition: $e_i(Y)$ and $e_i(X_k)$ reflect the part of each variable that is not linearly associated with the other predictors. See if they are linear or not.

-   Don't try to make one manually, just use `car::avPlot()`.

    ```{r}
    lm_bp <- lm(sbp ~ age + birthweight, data = bp)
    car::avPlot(model = lm_bp, variable = "age")
    car::avPlot(model = lm_bp, variable = "birthweight")
    ```

::: {.panel-tabset}
## Exercise
Evaluate the regression of body fat on tricep skinfold thickness, thigh circumference, and midarm circumference.

## Solution
It looks really good!
```{r}
lm_body <- lm(fat ~ triceps + thigh + midarm, data = body)
lm_body |>
  augment() |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 2, col = 2)
```
Added variable plots look good too!
```{r}
#| fig-width: 6
#| fig-height: 6
car::avPlots(lm_body)
```
Though, note that there is huge multicollinearity in these data. In STAT-415, you'll learn about variance inflation factors, which evaluate multicollinearity. In this case, the VIF's are all huge (rule of thumb is they should be less than 10)
```{r}
lm_fat <- lm(fat ~ triceps + thigh + midarm, data = body)
car::vif(lm_fat)
```
:::

## Including Categorical Variables

- Consider the [lead data](https://dcgerard.github.io/stat_320/data.html#lead) that you can download from <https://dcgerard.github.io/stat_320/data/lead.csv>.

-   We will be interested in these variables:
    - `maxfwt`: Finger-wrist tapping test in dominant hand (number of taps in one 10 second trial)
    - `Group`: Either `control` (low lead) or `exposed` (high lead)
    - `ageyrs`: Age in years
    - `sex`: Sex. Either `male` or `female`
  
    ```{r}
    #| message: false
    lead <- read_csv("https://dcgerard.github.io/stat_320/data/lead.csv") |>
      select(maxfwt, Group, ageyrs, sex)
    glimpse(lead)
    ```

- The goal is to assess the association between `maxfwt` and `Group`, adjusting for `ageyrs` and `sex`. 

- But group is a categorical variable. How do we include it?

::: {.callout-tip}
## Dummy Variables AKA Indicator Variables AKA One-hot Transformation
For a explanatory variable $X$ with $k$ levels, create $k-1$ new variables, where variable $j$ is
$$
X_{ij} = 
\begin{cases}
1 & \text{ if } X \text{ is level } j\\
0 & \text{ otherwise}
\end{cases}
$$
:::

- Just include the $k-1$ dummy variables in the model.

- E.g., set $X_{i1} = 1$ if in the exposed group and $0$ otherwise.

-   If our regression model is
    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_2 X_{i2} + \beta_3X_{i3} + \epsilon_i
    $$
    Then our model for the control group is
    $$
    Y_i = \beta_0 + \beta_2 X_{i2} + \beta_3X_{i3} + \epsilon_i
    $$
    And out model for the exposed group is
    $$
    Y_i = \beta_0 + \beta_1 + \beta_2 X_{i2} + \beta_3X_{i3} + \epsilon_i
    $$
    Thus, $\beta_1$ is the mean difference in `maxfwt` between exposed and control groups, adjusting for age and sex.
    
-   R will automatically do this dummy variable encoding for you if you pass it a character or factor variable.

    ```{r}
    lm(maxfwt ~ Group + ageyrs + sex, data = lead) |>
      tidy()
    ```

- Since the result is `Groupexposed`, this says that the coefficient corresponding to the `exposed` level in the `Group` variable is estimated to be -4.889.

::: {.panel-tabset}
## Exercise
Write out the entire model that was fit above. Note carefully that `sex` is also categorical. What is the model for males? What is the model for females?

## Solution
- Let $Y_i$ be the `maxfwt` for child $i$
- Let $X_{i1}$ be 1 if child $i$ is exposed and 0 if control.
- Let $X_{i2}$ be the age of child $i$.
- Let $X_{i3}$ be 1 if the child is male and 0 if female.

Then our model is
$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i
$$
where $\epsilon_i \sim N(0, \sigma^2)$

The model for males is
$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3 + \epsilon_i
$$
And out model for females is
$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i
$$

Thus, we estimate that Males have 1.641 more taps than females on average, adjusting for age and exposure status.
:::

::: {.callout-important}
If you have nothing but categorical explanatory variables in a model, then folks call this an **ANOVA model**.

If you have a mix of categorical and continuous explanatory variables (like here), then folks call this an **ANCOVA model** (analysis of covariance)

Some statisticians would be upset that I am being so simplistic. Don't tell them. This is just between you and me.
:::


-   Let's evaluate the fit of the model. There are some large values, but it doesn't look too bad:
    ```{r}
    lm_lead <- lm(maxfwt ~ Group + ageyrs + sex, data = lead)
    aout <- augment(lm_lead)
    ggplot(aout, aes(x = .fitted, y = .std.resid)) +
      geom_point() +
      geom_hline(yintercept = 0, lty = 2, col = 2)
    ```

# FEV Exercise

Refer to the FEV data described [here](https://dcgerard.github.io/stat_320/data.html#fev) and that can be downloaded from <https://dcgerard.github.io/stat_320/data/fev.csv>

::: {.panel-tabset}
## Exercise
Is FEV associated with age? Write down the model you use, state the hypotheses, assess assumptions, and make a conclusion.

## Solution
Let

- $Y_i$ be the FEV of individual $i$.
- $X_{i}$ be the age of individual $i$.

Then we assume

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
$$
where $\epsilon_i \sim N(0, \sigma^2)$

We fit the model
```{r}
#| message: false
fev <- read_csv("https://dcgerard.github.io/stat_320/data/fev.csv")
lmout <- lm(FEV ~ Age, data = fev)
tidy(lmout, conf.int = TRUE)
```

Assessing assumptions, we have
```{r}
aout <- augment(lmout)
ggplot(aout, aes(x = Age, y = FEV)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y~x)

ggplot(aout, aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y~x) +
  geom_hline(yintercept = 0, lty = 2, col = 2)
```

Everything looks good except the equal variance assumption. This is clearly violated.

We barely covered how to do this, so I didn't expect you to do it, but we need to do sandwich estimation.
```{r}
library(sandwich)
library(lmtest)
robust_se <- vcovHC(lmout, type = "HC1")
coeftest(lmout, vcov = robust_se) |>
  tidy(conf.int = TRUE)
```
Either way, we have strong evidence of an association between age and FEV (p < 0.001). Estiamte that individuals who are1 year older have an FEV 0.22 liters higher, on average (95% CI 0.20 to 0.24 liters higher).
:::

::: {.panel-tabset}
## Exercise
Is FEV associated with smoking status? Write down the model you use, state the hypotheses, assess assumptions, and make a conclusion.

## Solution
This is just a two-sample $t$-test. Let

- $X_i$ be the FEV in smoking individual $i$
- $Y_j$ be the FEV in non-smoking individual $j$.

We assume $X_i \sim N(\mu_1, \sigma_1)^2$ and $Y_j \sim N(\mu_2, \sigma_2^2)$

We are testing $H_0: \mu_1 = \mu_2$ versus $H_A: \mu_1 \neq \mu_2$

```{r}
t.test(FEV ~ Smoke, data = fev) |>
  tidy() |>
  select(p.value, estimate, conf.low, conf.high)
```


We assess assumptions with a boxplot
```{r}
ggplot(fev, aes(x = Smoke, y = FEV)) +
  geom_boxplot()
```

We have strong evidence that current smokers have higher FEV than non-current smokers (p < 0.001). Current smokers have about 0.71 liters larger FEV on average (95% CI 0.51 liters to 0.91 liters).

:::

::: {.panel-tabset}
## Exercise
Can you explain your results from the previous exercise? Are they counterintuitive?

## Solutions
If you are older, you are more likely to smoke. If you are older, you are also more likely to have a larger FEV. Thus, smoking status is confounded with age and we need to adjust for age to actually get the effect of smoking status on FEV.
:::

::: {.panel-tabset}
## Exercise
Is FEV associated with smoking status, adjusted for age and sex? Write down the model you use, state the hypotheses, assess assumptions, and make a conclusion.

## Solution
Let

- $Y_i$ be the FEV of individual $i$.
- $X_{i1}$ be 1 if individual $i$ does not smoke and 0 otherwise.
- $X_{i2}$ be the age of individual $i$.
- $X_{i3}$ be 1 if individual $i$ is male and 0 otherwise.

Then we assume

$$
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i
$$

```{r}
lmout <- lm(FEV ~ Smoke + Age + Sex, data = fev)
tidy(lmout, conf.int = TRUE)
```

Assessing assumptions, we have
```{r}
aout <- augment(lmout)
ggplot(aout, aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y~x) +
  geom_hline(yintercept = 0, lty = 2, col = 2)
```

```{r}
#| fig-width: 4
#| fig-height: 4
car::avPlot(lmout, variable = "Age")
car::avPlot(lmout, variable = "Smokenon-current")
car::avPlot(lmout, variable = "Sexmale")
```

Everything looks good except the equal variance assumption. Again, we definitely need sandwich.
```{r}
robust_se <- vcovHC(lmout, type = "HC1")
coeftest(lmout, vcov = robust_se) |>
  tidy(conf.int = TRUE)
```
 
 This actually changed the results of the model. We do not have evidence that smoking status is associated with FEV adjusting for age and sex (p = 0.14). The sign is at least correct. We estimate that non-smokers have on average an FEV that is 0.15 liters larger than smokers, adjusting for age and sex (95% CI 0.05 L lower to 0.36 L higher). We have more evidence that age and sex are associated with FEV (both p < 0.001). In particular, males who are expected to have an FEV that is 0.31 L larger than females (95% CI 0.23 L to 0.40 L), and each year folks get about 0.23 L larger FEV (95% CI 0.21 L to 0.24 L).
:::

# References
